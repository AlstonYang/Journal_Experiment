{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "## Set the GUP environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the display\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前設備： 0\n",
      "目前設備名： GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "## 查詢有無可用 GPU\n",
    "torch.cuda.is_available()\n",
    "## 查詢可用 GPU 的數量\n",
    "torch.cuda.device_count()\n",
    "##目前設備\n",
    "print(\"目前設備：\",torch.cuda.current_device())\n",
    "## 目前設備名\n",
    "print(\"目前設備名：\",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cacl(pred_value, actual_value):\n",
    "    \n",
    "#     yo, loss, tape = network.forward()\n",
    "    performance = []\n",
    "    performance.append(torch.mean(torch.abs(pred_value - actual_value)))\n",
    "    performance.append(torch.mean(torch.abs((pred_value - actual_value) / actual_value))) \n",
    "    performance.append(torch.sqrt(torch.mean((pred_value - actual_value)**2)))\n",
    "    \n",
    "    for i in range(2000,3001,1000):\n",
    "        correct_times = torch.nonzero(torch.abs(pred_value - actual_value) <= i)\n",
    "        accuracy = correct_times.shape[0]/pred_value.shape[0]\n",
    "        performance.append(accuracy)\n",
    "    \n",
    "    performance.append(torch.max(torch.abs(pred_value - actual_value)))\n",
    "    performance.append(torch.min(torch.abs(pred_value - actual_value)))\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(name, pred_value, actual_value,block_index):\n",
    "    \n",
    "#     fig, ax = plt.subplots(2,2,figsize=(20,10), sharex=True, sharey=True)\n",
    "    fig, ax = plt.subplots(1,figsize=(20,10), sharex=True, sharey=True)\n",
    "#     ax.set_xlim(0,pred_value.shape[0])  \n",
    "    \n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.plot(pred_value, label=\"LLAAT\")\n",
    "    ax.plot(actual_value, label=\"Actual\")\n",
    "    ax.set_title(\"Forecasted performance for l=%d\" %(1))\n",
    "    ax.legend()\n",
    "        \n",
    "    #fig.text(0.5, 0, \"Stage of training\", ha='center', fontsize=20)\n",
    "    #fig.text(0, 0.5, \"Copper price value\", va='center', rotation='vertical')\n",
    "\n",
    "    fig.suptitle(\"In the %s process in the M=%d window\"%(name, block_index))\n",
    "    fig.tight_layout()\n",
    "#     fig.savefig(\"In the %s process in the M=%d window.png\"%(name, block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adopted_node(network,block_index):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "#     ax.set_xticklabels([i for i in range(network.nb_node_acceptable.shape[0]+5)])\n",
    "    \n",
    "    ax.set_title(\"Total amount of adopted hidden nodes in the training process in the M=%d window\"%(block_index))\n",
    "    ax.plot(network.nb_node_acceptable,\"-o\")\n",
    "\n",
    "    ax.set_xlabel(\"Stage of training\")\n",
    "    ax.set_ylabel(\"Hidden nodes\")\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#     fig.savefig(\"hidden nodes in the training process in the M=%d window\"%(block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_table(evaluation_results, block_index, name, performance, nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node):\n",
    "\n",
    "    print(performance[3])\n",
    "    print(type(performance[3]))\n",
    "    \n",
    "    new_result = pd.DataFrame({\n",
    "\n",
    "        \"Window_index\":block_index,\n",
    "        \"Stage\":name,\n",
    "        \"MAE\" : round(performance[0].item(),2),\n",
    "        \"MAPE\" : \"%.2f\"%(performance[1]*100).item(),\n",
    "        \"RMSE\" : round(performance[2].item(),2),\n",
    "        \"Accuracy(2000)\" : [round(performance[3]*100,2)],\n",
    "        \"Accuracy(3000)\" : [round(performance[4]*100,2)],\n",
    "        \"Maximum error\" : round(performance[5].item(),2),\n",
    "        \"Minimum error\" : round(performance[6].item(),2),\n",
    "        \"Step4\":nb_step4,\n",
    "        \"Step6.1\":nb_step6_1,\n",
    "        \"Step6.2\":nb_step6_2,\n",
    "        \"Time\":time,\n",
    "        \"Adopted_hidden_node\":adopted_hidden_node\n",
    "    })\n",
    "\n",
    "    evaluation_results = evaluation_results.append(new_result, ignore_index=True)\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(last_date, network, nb_step4, nb_step6_1, nb_step6_2, x_train_scaled, y_train_scaled, x_test, y_test, start, end, block_index, evaluation_results_train, evaluation_results_test,forecasted_price):\n",
    "\n",
    "    ## Training_Step\n",
    "    print(\"<<Training step>>\")\n",
    "    print(\"The training time(s):\",end - start)\n",
    "    time = end - start\n",
    "    yo, loss= network.forward()\n",
    "    \n",
    "    ## N - outlier\n",
    "    pre_train = yo.data.cpu()\n",
    "    true_train = network.y.data.cpu()\n",
    "    \n",
    "    pred_value_train = torch.FloatTensor(sc.inverse_transform(pre_train))\n",
    "    actual_value_train = torch.FloatTensor(sc.inverse_transform(true_train))\n",
    "    accuracy_train = accuracy_cacl(pred_value_train,actual_value_train)\n",
    "\n",
    "    ## Outlier\n",
    "#     pre_outlier = torch.FloatTensor(sc.inverse_transform(network.forecast(x_train_scaled).data.cpu()))\n",
    "#     actual_outlier = torch.FloatTensor(sc.inverse_transform(y_train_scaled))\n",
    "#     accuracy_outlier = accuracy_cacl(pre_outlier,actual_outlier)\n",
    "    \n",
    "    ## B\n",
    "    pred_value_test = torch.FloatTensor(sc.inverse_transform(network.forecast(x_test).data.cpu()))\n",
    "    accuracy_test = accuracy_cacl(pred_value_test, y_test)\n",
    "    \n",
    "    total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "    print(\"<<The percentage of each step>>\")\n",
    "    print(\"Step 4: %.2f%%\"%((nb_step4/total_time)*100))\n",
    "    print(\"Step 6.1: %.2f%%\"%((nb_step6_1/total_time)*100))\n",
    "    print(\"Step 6.2: %.2f%%\"%((nb_step6_2/total_time)*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"Total frequency of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    adopted_hidden_node = network.nb_node_acceptable[-1].item()\n",
    "    print(\"The amount of adopted hidden nodes:\",network.nb_node_acceptable[-1].item())\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in training step>>\")\n",
    "    print(\"The MAE for l = 1: %.2f\" %(accuracy_train[0]))\n",
    "    print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_train[1]))\n",
    "    print(\"The RMSE for l = 1: %.2f\" %(accuracy_train[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_train[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_train[4]*100))\n",
    "    print(\"The maximum error:\", accuracy_train[5])\n",
    "    print(\"The minimum error:\", accuracy_train[6])\n",
    "#     print(\"The accuracy for l = 2: %.1f%%\" %(accuracy_train[1]*100))\n",
    "#     print(\"The accuracy for l = 3: %.1f%%\" %(accuracy_train[2]*100))\n",
    "#     print(\"The accuracy for l = 4: %.1f%%\" %(accuracy_train[3]*100))\n",
    "\n",
    "#     print(\"-\"*60)\n",
    "#     print(\"<<Accuracy in toutlier>>\")\n",
    "#     print(\"The MAE for l = 1: %.2f\" %(accuracy_outlier[0]))\n",
    "#     print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_outlier[1]))\n",
    "#     print(\"The RMSE for l = 1: %.2f\" %(accuracy_outlier[2]))\n",
    "#     print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_outlier[3]*100))\n",
    "#     print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_outlier[4]*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in inferencing step>>\")\n",
    "    print(\"The MAE for l = 1: %.1f\" %(accuracy_test[0]))\n",
    "    print(\"The MAPE for l = 1: %.1f%%\" %(accuracy_test[1]))\n",
    "    print(\"The RMSE for l = 1: %.1f\" %(accuracy_test[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.1f%%\" %(accuracy_test[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.1f%%\" %(accuracy_test[4]*100))\n",
    "    print(\"The maximum error:\", accuracy_test[5].item())\n",
    "    print(\"The minimum error:\", accuracy_test[6].item())\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    y_test = np.round_(y_test, decimals = 2) \n",
    "    \n",
    "    for i in range(pred_value_test.shape[0]):\n",
    "        \n",
    "        record = pd.DataFrame({\n",
    "            \n",
    "            \"Date\": [datetime.date(last_date[0]+timedelta(days=28+7*i))],\n",
    "            \"Actual\": y_test[i],\n",
    "            \"Forecasted_price\": round(pred_value_test[i].item(),2)\n",
    "        })\n",
    "    \n",
    "        forecasted_price = forecasted_price.append(record)\n",
    "    \n",
    "    evaluation_table_train = evaluation_table(evaluation_results_train, block_index, \"Training\", accuracy_train,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "#     evaluation_table_outlier = evaluation_table(evaluation_results_outlier, block_index, \"Outlier\", accuracy_outlier,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    evaluation_table_test = evaluation_table(evaluation_results_test, block_index, \"Inferencing\", accuracy_test,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    pre_LDSS = sc.inverse_transform(network.forecast(x_test).data.cpu())\n",
    "#     pd.DataFrame(pre_LDSS).to_csv(\"pre_LDSS_%d.csv\"%(block_index), index=False)\n",
    "    \n",
    "#     if block_index%5==0:\n",
    "    plot_result(\"training\",pred_value_train, actual_value_train,block_index)\n",
    "    plot_result(\"inferencing\",pred_value_test, y_test,block_index)\n",
    "    plot_adopted_node(network,block_index)\n",
    "    \n",
    "    return(evaluation_table_train, evaluation_table_test, forecasted_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek+1):\n",
    "        \n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = pd.DataFrame(data[\"Date\"][3:])\n",
    "    date[\"Date\"] = pd.to_datetime(date[\"Date\"], format=\"%Y/%m/%d\")\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "\n",
    "\n",
    "    return (date, x_data, y_data)\n",
    "\n",
    "#     return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(x_train_scaled.shape[1], nb_neuro).cuda()\n",
    "        self.linear2 = torch.nn.Linear(nb_neuro, 1).cuda()\n",
    "        \n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 0.12\n",
    "        self.threshold_for_lr = 1e-4\n",
    "        \n",
    "        # Input data \n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=torch.IntTensor([nb_neuro])\n",
    "        \n",
    "        self.limit = nb_neuro\n",
    "        \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test):\n",
    "    \n",
    "        x_test = torch.FloatTensor(x_test).cuda()\n",
    "        activation_value = self.linear1(x_test).clamp(min=0)\n",
    "        forecast_value = self.linear2(activation_value)\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = torch.cat([self.x, new_x_train.reshape(1,-1).cuda()],0)\n",
    "        self.y = torch.cat([self.y, new_y_train.reshape(-1,1).cuda()],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self, reg_strength=0):\n",
    "       \n",
    "        y1 = self.linear1(self.x).clamp(min=0)\n",
    "        yo = self.linear2(y1)\n",
    "\n",
    "        # performance measure\n",
    "        param_val= torch.sum(torch.pow(self.linear2.bias.data,2))+torch.sum(torch.pow(self.linear2.weight.data,2))+torch.sum(torch.pow(self.linear1.bias.data,2))+torch.sum(torch.pow(self.linear1.weight.data,2))\n",
    "        reg_term= reg_strength/((self.linear2.bias.data.shape[0]*(self.linear2.weight.data.shape[1]+1)) +(self.linear1.bias.data.shape[0]*(self.linear1.weight.data.shape[1]+1)))*param_val\n",
    "        loss = torch.nn.functional.mse_loss(yo,self.y)+reg_term\n",
    "        loss = loss.cuda()\n",
    "        return(yo, loss)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adadelta(self,loss):    \n",
    "\n",
    "        optimizer = optim.Adadelta(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "    print(\"Initializing module\")\n",
    "    ## Find each minimum output value y\n",
    "    min_y = torch.min(initial_y, axis=0)\n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y.values\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "    \n",
    "    ## Set up the initial parameter of the network\n",
    "    network.linear1.weight = torch.nn.Parameter(torch.FloatTensor(reg.coef_).cuda())\n",
    "    network.linear1.bias = torch.nn.Parameter(torch.FloatTensor(reg.intercept_).cuda())\n",
    "    network.linear2.weight=torch.nn.Parameter(torch.FloatTensor([[1]]).cuda())\n",
    "    network.linear2.bias = torch.nn.Parameter(torch.FloatTensor(min_y.values).cuda())\n",
    "    \n",
    "#     print(reg.coef_)\n",
    "#     print(reg.intercept_)\n",
    "\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "    print(\"<<Selecting module>>\")\n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(-1,1))\n",
    "        loss.append((temp_network.forward()[1].item(),i))\n",
    "#         print(network.state_dict())\n",
    "#         print(temp_network.y)\n",
    "#         print(\"-\"*20)\n",
    "#         print(temp_network.forward()[1])\n",
    "#         print(\"-\"*20)\n",
    "#     ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "    \n",
    "    \n",
    "    ## Print out some info for debug\n",
    "    print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "#     print(\"The second_loss value of k:\",loss[sorted_index[1]])\n",
    "    print(\"Selecting module finish!\")\n",
    "#     print(\"Loss\",loss)\n",
    "#     print(network.state_dict())\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matching(network):\n",
    "\n",
    "#     times_enlarge=0\n",
    "#     times_shrink=0\n",
    "    \n",
    "#     print(\"<<Matching module>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "#     ## Set up the learning rate of the network\n",
    "#     network.learning_rate = 1e-3\n",
    "#     network.acceptable = False\n",
    "#     initial_network = copy.deepcopy(network)\n",
    "\n",
    "#     yo, loss = network.forward()\n",
    "    \n",
    "#     if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "#         print(\"Matching finished (firstly) - the network is acceptable\")\n",
    "#         network.acceptable = True\n",
    "# #         print(\"Matching firstly finished - the network is acceptable\")\n",
    "#         print(\"Number of enlarge:\",times_enlarge)\n",
    "#         print(\"Number of shrink:\",times_shrink)\n",
    "#         return(network)\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         while True:\n",
    "\n",
    "#             yo, loss = network.forward()\n",
    "#             network_pre = copy.deepcopy(network)\n",
    "#             loss_pre = loss\n",
    "            \n",
    "#             # Backward and check the loss performance of the network with new learning rate\n",
    "#             network.backward_Adadelta(loss)\n",
    "#             yo, loss = network.forward()\n",
    "\n",
    "#             # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "#             if loss <= loss_pre and torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "       \n",
    "#                 network.acceptable = True\n",
    "#                 print(\"Matching finished - the network is acceptable\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "#                 return(network)\n",
    "\n",
    "#             elif loss <= loss_pre:\n",
    "                \n",
    "#                 times_enlarge+=1\n",
    "#                 network.learning_rate *= 1.2\n",
    "\n",
    "#             else:         \n",
    "\n",
    "#                 # Identify whether the current learning rate is less than the threshold\n",
    "#                 if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "#                     # If true, set the acceptable of the network as false and return it\n",
    "#                     network.acceptable = False\n",
    "#                     print(\"Matching finished - the network is Unacceptable\")\n",
    "#                     print(\"Number of enlarge:\",times_enlarge)\n",
    "#                     print(\"Number of shrink:\",times_shrink)\n",
    "#                     return(initial_network)\n",
    "\n",
    "#                 # On the contrary, restore w and adjust the learning rate\n",
    "#                 else:\n",
    "                    \n",
    "#                     # Restore the papameter of the network\n",
    "#                     network = copy.deepcopy(network_pre)\n",
    "#                     times_shrink+=1\n",
    "#                     network.learning_rate *= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "    print(\"<<Matching module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "        print(\"Matching(o) first finished - the network is acceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(10000):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "#             print(\"<前Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "#             print(\"<後Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "                print(\"Matching finished - the network is acceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "#                 print(\"*1.2\")\n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished - the network is Unacceptable\")\n",
    "                    print(\"Number of enlarge:\",times_enlarge)\n",
    "                    print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "#                     print(\"*0.7\")\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "        print(\"Matching的第%d回合\"%(i+1))\n",
    "        print(\"Matching finished - the network is Unacceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching for reorganizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_for_reorganizing(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "    print(\"<<Matching module for reorganizing>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "        print(\"Matching(o) first finished - the network is acceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(500):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "#             print(\"<前Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "#             print(\"<後Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "                print(\"Matching finished(o) - the network is acceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "#                 print(\"*1.2\")\n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished(o) - the network is Unacceptable\")\n",
    "                    print(\"Number of enlarge:\",times_enlarge)\n",
    "                    print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "#                     print(\"*0.7\")\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "        print(\"Matching的第%d回合\"%(i+1))\n",
    "        print(\"Matching finished - the network is Unacceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "    torch.random.manual_seed(0)\n",
    "    print(\"<<Cramming module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss = network.forward()\n",
    "    undesired_index = torch.nonzero(torch.abs(yo-network.y) > network.threshold_for_error+0.001, as_tuple =False)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "    print(\"不滿足個數：\",undesired_index.shape[0])\n",
    "    print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "    \n",
    "    if undesired_index.shape[0] == 1:\n",
    "        \n",
    "        # Unsatisfied situation\n",
    "        ## Find the index of the unsatisfied data\n",
    "        k_data_num = undesired_index[0][0]\n",
    "\n",
    "        undesired_data = torch.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "        ## Remove the data that does not meet the error term\n",
    "        left_data = network.x[:k_data_num,:]\n",
    "        right_data = network.x[k_data_num+1:,:]\n",
    "        remain_tensor = torch.cat([left_data, right_data], 0)\n",
    "\n",
    "\n",
    "        ## Use the random method to find out the gamma and zeta\n",
    "        while True:\n",
    "\n",
    "            ## Find m-vector gamma: r\n",
    "            ## Use the random method to generate the gamma that can make the conditions met\n",
    "            gamma = torch.rand(size=[1,network.x.shape[1]]).cuda()\n",
    "            subtract_undesired_data = torch.sub(remain_tensor, undesired_data)\n",
    "            matmul_value = torch.mm(gamma,torch.t(subtract_undesired_data))\n",
    "\n",
    "            if torch.all(matmul_value != 0):\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "\n",
    "            ## Find the tiny value: zeta\n",
    "            ## Use the random method to generate the zeta that can make the conditions met\n",
    "            zeta = torch.rand(size=[1]).cuda()\n",
    "\n",
    "            if torch.all(torch.mul(torch.add(zeta,matmul_value),torch.sub(zeta,matmul_value))<0):\n",
    "                break\n",
    "\n",
    "       \n",
    "\n",
    "        k_l = undesired_index[0][1]\n",
    "        \n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "\n",
    "        W1_new = torch.cat([w10,w11,w12],0)\n",
    "        \n",
    "\n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = torch.mm(gamma,torch.t(undesired_data))\n",
    "       \n",
    "        \n",
    "        b10 = torch.sub(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = torch.sub(-1*zeta,matual_value)\n",
    "\n",
    "        b1_new = torch.reshape(torch.cat([b10,b11,b12],0),[3])\n",
    "        \n",
    "#         print(\"b1_new\",b1_new)\n",
    "\n",
    "\n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "#         print(\"gap:\",gap)\n",
    "\n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        Wo_new = torch.reshape(torch.cat([wo0_value,wo1_value,wo2_value],0),[1,-1])\n",
    "\n",
    "        ## Add new neuroes to the network\n",
    "        network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight.data, W1_new]))\n",
    "        network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias.data, b1_new]))\n",
    "        network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight.data, Wo_new],1))\n",
    "\n",
    "\n",
    "        yo, loss = network.forward()\n",
    "        \n",
    "        ## Determine if cramming is successful and print out the corresponding information\n",
    "        if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            network.acceptable = True \n",
    "            print(\"Cramming success!\")\n",
    "\n",
    "        else:\n",
    "            print(\"Cramming failed!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"條件不合，不能Cramming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "    print(\"<<Regularizing module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 500\n",
    "    for i in range(500):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "        loss_pre = loss\n",
    "\n",
    "#         print(\"調整前的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "        \n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adadelta(loss)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "#         print(\"調整後的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "         # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "#                 print(\"*1.2\")\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "#                 print(\"Regularizing %d process - Enlarge\"%i)\n",
    "#                 print(\"第\\\"%d\\\"回合是成功執行regularizing\"%(i+1))\n",
    "#                 print(\"差異\")\n",
    "#                 print(torch.abs(yo-network.y))\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                print(\"Regularizing結束-因為沒有顧好預測誤差\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Unable to meet the error term\")\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = copy.deepcopy(network_pre)\n",
    "#                 print(\"*0.7\")\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "#                 print(\"把Learning rate變小\")\n",
    "#                 print(\"Regularizing %d process - Shrink\"%i)\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                print(\"Regularizing結束-Learning不能這麼小\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Less than the epsilon for the learning rate\")\n",
    "                return(network)\n",
    "\n",
    "    print(\"第\\\"%d\\\"回合Regularizing module完畢\"%(i+1))\n",
    "    print(\"Number of enlarge:\",times_enlarge)\n",
    "    print(\"Number of shrink:\",times_shrink)\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "    print(\"<<Reorganizing module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    limit = 4\n",
    "    if network.linear1.bias.shape[0] <= limit:\n",
    "        network = regularizing(network)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ## Set up the k = 1, and p = the number of hidden node\n",
    "        k = 1\n",
    "    #     p = network.W1.shape[1]\n",
    "        p = network.linear1.weight.data.shape[0]\n",
    "\n",
    "        while True:\n",
    "\n",
    "            ## If k > p, end of Process\n",
    "            if k > p or p<=limit:\n",
    "\n",
    "                print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "                return(network)\n",
    "\n",
    "            ## Else, Process is ongoing\n",
    "            else:\n",
    "\n",
    "                ## Using the regularizing module to adjust the network\n",
    "                network = regularizing(network)\n",
    "\n",
    "                ## Store the network and w\n",
    "                network_pre = copy.deepcopy(network)\n",
    "\n",
    "                ## Set up the acceptable of the network as false\n",
    "                network.acceptable = False\n",
    "                \n",
    "            \n",
    "                ## Ignore the K hidden node\n",
    "                network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight[:k-1],network.linear1.weight[k:]],0))\n",
    "                network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias[:k-1],network.linear1.bias[k:]]))\n",
    "                network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight[:,:k-1],network.linear2.weight[:,k:]],1))\n",
    "\n",
    "                \n",
    "                ## Using the matching module to adjust the network\n",
    "                network = matching_for_reorganizing(network)\n",
    "\n",
    "                print(\"是不是可以不要這個hidden node:\",network.acceptable)\n",
    "\n",
    "                ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "                if network.acceptable:\n",
    "\n",
    "                    print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                    network.nb_node_pruned += 1\n",
    "                    ## p--\n",
    "                    p-=1\n",
    "\n",
    "                ## Else, it means that the k hidden node cannot be removed\n",
    "                else:\n",
    "\n",
    "                    ## Restore the network and w\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "\n",
    "                    ## k++\n",
    "                    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <<1>> Block\n",
      "初始值 torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing module\n",
      "<<Initializing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "現在訓練到第幾筆資料: 20\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0, 72)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [72, 91, 93, 75, 87, 90, 94, 95, 73, 76, 86, 88, 92, 45, 69, 74, 77, 89, 96, 49] 數值 torch.Size([20, 1])\n",
      "目前模型的Data狀態 torch.Size([20, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7461],\n",
      "        [0.6914],\n",
      "        [0.7161],\n",
      "        [0.7463],\n",
      "        [0.7355],\n",
      "        [0.7034],\n",
      "        [0.7078],\n",
      "        [0.7050],\n",
      "        [0.7455],\n",
      "        [0.7586],\n",
      "        [0.7168],\n",
      "        [0.7342],\n",
      "        [0.7056],\n",
      "        [0.7079],\n",
      "        [0.7038],\n",
      "        [0.7606],\n",
      "        [0.7218],\n",
      "        [0.7118],\n",
      "        [0.6950],\n",
      "        [0.8462]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0062]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0006],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0019],\n",
      "        [    0.0009],\n",
      "        [    0.0004],\n",
      "        [    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0041]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.5213632583618164\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 21\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.8141045049778768e-11, 91)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [91, 93, 76, 77, 95, 94, 96, 90, 92, 89, 88, 87, 75, 74, 86, 73, 72, 69, 45, 49, 56] 數值 torch.Size([21, 1])\n",
      "目前模型的Data狀態 torch.Size([21, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6914],\n",
      "        [0.7161],\n",
      "        [0.7585],\n",
      "        [0.7217],\n",
      "        [0.7051],\n",
      "        [0.7079],\n",
      "        [0.6949],\n",
      "        [0.7032],\n",
      "        [0.7054],\n",
      "        [0.7115],\n",
      "        [0.7340],\n",
      "        [0.7352],\n",
      "        [0.7466],\n",
      "        [0.7610],\n",
      "        [0.7163],\n",
      "        [0.7461],\n",
      "        [0.7468],\n",
      "        [0.7048],\n",
      "        [0.7098],\n",
      "        [0.8483],\n",
      "        [0.9013]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0009],\n",
      "        [    0.0019],\n",
      "        [    0.0041],\n",
      "        [    0.0046]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0006],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0015],\n",
      "        [    0.0031],\n",
      "        [    0.0027],\n",
      "        [    0.0033]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.7905323505401611\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 22\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.298783551348606e-09, 76)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [76, 77, 93, 96, 91, 90, 95, 92, 94, 89, 88, 87, 75, 86, 74, 73, 72, 69, 49, 45, 56, 57] 數值 torch.Size([22, 1])\n",
      "目前模型的Data狀態 torch.Size([22, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7585],\n",
      "        [0.7217],\n",
      "        [0.7162],\n",
      "        [0.6949],\n",
      "        [0.6916],\n",
      "        [0.7032],\n",
      "        [0.7052],\n",
      "        [0.7054],\n",
      "        [0.7080],\n",
      "        [0.7114],\n",
      "        [0.7339],\n",
      "        [0.7351],\n",
      "        [0.7468],\n",
      "        [0.7161],\n",
      "        [0.7613],\n",
      "        [0.7465],\n",
      "        [0.7471],\n",
      "        [0.7054],\n",
      "        [0.8497],\n",
      "        [0.7110],\n",
      "        [0.9026],\n",
      "        [0.9135]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0015],\n",
      "        [    0.0027],\n",
      "        [    0.0031],\n",
      "        [    0.0033],\n",
      "        [    0.0065]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0021],\n",
      "        [    0.0013],\n",
      "        [    0.0043],\n",
      "        [    0.0019],\n",
      "        [    0.0050]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.0602223873138428\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 23\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.096225994653651e-10, 76)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [76, 77, 96, 93, 91, 92, 90, 95, 94, 88, 89, 87, 75, 86, 74, 73, 49, 72, 56, 69, 45, 57, 55] 數值 torch.Size([23, 1])\n",
      "目前模型的Data狀態 torch.Size([23, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7586],\n",
      "        [0.7217],\n",
      "        [0.6949],\n",
      "        [0.7163],\n",
      "        [0.6917],\n",
      "        [0.7054],\n",
      "        [0.7031],\n",
      "        [0.7053],\n",
      "        [0.7081],\n",
      "        [0.7338],\n",
      "        [0.7113],\n",
      "        [0.7350],\n",
      "        [0.7470],\n",
      "        [0.7160],\n",
      "        [0.7614],\n",
      "        [0.7467],\n",
      "        [0.8511],\n",
      "        [0.7475],\n",
      "        [0.9040],\n",
      "        [0.7060],\n",
      "        [0.7122],\n",
      "        [0.9150],\n",
      "        [0.9035]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0013],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0043],\n",
      "        [    0.0050],\n",
      "        [    0.0061]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0010],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0004],\n",
      "        [0.0014],\n",
      "        [0.0009],\n",
      "        [0.0024],\n",
      "        [0.0051],\n",
      "        [0.0040],\n",
      "        [0.0050]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.3293564319610596\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 24\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4683681825999884e-08, 93)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [93, 76, 77, 96, 95, 49, 94, 92, 91, 90, 88, 75, 89, 87, 74, 56, 86, 73, 72, 69, 57, 55, 45, 97] 數值 torch.Size([24, 1])\n",
      "目前模型的Data狀態 torch.Size([24, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7162],\n",
      "        [0.7585],\n",
      "        [0.7216],\n",
      "        [0.6948],\n",
      "        [0.7053],\n",
      "        [0.8521],\n",
      "        [0.7081],\n",
      "        [0.7053],\n",
      "        [0.6918],\n",
      "        [0.7030],\n",
      "        [0.7336],\n",
      "        [0.7469],\n",
      "        [0.7111],\n",
      "        [0.7348],\n",
      "        [0.7614],\n",
      "        [0.9051],\n",
      "        [0.7158],\n",
      "        [0.7467],\n",
      "        [0.7475],\n",
      "        [0.7063],\n",
      "        [0.9160],\n",
      "        [0.9046],\n",
      "        [0.7130],\n",
      "        [0.6918]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0001],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0024],\n",
      "        [0.0040],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0083]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0000],\n",
      "        [    0.0007],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0009],\n",
      "        [    0.0003],\n",
      "        [    0.0008],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0028],\n",
      "        [    0.0034],\n",
      "        [    0.0045],\n",
      "        [    0.0054],\n",
      "        [    0.0051]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.5986440181732178\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 25\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.1391778065881226e-11, 76)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [76, 49, 96, 77, 90, 92, 91, 56, 88, 93, 87, 89, 95, 94, 75, 86, 74, 73, 72, 69, 57, 55, 97, 45, 35] 數值 torch.Size([25, 1])\n",
      "目前模型的Data狀態 torch.Size([25, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7586],\n",
      "        [0.8524],\n",
      "        [0.6951],\n",
      "        [0.7217],\n",
      "        [0.7033],\n",
      "        [0.7055],\n",
      "        [0.6912],\n",
      "        [0.9056],\n",
      "        [0.7338],\n",
      "        [0.7165],\n",
      "        [0.7351],\n",
      "        [0.7113],\n",
      "        [0.7056],\n",
      "        [0.7084],\n",
      "        [0.7470],\n",
      "        [0.7160],\n",
      "        [0.7615],\n",
      "        [0.7469],\n",
      "        [0.7478],\n",
      "        [0.7067],\n",
      "        [0.9165],\n",
      "        [0.9051],\n",
      "        [0.6887],\n",
      "        [0.7134],\n",
      "        [0.8270]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0028],\n",
      "        [    0.0034],\n",
      "        [    0.0045],\n",
      "        [    0.0051],\n",
      "        [    0.0054],\n",
      "        [    0.0131]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0011],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0007],\n",
      "        [    0.0006],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0011],\n",
      "        [    0.0010],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0033],\n",
      "        [    0.0024],\n",
      "        [    0.0035],\n",
      "        [    0.0043],\n",
      "        [    0.0066],\n",
      "        [    0.0121]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.8692080974578857\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 26\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.781864042044617e-11, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 76, 77, 92, 90, 91, 93, 95, 88, 89, 87, 56, 94, 75, 74, 86, 49, 73, 72, 57, 69, 55, 97, 45, 35, 33] 數值 torch.Size([26, 1])\n",
      "目前模型的Data狀態 torch.Size([26, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6950],\n",
      "        [0.7586],\n",
      "        [0.7217],\n",
      "        [0.7055],\n",
      "        [0.7031],\n",
      "        [0.6911],\n",
      "        [0.7165],\n",
      "        [0.7056],\n",
      "        [0.7336],\n",
      "        [0.7111],\n",
      "        [0.7348],\n",
      "        [0.9066],\n",
      "        [0.7085],\n",
      "        [0.7471],\n",
      "        [0.7616],\n",
      "        [0.7157],\n",
      "        [0.8535],\n",
      "        [0.7472],\n",
      "        [0.7481],\n",
      "        [0.9176],\n",
      "        [0.7072],\n",
      "        [0.9061],\n",
      "        [0.6879],\n",
      "        [0.7145],\n",
      "        [0.8279],\n",
      "        [0.8073]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0010],\n",
      "        [    0.0011],\n",
      "        [    0.0011],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0024],\n",
      "        [    0.0033],\n",
      "        [    0.0035],\n",
      "        [    0.0043],\n",
      "        [    0.0066],\n",
      "        [    0.0121],\n",
      "        [    0.0127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0021],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0012],\n",
      "        [    0.0026],\n",
      "        [    0.0019],\n",
      "        [    0.0023],\n",
      "        [    0.0010],\n",
      "        [    0.0040],\n",
      "        [    0.0021],\n",
      "        [    0.0045],\n",
      "        [    0.0080],\n",
      "        [    0.0105],\n",
      "        [    0.0111]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.138648271560669\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 27\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.346531857710943e-10, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 92, 76, 77, 91, 90, 93, 95, 88, 87, 89, 94, 75, 57, 74, 86, 73, 55, 56, 72, 49, 69, 97, 45, 35, 33, 28] 數值 torch.Size([27, 1])\n",
      "目前模型的Data狀態 torch.Size([27, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6950],\n",
      "        [0.7056],\n",
      "        [0.7586],\n",
      "        [0.7217],\n",
      "        [0.6912],\n",
      "        [0.7031],\n",
      "        [0.7167],\n",
      "        [0.7056],\n",
      "        [0.7336],\n",
      "        [0.7348],\n",
      "        [0.7110],\n",
      "        [0.7086],\n",
      "        [0.7472],\n",
      "        [0.9190],\n",
      "        [0.7618],\n",
      "        [0.7156],\n",
      "        [0.7475],\n",
      "        [0.9075],\n",
      "        [0.9080],\n",
      "        [0.7485],\n",
      "        [0.8550],\n",
      "        [0.7078],\n",
      "        [0.6880],\n",
      "        [0.7159],\n",
      "        [0.8296],\n",
      "        [0.8089],\n",
      "        [0.8359]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0010],\n",
      "        [    0.0012],\n",
      "        [    0.0012],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0023],\n",
      "        [    0.0026],\n",
      "        [    0.0040],\n",
      "        [    0.0045],\n",
      "        [    0.0080],\n",
      "        [    0.0105],\n",
      "        [    0.0111],\n",
      "        [    0.0133]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0003],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0006],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0008],\n",
      "        [    0.0016],\n",
      "        [    0.0016],\n",
      "        [    0.0012],\n",
      "        [    0.0029],\n",
      "        [    0.0020],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0046],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0097],\n",
      "        [    0.0118]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.4077184200286865\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 28\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.3981137903538183e-09, 57)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [57, 92, 96, 95, 76, 75, 93, 91, 90, 77, 94, 74, 88, 87, 89, 55, 86, 73, 72, 56, 49, 69, 97, 45, 35, 33, 28, 27] 數值 torch.Size([28, 1])\n",
      "目前模型的Data狀態 torch.Size([28, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9199],\n",
      "        [0.7055],\n",
      "        [0.6947],\n",
      "        [0.7053],\n",
      "        [0.7583],\n",
      "        [0.7466],\n",
      "        [0.7165],\n",
      "        [0.6910],\n",
      "        [0.7029],\n",
      "        [0.7213],\n",
      "        [0.7084],\n",
      "        [0.7614],\n",
      "        [0.7333],\n",
      "        [0.7345],\n",
      "        [0.7107],\n",
      "        [0.9084],\n",
      "        [0.7152],\n",
      "        [0.7471],\n",
      "        [0.7482],\n",
      "        [0.9088],\n",
      "        [0.8559],\n",
      "        [0.7081],\n",
      "        [0.6881],\n",
      "        [0.7169],\n",
      "        [0.8308],\n",
      "        [0.8103],\n",
      "        [0.8374],\n",
      "        [0.8219]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0008],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0012],\n",
      "        [    0.0016],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0029],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0046],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0097],\n",
      "        [    0.0118],\n",
      "        [    0.0183]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0013],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0006],\n",
      "        [    0.0023],\n",
      "        [    0.0009],\n",
      "        [    0.0011],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0021],\n",
      "        [    0.0034],\n",
      "        [    0.0043],\n",
      "        [    0.0075],\n",
      "        [    0.0098],\n",
      "        [    0.0102],\n",
      "        [    0.0125],\n",
      "        [    0.0173]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.6750218868255615\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 29\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.8315091665499494e-09, 91)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [91, 90, 75, 96, 92, 95, 76, 74, 87, 88, 93, 77, 89, 94, 86, 73, 57, 72, 56, 49, 55, 69, 97, 45, 35, 33, 28, 27, 37] 數值 torch.Size([29, 1])\n",
      "目前模型的Data狀態 torch.Size([29, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6915],\n",
      "        [0.7032],\n",
      "        [0.7462],\n",
      "        [0.6948],\n",
      "        [0.7059],\n",
      "        [0.7053],\n",
      "        [0.7583],\n",
      "        [0.7610],\n",
      "        [0.7350],\n",
      "        [0.7338],\n",
      "        [0.7166],\n",
      "        [0.7213],\n",
      "        [0.7112],\n",
      "        [0.7084],\n",
      "        [0.7159],\n",
      "        [0.7466],\n",
      "        [0.9187],\n",
      "        [0.7476],\n",
      "        [0.9076],\n",
      "        [0.8545],\n",
      "        [0.9073],\n",
      "        [0.7073],\n",
      "        [0.6879],\n",
      "        [0.7154],\n",
      "        [0.8303],\n",
      "        [0.8098],\n",
      "        [0.8367],\n",
      "        [0.8209],\n",
      "        [0.8418]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0009],\n",
      "        [    0.0011],\n",
      "        [    0.0013],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0021],\n",
      "        [    0.0023],\n",
      "        [    0.0034],\n",
      "        [    0.0043],\n",
      "        [    0.0075],\n",
      "        [    0.0098],\n",
      "        [    0.0102],\n",
      "        [    0.0125],\n",
      "        [    0.0173],\n",
      "        [    0.0217]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0006],\n",
      "        [    0.0005],\n",
      "        [    0.0010],\n",
      "        [    0.0014],\n",
      "        [    0.0003],\n",
      "        [    0.0018],\n",
      "        [    0.0027],\n",
      "        [    0.0032],\n",
      "        [    0.0011],\n",
      "        [    0.0038],\n",
      "        [    0.0047],\n",
      "        [    0.0087],\n",
      "        [    0.0079],\n",
      "        [    0.0084],\n",
      "        [    0.0109],\n",
      "        [    0.0186],\n",
      "        [    0.0197]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.9419188499450684\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 30\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.708425897590132e-09, 91)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [91, 90, 75, 95, 76, 96, 57, 92, 77, 93, 88, 87, 94, 74, 89, 86, 55, 73, 72, 56, 49, 69, 97, 35, 33, 45, 28, 27, 37, 34] 數值 torch.Size([30, 1])\n",
      "目前模型的Data狀態 torch.Size([30, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6915],\n",
      "        [0.7032],\n",
      "        [0.7461],\n",
      "        [0.7052],\n",
      "        [0.7584],\n",
      "        [0.6947],\n",
      "        [0.9197],\n",
      "        [0.7060],\n",
      "        [0.7213],\n",
      "        [0.7166],\n",
      "        [0.7337],\n",
      "        [0.7350],\n",
      "        [0.7083],\n",
      "        [0.7612],\n",
      "        [0.7111],\n",
      "        [0.7158],\n",
      "        [0.9085],\n",
      "        [0.7469],\n",
      "        [0.7480],\n",
      "        [0.9086],\n",
      "        [0.8557],\n",
      "        [0.7076],\n",
      "        [0.6882],\n",
      "        [0.8321],\n",
      "        [0.8116],\n",
      "        [0.7166],\n",
      "        [0.8383],\n",
      "        [0.8222],\n",
      "        [0.8438],\n",
      "        [0.8383]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0010],\n",
      "        [    0.0011],\n",
      "        [    0.0014],\n",
      "        [    0.0018],\n",
      "        [    0.0027],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0047],\n",
      "        [    0.0079],\n",
      "        [    0.0084],\n",
      "        [    0.0087],\n",
      "        [    0.0109],\n",
      "        [    0.0186],\n",
      "        [    0.0197],\n",
      "        [    0.0239]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 27\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0005],\n",
      "        [    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0019],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0026],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0011],\n",
      "        [    0.0016],\n",
      "        [    0.0027],\n",
      "        [    0.0041],\n",
      "        [    0.0088],\n",
      "        [    0.0095],\n",
      "        [    0.0069],\n",
      "        [    0.0121],\n",
      "        [    0.0172],\n",
      "        [    0.0205],\n",
      "        [    0.0229]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.208299875259399\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 31\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.2567157031971874e-09, 95)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [95, 91, 90, 74, 94, 76, 93, 88, 87, 92, 96, 77, 89, 75, 86, 73, 56, 72, 49, 57, 55, 69, 97, 45, 35, 33, 28, 27, 37, 34, 36] 數值 torch.Size([31, 1])\n",
      "目前模型的Data狀態 torch.Size([31, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7050],\n",
      "        [0.6915],\n",
      "        [0.7032],\n",
      "        [0.7608],\n",
      "        [0.7080],\n",
      "        [0.7583],\n",
      "        [0.7164],\n",
      "        [0.7339],\n",
      "        [0.7352],\n",
      "        [0.7060],\n",
      "        [0.6946],\n",
      "        [0.7213],\n",
      "        [0.7112],\n",
      "        [0.7458],\n",
      "        [0.7160],\n",
      "        [0.7464],\n",
      "        [0.9070],\n",
      "        [0.7473],\n",
      "        [0.8540],\n",
      "        [0.9181],\n",
      "        [0.9070],\n",
      "        [0.7065],\n",
      "        [0.6877],\n",
      "        [0.7148],\n",
      "        [0.8312],\n",
      "        [0.8105],\n",
      "        [0.8371],\n",
      "        [0.8209],\n",
      "        [0.8430],\n",
      "        [0.8374],\n",
      "        [0.8670]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0011],\n",
      "        [    0.0012],\n",
      "        [    0.0016],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0027],\n",
      "        [    0.0041],\n",
      "        [    0.0069],\n",
      "        [    0.0088],\n",
      "        [    0.0095],\n",
      "        [    0.0121],\n",
      "        [    0.0172],\n",
      "        [    0.0205],\n",
      "        [    0.0229],\n",
      "        [    0.0244]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0006],\n",
      "        [    0.0009],\n",
      "        [    0.0006],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0002],\n",
      "        [    0.0036],\n",
      "        [    0.0043],\n",
      "        [    0.0016],\n",
      "        [    0.0038],\n",
      "        [    0.0049],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0137],\n",
      "        [    0.0156],\n",
      "        [    0.0220],\n",
      "        [    0.0213],\n",
      "        [    0.0229]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.475663423538208\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 32\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.255671122635249e-10, 93)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [93, 91, 92, 94, 49, 73, 90, 87, 88, 74, 95, 72, 76, 56, 89, 86, 77, 96, 75, 69, 57, 97, 55, 45, 35, 33, 28, 27, 34, 37, 36, 78] 數值 torch.Size([32, 1])\n",
      "目前模型的Data狀態 torch.Size([32, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7161],\n",
      "        [0.6914],\n",
      "        [0.7057],\n",
      "        [0.7076],\n",
      "        [0.8522],\n",
      "        [0.7458],\n",
      "        [0.7031],\n",
      "        [0.7352],\n",
      "        [0.7339],\n",
      "        [0.7603],\n",
      "        [0.7046],\n",
      "        [0.7466],\n",
      "        [0.7581],\n",
      "        [0.9054],\n",
      "        [0.7112],\n",
      "        [0.7161],\n",
      "        [0.7211],\n",
      "        [0.6943],\n",
      "        [0.7454],\n",
      "        [0.7054],\n",
      "        [0.9164],\n",
      "        [0.6874],\n",
      "        [0.9053],\n",
      "        [0.7128],\n",
      "        [0.8297],\n",
      "        [0.8090],\n",
      "        [0.8355],\n",
      "        [0.8192],\n",
      "        [0.8358],\n",
      "        [0.8415],\n",
      "        [0.8654],\n",
      "        [0.6874]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0016],\n",
      "        [    0.0036],\n",
      "        [    0.0038],\n",
      "        [    0.0043],\n",
      "        [    0.0049],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0137],\n",
      "        [    0.0156],\n",
      "        [    0.0213],\n",
      "        [    0.0220],\n",
      "        [    0.0229],\n",
      "        [    0.0243]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0012],\n",
      "        [    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0000],\n",
      "        [    0.0013],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0009],\n",
      "        [    0.0004],\n",
      "        [    0.0006],\n",
      "        [    0.0003],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0039],\n",
      "        [    0.0038],\n",
      "        [    0.0057],\n",
      "        [    0.0103],\n",
      "        [    0.0111],\n",
      "        [    0.0135],\n",
      "        [    0.0161],\n",
      "        [    0.0211],\n",
      "        [    0.0221],\n",
      "        [    0.0226],\n",
      "        [    0.0168]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.743495941162109\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 33\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.18506102100946e-10, 95)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [95, 91, 90, 56, 76, 92, 74, 94, 88, 87, 75, 93, 77, 89, 49, 96, 86, 73, 72, 69, 57, 55, 97, 45, 35, 33, 28, 27, 78, 34, 37, 36, 99] 數值 torch.Size([33, 1])\n",
      "目前模型的Data狀態 torch.Size([33, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7050],\n",
      "        [0.6915],\n",
      "        [0.7033],\n",
      "        [0.9060],\n",
      "        [0.7584],\n",
      "        [0.7058],\n",
      "        [0.7608],\n",
      "        [0.7080],\n",
      "        [0.7339],\n",
      "        [0.7352],\n",
      "        [0.7460],\n",
      "        [0.7165],\n",
      "        [0.7214],\n",
      "        [0.7113],\n",
      "        [0.8529],\n",
      "        [0.6945],\n",
      "        [0.7159],\n",
      "        [0.7467],\n",
      "        [0.7475],\n",
      "        [0.7064],\n",
      "        [0.9171],\n",
      "        [0.9058],\n",
      "        [0.6874],\n",
      "        [0.7137],\n",
      "        [0.8297],\n",
      "        [0.8089],\n",
      "        [0.8357],\n",
      "        [0.8198],\n",
      "        [0.6798],\n",
      "        [0.8356],\n",
      "        [0.8414],\n",
      "        [0.8652],\n",
      "        [0.6846]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0006],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0057],\n",
      "        [    0.0103],\n",
      "        [    0.0111],\n",
      "        [    0.0135],\n",
      "        [    0.0161],\n",
      "        [    0.0168],\n",
      "        [    0.0211],\n",
      "        [    0.0221],\n",
      "        [    0.0226],\n",
      "        [    0.0249]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(    0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 51\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0015],\n",
      "        [0.0017],\n",
      "        [0.0016],\n",
      "        [0.0004],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0009],\n",
      "        [0.0013],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0015],\n",
      "        [0.0012],\n",
      "        [0.0018],\n",
      "        [0.0021],\n",
      "        [0.0001],\n",
      "        [0.0023],\n",
      "        [0.0027],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0016],\n",
      "        [0.0034],\n",
      "        [0.0044],\n",
      "        [0.0021],\n",
      "        [0.0053],\n",
      "        [0.0111],\n",
      "        [0.0120],\n",
      "        [0.0143],\n",
      "        [0.0153],\n",
      "        [0.0138],\n",
      "        [0.0202],\n",
      "        [0.0229],\n",
      "        [0.0218],\n",
      "        [0.0230]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.985945701599121\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 34\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1307759112687563e-08, 49)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [49, 73, 56, 72, 74, 93, 94, 75, 95, 92, 69, 76, 90, 91, 77, 88, 87, 97, 89, 96, 86, 57, 55, 45, 35, 33, 78, 28, 27, 34, 36, 37, 99, 98] 數值 torch.Size([34, 1])\n",
      "目前模型的Data狀態 torch.Size([34, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8526],\n",
      "        [0.7458],\n",
      "        [0.9055],\n",
      "        [0.7466],\n",
      "        [0.7597],\n",
      "        [0.7149],\n",
      "        [0.7065],\n",
      "        [0.7448],\n",
      "        [0.7035],\n",
      "        [0.7041],\n",
      "        [0.7054],\n",
      "        [0.7570],\n",
      "        [0.7017],\n",
      "        [0.6898],\n",
      "        [0.7200],\n",
      "        [0.7323],\n",
      "        [0.7335],\n",
      "        [0.6856],\n",
      "        [0.7096],\n",
      "        [0.6928],\n",
      "        [0.7140],\n",
      "        [0.9166],\n",
      "        [0.9052],\n",
      "        [0.7132],\n",
      "        [0.8289],\n",
      "        [0.8080],\n",
      "        [0.6769],\n",
      "        [0.8349],\n",
      "        [0.8190],\n",
      "        [0.8346],\n",
      "        [0.8643],\n",
      "        [0.8406],\n",
      "        [0.6827],\n",
      "        [0.7009]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0005],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0015],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0027],\n",
      "        [0.0034],\n",
      "        [0.0044],\n",
      "        [0.0053],\n",
      "        [0.0111],\n",
      "        [0.0120],\n",
      "        [0.0138],\n",
      "        [0.0143],\n",
      "        [0.0153],\n",
      "        [0.0202],\n",
      "        [0.0218],\n",
      "        [0.0229],\n",
      "        [0.0230],\n",
      "        [0.0237]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 60\n",
      "Number of shrink: 38\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0003],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0016],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0010],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0010],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0116],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0149],\n",
      "        [0.0148],\n",
      "        [0.0196],\n",
      "        [0.0213],\n",
      "        [0.0234],\n",
      "        [0.0219],\n",
      "        [0.0226]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.258852005004883\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 35\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.8033606608014452e-08, 49)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [49, 72, 73, 56, 69, 97, 74, 93, 75, 94, 95, 76, 90, 92, 77, 91, 88, 87, 89, 96, 86, 57, 55, 45, 35, 33, 78, 27, 28, 34, 36, 99, 98, 37, 32] 數值 torch.Size([35, 1])\n",
      "目前模型的Data狀態 torch.Size([35, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8523],\n",
      "        [0.7460],\n",
      "        [0.7452],\n",
      "        [0.9051],\n",
      "        [0.7048],\n",
      "        [0.6846],\n",
      "        [0.7590],\n",
      "        [0.7139],\n",
      "        [0.7440],\n",
      "        [0.7055],\n",
      "        [0.7025],\n",
      "        [0.7561],\n",
      "        [0.7008],\n",
      "        [0.7030],\n",
      "        [0.7192],\n",
      "        [0.6887],\n",
      "        [0.7313],\n",
      "        [0.7325],\n",
      "        [0.7086],\n",
      "        [0.6917],\n",
      "        [0.7130],\n",
      "        [0.9162],\n",
      "        [0.9048],\n",
      "        [0.7129],\n",
      "        [0.8284],\n",
      "        [0.8075],\n",
      "        [0.6760],\n",
      "        [0.8184],\n",
      "        [0.8343],\n",
      "        [0.8341],\n",
      "        [0.8639],\n",
      "        [0.6816],\n",
      "        [0.6998],\n",
      "        [0.8401],\n",
      "        [0.7809]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0022],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0116],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0196],\n",
      "        [0.0213],\n",
      "        [0.0219],\n",
      "        [0.0226],\n",
      "        [0.0234],\n",
      "        [0.0286]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0021],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0012],\n",
      "        [0.0020],\n",
      "        [0.0007],\n",
      "        [0.0012],\n",
      "        [0.0022],\n",
      "        [0.0019],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0019],\n",
      "        [0.0029],\n",
      "        [0.0072],\n",
      "        [0.0099],\n",
      "        [0.0108],\n",
      "        [0.0130],\n",
      "        [0.0165],\n",
      "        [0.0131],\n",
      "        [0.0213],\n",
      "        [0.0231],\n",
      "        [0.0215],\n",
      "        [0.0222],\n",
      "        [0.0216],\n",
      "        [0.0268]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.531619071960449\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 36\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.628809170346358e-07, 73)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [73, 97, 72, 74, 56, 75, 57, 69, 49, 93, 94, 76, 95, 77, 90, 55, 92, 91, 88, 87, 89, 96, 86, 45, 35, 33, 78, 28, 27, 34, 99, 37, 98, 36, 32, 101] 數值 torch.Size([36, 1])\n",
      "目前模型的Data狀態 torch.Size([36, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7460],\n",
      "        [0.6842],\n",
      "        [0.7468],\n",
      "        [0.7594],\n",
      "        [0.9071],\n",
      "        [0.7444],\n",
      "        [0.9181],\n",
      "        [0.7059],\n",
      "        [0.8545],\n",
      "        [0.7139],\n",
      "        [0.7055],\n",
      "        [0.7561],\n",
      "        [0.7025],\n",
      "        [0.7192],\n",
      "        [0.7006],\n",
      "        [0.9067],\n",
      "        [0.7027],\n",
      "        [0.6885],\n",
      "        [0.7311],\n",
      "        [0.7322],\n",
      "        [0.7084],\n",
      "        [0.6914],\n",
      "        [0.7125],\n",
      "        [0.7151],\n",
      "        [0.8301],\n",
      "        [0.8092],\n",
      "        [0.6760],\n",
      "        [0.8361],\n",
      "        [0.8201],\n",
      "        [0.8357],\n",
      "        [0.6812],\n",
      "        [0.8419],\n",
      "        [0.6994],\n",
      "        [0.8656],\n",
      "        [0.7827],\n",
      "        [0.6760]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0019],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0072],\n",
      "        [0.0099],\n",
      "        [0.0108],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0165],\n",
      "        [0.0213],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0231],\n",
      "        [0.0268],\n",
      "        [0.0285]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 59\n",
      "Number of shrink: 37\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0016],\n",
      "        [0.0009],\n",
      "        [0.0024],\n",
      "        [0.0022],\n",
      "        [0.0016],\n",
      "        [0.0019],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0048],\n",
      "        [0.0069],\n",
      "        [0.0102],\n",
      "        [0.0111],\n",
      "        [0.0125],\n",
      "        [0.0134],\n",
      "        [0.0162],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0219],\n",
      "        [0.0217],\n",
      "        [0.0228],\n",
      "        [0.0271],\n",
      "        [0.0279]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.801050901412964\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 37\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0170936093345517e-08, 97)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [97, 73, 72, 56, 74, 69, 49, 57, 75, 93, 94, 76, 95, 77, 55, 90, 92, 91, 88, 89, 87, 96, 86, 45, 35, 33, 78, 28, 27, 99, 34, 98, 37, 36, 32, 101, 62] 數值 torch.Size([37, 1])\n",
      "目前模型的Data狀態 torch.Size([37, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6836],\n",
      "        [0.7456],\n",
      "        [0.7464],\n",
      "        [0.9068],\n",
      "        [0.7590],\n",
      "        [0.7055],\n",
      "        [0.8543],\n",
      "        [0.9178],\n",
      "        [0.7439],\n",
      "        [0.7134],\n",
      "        [0.7050],\n",
      "        [0.7556],\n",
      "        [0.7020],\n",
      "        [0.7187],\n",
      "        [0.9064],\n",
      "        [0.7001],\n",
      "        [0.7022],\n",
      "        [0.6879],\n",
      "        [0.7306],\n",
      "        [0.7079],\n",
      "        [0.7316],\n",
      "        [0.6909],\n",
      "        [0.7120],\n",
      "        [0.7148],\n",
      "        [0.8298],\n",
      "        [0.8089],\n",
      "        [0.6755],\n",
      "        [0.8358],\n",
      "        [0.8198],\n",
      "        [0.6806],\n",
      "        [0.8354],\n",
      "        [0.6988],\n",
      "        [0.8416],\n",
      "        [0.8653],\n",
      "        [0.7824],\n",
      "        [0.6754],\n",
      "        [0.8520]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0048],\n",
      "        [0.0069],\n",
      "        [0.0102],\n",
      "        [0.0111],\n",
      "        [0.0125],\n",
      "        [0.0134],\n",
      "        [0.0162],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0228],\n",
      "        [0.0271],\n",
      "        [0.0279],\n",
      "        [0.0313]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0026],\n",
      "        [0.0011],\n",
      "        [0.0026],\n",
      "        [0.0037],\n",
      "        [0.0006],\n",
      "        [0.0020],\n",
      "        [0.0028],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0029],\n",
      "        [0.0016],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0098],\n",
      "        [0.0126],\n",
      "        [0.0122],\n",
      "        [0.0175],\n",
      "        [0.0206],\n",
      "        [0.0222],\n",
      "        [0.0214],\n",
      "        [0.0206],\n",
      "        [0.0241],\n",
      "        [0.0257],\n",
      "        [0.0277],\n",
      "        [0.0299]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.072817087173462\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 38\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.5099794243033102e-08, 97)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [97, 57, 73, 72, 74, 55, 75, 69, 56, 93, 94, 76, 77, 95, 90, 92, 49, 91, 88, 89, 87, 96, 86, 45, 35, 33, 28, 78, 27, 99, 37, 98, 34, 36, 32, 101, 47, 62] 數值 torch.Size([38, 1])\n",
      "目前模型的Data狀態 torch.Size([38, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6834],\n",
      "        [0.9194],\n",
      "        [0.7464],\n",
      "        [0.7472],\n",
      "        [0.7595],\n",
      "        [0.9080],\n",
      "        [0.7443],\n",
      "        [0.7064],\n",
      "        [0.9085],\n",
      "        [0.7133],\n",
      "        [0.7050],\n",
      "        [0.7557],\n",
      "        [0.7189],\n",
      "        [0.7020],\n",
      "        [0.7000],\n",
      "        [0.7019],\n",
      "        [0.8561],\n",
      "        [0.6877],\n",
      "        [0.7303],\n",
      "        [0.7077],\n",
      "        [0.7313],\n",
      "        [0.6907],\n",
      "        [0.7116],\n",
      "        [0.7167],\n",
      "        [0.8311],\n",
      "        [0.8101],\n",
      "        [0.8370],\n",
      "        [0.6757],\n",
      "        [0.8211],\n",
      "        [0.6803],\n",
      "        [0.8429],\n",
      "        [0.6986],\n",
      "        [0.8366],\n",
      "        [0.8666],\n",
      "        [0.7837],\n",
      "        [0.6751],\n",
      "        [0.8000],\n",
      "        [0.8534]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0011],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0098],\n",
      "        [0.0122],\n",
      "        [0.0126],\n",
      "        [0.0175],\n",
      "        [0.0206],\n",
      "        [0.0206],\n",
      "        [0.0214],\n",
      "        [0.0222],\n",
      "        [0.0241],\n",
      "        [0.0257],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0299]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0019],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0040],\n",
      "        [0.0052],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0029],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0043],\n",
      "        [0.0066],\n",
      "        [0.0043],\n",
      "        [0.0046],\n",
      "        [0.0047],\n",
      "        [0.0049],\n",
      "        [0.0049],\n",
      "        [0.0062],\n",
      "        [0.0117],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0102],\n",
      "        [0.0127],\n",
      "        [0.0196],\n",
      "        [0.0199],\n",
      "        [0.0186],\n",
      "        [0.0207],\n",
      "        [0.0240],\n",
      "        [0.0261],\n",
      "        [0.0237],\n",
      "        [0.0270],\n",
      "        [0.0265],\n",
      "        [0.0277]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.345041036605835\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 39\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.391168093003216e-07, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 97, 55, 75, 73, 57, 72, 77, 76, 94, 93, 95, 90, 69, 92, 91, 88, 89, 87, 96, 56, 86, 49, 35, 33, 28, 45, 78, 37, 27, 99, 98, 32, 34, 36, 47, 101, 62, 48] 數值 torch.Size([39, 1])\n",
      "目前模型的Data狀態 torch.Size([39, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7600],\n",
      "        [0.6827],\n",
      "        [0.9105],\n",
      "        [0.7448],\n",
      "        [0.7474],\n",
      "        [0.9219],\n",
      "        [0.7484],\n",
      "        [0.7189],\n",
      "        [0.7556],\n",
      "        [0.7048],\n",
      "        [0.7130],\n",
      "        [0.7018],\n",
      "        [0.6995],\n",
      "        [0.7078],\n",
      "        [0.7013],\n",
      "        [0.6871],\n",
      "        [0.7296],\n",
      "        [0.7070],\n",
      "        [0.7306],\n",
      "        [0.6901],\n",
      "        [0.9111],\n",
      "        [0.7106],\n",
      "        [0.8591],\n",
      "        [0.8331],\n",
      "        [0.8121],\n",
      "        [0.8390],\n",
      "        [0.7196],\n",
      "        [0.6757],\n",
      "        [0.8449],\n",
      "        [0.8232],\n",
      "        [0.6797],\n",
      "        [0.6979],\n",
      "        [0.7858],\n",
      "        [0.8384],\n",
      "        [0.8686],\n",
      "        [0.8031],\n",
      "        [0.6745],\n",
      "        [0.8556],\n",
      "        [0.8181]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0046],\n",
      "        [0.0047],\n",
      "        [0.0049],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0102],\n",
      "        [0.0117],\n",
      "        [0.0127],\n",
      "        [0.0186],\n",
      "        [0.0196],\n",
      "        [0.0199],\n",
      "        [0.0207],\n",
      "        [0.0237],\n",
      "        [0.0240],\n",
      "        [0.0261],\n",
      "        [0.0265],\n",
      "        [0.0270],\n",
      "        [0.0277],\n",
      "        [0.0281]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0016],\n",
      "        [0.0032],\n",
      "        [0.0013],\n",
      "        [0.0028],\n",
      "        [0.0042],\n",
      "        [0.0033],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0076],\n",
      "        [0.0072],\n",
      "        [0.0093],\n",
      "        [0.0052],\n",
      "        [0.0063],\n",
      "        [0.0086],\n",
      "        [0.0144],\n",
      "        [0.0126],\n",
      "        [0.0170],\n",
      "        [0.0213],\n",
      "        [0.0191],\n",
      "        [0.0199],\n",
      "        [0.0219],\n",
      "        [0.0255],\n",
      "        [0.0277],\n",
      "        [0.0237],\n",
      "        [0.0262],\n",
      "        [0.0257],\n",
      "        [0.0251]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.617416858673096\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 40\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.187046798165284e-08, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 75, 97, 73, 77, 55, 76, 72, 94, 93, 95, 57, 90, 91, 92, 69, 35, 88, 89, 96, 87, 33, 86, 56, 28, 49, 78, 45, 37, 99, 98, 27, 32, 47, 48, 34, 62, 101, 36, 26] 數值 torch.Size([40, 1])\n",
      "目前模型的Data狀態 torch.Size([40, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7604],\n",
      "        [0.7450],\n",
      "        [0.6819],\n",
      "        [0.7483],\n",
      "        [0.7187],\n",
      "        [0.9128],\n",
      "        [0.7554],\n",
      "        [0.7494],\n",
      "        [0.7045],\n",
      "        [0.7127],\n",
      "        [0.7015],\n",
      "        [0.9241],\n",
      "        [0.6990],\n",
      "        [0.6864],\n",
      "        [0.7006],\n",
      "        [0.7090],\n",
      "        [0.8348],\n",
      "        [0.7289],\n",
      "        [0.7064],\n",
      "        [0.6894],\n",
      "        [0.7298],\n",
      "        [0.8137],\n",
      "        [0.7096],\n",
      "        [0.9135],\n",
      "        [0.8406],\n",
      "        [0.8618],\n",
      "        [0.6756],\n",
      "        [0.7223],\n",
      "        [0.8466],\n",
      "        [0.6788],\n",
      "        [0.6971],\n",
      "        [0.8250],\n",
      "        [0.7875],\n",
      "        [0.8060],\n",
      "        [0.8212],\n",
      "        [0.8400],\n",
      "        [0.8576],\n",
      "        [0.6737],\n",
      "        [0.8703],\n",
      "        [0.8017]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0013],\n",
      "        [0.0016],\n",
      "        [0.0028],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0052],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0063],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0086],\n",
      "        [0.0093],\n",
      "        [0.0126],\n",
      "        [0.0144],\n",
      "        [0.0170],\n",
      "        [0.0191],\n",
      "        [0.0199],\n",
      "        [0.0213],\n",
      "        [0.0219],\n",
      "        [0.0237],\n",
      "        [0.0251],\n",
      "        [0.0255],\n",
      "        [0.0257],\n",
      "        [0.0262],\n",
      "        [0.0277],\n",
      "        [0.0299]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0015],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0051],\n",
      "        [0.0039],\n",
      "        [0.0041],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0063],\n",
      "        [0.0046],\n",
      "        [0.0059],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0040],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0072],\n",
      "        [0.0063],\n",
      "        [0.0051],\n",
      "        [0.0083],\n",
      "        [0.0099],\n",
      "        [0.0071],\n",
      "        [0.0123],\n",
      "        [0.0119],\n",
      "        [0.0173],\n",
      "        [0.0158],\n",
      "        [0.0172],\n",
      "        [0.0180],\n",
      "        [0.0230],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0215],\n",
      "        [0.0264],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0288],\n",
      "        [0.0278]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.889953851699829\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 41\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.6416603304824093e-07, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 75, 97, 73, 77, 76, 94, 35, 93, 72, 95, 90, 55, 33, 89, 91, 88, 92, 87, 57, 69, 28, 96, 86, 56, 78, 49, 37, 99, 45, 98, 32, 47, 48, 27, 62, 101, 34, 26, 36, 63] 數值 torch.Size([41, 1])\n",
      "目前模型的Data狀態 torch.Size([41, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7601],\n",
      "        [0.7448],\n",
      "        [0.6801],\n",
      "        [0.7491],\n",
      "        [0.7181],\n",
      "        [0.7547],\n",
      "        [0.7038],\n",
      "        [0.8360],\n",
      "        [0.7121],\n",
      "        [0.7503],\n",
      "        [0.7008],\n",
      "        [0.6987],\n",
      "        [0.9147],\n",
      "        [0.8149],\n",
      "        [0.7059],\n",
      "        [0.6855],\n",
      "        [0.7283],\n",
      "        [0.6994],\n",
      "        [0.7292],\n",
      "        [0.9263],\n",
      "        [0.7103],\n",
      "        [0.8421],\n",
      "        [0.6878],\n",
      "        [0.7085],\n",
      "        [0.9158],\n",
      "        [0.6749],\n",
      "        [0.8647],\n",
      "        [0.8478],\n",
      "        [0.6769],\n",
      "        [0.7252],\n",
      "        [0.6952],\n",
      "        [0.7891],\n",
      "        [0.8092],\n",
      "        [0.8248],\n",
      "        [0.8267],\n",
      "        [0.8594],\n",
      "        [0.6717],\n",
      "        [0.8409],\n",
      "        [0.8037],\n",
      "        [0.8713],\n",
      "        [0.8317]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0015],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0051],\n",
      "        [0.0059],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0063],\n",
      "        [0.0063],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0083],\n",
      "        [0.0099],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0158],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0180],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0215],\n",
      "        [0.0230],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0264],\n",
      "        [0.0278],\n",
      "        [0.0288],\n",
      "        [0.0300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0011],\n",
      "        [0.0040],\n",
      "        [0.0045],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0053],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0065],\n",
      "        [0.0045],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0058],\n",
      "        [0.0064],\n",
      "        [0.0061],\n",
      "        [0.0079],\n",
      "        [0.0077],\n",
      "        [0.0065],\n",
      "        [0.0078],\n",
      "        [0.0083],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0141],\n",
      "        [0.0152],\n",
      "        [0.0165],\n",
      "        [0.0192],\n",
      "        [0.0174],\n",
      "        [0.0195],\n",
      "        [0.0183],\n",
      "        [0.0190],\n",
      "        [0.0240],\n",
      "        [0.0222],\n",
      "        [0.0235],\n",
      "        [0.0269],\n",
      "        [0.0265],\n",
      "        [0.0292],\n",
      "        [0.0283]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.1626505851745605\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 42\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.572022925865895e-08, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 75, 77, 35, 76, 94, 93, 97, 95, 90, 33, 73, 72, 89, 88, 91, 87, 92, 28, 55, 69, 96, 57, 86, 56, 78, 49, 37, 99, 98, 47, 48, 45, 32, 62, 101, 27, 26, 34, 63, 36, 61] 數值 torch.Size([42, 1])\n",
      "目前模型的Data狀態 torch.Size([42, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7603],\n",
      "        [0.7452],\n",
      "        [0.7185],\n",
      "        [0.8367],\n",
      "        [0.7552],\n",
      "        [0.7041],\n",
      "        [0.7123],\n",
      "        [0.6795],\n",
      "        [0.7009],\n",
      "        [0.6992],\n",
      "        [0.8155],\n",
      "        [0.7501],\n",
      "        [0.7514],\n",
      "        [0.7062],\n",
      "        [0.7285],\n",
      "        [0.6856],\n",
      "        [0.7294],\n",
      "        [0.6993],\n",
      "        [0.8427],\n",
      "        [0.9161],\n",
      "        [0.7116],\n",
      "        [0.6872],\n",
      "        [0.9279],\n",
      "        [0.7085],\n",
      "        [0.9174],\n",
      "        [0.6753],\n",
      "        [0.8665],\n",
      "        [0.8483],\n",
      "        [0.6763],\n",
      "        [0.6945],\n",
      "        [0.8113],\n",
      "        [0.8272],\n",
      "        [0.7272],\n",
      "        [0.7899],\n",
      "        [0.8612],\n",
      "        [0.6709],\n",
      "        [0.8277],\n",
      "        [0.8050],\n",
      "        [0.8413],\n",
      "        [0.8333],\n",
      "        [0.8717],\n",
      "        [0.8861]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0011],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0045],\n",
      "        [0.0045],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0058],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0065],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0083],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0141],\n",
      "        [0.0152],\n",
      "        [0.0165],\n",
      "        [0.0174],\n",
      "        [0.0183],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0195],\n",
      "        [0.0222],\n",
      "        [0.0235],\n",
      "        [0.0240],\n",
      "        [0.0265],\n",
      "        [0.0269],\n",
      "        [0.0283],\n",
      "        [0.0292],\n",
      "        [0.0342]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0028],\n",
      "        [    0.0027],\n",
      "        [    0.0029],\n",
      "        [    0.0035],\n",
      "        [    0.0035],\n",
      "        [    0.0047],\n",
      "        [    0.0041],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0054],\n",
      "        [    0.0065],\n",
      "        [    0.0055],\n",
      "        [    0.0058],\n",
      "        [    0.0058],\n",
      "        [    0.0060],\n",
      "        [    0.0065],\n",
      "        [    0.0058],\n",
      "        [    0.0080],\n",
      "        [    0.0090],\n",
      "        [    0.0084],\n",
      "        [    0.0098],\n",
      "        [    0.0085],\n",
      "        [    0.0134],\n",
      "        [    0.0126],\n",
      "        [    0.0158],\n",
      "        [    0.0147],\n",
      "        [    0.0160],\n",
      "        [    0.0168],\n",
      "        [    0.0161],\n",
      "        [    0.0166],\n",
      "        [    0.0212],\n",
      "        [    0.0188],\n",
      "        [    0.0201],\n",
      "        [    0.0229],\n",
      "        [    0.0251],\n",
      "        [    0.0251],\n",
      "        [    0.0272],\n",
      "        [    0.0264],\n",
      "        [    0.0296],\n",
      "        [    0.0321]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.435256242752075\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 43\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.474820679613913e-11, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 75, 35, 77, 76, 93, 94, 90, 33, 95, 97, 73, 89, 28, 88, 91, 87, 72, 92, 55, 96, 86, 69, 57, 78, 56, 37, 49, 99, 47, 48, 98, 32, 62, 45, 101, 26, 27, 63, 34, 36, 61, 100] 數值 torch.Size([43, 1])\n",
      "目前模型的Data狀態 torch.Size([43, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7606],\n",
      "        [0.7457],\n",
      "        [0.8373],\n",
      "        [0.7190],\n",
      "        [0.7557],\n",
      "        [0.7126],\n",
      "        [0.7043],\n",
      "        [0.6995],\n",
      "        [0.8160],\n",
      "        [0.7009],\n",
      "        [0.6789],\n",
      "        [0.7509],\n",
      "        [0.7063],\n",
      "        [0.8434],\n",
      "        [0.7285],\n",
      "        [0.6856],\n",
      "        [0.7295],\n",
      "        [0.7526],\n",
      "        [0.6992],\n",
      "        [0.9176],\n",
      "        [0.6866],\n",
      "        [0.7082],\n",
      "        [0.7128],\n",
      "        [0.9297],\n",
      "        [0.6756],\n",
      "        [0.9193],\n",
      "        [0.8489],\n",
      "        [0.8682],\n",
      "        [0.6757],\n",
      "        [0.8136],\n",
      "        [0.8297],\n",
      "        [0.6940],\n",
      "        [0.7907],\n",
      "        [0.8632],\n",
      "        [0.7291],\n",
      "        [0.6703],\n",
      "        [0.8065],\n",
      "        [0.8287],\n",
      "        [0.8353],\n",
      "        [0.8417],\n",
      "        [0.8721],\n",
      "        [0.8882],\n",
      "        [0.6680]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0027],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0035],\n",
      "        [    0.0035],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0041],\n",
      "        [    0.0047],\n",
      "        [    0.0054],\n",
      "        [    0.0055],\n",
      "        [    0.0058],\n",
      "        [    0.0058],\n",
      "        [    0.0058],\n",
      "        [    0.0060],\n",
      "        [    0.0065],\n",
      "        [    0.0065],\n",
      "        [    0.0080],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0090],\n",
      "        [    0.0098],\n",
      "        [    0.0126],\n",
      "        [    0.0134],\n",
      "        [    0.0147],\n",
      "        [    0.0158],\n",
      "        [    0.0160],\n",
      "        [    0.0161],\n",
      "        [    0.0166],\n",
      "        [    0.0168],\n",
      "        [    0.0188],\n",
      "        [    0.0201],\n",
      "        [    0.0212],\n",
      "        [    0.0229],\n",
      "        [    0.0251],\n",
      "        [    0.0251],\n",
      "        [    0.0264],\n",
      "        [    0.0272],\n",
      "        [    0.0296],\n",
      "        [    0.0321],\n",
      "        [    0.0327]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0081],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0101],\n",
      "        [0.0124],\n",
      "        [0.0137],\n",
      "        [0.0150],\n",
      "        [0.0161],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0190],\n",
      "        [0.0196],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0258],\n",
      "        [0.0268],\n",
      "        [0.0291],\n",
      "        [0.0315],\n",
      "        [0.0288]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.7073283195495605\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 44\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.599556456876599e-08, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 75, 77, 35, 76, 93, 94, 90, 33, 95, 97, 73, 89, 28, 88, 91, 87, 72, 92, 55, 86, 96, 69, 57, 78, 56, 37, 99, 47, 48, 49, 98, 32, 62, 45, 101, 26, 27, 63, 34, 100, 36, 61, 58] 數值 torch.Size([44, 1])\n",
      "目前模型的Data狀態 torch.Size([44, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7603],\n",
      "        [0.7455],\n",
      "        [0.7188],\n",
      "        [0.8370],\n",
      "        [0.7555],\n",
      "        [0.7124],\n",
      "        [0.7040],\n",
      "        [0.6993],\n",
      "        [0.8157],\n",
      "        [0.7005],\n",
      "        [0.6782],\n",
      "        [0.7511],\n",
      "        [0.7059],\n",
      "        [0.8432],\n",
      "        [0.7281],\n",
      "        [0.6852],\n",
      "        [0.7291],\n",
      "        [0.7529],\n",
      "        [0.6987],\n",
      "        [0.9177],\n",
      "        [0.7077],\n",
      "        [0.6859],\n",
      "        [0.7132],\n",
      "        [0.9301],\n",
      "        [0.6754],\n",
      "        [0.9196],\n",
      "        [0.8485],\n",
      "        [0.6750],\n",
      "        [0.8142],\n",
      "        [0.8304],\n",
      "        [0.8685],\n",
      "        [0.6933],\n",
      "        [0.7904],\n",
      "        [0.8638],\n",
      "        [0.7296],\n",
      "        [0.6695],\n",
      "        [0.8068],\n",
      "        [0.8288],\n",
      "        [0.8358],\n",
      "        [0.8412],\n",
      "        [0.6642],\n",
      "        [0.8716],\n",
      "        [0.8888],\n",
      "        [0.8863]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0081],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0101],\n",
      "        [0.0124],\n",
      "        [0.0137],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        [0.0190],\n",
      "        [0.0196],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0258],\n",
      "        [0.0268],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0315],\n",
      "        [0.0492]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0004],\n",
      "        [0.0021],\n",
      "        [0.0011],\n",
      "        [0.0021],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0025],\n",
      "        [0.0045],\n",
      "        [0.0062],\n",
      "        [0.0073],\n",
      "        [0.0061],\n",
      "        [0.0038],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0088],\n",
      "        [0.0074],\n",
      "        [0.0115],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0117],\n",
      "        [0.0142],\n",
      "        [0.0130],\n",
      "        [0.0177],\n",
      "        [0.0133],\n",
      "        [0.0145],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0197],\n",
      "        [0.0153],\n",
      "        [0.0169],\n",
      "        [0.0158],\n",
      "        [0.0255],\n",
      "        [0.0213],\n",
      "        [0.0217],\n",
      "        [0.0278],\n",
      "        [0.0222],\n",
      "        [0.0283],\n",
      "        [0.0279],\n",
      "        [0.0307],\n",
      "        [0.0276],\n",
      "        [0.0448]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.979357719421387\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 45\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5419396959259757e-07, 75)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [75, 74, 35, 77, 76, 33, 93, 94, 28, 90, 95, 89, 97, 88, 87, 91, 73, 92, 72, 86, 96, 47, 48, 55, 69, 78, 37, 57, 99, 98, 62, 32, 56, 49, 101, 26, 63, 45, 61, 27, 100, 34, 36, 58, 24] 數值 torch.Size([45, 1])\n",
      "目前模型的Data狀態 torch.Size([45, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7467],\n",
      "        [0.7613],\n",
      "        [0.8389],\n",
      "        [0.7197],\n",
      "        [0.7565],\n",
      "        [0.8175],\n",
      "        [0.7126],\n",
      "        [0.7042],\n",
      "        [0.8454],\n",
      "        [0.6994],\n",
      "        [0.7005],\n",
      "        [0.7057],\n",
      "        [0.6774],\n",
      "        [0.7278],\n",
      "        [0.7289],\n",
      "        [0.6848],\n",
      "        [0.7528],\n",
      "        [0.6982],\n",
      "        [0.7550],\n",
      "        [0.7071],\n",
      "        [0.6852],\n",
      "        [0.8185],\n",
      "        [0.8351],\n",
      "        [0.9211],\n",
      "        [0.7156],\n",
      "        [0.6760],\n",
      "        [0.8502],\n",
      "        [0.9342],\n",
      "        [0.6742],\n",
      "        [0.6925],\n",
      "        [0.8675],\n",
      "        [0.7926],\n",
      "        [0.9236],\n",
      "        [0.8721],\n",
      "        [0.6687],\n",
      "        [0.8099],\n",
      "        [0.8394],\n",
      "        [0.7334],\n",
      "        [0.8927],\n",
      "        [0.8314],\n",
      "        [0.6632],\n",
      "        [0.8428],\n",
      "        [0.8733],\n",
      "        [0.8906],\n",
      "        [0.7429]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0021],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0073],\n",
      "        [0.0074],\n",
      "        [0.0088],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0117],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0142],\n",
      "        [0.0145],\n",
      "        [0.0153],\n",
      "        [0.0158],\n",
      "        [0.0169],\n",
      "        [0.0177],\n",
      "        [0.0197],\n",
      "        [0.0213],\n",
      "        [0.0217],\n",
      "        [0.0222],\n",
      "        [0.0255],\n",
      "        [0.0276],\n",
      "        [0.0278],\n",
      "        [0.0279],\n",
      "        [0.0283],\n",
      "        [0.0307],\n",
      "        [0.0448],\n",
      "        [0.0499]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0012],\n",
      "        [0.0029],\n",
      "        [0.0031],\n",
      "        [0.0018],\n",
      "        [0.0031],\n",
      "        [0.0043],\n",
      "        [0.0055],\n",
      "        [0.0066],\n",
      "        [0.0060],\n",
      "        [0.0060],\n",
      "        [0.0063],\n",
      "        [0.0082],\n",
      "        [0.0074],\n",
      "        [0.0100],\n",
      "        [0.0094],\n",
      "        [0.0102],\n",
      "        [0.0080],\n",
      "        [0.0078],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0132],\n",
      "        [0.0125],\n",
      "        [0.0171],\n",
      "        [0.0142],\n",
      "        [0.0150],\n",
      "        [0.0134],\n",
      "        [0.0153],\n",
      "        [0.0203],\n",
      "        [0.0221],\n",
      "        [0.0209],\n",
      "        [0.0188],\n",
      "        [0.0199],\n",
      "        [0.0284],\n",
      "        [0.0250],\n",
      "        [0.0302],\n",
      "        [0.0274],\n",
      "        [0.0293],\n",
      "        [0.0314],\n",
      "        [0.0416],\n",
      "        [0.0464]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.251951456069946\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 46\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.1896918762395217e-08, 35)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [35, 75, 74, 33, 77, 76, 28, 93, 94, 90, 95, 89, 88, 87, 91, 97, 92, 48, 47, 73, 86, 72, 96, 37, 78, 62, 55, 69, 99, 98, 32, 57, 26, 63, 56, 101, 49, 61, 100, 45, 34, 27, 36, 58, 24, 67] 數值 torch.Size([46, 1])\n",
      "目前模型的Data狀態 torch.Size([46, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8398],\n",
      "        [0.7470],\n",
      "        [0.7614],\n",
      "        [0.8187],\n",
      "        [0.7201],\n",
      "        [0.7569],\n",
      "        [0.8474],\n",
      "        [0.7132],\n",
      "        [0.7047],\n",
      "        [0.7003],\n",
      "        [0.7007],\n",
      "        [0.7062],\n",
      "        [0.7283],\n",
      "        [0.7295],\n",
      "        [0.6851],\n",
      "        [0.6770],\n",
      "        [0.6983],\n",
      "        [0.8384],\n",
      "        [0.8216],\n",
      "        [0.7537],\n",
      "        [0.7074],\n",
      "        [0.7561],\n",
      "        [0.6848],\n",
      "        [0.8511],\n",
      "        [0.6762],\n",
      "        [0.8699],\n",
      "        [0.9232],\n",
      "        [0.7178],\n",
      "        [0.6739],\n",
      "        [0.6921],\n",
      "        [0.7942],\n",
      "        [0.9371],\n",
      "        [0.8127],\n",
      "        [0.8417],\n",
      "        [0.9262],\n",
      "        [0.6683],\n",
      "        [0.8745],\n",
      "        [0.8953],\n",
      "        [0.6627],\n",
      "        [0.7363],\n",
      "        [0.8437],\n",
      "        [0.8338],\n",
      "        [0.8739],\n",
      "        [0.8938],\n",
      "        [0.7464],\n",
      "        [0.7409]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0029],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0043],\n",
      "        [0.0055],\n",
      "        [0.0060],\n",
      "        [0.0060],\n",
      "        [0.0063],\n",
      "        [0.0066],\n",
      "        [0.0074],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0094],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0125],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0142],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0171],\n",
      "        [0.0188],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0209],\n",
      "        [0.0221],\n",
      "        [0.0250],\n",
      "        [0.0274],\n",
      "        [0.0284],\n",
      "        [0.0293],\n",
      "        [0.0302],\n",
      "        [0.0314],\n",
      "        [0.0416],\n",
      "        [0.0464],\n",
      "        [0.0527]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0020],\n",
      "        [0.0015],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0030],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0084],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0078],\n",
      "        [0.0100],\n",
      "        [0.0099],\n",
      "        [0.0115],\n",
      "        [0.0140],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0135],\n",
      "        [0.0153],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0158],\n",
      "        [0.0182],\n",
      "        [0.0174],\n",
      "        [0.0193],\n",
      "        [0.0208],\n",
      "        [0.0196],\n",
      "        [0.0225],\n",
      "        [0.0240],\n",
      "        [0.0261],\n",
      "        [0.0292],\n",
      "        [0.0279],\n",
      "        [0.0310],\n",
      "        [0.0296],\n",
      "        [0.0400],\n",
      "        [0.0444],\n",
      "        [0.0500]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.523718118667603\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 47\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.6357054256086485e-08, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 75, 28, 35, 76, 77, 33, 90, 93, 94, 95, 89, 87, 88, 48, 47, 91, 73, 97, 92, 72, 86, 96, 78, 62, 99, 55, 98, 37, 69, 32, 26, 57, 63, 101, 56, 49, 61, 100, 34, 45, 36, 27, 58, 24, 67, 25] 數值 torch.Size([47, 1])\n",
      "目前模型的Data狀態 torch.Size([47, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7604],\n",
      "        [0.7465],\n",
      "        [0.8477],\n",
      "        [0.8384],\n",
      "        [0.7566],\n",
      "        [0.7196],\n",
      "        [0.8178],\n",
      "        [0.7003],\n",
      "        [0.7128],\n",
      "        [0.7043],\n",
      "        [0.6999],\n",
      "        [0.7061],\n",
      "        [0.7293],\n",
      "        [0.7280],\n",
      "        [0.8397],\n",
      "        [0.8227],\n",
      "        [0.6845],\n",
      "        [0.7533],\n",
      "        [0.6757],\n",
      "        [0.6973],\n",
      "        [0.7560],\n",
      "        [0.7068],\n",
      "        [0.6835],\n",
      "        [0.6754],\n",
      "        [0.8706],\n",
      "        [0.6728],\n",
      "        [0.9231],\n",
      "        [0.6909],\n",
      "        [0.8495],\n",
      "        [0.7191],\n",
      "        [0.7936],\n",
      "        [0.8142],\n",
      "        [0.9382],\n",
      "        [0.8423],\n",
      "        [0.6671],\n",
      "        [0.9267],\n",
      "        [0.8749],\n",
      "        [0.8963],\n",
      "        [0.6614],\n",
      "        [0.8424],\n",
      "        [0.7371],\n",
      "        [0.8721],\n",
      "        [0.8346],\n",
      "        [0.8954],\n",
      "        [0.7485],\n",
      "        [0.7436],\n",
      "        [0.7553]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0084],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0153],\n",
      "        [0.0158],\n",
      "        [0.0174],\n",
      "        [0.0182],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0208],\n",
      "        [0.0225],\n",
      "        [0.0240],\n",
      "        [0.0261],\n",
      "        [0.0279],\n",
      "        [0.0292],\n",
      "        [0.0296],\n",
      "        [0.0310],\n",
      "        [0.0400],\n",
      "        [0.0444],\n",
      "        [0.0500],\n",
      "        [0.0513]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0002],\n",
      "        [0.0008],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0050],\n",
      "        [0.0049],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0066],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0104],\n",
      "        [0.0098],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0108],\n",
      "        [0.0129],\n",
      "        [0.0146],\n",
      "        [0.0135],\n",
      "        [0.0142],\n",
      "        [0.0180],\n",
      "        [0.0148],\n",
      "        [0.0138],\n",
      "        [0.0207],\n",
      "        [0.0177],\n",
      "        [0.0194],\n",
      "        [0.0225],\n",
      "        [0.0245],\n",
      "        [0.0218],\n",
      "        [0.0258],\n",
      "        [0.0281],\n",
      "        [0.0317],\n",
      "        [0.0291],\n",
      "        [0.0338],\n",
      "        [0.0370],\n",
      "        [0.0399],\n",
      "        [0.0455],\n",
      "        [0.0467]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.796764850616455\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 48\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.645862551275968e-08, 75)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [75, 74, 28, 35, 33, 90, 76, 77, 93, 94, 48, 47, 89, 95, 87, 88, 91, 73, 97, 92, 86, 72, 62, 78, 96, 99, 98, 26, 37, 55, 32, 63, 69, 101, 57, 61, 56, 49, 100, 34, 36, 45, 27, 58, 24, 67, 25, 46] 數值 torch.Size([48, 1])\n",
      "目前模型的Data狀態 torch.Size([48, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7465],\n",
      "        [0.7600],\n",
      "        [0.8500],\n",
      "        [0.8384],\n",
      "        [0.8183],\n",
      "        [0.7013],\n",
      "        [0.7565],\n",
      "        [0.7195],\n",
      "        [0.7134],\n",
      "        [0.7048],\n",
      "        [0.8427],\n",
      "        [0.8255],\n",
      "        [0.7069],\n",
      "        [0.7001],\n",
      "        [0.7299],\n",
      "        [0.7286],\n",
      "        [0.6848],\n",
      "        [0.7536],\n",
      "        [0.6753],\n",
      "        [0.6972],\n",
      "        [0.7070],\n",
      "        [0.7566],\n",
      "        [0.8725],\n",
      "        [0.6749],\n",
      "        [0.6831],\n",
      "        [0.6727],\n",
      "        [0.6906],\n",
      "        [0.8178],\n",
      "        [0.8493],\n",
      "        [0.9242],\n",
      "        [0.7947],\n",
      "        [0.8439],\n",
      "        [0.7218],\n",
      "        [0.6669],\n",
      "        [0.9407],\n",
      "        [0.8984],\n",
      "        [0.9285],\n",
      "        [0.8769],\n",
      "        [0.6612],\n",
      "        [0.8425],\n",
      "        [0.8716],\n",
      "        [0.7396],\n",
      "        [0.8375],\n",
      "        [0.8984],\n",
      "        [0.7529],\n",
      "        [0.7481],\n",
      "        [0.7598],\n",
      "        [0.7906]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0066],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0142],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0194],\n",
      "        [0.0207],\n",
      "        [0.0218],\n",
      "        [0.0225],\n",
      "        [0.0245],\n",
      "        [0.0258],\n",
      "        [0.0281],\n",
      "        [0.0291],\n",
      "        [0.0317],\n",
      "        [0.0338],\n",
      "        [0.0370],\n",
      "        [0.0399],\n",
      "        [0.0455],\n",
      "        [0.0467],\n",
      "        [0.0527]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0012],\n",
      "        [    0.0030],\n",
      "        [    0.0012],\n",
      "        [    0.0008],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0029],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0002],\n",
      "        [    0.0007],\n",
      "        [    0.0051],\n",
      "        [    0.0053],\n",
      "        [    0.0060],\n",
      "        [    0.0060],\n",
      "        [    0.0072],\n",
      "        [    0.0084],\n",
      "        [    0.0093],\n",
      "        [    0.0093],\n",
      "        [    0.0108],\n",
      "        [    0.0110],\n",
      "        [    0.0085],\n",
      "        [    0.0112],\n",
      "        [    0.0130],\n",
      "        [    0.0122],\n",
      "        [    0.0125],\n",
      "        [    0.0102],\n",
      "        [    0.0140],\n",
      "        [    0.0162],\n",
      "        [    0.0134],\n",
      "        [    0.0156],\n",
      "        [    0.0206],\n",
      "        [    0.0186],\n",
      "        [    0.0236],\n",
      "        [    0.0193],\n",
      "        [    0.0249],\n",
      "        [    0.0272],\n",
      "        [    0.0250],\n",
      "        [    0.0284],\n",
      "        [    0.0290],\n",
      "        [    0.0351],\n",
      "        [    0.0367],\n",
      "        [    0.0337],\n",
      "        [    0.0356],\n",
      "        [    0.0412],\n",
      "        [    0.0423],\n",
      "        [    0.0488]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.069409370422363\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 49\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.438703875095598e-09, 75)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [75, 48, 47, 33, 74, 35, 90, 76, 93, 77, 94, 28, 89, 95, 87, 88, 91, 73, 62, 97, 92, 26, 86, 72, 78, 99, 98, 96, 32, 37, 63, 55, 101, 61, 69, 57, 56, 100, 49, 34, 36, 58, 45, 24, 27, 67, 25, 46, 53] 數值 torch.Size([49, 1])\n",
      "目前模型的Data狀態 torch.Size([49, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7462],\n",
      "        [0.8464],\n",
      "        [0.8289],\n",
      "        [0.8192],\n",
      "        [0.7594],\n",
      "        [0.8388],\n",
      "        [0.7014],\n",
      "        [0.7560],\n",
      "        [0.7133],\n",
      "        [0.7189],\n",
      "        [0.7048],\n",
      "        [0.8522],\n",
      "        [0.7066],\n",
      "        [0.6997],\n",
      "        [0.7295],\n",
      "        [0.7282],\n",
      "        [0.6843],\n",
      "        [0.7539],\n",
      "        [0.8748],\n",
      "        [0.6743],\n",
      "        [0.6963],\n",
      "        [0.8214],\n",
      "        [0.7060],\n",
      "        [0.7572],\n",
      "        [0.6742],\n",
      "        [0.6719],\n",
      "        [0.6897],\n",
      "        [0.6821],\n",
      "        [0.7960],\n",
      "        [0.8496],\n",
      "        [0.8461],\n",
      "        [0.9258],\n",
      "        [0.6661],\n",
      "        [0.9010],\n",
      "        [0.7244],\n",
      "        [0.9435],\n",
      "        [0.9308],\n",
      "        [0.6603],\n",
      "        [0.8796],\n",
      "        [0.8429],\n",
      "        [0.8715],\n",
      "        [0.9018],\n",
      "        [0.7430],\n",
      "        [0.7573],\n",
      "        [0.8403],\n",
      "        [0.7525],\n",
      "        [0.7642],\n",
      "        [0.7945],\n",
      "        [0.9222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0012],\n",
      "        [    0.0012],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0030],\n",
      "        [    0.0030],\n",
      "        [    0.0051],\n",
      "        [    0.0053],\n",
      "        [    0.0060],\n",
      "        [    0.0060],\n",
      "        [    0.0072],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0093],\n",
      "        [    0.0093],\n",
      "        [    0.0102],\n",
      "        [    0.0108],\n",
      "        [    0.0110],\n",
      "        [    0.0112],\n",
      "        [    0.0122],\n",
      "        [    0.0125],\n",
      "        [    0.0130],\n",
      "        [    0.0134],\n",
      "        [    0.0140],\n",
      "        [    0.0156],\n",
      "        [    0.0162],\n",
      "        [    0.0186],\n",
      "        [    0.0193],\n",
      "        [    0.0206],\n",
      "        [    0.0236],\n",
      "        [    0.0249],\n",
      "        [    0.0250],\n",
      "        [    0.0272],\n",
      "        [    0.0284],\n",
      "        [    0.0290],\n",
      "        [    0.0337],\n",
      "        [    0.0351],\n",
      "        [    0.0356],\n",
      "        [    0.0367],\n",
      "        [    0.0412],\n",
      "        [    0.0423],\n",
      "        [    0.0488],\n",
      "        [    0.0548]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0001],\n",
      "        [    0.0009],\n",
      "        [    0.0021],\n",
      "        [    0.0012],\n",
      "        [    0.0029],\n",
      "        [    0.0005],\n",
      "        [    0.0013],\n",
      "        [    0.0008],\n",
      "        [    0.0017],\n",
      "        [    0.0013],\n",
      "        [    0.0032],\n",
      "        [    0.0027],\n",
      "        [    0.0041],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0053],\n",
      "        [    0.0088],\n",
      "        [    0.0085],\n",
      "        [    0.0083],\n",
      "        [    0.0077],\n",
      "        [    0.0090],\n",
      "        [    0.0087],\n",
      "        [    0.0115],\n",
      "        [    0.0120],\n",
      "        [    0.0134],\n",
      "        [    0.0135],\n",
      "        [    0.0121],\n",
      "        [    0.0143],\n",
      "        [    0.0157],\n",
      "        [    0.0157],\n",
      "        [    0.0145],\n",
      "        [    0.0197],\n",
      "        [    0.0191],\n",
      "        [    0.0224],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0261],\n",
      "        [    0.0262],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0333],\n",
      "        [    0.0350],\n",
      "        [    0.0337],\n",
      "        [    0.0372],\n",
      "        [    0.0381],\n",
      "        [    0.0402],\n",
      "        [    0.0486],\n",
      "        [    0.0527]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.343266725540161\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 50\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.489116716717035e-09, 48)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [48, 75, 90, 93, 47, 74, 94, 76, 77, 33, 89, 35, 28, 87, 88, 95, 91, 92, 97, 62, 86, 73, 26, 72, 78, 96, 99, 98, 32, 55, 37, 63, 61, 101, 69, 57, 56, 100, 49, 34, 36, 58, 24, 45, 27, 67, 25, 46, 53, 102] 數值 torch.Size([50, 1])\n",
      "目前模型的Data狀態 torch.Size([50, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8463],\n",
      "        [0.7467],\n",
      "        [0.7039],\n",
      "        [0.7152],\n",
      "        [0.8288],\n",
      "        [0.7594],\n",
      "        [0.7065],\n",
      "        [0.7573],\n",
      "        [0.7200],\n",
      "        [0.8179],\n",
      "        [0.7091],\n",
      "        [0.8372],\n",
      "        [0.8524],\n",
      "        [0.7319],\n",
      "        [0.7305],\n",
      "        [0.7009],\n",
      "        [0.6862],\n",
      "        [0.6980],\n",
      "        [0.6753],\n",
      "        [0.8748],\n",
      "        [0.7081],\n",
      "        [0.7543],\n",
      "        [0.8225],\n",
      "        [0.7577],\n",
      "        [0.6750],\n",
      "        [0.6829],\n",
      "        [0.6731],\n",
      "        [0.6907],\n",
      "        [0.7951],\n",
      "        [0.9241],\n",
      "        [0.8478],\n",
      "        [0.8459],\n",
      "        [0.9012],\n",
      "        [0.6672],\n",
      "        [0.7263],\n",
      "        [0.9432],\n",
      "        [0.9297],\n",
      "        [0.6614],\n",
      "        [0.8786],\n",
      "        [0.8413],\n",
      "        [0.8694],\n",
      "        [0.9021],\n",
      "        [0.7592],\n",
      "        [0.7429],\n",
      "        [0.8409],\n",
      "        [0.7555],\n",
      "        [0.7663],\n",
      "        [0.7947],\n",
      "        [0.9201],\n",
      "        [0.7118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0013],\n",
      "        [    0.0017],\n",
      "        [    0.0021],\n",
      "        [    0.0027],\n",
      "        [    0.0029],\n",
      "        [    0.0032],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0041],\n",
      "        [    0.0053],\n",
      "        [    0.0077],\n",
      "        [    0.0083],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0088],\n",
      "        [    0.0090],\n",
      "        [    0.0115],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0134],\n",
      "        [    0.0135],\n",
      "        [    0.0143],\n",
      "        [    0.0145],\n",
      "        [    0.0157],\n",
      "        [    0.0157],\n",
      "        [    0.0191],\n",
      "        [    0.0197],\n",
      "        [    0.0224],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0261],\n",
      "        [    0.0262],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0333],\n",
      "        [    0.0337],\n",
      "        [    0.0350],\n",
      "        [    0.0372],\n",
      "        [    0.0381],\n",
      "        [    0.0402],\n",
      "        [    0.0486],\n",
      "        [    0.0527],\n",
      "        [    0.0707]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0031],\n",
      "        [0.0027],\n",
      "        [0.0047],\n",
      "        [0.0017],\n",
      "        [0.0048],\n",
      "        [0.0053],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0051],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0010],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0083],\n",
      "        [0.0093],\n",
      "        [0.0122],\n",
      "        [0.0133],\n",
      "        [0.0107],\n",
      "        [0.0126],\n",
      "        [0.0062],\n",
      "        [0.0104],\n",
      "        [0.0091],\n",
      "        [0.0082],\n",
      "        [0.0170],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0167],\n",
      "        [0.0117],\n",
      "        [0.0189],\n",
      "        [0.0181],\n",
      "        [0.0210],\n",
      "        [0.0144],\n",
      "        [0.0210],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0206],\n",
      "        [0.0246],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0347],\n",
      "        [0.0341],\n",
      "        [0.0341],\n",
      "        [0.0353],\n",
      "        [0.0384],\n",
      "        [0.0408],\n",
      "        [0.0492],\n",
      "        [0.0494],\n",
      "        [0.0653]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.616455316543579\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 51\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.0417999735400372e-07, 48)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [48, 28, 47, 90, 75, 93, 74, 76, 33, 94, 77, 89, 35, 73, 87, 88, 78, 99, 95, 98, 72, 91, 26, 62, 55, 92, 86, 97, 101, 32, 96, 63, 37, 100, 69, 61, 57, 56, 36, 34, 49, 45, 24, 58, 27, 67, 25, 46, 53, 102, 85] 數值 torch.Size([51, 1])\n",
      "目前模型的Data狀態 torch.Size([51, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8458],\n",
      "        [0.8502],\n",
      "        [0.8279],\n",
      "        [0.7006],\n",
      "        [0.7432],\n",
      "        [0.7114],\n",
      "        [0.7558],\n",
      "        [0.7536],\n",
      "        [0.8149],\n",
      "        [0.7025],\n",
      "        [0.7165],\n",
      "        [0.7057],\n",
      "        [0.8340],\n",
      "        [0.7517],\n",
      "        [0.7284],\n",
      "        [0.7270],\n",
      "        [0.6712],\n",
      "        [0.6680],\n",
      "        [0.6967],\n",
      "        [0.6855],\n",
      "        [0.7552],\n",
      "        [0.6821],\n",
      "        [0.8211],\n",
      "        [0.8727],\n",
      "        [0.9213],\n",
      "        [0.6935],\n",
      "        [0.7042],\n",
      "        [0.6702],\n",
      "        [0.6619],\n",
      "        [0.7927],\n",
      "        [0.6780],\n",
      "        [0.8436],\n",
      "        [0.8446],\n",
      "        [0.6560],\n",
      "        [0.7248],\n",
      "        [0.8993],\n",
      "        [0.9413],\n",
      "        [0.9275],\n",
      "        [0.8660],\n",
      "        [0.8379],\n",
      "        [0.8770],\n",
      "        [0.7420],\n",
      "        [0.7587],\n",
      "        [0.9007],\n",
      "        [0.8389],\n",
      "        [0.7552],\n",
      "        [0.7658],\n",
      "        [0.7941],\n",
      "        [0.9168],\n",
      "        [0.7065],\n",
      "        [0.7300]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0010],\n",
      "        [0.0017],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0047],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0082],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0104],\n",
      "        [0.0107],\n",
      "        [0.0117],\n",
      "        [0.0122],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0144],\n",
      "        [0.0167],\n",
      "        [0.0170],\n",
      "        [0.0181],\n",
      "        [0.0189],\n",
      "        [0.0206],\n",
      "        [0.0210],\n",
      "        [0.0210],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0246],\n",
      "        [0.0341],\n",
      "        [0.0341],\n",
      "        [0.0347],\n",
      "        [0.0353],\n",
      "        [0.0384],\n",
      "        [0.0408],\n",
      "        [0.0492],\n",
      "        [0.0494],\n",
      "        [0.0653],\n",
      "        [0.0689]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0029],\n",
      "        [0.0024],\n",
      "        [0.0016],\n",
      "        [0.0045],\n",
      "        [0.0042],\n",
      "        [0.0060],\n",
      "        [0.0055],\n",
      "        [0.0064],\n",
      "        [0.0047],\n",
      "        [0.0068],\n",
      "        [0.0065],\n",
      "        [0.0079],\n",
      "        [0.0056],\n",
      "        [0.0064],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0071],\n",
      "        [0.0057],\n",
      "        [0.0102],\n",
      "        [0.0058],\n",
      "        [0.0097],\n",
      "        [0.0114],\n",
      "        [0.0081],\n",
      "        [0.0089],\n",
      "        [0.0125],\n",
      "        [0.0143],\n",
      "        [0.0150],\n",
      "        [0.0159],\n",
      "        [0.0119],\n",
      "        [0.0157],\n",
      "        [0.0194],\n",
      "        [0.0164],\n",
      "        [0.0185],\n",
      "        [0.0179],\n",
      "        [0.0228],\n",
      "        [0.0191],\n",
      "        [0.0232],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0237],\n",
      "        [0.0265],\n",
      "        [0.0375],\n",
      "        [0.0310],\n",
      "        [0.0324],\n",
      "        [0.0369],\n",
      "        [0.0352],\n",
      "        [0.0376],\n",
      "        [0.0456],\n",
      "        [0.0495],\n",
      "        [0.0625],\n",
      "        [0.0659]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.886102437973022\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 52\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.4225555534940213e-06, 47)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [47, 28, 48, 75, 90, 33, 74, 35, 99, 98, 93, 73, 76, 77, 94, 78, 89, 26, 62, 87, 88, 72, 95, 91, 101, 55, 92, 86, 32, 97, 63, 100, 37, 61, 96, 69, 56, 57, 34, 36, 49, 24, 58, 67, 27, 45, 25, 46, 53, 102, 85, 103] 數值 torch.Size([52, 1])\n",
      "目前模型的Data狀態 torch.Size([52, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8312],\n",
      "        [0.8516],\n",
      "        [0.8492],\n",
      "        [0.7420],\n",
      "        [0.6989],\n",
      "        [0.8153],\n",
      "        [0.7551],\n",
      "        [0.8344],\n",
      "        [0.6655],\n",
      "        [0.6830],\n",
      "        [0.7100],\n",
      "        [0.7519],\n",
      "        [0.7522],\n",
      "        [0.7153],\n",
      "        [0.7009],\n",
      "        [0.6701],\n",
      "        [0.7039],\n",
      "        [0.8235],\n",
      "        [0.8744],\n",
      "        [0.7263],\n",
      "        [0.7249],\n",
      "        [0.7558],\n",
      "        [0.6948],\n",
      "        [0.6800],\n",
      "        [0.6593],\n",
      "        [0.9220],\n",
      "        [0.6914],\n",
      "        [0.7018],\n",
      "        [0.7937],\n",
      "        [0.6677],\n",
      "        [0.8452],\n",
      "        [0.6532],\n",
      "        [0.8450],\n",
      "        [0.9012],\n",
      "        [0.6756],\n",
      "        [0.7266],\n",
      "        [0.9289],\n",
      "        [0.9432],\n",
      "        [0.8382],\n",
      "        [0.8664],\n",
      "        [0.8789],\n",
      "        [0.7619],\n",
      "        [0.9030],\n",
      "        [0.7584],\n",
      "        [0.8406],\n",
      "        [0.7454],\n",
      "        [0.7689],\n",
      "        [0.7977],\n",
      "        [0.9170],\n",
      "        [0.7037],\n",
      "        [0.7270],\n",
      "        [0.7279]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0047],\n",
      "        [0.0055],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0064],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0102],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0143],\n",
      "        [0.0150],\n",
      "        [0.0157],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0179],\n",
      "        [0.0185],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0238],\n",
      "        [0.0265],\n",
      "        [0.0310],\n",
      "        [0.0324],\n",
      "        [0.0352],\n",
      "        [0.0369],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0456],\n",
      "        [0.0495],\n",
      "        [0.0625],\n",
      "        [0.0659],\n",
      "        [0.0688]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0025],\n",
      "        [0.0016],\n",
      "        [0.0040],\n",
      "        [0.0061],\n",
      "        [0.0060],\n",
      "        [0.0058],\n",
      "        [0.0069],\n",
      "        [0.0067],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0081],\n",
      "        [0.0062],\n",
      "        [0.0083],\n",
      "        [0.0079],\n",
      "        [0.0093],\n",
      "        [0.0057],\n",
      "        [0.0094],\n",
      "        [0.0084],\n",
      "        [0.0094],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0097],\n",
      "        [0.0127],\n",
      "        [0.0136],\n",
      "        [0.0079],\n",
      "        [0.0112],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0164],\n",
      "        [0.0191],\n",
      "        [0.0170],\n",
      "        [0.0138],\n",
      "        [0.0194],\n",
      "        [0.0193],\n",
      "        [0.0224],\n",
      "        [0.0232],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0226],\n",
      "        [0.0229],\n",
      "        [0.0263],\n",
      "        [0.0300],\n",
      "        [0.0328],\n",
      "        [0.0342],\n",
      "        [0.0361],\n",
      "        [0.0387],\n",
      "        [0.0370],\n",
      "        [0.0446],\n",
      "        [0.0478],\n",
      "        [0.0582],\n",
      "        [0.0629],\n",
      "        [0.0640]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.15668797492981\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 53\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.696714318517479e-06, 28)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [28, 99, 98, 47, 48, 78, 33, 90, 75, 73, 35, 74, 101, 77, 93, 76, 26, 94, 89, 62, 72, 87, 88, 55, 95, 91, 100, 32, 92, 86, 63, 97, 61, 37, 56, 96, 57, 34, 36, 69, 49, 24, 58, 67, 27, 25, 45, 46, 53, 102, 85, 103, 30] 數值 torch.Size([53, 1])\n",
      "目前模型的Data狀態 torch.Size([53, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8508],\n",
      "        [0.6618],\n",
      "        [0.6795],\n",
      "        [0.8321],\n",
      "        [0.8503],\n",
      "        [0.6688],\n",
      "        [0.8142],\n",
      "        [0.6973],\n",
      "        [0.7402],\n",
      "        [0.7518],\n",
      "        [0.8333],\n",
      "        [0.7537],\n",
      "        [0.6553],\n",
      "        [0.7138],\n",
      "        [0.7080],\n",
      "        [0.7503],\n",
      "        [0.8231],\n",
      "        [0.6985],\n",
      "        [0.7024],\n",
      "        [0.8739],\n",
      "        [0.7559],\n",
      "        [0.7245],\n",
      "        [0.7233],\n",
      "        [0.9208],\n",
      "        [0.6923],\n",
      "        [0.6778],\n",
      "        [0.6491],\n",
      "        [0.7931],\n",
      "        [0.6889],\n",
      "        [0.6998],\n",
      "        [0.8446],\n",
      "        [0.6644],\n",
      "        [0.9010],\n",
      "        [0.8442],\n",
      "        [0.9281],\n",
      "        [0.6726],\n",
      "        [0.9424],\n",
      "        [0.8371],\n",
      "        [0.8655],\n",
      "        [0.7270],\n",
      "        [0.8787],\n",
      "        [0.7629],\n",
      "        [0.9027],\n",
      "        [0.7594],\n",
      "        [0.8397],\n",
      "        [0.7696],\n",
      "        [0.7466],\n",
      "        [0.7987],\n",
      "        [0.9152],\n",
      "        [0.6994],\n",
      "        [0.7240],\n",
      "        [0.7231],\n",
      "        [0.9201]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0040],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0097],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0112],\n",
      "        [0.0127],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0164],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0224],\n",
      "        [0.0226],\n",
      "        [0.0229],\n",
      "        [0.0232],\n",
      "        [0.0263],\n",
      "        [0.0300],\n",
      "        [0.0328],\n",
      "        [0.0342],\n",
      "        [0.0361],\n",
      "        [0.0370],\n",
      "        [0.0387],\n",
      "        [0.0446],\n",
      "        [0.0478],\n",
      "        [0.0582],\n",
      "        [0.0629],\n",
      "        [0.0640],\n",
      "        [0.0705]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0015],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0010],\n",
      "        [0.0026],\n",
      "        [0.0067],\n",
      "        [0.0089],\n",
      "        [0.0054],\n",
      "        [0.0063],\n",
      "        [0.0070],\n",
      "        [0.0098],\n",
      "        [0.0068],\n",
      "        [0.0061],\n",
      "        [0.0073],\n",
      "        [0.0081],\n",
      "        [0.0080],\n",
      "        [0.0107],\n",
      "        [0.0097],\n",
      "        [0.0085],\n",
      "        [0.0109],\n",
      "        [0.0106],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0080],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0121],\n",
      "        [0.0190],\n",
      "        [0.0168],\n",
      "        [0.0161],\n",
      "        [0.0184],\n",
      "        [0.0201],\n",
      "        [0.0208],\n",
      "        [0.0223],\n",
      "        [0.0194],\n",
      "        [0.0232],\n",
      "        [0.0196],\n",
      "        [0.0195],\n",
      "        [0.0199],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0311],\n",
      "        [0.0350],\n",
      "        [0.0338],\n",
      "        [0.0332],\n",
      "        [0.0382],\n",
      "        [0.0376],\n",
      "        [0.0459],\n",
      "        [0.0442],\n",
      "        [0.0561],\n",
      "        [0.0625],\n",
      "        [0.0614],\n",
      "        [0.0668]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.42621922492981\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 54\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.561439249708201e-07, 99)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [99, 47, 98, 28, 48, 90, 101, 75, 78, 74, 73, 77, 76, 55, 93, 89, 33, 94, 35, 87, 88, 72, 26, 62, 100, 95, 91, 86, 92, 63, 32, 56, 34, 57, 36, 97, 61, 37, 96, 69, 49, 24, 27, 67, 58, 45, 25, 53, 46, 102, 103, 85, 30, 29] 數值 torch.Size([54, 1])\n",
      "目前模型的Data狀態 torch.Size([54, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6605],\n",
      "        [0.8306],\n",
      "        [0.6784],\n",
      "        [0.8477],\n",
      "        [0.8489],\n",
      "        [0.6980],\n",
      "        [0.6536],\n",
      "        [0.7400],\n",
      "        [0.6697],\n",
      "        [0.7538],\n",
      "        [0.7525],\n",
      "        [0.7145],\n",
      "        [0.7506],\n",
      "        [0.9176],\n",
      "        [0.7080],\n",
      "        [0.7033],\n",
      "        [0.8111],\n",
      "        [0.6981],\n",
      "        [0.8303],\n",
      "        [0.7253],\n",
      "        [0.7239],\n",
      "        [0.7568],\n",
      "        [0.8208],\n",
      "        [0.8724],\n",
      "        [0.6475],\n",
      "        [0.6918],\n",
      "        [0.6781],\n",
      "        [0.7007],\n",
      "        [0.6889],\n",
      "        [0.8433],\n",
      "        [0.7904],\n",
      "        [0.9253],\n",
      "        [0.8340],\n",
      "        [0.9396],\n",
      "        [0.8625],\n",
      "        [0.6635],\n",
      "        [0.8995],\n",
      "        [0.8413],\n",
      "        [0.6718],\n",
      "        [0.7274],\n",
      "        [0.8762],\n",
      "        [0.7618],\n",
      "        [0.8368],\n",
      "        [0.7599],\n",
      "        [0.9004],\n",
      "        [0.7455],\n",
      "        [0.7683],\n",
      "        [0.9116],\n",
      "        [0.7974],\n",
      "        [0.6973],\n",
      "        [0.7205],\n",
      "        [0.7236],\n",
      "        [0.9164],\n",
      "        [0.9006]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0026],\n",
      "        [0.0054],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0067],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0121],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0161],\n",
      "        [0.0168],\n",
      "        [0.0184],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0196],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0208],\n",
      "        [0.0223],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0311],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0350],\n",
      "        [0.0376],\n",
      "        [0.0382],\n",
      "        [0.0442],\n",
      "        [0.0459],\n",
      "        [0.0561],\n",
      "        [0.0614],\n",
      "        [0.0625],\n",
      "        [0.0668],\n",
      "        [0.0711]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0020],\n",
      "        [0.0011],\n",
      "        [0.0069],\n",
      "        [0.0003],\n",
      "        [0.0062],\n",
      "        [0.0031],\n",
      "        [0.0075],\n",
      "        [0.0068],\n",
      "        [0.0079],\n",
      "        [0.0066],\n",
      "        [0.0077],\n",
      "        [0.0089],\n",
      "        [0.0036],\n",
      "        [0.0094],\n",
      "        [0.0092],\n",
      "        [0.0136],\n",
      "        [0.0114],\n",
      "        [0.0144],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0104],\n",
      "        [0.0150],\n",
      "        [0.0136],\n",
      "        [0.0093],\n",
      "        [0.0150],\n",
      "        [0.0144],\n",
      "        [0.0167],\n",
      "        [0.0181],\n",
      "        [0.0208],\n",
      "        [0.0235],\n",
      "        [0.0153],\n",
      "        [0.0146],\n",
      "        [0.0152],\n",
      "        [0.0151],\n",
      "        [0.0223],\n",
      "        [0.0237],\n",
      "        [0.0269],\n",
      "        [0.0252],\n",
      "        [0.0224],\n",
      "        [0.0198],\n",
      "        [0.0343],\n",
      "        [0.0282],\n",
      "        [0.0350],\n",
      "        [0.0388],\n",
      "        [0.0350],\n",
      "        [0.0416],\n",
      "        [0.0393],\n",
      "        [0.0487],\n",
      "        [0.0527],\n",
      "        [0.0576],\n",
      "        [0.0608],\n",
      "        [0.0607],\n",
      "        [0.0652]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.69706392288208\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 55\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.761356357605109e-08, 48)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [48, 98, 99, 47, 101, 55, 90, 73, 78, 28, 75, 77, 74, 76, 89, 100, 93, 72, 87, 88, 94, 33, 62, 35, 91, 34, 26, 95, 36, 57, 56, 86, 92, 49, 63, 97, 69, 32, 61, 96, 37, 27, 24, 45, 67, 58, 53, 25, 46, 102, 103, 30, 85, 29, 52] 數值 torch.Size([55, 1])\n",
      "目前模型的Data狀態 torch.Size([55, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8460],\n",
      "        [0.6760],\n",
      "        [0.6581],\n",
      "        [0.8276],\n",
      "        [0.6506],\n",
      "        [0.9132],\n",
      "        [0.6971],\n",
      "        [0.7522],\n",
      "        [0.6698],\n",
      "        [0.8423],\n",
      "        [0.7388],\n",
      "        [0.7141],\n",
      "        [0.7527],\n",
      "        [0.7497],\n",
      "        [0.7025],\n",
      "        [0.6447],\n",
      "        [0.7067],\n",
      "        [0.7565],\n",
      "        [0.7244],\n",
      "        [0.7230],\n",
      "        [0.6964],\n",
      "        [0.8064],\n",
      "        [0.8697],\n",
      "        [0.8256],\n",
      "        [0.6770],\n",
      "        [0.8291],\n",
      "        [0.8166],\n",
      "        [0.6900],\n",
      "        [0.8576],\n",
      "        [0.9352],\n",
      "        [0.9212],\n",
      "        [0.7001],\n",
      "        [0.6875],\n",
      "        [0.8722],\n",
      "        [0.8409],\n",
      "        [0.6613],\n",
      "        [0.7262],\n",
      "        [0.7859],\n",
      "        [0.8966],\n",
      "        [0.6698],\n",
      "        [0.8366],\n",
      "        [0.8318],\n",
      "        [0.7586],\n",
      "        [0.7429],\n",
      "        [0.7586],\n",
      "        [0.8967],\n",
      "        [0.9067],\n",
      "        [0.7649],\n",
      "        [0.7946],\n",
      "        [0.6939],\n",
      "        [0.7168],\n",
      "        [0.9103],\n",
      "        [0.7218],\n",
      "        [0.8948],\n",
      "        [0.9507]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0011],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0031],\n",
      "        [0.0036],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0104],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0136],\n",
      "        [0.0136],\n",
      "        [0.0144],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0167],\n",
      "        [0.0181],\n",
      "        [0.0198],\n",
      "        [0.0208],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0252],\n",
      "        [0.0269],\n",
      "        [0.0282],\n",
      "        [0.0343],\n",
      "        [0.0350],\n",
      "        [0.0350],\n",
      "        [0.0388],\n",
      "        [0.0393],\n",
      "        [0.0416],\n",
      "        [0.0487],\n",
      "        [0.0527],\n",
      "        [0.0576],\n",
      "        [0.0607],\n",
      "        [0.0608],\n",
      "        [0.0652],\n",
      "        [0.0673]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0002],\n",
      "        [0.0008],\n",
      "        [0.0019],\n",
      "        [0.0037],\n",
      "        [0.0015],\n",
      "        [0.0043],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0086],\n",
      "        [0.0057],\n",
      "        [0.0053],\n",
      "        [0.0061],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0100],\n",
      "        [0.0075],\n",
      "        [0.0129],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0099],\n",
      "        [0.0153],\n",
      "        [0.0133],\n",
      "        [0.0159],\n",
      "        [0.0128],\n",
      "        [0.0127],\n",
      "        [0.0155],\n",
      "        [0.0138],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0136],\n",
      "        [0.0150],\n",
      "        [0.0164],\n",
      "        [0.0186],\n",
      "        [0.0203],\n",
      "        [0.0212],\n",
      "        [0.0246],\n",
      "        [0.0249],\n",
      "        [0.0235],\n",
      "        [0.0241],\n",
      "        [0.0284],\n",
      "        [0.0270],\n",
      "        [0.0340],\n",
      "        [0.0357],\n",
      "        [0.0325],\n",
      "        [0.0395],\n",
      "        [0.0366],\n",
      "        [0.0414],\n",
      "        [0.0481],\n",
      "        [0.0528],\n",
      "        [0.0572],\n",
      "        [0.0580],\n",
      "        [0.0616],\n",
      "        [0.0629],\n",
      "        [0.0642]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.967484712600708\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 56\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.19260163148283e-08, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 48, 99, 55, 47, 101, 90, 77, 75, 74, 76, 89, 93, 28, 73, 87, 78, 88, 94, 100, 34, 91, 72, 62, 36, 56, 95, 57, 86, 33, 26, 35, 92, 49, 63, 97, 61, 96, 69, 32, 27, 37, 67, 24, 45, 53, 58, 25, 46, 102, 103, 30, 85, 29, 52, 51] 數值 torch.Size([56, 1])\n",
      "目前模型的Data狀態 torch.Size([56, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6769],\n",
      "        [0.8460],\n",
      "        [0.6589],\n",
      "        [0.9111],\n",
      "        [0.8278],\n",
      "        [0.6511],\n",
      "        [0.6990],\n",
      "        [0.7165],\n",
      "        [0.7406],\n",
      "        [0.7545],\n",
      "        [0.7517],\n",
      "        [0.7045],\n",
      "        [0.7085],\n",
      "        [0.8406],\n",
      "        [0.7546],\n",
      "        [0.7263],\n",
      "        [0.6724],\n",
      "        [0.7247],\n",
      "        [0.6979],\n",
      "        [0.6454],\n",
      "        [0.8272],\n",
      "        [0.6786],\n",
      "        [0.7590],\n",
      "        [0.8700],\n",
      "        [0.8559],\n",
      "        [0.9195],\n",
      "        [0.6913],\n",
      "        [0.9337],\n",
      "        [0.7018],\n",
      "        [0.8047],\n",
      "        [0.8160],\n",
      "        [0.8242],\n",
      "        [0.6892],\n",
      "        [0.8710],\n",
      "        [0.8414],\n",
      "        [0.6623],\n",
      "        [0.8968],\n",
      "        [0.6709],\n",
      "        [0.7284],\n",
      "        [0.7845],\n",
      "        [0.8306],\n",
      "        [0.8351],\n",
      "        [0.7611],\n",
      "        [0.7589],\n",
      "        [0.7436],\n",
      "        [0.9040],\n",
      "        [0.8960],\n",
      "        [0.7652],\n",
      "        [0.7951],\n",
      "        [0.6940],\n",
      "        [0.7163],\n",
      "        [0.9076],\n",
      "        [0.7227],\n",
      "        [0.8924],\n",
      "        [0.9477],\n",
      "        [0.9693]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0008],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0037],\n",
      "        [0.0043],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0061],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0075],\n",
      "        [0.0086],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0127],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0155],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0186],\n",
      "        [0.0203],\n",
      "        [0.0212],\n",
      "        [0.0235],\n",
      "        [0.0241],\n",
      "        [0.0246],\n",
      "        [0.0249],\n",
      "        [0.0270],\n",
      "        [0.0284],\n",
      "        [0.0325],\n",
      "        [0.0340],\n",
      "        [0.0357],\n",
      "        [0.0366],\n",
      "        [0.0395],\n",
      "        [0.0414],\n",
      "        [0.0481],\n",
      "        [0.0528],\n",
      "        [0.0572],\n",
      "        [0.0580],\n",
      "        [0.0616],\n",
      "        [0.0629],\n",
      "        [0.0642],\n",
      "        [0.0714]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0019],\n",
      "        [    0.0006],\n",
      "        [    0.0025],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0038],\n",
      "        [    0.0038],\n",
      "        [    0.0053],\n",
      "        [    0.0053],\n",
      "        [    0.0059],\n",
      "        [    0.0065],\n",
      "        [    0.0065],\n",
      "        [    0.0109],\n",
      "        [    0.0101],\n",
      "        [    0.0087],\n",
      "        [    0.0111],\n",
      "        [    0.0090],\n",
      "        [    0.0093],\n",
      "        [    0.0102],\n",
      "        [    0.0099],\n",
      "        [    0.0122],\n",
      "        [    0.0141],\n",
      "        [    0.0144],\n",
      "        [    0.0109],\n",
      "        [    0.0102],\n",
      "        [    0.0136],\n",
      "        [    0.0108],\n",
      "        [    0.0143],\n",
      "        [    0.0181],\n",
      "        [    0.0170],\n",
      "        [    0.0182],\n",
      "        [    0.0155],\n",
      "        [    0.0154],\n",
      "        [    0.0213],\n",
      "        [    0.0209],\n",
      "        [    0.0247],\n",
      "        [    0.0237],\n",
      "        [    0.0256],\n",
      "        [    0.0272],\n",
      "        [    0.0248],\n",
      "        [    0.0308],\n",
      "        [    0.0312],\n",
      "        [    0.0347],\n",
      "        [    0.0352],\n",
      "        [    0.0321],\n",
      "        [    0.0416],\n",
      "        [    0.0422],\n",
      "        [    0.0490],\n",
      "        [    0.0524],\n",
      "        [    0.0560],\n",
      "        [    0.0544],\n",
      "        [    0.0607],\n",
      "        [    0.0598],\n",
      "        [    0.0591],\n",
      "        [    0.0652]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.239872217178345\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 57\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.7070828890791745e-09, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 99, 48, 55, 47, 90, 101, 77, 75, 74, 76, 93, 89, 87, 88, 94, 34, 73, 56, 100, 57, 36, 28, 78, 91, 95, 72, 86, 62, 49, 92, 26, 33, 35, 97, 63, 96, 61, 27, 69, 32, 37, 67, 53, 24, 45, 58, 25, 46, 102, 30, 103, 52, 29, 85, 51, 54] 數值 torch.Size([57, 1])\n",
      "目前模型的Data狀態 torch.Size([57, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6771],\n",
      "        [0.6591],\n",
      "        [0.8443],\n",
      "        [0.9071],\n",
      "        [0.8264],\n",
      "        [0.6996],\n",
      "        [0.6513],\n",
      "        [0.7179],\n",
      "        [0.7410],\n",
      "        [0.7553],\n",
      "        [0.7527],\n",
      "        [0.7096],\n",
      "        [0.7053],\n",
      "        [0.7268],\n",
      "        [0.7253],\n",
      "        [0.6984],\n",
      "        [0.8243],\n",
      "        [0.7556],\n",
      "        [0.9161],\n",
      "        [0.6455],\n",
      "        [0.9308],\n",
      "        [0.8534],\n",
      "        [0.8383],\n",
      "        [0.6741],\n",
      "        [0.6792],\n",
      "        [0.6915],\n",
      "        [0.7602],\n",
      "        [0.7024],\n",
      "        [0.8689],\n",
      "        [0.8678],\n",
      "        [0.6902],\n",
      "        [0.8145],\n",
      "        [0.8019],\n",
      "        [0.8218],\n",
      "        [0.6627],\n",
      "        [0.8404],\n",
      "        [0.6713],\n",
      "        [0.8956],\n",
      "        [0.8285],\n",
      "        [0.7295],\n",
      "        [0.7823],\n",
      "        [0.8327],\n",
      "        [0.7624],\n",
      "        [0.8995],\n",
      "        [0.7582],\n",
      "        [0.7431],\n",
      "        [0.8939],\n",
      "        [0.7644],\n",
      "        [0.7942],\n",
      "        [0.6936],\n",
      "        [0.9040],\n",
      "        [0.7151],\n",
      "        [0.9426],\n",
      "        [0.8894],\n",
      "        [0.7218],\n",
      "        [0.9631],\n",
      "        [0.9602]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0019],\n",
      "        [    0.0025],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0038],\n",
      "        [    0.0038],\n",
      "        [    0.0053],\n",
      "        [    0.0053],\n",
      "        [    0.0059],\n",
      "        [    0.0065],\n",
      "        [    0.0065],\n",
      "        [    0.0087],\n",
      "        [    0.0090],\n",
      "        [    0.0093],\n",
      "        [    0.0099],\n",
      "        [    0.0101],\n",
      "        [    0.0102],\n",
      "        [    0.0102],\n",
      "        [    0.0108],\n",
      "        [    0.0109],\n",
      "        [    0.0109],\n",
      "        [    0.0111],\n",
      "        [    0.0122],\n",
      "        [    0.0136],\n",
      "        [    0.0141],\n",
      "        [    0.0143],\n",
      "        [    0.0144],\n",
      "        [    0.0154],\n",
      "        [    0.0155],\n",
      "        [    0.0170],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0209],\n",
      "        [    0.0213],\n",
      "        [    0.0237],\n",
      "        [    0.0247],\n",
      "        [    0.0248],\n",
      "        [    0.0256],\n",
      "        [    0.0272],\n",
      "        [    0.0308],\n",
      "        [    0.0312],\n",
      "        [    0.0321],\n",
      "        [    0.0347],\n",
      "        [    0.0352],\n",
      "        [    0.0416],\n",
      "        [    0.0422],\n",
      "        [    0.0490],\n",
      "        [    0.0524],\n",
      "        [    0.0544],\n",
      "        [    0.0560],\n",
      "        [    0.0591],\n",
      "        [    0.0598],\n",
      "        [    0.0607],\n",
      "        [    0.0652],\n",
      "        [    0.0676]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0024],\n",
      "        [0.0027],\n",
      "        [0.0062],\n",
      "        [0.0094],\n",
      "        [0.0071],\n",
      "        [0.0058],\n",
      "        [0.0017],\n",
      "        [0.0052],\n",
      "        [0.0077],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0083],\n",
      "        [0.0107],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0048],\n",
      "        [0.0082],\n",
      "        [0.0039],\n",
      "        [0.0081],\n",
      "        [0.0052],\n",
      "        [0.0061],\n",
      "        [0.0153],\n",
      "        [0.0099],\n",
      "        [0.0142],\n",
      "        [0.0160],\n",
      "        [0.0123],\n",
      "        [0.0162],\n",
      "        [0.0183],\n",
      "        [0.0097],\n",
      "        [0.0170],\n",
      "        [0.0207],\n",
      "        [0.0230],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0251],\n",
      "        [0.0260],\n",
      "        [0.0286],\n",
      "        [0.0206],\n",
      "        [0.0240],\n",
      "        [0.0316],\n",
      "        [0.0354],\n",
      "        [0.0324],\n",
      "        [0.0248],\n",
      "        [0.0378],\n",
      "        [0.0322],\n",
      "        [0.0464],\n",
      "        [0.0452],\n",
      "        [0.0525],\n",
      "        [0.0497],\n",
      "        [0.0486],\n",
      "        [0.0525],\n",
      "        [0.0512],\n",
      "        [0.0546],\n",
      "        [0.0572],\n",
      "        [0.0563],\n",
      "        [0.0601]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.514257431030273\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 58\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.9633617941726698e-06, 101)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [101, 98, 99, 56, 34, 57, 77, 90, 36, 48, 47, 74, 76, 75, 93, 100, 73, 89, 55, 49, 78, 87, 88, 94, 72, 91, 28, 95, 86, 92, 62, 27, 26, 35, 33, 97, 69, 53, 63, 96, 61, 32, 45, 67, 37, 24, 25, 58, 30, 102, 52, 46, 103, 29, 51, 85, 54, 31] 數值 torch.Size([58, 1])\n",
      "目前模型的Data狀態 torch.Size([58, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6492],\n",
      "        [0.6747],\n",
      "        [0.6570],\n",
      "        [0.9098],\n",
      "        [0.8192],\n",
      "        [0.9252],\n",
      "        [0.7165],\n",
      "        [0.6975],\n",
      "        [0.8486],\n",
      "        [0.8401],\n",
      "        [0.8225],\n",
      "        [0.7532],\n",
      "        [0.7509],\n",
      "        [0.7386],\n",
      "        [0.7081],\n",
      "        [0.6435],\n",
      "        [0.7537],\n",
      "        [0.7035],\n",
      "        [0.9002],\n",
      "        [0.8621],\n",
      "        [0.6729],\n",
      "        [0.7248],\n",
      "        [0.7232],\n",
      "        [0.6965],\n",
      "        [0.7584],\n",
      "        [0.6772],\n",
      "        [0.8339],\n",
      "        [0.6890],\n",
      "        [0.7005],\n",
      "        [0.6886],\n",
      "        [0.8650],\n",
      "        [0.8242],\n",
      "        [0.8108],\n",
      "        [0.8173],\n",
      "        [0.7970],\n",
      "        [0.6604],\n",
      "        [0.7279],\n",
      "        [0.8922],\n",
      "        [0.8366],\n",
      "        [0.6690],\n",
      "        [0.8917],\n",
      "        [0.7778],\n",
      "        [0.7401],\n",
      "        [0.7612],\n",
      "        [0.8281],\n",
      "        [0.7551],\n",
      "        [0.7614],\n",
      "        [0.8891],\n",
      "        [0.8982],\n",
      "        [0.6909],\n",
      "        [0.9346],\n",
      "        [0.7908],\n",
      "        [0.7117],\n",
      "        [0.8841],\n",
      "        [0.9541],\n",
      "        [0.7183],\n",
      "        [0.9527],\n",
      "        [0.8723]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0039],\n",
      "        [0.0048],\n",
      "        [0.0052],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0107],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0123],\n",
      "        [0.0142],\n",
      "        [0.0153],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0170],\n",
      "        [0.0183],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0227],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0240],\n",
      "        [0.0248],\n",
      "        [0.0251],\n",
      "        [0.0260],\n",
      "        [0.0286],\n",
      "        [0.0316],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0354],\n",
      "        [0.0378],\n",
      "        [0.0452],\n",
      "        [0.0464],\n",
      "        [0.0486],\n",
      "        [0.0497],\n",
      "        [0.0512],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0546],\n",
      "        [0.0563],\n",
      "        [0.0572],\n",
      "        [0.0601],\n",
      "        [0.0680]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0039],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0036],\n",
      "        [0.0045],\n",
      "        [0.0054],\n",
      "        [0.0022],\n",
      "        [0.0036],\n",
      "        [0.0060],\n",
      "        [0.0043],\n",
      "        [0.0050],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0104],\n",
      "        [0.0115],\n",
      "        [0.0060],\n",
      "        [0.0104],\n",
      "        [0.0101],\n",
      "        [0.0133],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0087],\n",
      "        [0.0158],\n",
      "        [0.0120],\n",
      "        [0.0148],\n",
      "        [0.0139],\n",
      "        [0.0145],\n",
      "        [0.0144],\n",
      "        [0.0163],\n",
      "        [0.0218],\n",
      "        [0.0186],\n",
      "        [0.0223],\n",
      "        [0.0231],\n",
      "        [0.0210],\n",
      "        [0.0279],\n",
      "        [0.0232],\n",
      "        [0.0229],\n",
      "        [0.0239],\n",
      "        [0.0267],\n",
      "        [0.0312],\n",
      "        [0.0351],\n",
      "        [0.0277],\n",
      "        [0.0352],\n",
      "        [0.0351],\n",
      "        [0.0424],\n",
      "        [0.0452],\n",
      "        [0.0476],\n",
      "        [0.0515],\n",
      "        [0.0489],\n",
      "        [0.0498],\n",
      "        [0.0536],\n",
      "        [0.0543],\n",
      "        [0.0533],\n",
      "        [0.0580],\n",
      "        [0.0583],\n",
      "        [0.0669]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.788908243179321\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 59\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0943246309125243e-07, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 99, 77, 56, 90, 101, 48, 34, 74, 76, 47, 75, 93, 57, 89, 36, 94, 87, 88, 49, 100, 55, 73, 91, 78, 95, 92, 86, 28, 72, 62, 26, 97, 27, 35, 63, 33, 53, 96, 61, 67, 69, 32, 45, 24, 37, 25, 58, 30, 52, 46, 102, 51, 103, 29, 85, 54, 31, 23] 數值 torch.Size([59, 1])\n",
      "目前模型的Data狀態 torch.Size([59, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6768],\n",
      "        [0.6593],\n",
      "        [0.7196],\n",
      "        [0.9095],\n",
      "        [0.6997],\n",
      "        [0.6513],\n",
      "        [0.8420],\n",
      "        [0.8189],\n",
      "        [0.7561],\n",
      "        [0.7537],\n",
      "        [0.8247],\n",
      "        [0.7413],\n",
      "        [0.7111],\n",
      "        [0.9254],\n",
      "        [0.7058],\n",
      "        [0.8486],\n",
      "        [0.6991],\n",
      "        [0.7267],\n",
      "        [0.7251],\n",
      "        [0.8625],\n",
      "        [0.6457],\n",
      "        [0.8991],\n",
      "        [0.7570],\n",
      "        [0.6794],\n",
      "        [0.6763],\n",
      "        [0.6912],\n",
      "        [0.6912],\n",
      "        [0.7023],\n",
      "        [0.8344],\n",
      "        [0.7620],\n",
      "        [0.8670],\n",
      "        [0.8129],\n",
      "        [0.6626],\n",
      "        [0.8254],\n",
      "        [0.8177],\n",
      "        [0.8387],\n",
      "        [0.7969],\n",
      "        [0.8906],\n",
      "        [0.6711],\n",
      "        [0.8935],\n",
      "        [0.7659],\n",
      "        [0.7317],\n",
      "        [0.7782],\n",
      "        [0.7430],\n",
      "        [0.7577],\n",
      "        [0.8283],\n",
      "        [0.7642],\n",
      "        [0.8903],\n",
      "        [0.8972],\n",
      "        [0.9324],\n",
      "        [0.7935],\n",
      "        [0.6926],\n",
      "        [0.9511],\n",
      "        [0.7128],\n",
      "        [0.8839],\n",
      "        [0.7191],\n",
      "        [0.9509],\n",
      "        [0.8712],\n",
      "        [0.7652]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0005],\n",
      "        [0.0022],\n",
      "        [0.0036],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0060],\n",
      "        [0.0060],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0104],\n",
      "        [0.0115],\n",
      "        [0.0120],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0148],\n",
      "        [0.0158],\n",
      "        [0.0163],\n",
      "        [0.0186],\n",
      "        [0.0210],\n",
      "        [0.0218],\n",
      "        [0.0223],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0239],\n",
      "        [0.0267],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0312],\n",
      "        [0.0351],\n",
      "        [0.0351],\n",
      "        [0.0352],\n",
      "        [0.0424],\n",
      "        [0.0452],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0498],\n",
      "        [0.0515],\n",
      "        [0.0533],\n",
      "        [0.0536],\n",
      "        [0.0543],\n",
      "        [0.0580],\n",
      "        [0.0583],\n",
      "        [0.0669],\n",
      "        [0.0802]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0025],\n",
      "        [0.0023],\n",
      "        [0.0036],\n",
      "        [0.0021],\n",
      "        [0.0055],\n",
      "        [0.0021],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0056],\n",
      "        [0.0067],\n",
      "        [0.0033],\n",
      "        [0.0064],\n",
      "        [0.0062],\n",
      "        [0.0046],\n",
      "        [0.0079],\n",
      "        [0.0045],\n",
      "        [0.0101],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0098],\n",
      "        [0.0085],\n",
      "        [0.0126],\n",
      "        [0.0112],\n",
      "        [0.0141],\n",
      "        [0.0122],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0174],\n",
      "        [0.0148],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0172],\n",
      "        [0.0230],\n",
      "        [0.0223],\n",
      "        [0.0234],\n",
      "        [0.0228],\n",
      "        [0.0243],\n",
      "        [0.0204],\n",
      "        [0.0260],\n",
      "        [0.0267],\n",
      "        [0.0247],\n",
      "        [0.0295],\n",
      "        [0.0318],\n",
      "        [0.0375],\n",
      "        [0.0326],\n",
      "        [0.0364],\n",
      "        [0.0398],\n",
      "        [0.0452],\n",
      "        [0.0459],\n",
      "        [0.0456],\n",
      "        [0.0476],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0509],\n",
      "        [0.0534],\n",
      "        [0.0545],\n",
      "        [0.0553],\n",
      "        [0.0648],\n",
      "        [0.0775]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.062614679336548\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 60\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.324761448515346e-06, 101)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [101, 56, 99, 98, 34, 48, 47, 77, 36, 57, 90, 74, 93, 75, 76, 89, 100, 49, 94, 73, 87, 88, 78, 55, 91, 28, 95, 72, 92, 62, 26, 86, 53, 27, 63, 97, 35, 33, 67, 96, 61, 69, 32, 24, 37, 45, 25, 58, 52, 30, 46, 102, 51, 103, 29, 85, 54, 31, 23, 68] 數值 torch.Size([60, 1])\n",
      "目前模型的Data狀態 torch.Size([60, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6495],\n",
      "        [0.9081],\n",
      "        [0.6574],\n",
      "        [0.6747],\n",
      "        [0.8173],\n",
      "        [0.8433],\n",
      "        [0.8263],\n",
      "        [0.7181],\n",
      "        [0.8470],\n",
      "        [0.9246],\n",
      "        [0.6979],\n",
      "        [0.7550],\n",
      "        [0.7099],\n",
      "        [0.7399],\n",
      "        [0.7519],\n",
      "        [0.7039],\n",
      "        [0.6439],\n",
      "        [0.8622],\n",
      "        [0.6976],\n",
      "        [0.7567],\n",
      "        [0.7242],\n",
      "        [0.7227],\n",
      "        [0.6752],\n",
      "        [0.8970],\n",
      "        [0.6773],\n",
      "        [0.8344],\n",
      "        [0.6893],\n",
      "        [0.7620],\n",
      "        [0.6894],\n",
      "        [0.8671],\n",
      "        [0.8144],\n",
      "        [0.6994],\n",
      "        [0.8879],\n",
      "        [0.8260],\n",
      "        [0.8388],\n",
      "        [0.6605],\n",
      "        [0.8166],\n",
      "        [0.7957],\n",
      "        [0.7690],\n",
      "        [0.6691],\n",
      "        [0.8935],\n",
      "        [0.7334],\n",
      "        [0.7777],\n",
      "        [0.7602],\n",
      "        [0.8271],\n",
      "        [0.7454],\n",
      "        [0.7667],\n",
      "        [0.8903],\n",
      "        [0.9290],\n",
      "        [0.8955],\n",
      "        [0.7957],\n",
      "        [0.6904],\n",
      "        [0.9474],\n",
      "        [0.7100],\n",
      "        [0.8829],\n",
      "        [0.7155],\n",
      "        [0.9479],\n",
      "        [0.8690],\n",
      "        [0.7679],\n",
      "        [0.7316]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0021],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0033],\n",
      "        [0.0036],\n",
      "        [0.0045],\n",
      "        [0.0046],\n",
      "        [0.0055],\n",
      "        [0.0056],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0079],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0112],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0126],\n",
      "        [0.0141],\n",
      "        [0.0148],\n",
      "        [0.0157],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0162],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0204],\n",
      "        [0.0223],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0234],\n",
      "        [0.0243],\n",
      "        [0.0247],\n",
      "        [0.0260],\n",
      "        [0.0267],\n",
      "        [0.0295],\n",
      "        [0.0318],\n",
      "        [0.0326],\n",
      "        [0.0364],\n",
      "        [0.0375],\n",
      "        [0.0398],\n",
      "        [0.0452],\n",
      "        [0.0456],\n",
      "        [0.0459],\n",
      "        [0.0476],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0509],\n",
      "        [0.0534],\n",
      "        [0.0545],\n",
      "        [0.0553],\n",
      "        [0.0648],\n",
      "        [0.0775],\n",
      "        [0.0850]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0050],\n",
      "        [0.0046],\n",
      "        [0.0007],\n",
      "        [0.0003],\n",
      "        [0.0048],\n",
      "        [0.0024],\n",
      "        [0.0023],\n",
      "        [0.0002],\n",
      "        [0.0062],\n",
      "        [0.0079],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0030],\n",
      "        [0.0035],\n",
      "        [0.0047],\n",
      "        [0.0115],\n",
      "        [0.0133],\n",
      "        [0.0067],\n",
      "        [0.0155],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0159],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0109],\n",
      "        [0.0127],\n",
      "        [0.0206],\n",
      "        [0.0131],\n",
      "        [0.0119],\n",
      "        [0.0113],\n",
      "        [0.0154],\n",
      "        [0.0213],\n",
      "        [0.0271],\n",
      "        [0.0183],\n",
      "        [0.0202],\n",
      "        [0.0211],\n",
      "        [0.0219],\n",
      "        [0.0164],\n",
      "        [0.0232],\n",
      "        [0.0224],\n",
      "        [0.0362],\n",
      "        [0.0287],\n",
      "        [0.0255],\n",
      "        [0.0342],\n",
      "        [0.0439],\n",
      "        [0.0326],\n",
      "        [0.0409],\n",
      "        [0.0458],\n",
      "        [0.0479],\n",
      "        [0.0412],\n",
      "        [0.0517],\n",
      "        [0.0494],\n",
      "        [0.0528],\n",
      "        [0.0564],\n",
      "        [0.0561],\n",
      "        [0.0560],\n",
      "        [0.0662],\n",
      "        [0.0700],\n",
      "        [0.0771]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.337382316589355\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 61\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.3318686354941747e-08, 77)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [77, 98, 99, 74, 90, 47, 48, 93, 75, 76, 56, 89, 34, 101, 36, 94, 57, 87, 88, 28, 55, 91, 26, 100, 62, 95, 92, 49, 86, 73, 78, 67, 63, 97, 72, 35, 53, 33, 61, 96, 24, 27, 32, 25, 37, 69, 58, 46, 45, 52, 30, 51, 102, 103, 54, 85, 29, 31, 23, 68, 64] 數值 torch.Size([61, 1])\n",
      "目前模型的Data狀態 torch.Size([61, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7215],\n",
      "        [0.6775],\n",
      "        [0.6605],\n",
      "        [0.7585],\n",
      "        [0.7011],\n",
      "        [0.8320],\n",
      "        [0.8487],\n",
      "        [0.7136],\n",
      "        [0.7433],\n",
      "        [0.7551],\n",
      "        [0.9105],\n",
      "        [0.7071],\n",
      "        [0.8193],\n",
      "        [0.6525],\n",
      "        [0.8488],\n",
      "        [0.7011],\n",
      "        [0.9278],\n",
      "        [0.7268],\n",
      "        [0.7253],\n",
      "        [0.8383],\n",
      "        [0.8987],\n",
      "        [0.6803],\n",
      "        [0.8202],\n",
      "        [0.6468],\n",
      "        [0.8714],\n",
      "        [0.6923],\n",
      "        [0.6925],\n",
      "        [0.8657],\n",
      "        [0.7014],\n",
      "        [0.7610],\n",
      "        [0.6789],\n",
      "        [0.7773],\n",
      "        [0.8433],\n",
      "        [0.6633],\n",
      "        [0.7667],\n",
      "        [0.8189],\n",
      "        [0.8887],\n",
      "        [0.7981],\n",
      "        [0.8978],\n",
      "        [0.6719],\n",
      "        [0.7674],\n",
      "        [0.8307],\n",
      "        [0.7807],\n",
      "        [0.7739],\n",
      "        [0.8293],\n",
      "        [0.7401],\n",
      "        [0.8946],\n",
      "        [0.8020],\n",
      "        [0.7519],\n",
      "        [0.9292],\n",
      "        [0.8975],\n",
      "        [0.9473],\n",
      "        [0.6929],\n",
      "        [0.7120],\n",
      "        [0.9486],\n",
      "        [0.7171],\n",
      "        [0.8859],\n",
      "        [0.8704],\n",
      "        [0.7754],\n",
      "        [0.7396],\n",
      "        [0.7927]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0030],\n",
      "        [0.0035],\n",
      "        [0.0046],\n",
      "        [0.0047],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0062],\n",
      "        [0.0067],\n",
      "        [0.0079],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0109],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0119],\n",
      "        [0.0127],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0183],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0219],\n",
      "        [0.0224],\n",
      "        [0.0232],\n",
      "        [0.0255],\n",
      "        [0.0271],\n",
      "        [0.0287],\n",
      "        [0.0326],\n",
      "        [0.0342],\n",
      "        [0.0362],\n",
      "        [0.0409],\n",
      "        [0.0412],\n",
      "        [0.0439],\n",
      "        [0.0458],\n",
      "        [0.0479],\n",
      "        [0.0494],\n",
      "        [0.0517],\n",
      "        [0.0528],\n",
      "        [0.0560],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0662],\n",
      "        [0.0700],\n",
      "        [0.0771],\n",
      "        [0.0817]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0003],\n",
      "        [0.0011],\n",
      "        [0.0009],\n",
      "        [0.0024],\n",
      "        [0.0063],\n",
      "        [0.0061],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0027],\n",
      "        [0.0056],\n",
      "        [0.0050],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0097],\n",
      "        [0.0108],\n",
      "        [0.0113],\n",
      "        [0.0080],\n",
      "        [0.0119],\n",
      "        [0.0090],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0149],\n",
      "        [0.0168],\n",
      "        [0.0175],\n",
      "        [0.0174],\n",
      "        [0.0107],\n",
      "        [0.0152],\n",
      "        [0.0202],\n",
      "        [0.0231],\n",
      "        [0.0207],\n",
      "        [0.0206],\n",
      "        [0.0215],\n",
      "        [0.0197],\n",
      "        [0.0233],\n",
      "        [0.0212],\n",
      "        [0.0293],\n",
      "        [0.0278],\n",
      "        [0.0284],\n",
      "        [0.0340],\n",
      "        [0.0403],\n",
      "        [0.0383],\n",
      "        [0.0366],\n",
      "        [0.0486],\n",
      "        [0.0444],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0516],\n",
      "        [0.0524],\n",
      "        [0.0552],\n",
      "        [0.0544],\n",
      "        [0.0566],\n",
      "        [0.0654],\n",
      "        [0.0655],\n",
      "        [0.0719],\n",
      "        [0.0774]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.612282037734985\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 62\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1823914292108384e-07, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 77, 74, 99, 93, 75, 90, 76, 34, 89, 101, 56, 36, 94, 48, 47, 26, 62, 57, 87, 28, 88, 67, 55, 91, 100, 95, 92, 49, 63, 86, 78, 73, 61, 97, 53, 35, 24, 33, 72, 96, 32, 25, 27, 37, 46, 58, 69, 52, 30, 51, 45, 102, 103, 85, 54, 29, 31, 23, 68, 64, 66] 數值 torch.Size([62, 1])\n",
      "目前模型的Data狀態 torch.Size([62, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6775],\n",
      "        [0.7226],\n",
      "        [0.7597],\n",
      "        [0.6608],\n",
      "        [0.7144],\n",
      "        [0.7444],\n",
      "        [0.7010],\n",
      "        [0.7559],\n",
      "        [0.8191],\n",
      "        [0.7067],\n",
      "        [0.6527],\n",
      "        [0.9115],\n",
      "        [0.8485],\n",
      "        [0.7017],\n",
      "        [0.8523],\n",
      "        [0.8359],\n",
      "        [0.8236],\n",
      "        [0.8743],\n",
      "        [0.9294],\n",
      "        [0.7258],\n",
      "        [0.8395],\n",
      "        [0.7244],\n",
      "        [0.7829],\n",
      "        [0.8988],\n",
      "        [0.6801],\n",
      "        [0.6472],\n",
      "        [0.6924],\n",
      "        [0.6927],\n",
      "        [0.8673],\n",
      "        [0.8465],\n",
      "        [0.7000],\n",
      "        [0.6804],\n",
      "        [0.7630],\n",
      "        [0.9006],\n",
      "        [0.6634],\n",
      "        [0.8880],\n",
      "        [0.8193],\n",
      "        [0.7716],\n",
      "        [0.7984],\n",
      "        [0.7692],\n",
      "        [0.6718],\n",
      "        [0.7816],\n",
      "        [0.7782],\n",
      "        [0.8330],\n",
      "        [0.8295],\n",
      "        [0.8067],\n",
      "        [0.8971],\n",
      "        [0.7441],\n",
      "        [0.9279],\n",
      "        [0.8969],\n",
      "        [0.9456],\n",
      "        [0.7565],\n",
      "        [0.6928],\n",
      "        [0.7115],\n",
      "        [0.7155],\n",
      "        [0.9478],\n",
      "        [0.8861],\n",
      "        [0.8696],\n",
      "        [0.7799],\n",
      "        [0.7447],\n",
      "        [0.7969],\n",
      "        [0.7538]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0047],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0080],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0168],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0278],\n",
      "        [0.0284],\n",
      "        [0.0293],\n",
      "        [0.0340],\n",
      "        [0.0366],\n",
      "        [0.0383],\n",
      "        [0.0403],\n",
      "        [0.0444],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0486],\n",
      "        [0.0516],\n",
      "        [0.0524],\n",
      "        [0.0544],\n",
      "        [0.0552],\n",
      "        [0.0566],\n",
      "        [0.0654],\n",
      "        [0.0655],\n",
      "        [0.0719],\n",
      "        [0.0774],\n",
      "        [0.0817]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0024],\n",
      "        [0.0007],\n",
      "        [0.0021],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0019],\n",
      "        [0.0014],\n",
      "        [0.0041],\n",
      "        [0.0048],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0051],\n",
      "        [0.0049],\n",
      "        [0.0097],\n",
      "        [0.0101],\n",
      "        [0.0043],\n",
      "        [0.0060],\n",
      "        [0.0111],\n",
      "        [0.0100],\n",
      "        [0.0086],\n",
      "        [0.0102],\n",
      "        [0.0042],\n",
      "        [0.0107],\n",
      "        [0.0110],\n",
      "        [0.0129],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0165],\n",
      "        [0.0119],\n",
      "        [0.0175],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0169],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0207],\n",
      "        [0.0166],\n",
      "        [0.0214],\n",
      "        [0.0260],\n",
      "        [0.0227],\n",
      "        [0.0271],\n",
      "        [0.0236],\n",
      "        [0.0317],\n",
      "        [0.0344],\n",
      "        [0.0319],\n",
      "        [0.0355],\n",
      "        [0.0449],\n",
      "        [0.0428],\n",
      "        [0.0464],\n",
      "        [0.0460],\n",
      "        [0.0532],\n",
      "        [0.0522],\n",
      "        [0.0526],\n",
      "        [0.0535],\n",
      "        [0.0541],\n",
      "        [0.0567],\n",
      "        [0.0642],\n",
      "        [0.0606],\n",
      "        [0.0660],\n",
      "        [0.0730],\n",
      "        [0.0748]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.887929916381836\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 63\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.9508091848338154e-08, 75)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [75, 93, 74, 98, 76, 90, 99, 77, 34, 67, 26, 89, 94, 36, 62, 101, 56, 28, 48, 87, 47, 88, 55, 91, 57, 63, 95, 92, 100, 49, 24, 61, 86, 78, 97, 53, 73, 35, 33, 96, 25, 72, 32, 27, 46, 37, 58, 52, 69, 51, 30, 102, 103, 45, 85, 54, 29, 23, 31, 68, 64, 66, 22] 數值 torch.Size([63, 1])\n",
      "目前模型的Data狀態 torch.Size([63, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7461],\n",
      "        [0.7157],\n",
      "        [0.7613],\n",
      "        [0.6782],\n",
      "        [0.7572],\n",
      "        [0.7015],\n",
      "        [0.6618],\n",
      "        [0.7242],\n",
      "        [0.8186],\n",
      "        [0.7894],\n",
      "        [0.8272],\n",
      "        [0.7070],\n",
      "        [0.7029],\n",
      "        [0.8477],\n",
      "        [0.8773],\n",
      "        [0.6537],\n",
      "        [0.9125],\n",
      "        [0.8406],\n",
      "        [0.8560],\n",
      "        [0.7255],\n",
      "        [0.8398],\n",
      "        [0.7240],\n",
      "        [0.8989],\n",
      "        [0.6804],\n",
      "        [0.9310],\n",
      "        [0.8498],\n",
      "        [0.6931],\n",
      "        [0.6934],\n",
      "        [0.6482],\n",
      "        [0.8690],\n",
      "        [0.7763],\n",
      "        [0.9034],\n",
      "        [0.6992],\n",
      "        [0.6824],\n",
      "        [0.6640],\n",
      "        [0.8872],\n",
      "        [0.7654],\n",
      "        [0.8194],\n",
      "        [0.7986],\n",
      "        [0.6723],\n",
      "        [0.7829],\n",
      "        [0.7721],\n",
      "        [0.7823],\n",
      "        [0.8353],\n",
      "        [0.8114],\n",
      "        [0.8291],\n",
      "        [0.8999],\n",
      "        [0.9263],\n",
      "        [0.7488],\n",
      "        [0.9438],\n",
      "        [0.8960],\n",
      "        [0.6933],\n",
      "        [0.7118],\n",
      "        [0.7611],\n",
      "        [0.7145],\n",
      "        [0.9467],\n",
      "        [0.8862],\n",
      "        [0.7848],\n",
      "        [0.8685],\n",
      "        [0.7506],\n",
      "        [0.8014],\n",
      "        [0.7607],\n",
      "        [0.7783]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0010],\n",
      "        [0.0014],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0024],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0086],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0107],\n",
      "        [0.0110],\n",
      "        [0.0111],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0129],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0175],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0207],\n",
      "        [0.0214],\n",
      "        [0.0227],\n",
      "        [0.0236],\n",
      "        [0.0260],\n",
      "        [0.0271],\n",
      "        [0.0317],\n",
      "        [0.0319],\n",
      "        [0.0344],\n",
      "        [0.0355],\n",
      "        [0.0428],\n",
      "        [0.0449],\n",
      "        [0.0460],\n",
      "        [0.0464],\n",
      "        [0.0522],\n",
      "        [0.0526],\n",
      "        [0.0532],\n",
      "        [0.0535],\n",
      "        [0.0541],\n",
      "        [0.0567],\n",
      "        [0.0606],\n",
      "        [0.0642],\n",
      "        [0.0660],\n",
      "        [0.0730],\n",
      "        [0.0748],\n",
      "        [0.0765]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0019],\n",
      "        [0.0018],\n",
      "        [0.0010],\n",
      "        [0.0010],\n",
      "        [0.0035],\n",
      "        [0.0039],\n",
      "        [0.0005],\n",
      "        [0.0005],\n",
      "        [0.0016],\n",
      "        [0.0007],\n",
      "        [0.0027],\n",
      "        [0.0069],\n",
      "        [0.0064],\n",
      "        [0.0022],\n",
      "        [0.0062],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0091],\n",
      "        [0.0107],\n",
      "        [0.0127],\n",
      "        [0.0114],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0101],\n",
      "        [0.0118],\n",
      "        [0.0139],\n",
      "        [0.0143],\n",
      "        [0.0113],\n",
      "        [0.0158],\n",
      "        [0.0138],\n",
      "        [0.0172],\n",
      "        [0.0209],\n",
      "        [0.0178],\n",
      "        [0.0216],\n",
      "        [0.0165],\n",
      "        [0.0189],\n",
      "        [0.0229],\n",
      "        [0.0234],\n",
      "        [0.0249],\n",
      "        [0.0208],\n",
      "        [0.0254],\n",
      "        [0.0285],\n",
      "        [0.0322],\n",
      "        [0.0299],\n",
      "        [0.0369],\n",
      "        [0.0356],\n",
      "        [0.0390],\n",
      "        [0.0467],\n",
      "        [0.0420],\n",
      "        [0.0438],\n",
      "        [0.0501],\n",
      "        [0.0503],\n",
      "        [0.0552],\n",
      "        [0.0500],\n",
      "        [0.0505],\n",
      "        [0.0551],\n",
      "        [0.0576],\n",
      "        [0.0612],\n",
      "        [0.0629],\n",
      "        [0.0718],\n",
      "        [0.0709],\n",
      "        [0.0728]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.166007041931152\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 64\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1836967789568007e-07, 77)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [77, 99, 67, 98, 74, 34, 93, 75, 36, 26, 76, 90, 101, 56, 62, 94, 89, 28, 57, 48, 100, 47, 63, 87, 88, 91, 55, 24, 95, 92, 49, 53, 61, 78, 73, 25, 86, 97, 35, 33, 96, 72, 32, 46, 27, 58, 37, 52, 51, 30, 69, 85, 102, 103, 54, 29, 45, 23, 31, 68, 66, 64, 22, 65] 數值 torch.Size([64, 1])\n",
      "目前模型的Data狀態 torch.Size([64, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7222],\n",
      "        [0.6602],\n",
      "        [0.7929],\n",
      "        [0.6762],\n",
      "        [0.7596],\n",
      "        [0.8161],\n",
      "        [0.7143],\n",
      "        [0.7444],\n",
      "        [0.8447],\n",
      "        [0.8288],\n",
      "        [0.7551],\n",
      "        [0.6995],\n",
      "        [0.6520],\n",
      "        [0.9108],\n",
      "        [0.8771],\n",
      "        [0.7014],\n",
      "        [0.7048],\n",
      "        [0.8401],\n",
      "        [0.9300],\n",
      "        [0.8570],\n",
      "        [0.6466],\n",
      "        [0.8411],\n",
      "        [0.8498],\n",
      "        [0.7227],\n",
      "        [0.7213],\n",
      "        [0.6782],\n",
      "        [0.8964],\n",
      "        [0.7790],\n",
      "        [0.6911],\n",
      "        [0.6913],\n",
      "        [0.8682],\n",
      "        [0.8839],\n",
      "        [0.9031],\n",
      "        [0.6809],\n",
      "        [0.7644],\n",
      "        [0.7858],\n",
      "        [0.6959],\n",
      "        [0.6620],\n",
      "        [0.8171],\n",
      "        [0.7966],\n",
      "        [0.6701],\n",
      "        [0.7716],\n",
      "        [0.7810],\n",
      "        [0.8134],\n",
      "        [0.8358],\n",
      "        [0.8999],\n",
      "        [0.8267],\n",
      "        [0.9225],\n",
      "        [0.9398],\n",
      "        [0.8934],\n",
      "        [0.7505],\n",
      "        [0.7111],\n",
      "        [0.6913],\n",
      "        [0.7094],\n",
      "        [0.9431],\n",
      "        [0.8847],\n",
      "        [0.7631],\n",
      "        [0.7878],\n",
      "        [0.8654],\n",
      "        [0.7537],\n",
      "        [0.7646],\n",
      "        [0.8025],\n",
      "        [0.7819],\n",
      "        [0.7568]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0010],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0027],\n",
      "        [0.0035],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0069],\n",
      "        [0.0091],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0143],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0172],\n",
      "        [0.0178],\n",
      "        [0.0189],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0216],\n",
      "        [0.0229],\n",
      "        [0.0234],\n",
      "        [0.0249],\n",
      "        [0.0254],\n",
      "        [0.0285],\n",
      "        [0.0299],\n",
      "        [0.0322],\n",
      "        [0.0356],\n",
      "        [0.0369],\n",
      "        [0.0390],\n",
      "        [0.0420],\n",
      "        [0.0438],\n",
      "        [0.0467],\n",
      "        [0.0500],\n",
      "        [0.0501],\n",
      "        [0.0503],\n",
      "        [0.0505],\n",
      "        [0.0551],\n",
      "        [0.0552],\n",
      "        [0.0576],\n",
      "        [0.0612],\n",
      "        [0.0629],\n",
      "        [0.0709],\n",
      "        [0.0718],\n",
      "        [0.0728],\n",
      "        [0.0789]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0030],\n",
      "        [0.0031],\n",
      "        [0.0074],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0026],\n",
      "        [0.0011],\n",
      "        [0.0009],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0012],\n",
      "        [0.0019],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0017],\n",
      "        [0.0036],\n",
      "        [0.0052],\n",
      "        [0.0063],\n",
      "        [0.0131],\n",
      "        [0.0160],\n",
      "        [0.0139],\n",
      "        [0.0170],\n",
      "        [0.0070],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0072],\n",
      "        [0.0116],\n",
      "        [0.0121],\n",
      "        [0.0191],\n",
      "        [0.0171],\n",
      "        [0.0130],\n",
      "        [0.0209],\n",
      "        [0.0225],\n",
      "        [0.0140],\n",
      "        [0.0204],\n",
      "        [0.0194],\n",
      "        [0.0214],\n",
      "        [0.0218],\n",
      "        [0.0230],\n",
      "        [0.0296],\n",
      "        [0.0263],\n",
      "        [0.0235],\n",
      "        [0.0363],\n",
      "        [0.0314],\n",
      "        [0.0358],\n",
      "        [0.0390],\n",
      "        [0.0417],\n",
      "        [0.0444],\n",
      "        [0.0528],\n",
      "        [0.0504],\n",
      "        [0.0523],\n",
      "        [0.0522],\n",
      "        [0.0508],\n",
      "        [0.0568],\n",
      "        [0.0616],\n",
      "        [0.0507],\n",
      "        [0.0615],\n",
      "        [0.0555],\n",
      "        [0.0623],\n",
      "        [0.0657],\n",
      "        [0.0652],\n",
      "        [0.0710]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.442183494567871\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 65\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.280193585756933e-07, 75)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [75, 93, 76, 98, 74, 62, 90, 34, 36, 26, 77, 99, 94, 89, 28, 63, 101, 24, 56, 67, 91, 95, 55, 87, 88, 92, 61, 57, 100, 25, 48, 47, 53, 49, 97, 86, 78, 35, 33, 73, 96, 46, 32, 72, 58, 37, 27, 52, 51, 30, 85, 23, 54, 103, 102, 69, 68, 29, 31, 45, 66, 22, 64, 65, 59] 數值 torch.Size([65, 1])\n",
      "目前模型的Data狀態 torch.Size([65, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7472],\n",
      "        [0.7172],\n",
      "        [0.7574],\n",
      "        [0.6784],\n",
      "        [0.7623],\n",
      "        [0.8816],\n",
      "        [0.7015],\n",
      "        [0.8171],\n",
      "        [0.8452],\n",
      "        [0.8343],\n",
      "        [0.7248],\n",
      "        [0.6628],\n",
      "        [0.7042],\n",
      "        [0.7065],\n",
      "        [0.8429],\n",
      "        [0.8547],\n",
      "        [0.6545],\n",
      "        [0.7857],\n",
      "        [0.9132],\n",
      "        [0.8010],\n",
      "        [0.6801],\n",
      "        [0.6934],\n",
      "        [0.8979],\n",
      "        [0.7237],\n",
      "        [0.7223],\n",
      "        [0.6936],\n",
      "        [0.9073],\n",
      "        [0.9331],\n",
      "        [0.6493],\n",
      "        [0.7925],\n",
      "        [0.8623],\n",
      "        [0.8467],\n",
      "        [0.8845],\n",
      "        [0.8715],\n",
      "        [0.6641],\n",
      "        [0.6964],\n",
      "        [0.6840],\n",
      "        [0.8186],\n",
      "        [0.7982],\n",
      "        [0.7680],\n",
      "        [0.6721],\n",
      "        [0.8198],\n",
      "        [0.7832],\n",
      "        [0.7757],\n",
      "        [0.9041],\n",
      "        [0.8277],\n",
      "        [0.8400],\n",
      "        [0.9225],\n",
      "        [0.9396],\n",
      "        [0.8940],\n",
      "        [0.7115],\n",
      "        [0.7948],\n",
      "        [0.9434],\n",
      "        [0.7113],\n",
      "        [0.6935],\n",
      "        [0.7567],\n",
      "        [0.7612],\n",
      "        [0.8864],\n",
      "        [0.8658],\n",
      "        [0.7695],\n",
      "        [0.7732],\n",
      "        [0.7895],\n",
      "        [0.8086],\n",
      "        [0.7646],\n",
      "        [0.8803]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0036],\n",
      "        [0.0052],\n",
      "        [0.0063],\n",
      "        [0.0070],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0073],\n",
      "        [0.0074],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0139],\n",
      "        [0.0140],\n",
      "        [0.0160],\n",
      "        [0.0170],\n",
      "        [0.0171],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0204],\n",
      "        [0.0209],\n",
      "        [0.0214],\n",
      "        [0.0218],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0235],\n",
      "        [0.0263],\n",
      "        [0.0296],\n",
      "        [0.0314],\n",
      "        [0.0358],\n",
      "        [0.0363],\n",
      "        [0.0390],\n",
      "        [0.0417],\n",
      "        [0.0444],\n",
      "        [0.0504],\n",
      "        [0.0507],\n",
      "        [0.0508],\n",
      "        [0.0522],\n",
      "        [0.0523],\n",
      "        [0.0528],\n",
      "        [0.0555],\n",
      "        [0.0568],\n",
      "        [0.0615],\n",
      "        [0.0616],\n",
      "        [0.0623],\n",
      "        [0.0652],\n",
      "        [0.0657],\n",
      "        [0.0710],\n",
      "        [0.0790]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0020],\n",
      "        [0.0005],\n",
      "        [0.0015],\n",
      "        [0.0025],\n",
      "        [0.0012],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0017],\n",
      "        [0.0062],\n",
      "        [0.0039],\n",
      "        [0.0037],\n",
      "        [0.0029],\n",
      "        [0.0056],\n",
      "        [0.0052],\n",
      "        [0.0037],\n",
      "        [0.0076],\n",
      "        [0.0029],\n",
      "        [0.0085],\n",
      "        [0.0131],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0114],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0118],\n",
      "        [0.0102],\n",
      "        [0.0149],\n",
      "        [0.0146],\n",
      "        [0.0097],\n",
      "        [0.0194],\n",
      "        [0.0207],\n",
      "        [0.0165],\n",
      "        [0.0205],\n",
      "        [0.0193],\n",
      "        [0.0217],\n",
      "        [0.0222],\n",
      "        [0.0215],\n",
      "        [0.0217],\n",
      "        [0.0240],\n",
      "        [0.0231],\n",
      "        [0.0191],\n",
      "        [0.0256],\n",
      "        [0.0317],\n",
      "        [0.0286],\n",
      "        [0.0363],\n",
      "        [0.0386],\n",
      "        [0.0379],\n",
      "        [0.0404],\n",
      "        [0.0436],\n",
      "        [0.0491],\n",
      "        [0.0462],\n",
      "        [0.0500],\n",
      "        [0.0524],\n",
      "        [0.0527],\n",
      "        [0.0567],\n",
      "        [0.0503],\n",
      "        [0.0570],\n",
      "        [0.0605],\n",
      "        [0.0657],\n",
      "        [0.0562],\n",
      "        [0.0602],\n",
      "        [0.0614],\n",
      "        [0.0653],\n",
      "        [0.0761]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.716642141342163\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 66\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1864829591322632e-07, 76)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [76, 62, 98, 36, 75, 90, 93, 34, 74, 94, 24, 99, 63, 77, 28, 89, 26, 101, 56, 25, 61, 55, 91, 95, 92, 87, 88, 67, 100, 57, 53, 46, 97, 48, 49, 47, 35, 33, 86, 78, 96, 73, 32, 58, 72, 37, 52, 27, 51, 30, 23, 85, 54, 68, 103, 102, 66, 69, 29, 22, 31, 64, 65, 45, 59, 60] 數值 torch.Size([66, 1])\n",
      "目前模型的Data狀態 torch.Size([66, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7581],\n",
      "        [0.8846],\n",
      "        [0.6787],\n",
      "        [0.8443],\n",
      "        [0.7480],\n",
      "        [0.7014],\n",
      "        [0.7181],\n",
      "        [0.8166],\n",
      "        [0.7631],\n",
      "        [0.7049],\n",
      "        [0.7900],\n",
      "        [0.6634],\n",
      "        [0.8579],\n",
      "        [0.7256],\n",
      "        [0.8440],\n",
      "        [0.7061],\n",
      "        [0.8378],\n",
      "        [0.6550],\n",
      "        [0.9144],\n",
      "        [0.7969],\n",
      "        [0.9101],\n",
      "        [0.8982],\n",
      "        [0.6800],\n",
      "        [0.6935],\n",
      "        [0.6938],\n",
      "        [0.7228],\n",
      "        [0.7213],\n",
      "        [0.8067],\n",
      "        [0.6500],\n",
      "        [0.9349],\n",
      "        [0.8839],\n",
      "        [0.8242],\n",
      "        [0.6642],\n",
      "        [0.8657],\n",
      "        [0.8730],\n",
      "        [0.8504],\n",
      "        [0.8186],\n",
      "        [0.7983],\n",
      "        [0.6951],\n",
      "        [0.6852],\n",
      "        [0.6720],\n",
      "        [0.7695],\n",
      "        [0.7838],\n",
      "        [0.9069],\n",
      "        [0.7778],\n",
      "        [0.8272],\n",
      "        [0.9214],\n",
      "        [0.8423],\n",
      "        [0.9383],\n",
      "        [0.8932],\n",
      "        [0.7993],\n",
      "        [0.7101],\n",
      "        [0.9426],\n",
      "        [0.7663],\n",
      "        [0.7116],\n",
      "        [0.6938],\n",
      "        [0.7793],\n",
      "        [0.7605],\n",
      "        [0.8865],\n",
      "        [0.7946],\n",
      "        [0.8648],\n",
      "        [0.8129],\n",
      "        [0.7703],\n",
      "        [0.7736],\n",
      "        [0.8833],\n",
      "        [0.8780]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0062],\n",
      "        [0.0076],\n",
      "        [0.0085],\n",
      "        [0.0097],\n",
      "        [0.0102],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0165],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0215],\n",
      "        [0.0217],\n",
      "        [0.0217],\n",
      "        [0.0222],\n",
      "        [0.0231],\n",
      "        [0.0240],\n",
      "        [0.0256],\n",
      "        [0.0286],\n",
      "        [0.0317],\n",
      "        [0.0363],\n",
      "        [0.0379],\n",
      "        [0.0386],\n",
      "        [0.0404],\n",
      "        [0.0436],\n",
      "        [0.0462],\n",
      "        [0.0491],\n",
      "        [0.0500],\n",
      "        [0.0503],\n",
      "        [0.0524],\n",
      "        [0.0527],\n",
      "        [0.0562],\n",
      "        [0.0567],\n",
      "        [0.0570],\n",
      "        [0.0602],\n",
      "        [0.0605],\n",
      "        [0.0614],\n",
      "        [0.0653],\n",
      "        [0.0657],\n",
      "        [0.0761],\n",
      "        [0.0773]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0039],\n",
      "        [0.0017],\n",
      "        [0.0008],\n",
      "        [0.0023],\n",
      "        [0.0021],\n",
      "        [0.0027],\n",
      "        [0.0017],\n",
      "        [0.0030],\n",
      "        [0.0024],\n",
      "        [0.0004],\n",
      "        [0.0042],\n",
      "        [0.0008],\n",
      "        [0.0048],\n",
      "        [0.0045],\n",
      "        [0.0061],\n",
      "        [0.0089],\n",
      "        [0.0080],\n",
      "        [0.0095],\n",
      "        [0.0064],\n",
      "        [0.0077],\n",
      "        [0.0111],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0135],\n",
      "        [0.0139],\n",
      "        [0.0179],\n",
      "        [0.0152],\n",
      "        [0.0165],\n",
      "        [0.0159],\n",
      "        [0.0155],\n",
      "        [0.0193],\n",
      "        [0.0221],\n",
      "        [0.0214],\n",
      "        [0.0236],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0228],\n",
      "        [0.0233],\n",
      "        [0.0233],\n",
      "        [0.0250],\n",
      "        [0.0253],\n",
      "        [0.0261],\n",
      "        [0.0334],\n",
      "        [0.0369],\n",
      "        [0.0370],\n",
      "        [0.0404],\n",
      "        [0.0393],\n",
      "        [0.0428],\n",
      "        [0.0427],\n",
      "        [0.0478],\n",
      "        [0.0494],\n",
      "        [0.0461],\n",
      "        [0.0527],\n",
      "        [0.0530],\n",
      "        [0.0511],\n",
      "        [0.0598],\n",
      "        [0.0570],\n",
      "        [0.0564],\n",
      "        [0.0597],\n",
      "        [0.0578],\n",
      "        [0.0606],\n",
      "        [0.0690],\n",
      "        [0.0734],\n",
      "        [0.0746]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.98772406578064\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 67\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1571134450605314e-07, 76)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [76, 24, 63, 36, 34, 98, 90, 75, 94, 93, 74, 62, 99, 28, 77, 89, 25, 61, 101, 26, 56, 55, 91, 95, 92, 87, 88, 100, 46, 53, 57, 67, 97, 49, 35, 33, 48, 86, 78, 96, 47, 73, 32, 58, 72, 37, 52, 51, 27, 23, 30, 68, 85, 54, 66, 103, 102, 22, 29, 64, 31, 69, 65, 45, 59, 60, 21] 數值 torch.Size([67, 1])\n",
      "目前模型的Data狀態 torch.Size([67, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7589],\n",
      "        [0.7933],\n",
      "        [0.8609],\n",
      "        [0.8434],\n",
      "        [0.8162],\n",
      "        [0.6789],\n",
      "        [0.7013],\n",
      "        [0.7486],\n",
      "        [0.7054],\n",
      "        [0.7188],\n",
      "        [0.7636],\n",
      "        [0.8872],\n",
      "        [0.6639],\n",
      "        [0.8447],\n",
      "        [0.7266],\n",
      "        [0.7056],\n",
      "        [0.8002],\n",
      "        [0.9126],\n",
      "        [0.6554],\n",
      "        [0.8404],\n",
      "        [0.9154],\n",
      "        [0.8984],\n",
      "        [0.6799],\n",
      "        [0.6934],\n",
      "        [0.6940],\n",
      "        [0.7220],\n",
      "        [0.7204],\n",
      "        [0.6505],\n",
      "        [0.8277],\n",
      "        [0.8833],\n",
      "        [0.9364],\n",
      "        [0.8115],\n",
      "        [0.6643],\n",
      "        [0.8738],\n",
      "        [0.8184],\n",
      "        [0.7983],\n",
      "        [0.8684],\n",
      "        [0.6940],\n",
      "        [0.6863],\n",
      "        [0.6717],\n",
      "        [0.8533],\n",
      "        [0.7705],\n",
      "        [0.7842],\n",
      "        [0.9093],\n",
      "        [0.7795],\n",
      "        [0.8266],\n",
      "        [0.9205],\n",
      "        [0.9371],\n",
      "        [0.8440],\n",
      "        [0.8028],\n",
      "        [0.8924],\n",
      "        [0.7705],\n",
      "        [0.7089],\n",
      "        [0.9420],\n",
      "        [0.7844],\n",
      "        [0.7118],\n",
      "        [0.6941],\n",
      "        [0.7984],\n",
      "        [0.8865],\n",
      "        [0.8166],\n",
      "        [0.8639],\n",
      "        [0.7637],\n",
      "        [0.7751],\n",
      "        [0.7769],\n",
      "        [0.8860],\n",
      "        [0.8806],\n",
      "        [0.7767]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0004],\n",
      "        [0.0008],\n",
      "        [0.0008],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0048],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0089],\n",
      "        [0.0095],\n",
      "        [0.0111],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0135],\n",
      "        [0.0139],\n",
      "        [0.0152],\n",
      "        [0.0155],\n",
      "        [0.0159],\n",
      "        [0.0165],\n",
      "        [0.0179],\n",
      "        [0.0193],\n",
      "        [0.0214],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0228],\n",
      "        [0.0233],\n",
      "        [0.0233],\n",
      "        [0.0236],\n",
      "        [0.0250],\n",
      "        [0.0253],\n",
      "        [0.0261],\n",
      "        [0.0334],\n",
      "        [0.0369],\n",
      "        [0.0370],\n",
      "        [0.0393],\n",
      "        [0.0404],\n",
      "        [0.0427],\n",
      "        [0.0428],\n",
      "        [0.0461],\n",
      "        [0.0478],\n",
      "        [0.0494],\n",
      "        [0.0511],\n",
      "        [0.0527],\n",
      "        [0.0530],\n",
      "        [0.0564],\n",
      "        [0.0570],\n",
      "        [0.0578],\n",
      "        [0.0597],\n",
      "        [0.0598],\n",
      "        [0.0606],\n",
      "        [0.0690],\n",
      "        [0.0734],\n",
      "        [0.0746],\n",
      "        [0.0767]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0015],\n",
      "        [0.0031],\n",
      "        [0.0003],\n",
      "        [0.0019],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0035],\n",
      "        [0.0003],\n",
      "        [0.0035],\n",
      "        [0.0016],\n",
      "        [0.0009],\n",
      "        [0.0042],\n",
      "        [0.0029],\n",
      "        [0.0048],\n",
      "        [0.0031],\n",
      "        [0.0077],\n",
      "        [0.0036],\n",
      "        [0.0075],\n",
      "        [0.0067],\n",
      "        [0.0104],\n",
      "        [0.0085],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0130],\n",
      "        [0.0156],\n",
      "        [0.0159],\n",
      "        [0.0140],\n",
      "        [0.0137],\n",
      "        [0.0134],\n",
      "        [0.0160],\n",
      "        [0.0211],\n",
      "        [0.0210],\n",
      "        [0.0208],\n",
      "        [0.0236],\n",
      "        [0.0232],\n",
      "        [0.0231],\n",
      "        [0.0253],\n",
      "        [0.0219],\n",
      "        [0.0253],\n",
      "        [0.0248],\n",
      "        [0.0237],\n",
      "        [0.0264],\n",
      "        [0.0258],\n",
      "        [0.0325],\n",
      "        [0.0392],\n",
      "        [0.0344],\n",
      "        [0.0365],\n",
      "        [0.0409],\n",
      "        [0.0399],\n",
      "        [0.0409],\n",
      "        [0.0433],\n",
      "        [0.0451],\n",
      "        [0.0468],\n",
      "        [0.0476],\n",
      "        [0.0511],\n",
      "        [0.0515],\n",
      "        [0.0531],\n",
      "        [0.0558],\n",
      "        [0.0565],\n",
      "        [0.0573],\n",
      "        [0.0613],\n",
      "        [0.0580],\n",
      "        [0.0707],\n",
      "        [0.0728],\n",
      "        [0.0741],\n",
      "        [0.0725]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.256553411483765\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 68\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.771485713675247e-08, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 34, 75, 63, 74, 76, 93, 36, 99, 24, 77, 90, 94, 25, 62, 28, 101, 61, 89, 56, 26, 55, 91, 92, 95, 53, 46, 100, 87, 88, 57, 49, 97, 67, 78, 48, 33, 35, 73, 47, 86, 96, 58, 32, 72, 52, 51, 37, 23, 30, 27, 68, 85, 54, 66, 103, 102, 22, 29, 64, 31, 65, 69, 45, 21, 59, 60, 106] 數值 torch.Size([68, 1])\n",
      "目前模型的Data狀態 torch.Size([68, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6773],\n",
      "        [0.8142],\n",
      "        [0.7466],\n",
      "        [0.8613],\n",
      "        [0.7615],\n",
      "        [0.7571],\n",
      "        [0.7177],\n",
      "        [0.8407],\n",
      "        [0.6627],\n",
      "        [0.7959],\n",
      "        [0.7249],\n",
      "        [0.6999],\n",
      "        [0.7042],\n",
      "        [0.8029],\n",
      "        [0.8875],\n",
      "        [0.8444],\n",
      "        [0.6541],\n",
      "        [0.9128],\n",
      "        [0.7040],\n",
      "        [0.9144],\n",
      "        [0.8420],\n",
      "        [0.8967],\n",
      "        [0.6785],\n",
      "        [0.6926],\n",
      "        [0.6917],\n",
      "        [0.8808],\n",
      "        [0.8295],\n",
      "        [0.6493],\n",
      "        [0.7199],\n",
      "        [0.7183],\n",
      "        [0.9360],\n",
      "        [0.8732],\n",
      "        [0.6625],\n",
      "        [0.8147],\n",
      "        [0.6849],\n",
      "        [0.8694],\n",
      "        [0.7967],\n",
      "        [0.8164],\n",
      "        [0.7692],\n",
      "        [0.8545],\n",
      "        [0.6915],\n",
      "        [0.6697],\n",
      "        [0.9097],\n",
      "        [0.7831],\n",
      "        [0.7787],\n",
      "        [0.9179],\n",
      "        [0.9343],\n",
      "        [0.8243],\n",
      "        [0.8056],\n",
      "        [0.8904],\n",
      "        [0.8446],\n",
      "        [0.7733],\n",
      "        [0.7062],\n",
      "        [0.9394],\n",
      "        [0.7879],\n",
      "        [0.7102],\n",
      "        [0.6927],\n",
      "        [0.8016],\n",
      "        [0.8854],\n",
      "        [0.8179],\n",
      "        [0.8616],\n",
      "        [0.7777],\n",
      "        [0.7652],\n",
      "        [0.7787],\n",
      "        [0.7808],\n",
      "        [0.8866],\n",
      "        [0.8812],\n",
      "        [0.7789]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0019],\n",
      "        [0.0029],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0048],\n",
      "        [0.0067],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0085],\n",
      "        [0.0104],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0156],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0219],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0248],\n",
      "        [0.0253],\n",
      "        [0.0253],\n",
      "        [0.0258],\n",
      "        [0.0264],\n",
      "        [0.0325],\n",
      "        [0.0344],\n",
      "        [0.0365],\n",
      "        [0.0392],\n",
      "        [0.0399],\n",
      "        [0.0409],\n",
      "        [0.0409],\n",
      "        [0.0433],\n",
      "        [0.0451],\n",
      "        [0.0468],\n",
      "        [0.0476],\n",
      "        [0.0511],\n",
      "        [0.0515],\n",
      "        [0.0531],\n",
      "        [0.0558],\n",
      "        [0.0565],\n",
      "        [0.0573],\n",
      "        [0.0580],\n",
      "        [0.0613],\n",
      "        [0.0707],\n",
      "        [0.0725],\n",
      "        [0.0728],\n",
      "        [0.0741],\n",
      "        [0.0823]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0031],\n",
      "        [    0.0026],\n",
      "        [    0.0020],\n",
      "        [    0.0003],\n",
      "        [    0.0012],\n",
      "        [    0.0040],\n",
      "        [    0.0008],\n",
      "        [    0.0047],\n",
      "        [    0.0001],\n",
      "        [    0.0049],\n",
      "        [    0.0010],\n",
      "        [    0.0061],\n",
      "        [    0.0062],\n",
      "        [    0.0020],\n",
      "        [    0.0040],\n",
      "        [    0.0060],\n",
      "        [    0.0034],\n",
      "        [    0.0077],\n",
      "        [    0.0105],\n",
      "        [    0.0072],\n",
      "        [    0.0108],\n",
      "        [    0.0148],\n",
      "        [    0.0158],\n",
      "        [    0.0159],\n",
      "        [    0.0163],\n",
      "        [    0.0108],\n",
      "        [    0.0120],\n",
      "        [    0.0108],\n",
      "        [    0.0189],\n",
      "        [    0.0192],\n",
      "        [    0.0151],\n",
      "        [    0.0201],\n",
      "        [    0.0243],\n",
      "        [    0.0234],\n",
      "        [    0.0202],\n",
      "        [    0.0242],\n",
      "        [    0.0252],\n",
      "        [    0.0259],\n",
      "        [    0.0226],\n",
      "        [    0.0260],\n",
      "        [    0.0289],\n",
      "        [    0.0286],\n",
      "        [    0.0259],\n",
      "        [    0.0278],\n",
      "        [    0.0319],\n",
      "        [    0.0316],\n",
      "        [    0.0334],\n",
      "        [    0.0417],\n",
      "        [    0.0379],\n",
      "        [    0.0382],\n",
      "        [    0.0404],\n",
      "        [    0.0413],\n",
      "        [    0.0411],\n",
      "        [    0.0440],\n",
      "        [    0.0450],\n",
      "        [    0.0472],\n",
      "        [    0.0479],\n",
      "        [    0.0509],\n",
      "        [    0.0538],\n",
      "        [    0.0556],\n",
      "        [    0.0545],\n",
      "        [    0.0560],\n",
      "        [    0.0623],\n",
      "        [    0.0725],\n",
      "        [    0.0694],\n",
      "        [    0.0729],\n",
      "        [    0.0741],\n",
      "        [    0.0765]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.525338649749756\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 69\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.090360133408467e-09, 99)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [99, 63, 93, 77, 74, 25, 75, 34, 98, 101, 76, 62, 36, 24, 28, 90, 94, 56, 61, 89, 100, 53, 26, 46, 55, 57, 91, 92, 95, 87, 88, 49, 78, 73, 67, 48, 97, 33, 35, 58, 47, 32, 96, 86, 52, 72, 51, 23, 30, 27, 85, 68, 37, 54, 66, 103, 102, 22, 29, 31, 64, 65, 69, 21, 45, 59, 60, 106, 108] 數值 torch.Size([69, 1])\n",
      "目前模型的Data狀態 torch.Size([69, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6597],\n",
      "        [0.8614],\n",
      "        [0.7153],\n",
      "        [0.7228],\n",
      "        [0.7594],\n",
      "        [0.8046],\n",
      "        [0.7443],\n",
      "        [0.8118],\n",
      "        [0.6741],\n",
      "        [0.6508],\n",
      "        [0.7546],\n",
      "        [0.8874],\n",
      "        [0.8379],\n",
      "        [0.7977],\n",
      "        [0.8432],\n",
      "        [0.6973],\n",
      "        [0.7016],\n",
      "        [0.9131],\n",
      "        [0.9126],\n",
      "        [0.7012],\n",
      "        [0.6461],\n",
      "        [0.8782],\n",
      "        [0.8424],\n",
      "        [0.8313],\n",
      "        [0.8948],\n",
      "        [0.9350],\n",
      "        [0.6756],\n",
      "        [0.6897],\n",
      "        [0.6888],\n",
      "        [0.7166],\n",
      "        [0.7150],\n",
      "        [0.8725],\n",
      "        [0.6832],\n",
      "        [0.7681],\n",
      "        [0.8170],\n",
      "        [0.8704],\n",
      "        [0.6593],\n",
      "        [0.7948],\n",
      "        [0.8141],\n",
      "        [0.9095],\n",
      "        [0.8556],\n",
      "        [0.7816],\n",
      "        [0.6664],\n",
      "        [0.6878],\n",
      "        [0.9150],\n",
      "        [0.7781],\n",
      "        [0.9312],\n",
      "        [0.8075],\n",
      "        [0.8878],\n",
      "        [0.8440],\n",
      "        [0.7022],\n",
      "        [0.7753],\n",
      "        [0.8218],\n",
      "        [0.9366],\n",
      "        [0.7905],\n",
      "        [0.7063],\n",
      "        [0.6891],\n",
      "        [0.8038],\n",
      "        [0.8833],\n",
      "        [0.8588],\n",
      "        [0.8187],\n",
      "        [0.7796],\n",
      "        [0.7662],\n",
      "        [0.7839],\n",
      "        [0.7804],\n",
      "        [0.8865],\n",
      "        [0.8811],\n",
      "        [0.7731],\n",
      "        [0.7724]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0008],\n",
      "        [    0.0010],\n",
      "        [    0.0012],\n",
      "        [    0.0020],\n",
      "        [    0.0020],\n",
      "        [    0.0026],\n",
      "        [    0.0031],\n",
      "        [    0.0034],\n",
      "        [    0.0040],\n",
      "        [    0.0040],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0060],\n",
      "        [    0.0061],\n",
      "        [    0.0062],\n",
      "        [    0.0072],\n",
      "        [    0.0077],\n",
      "        [    0.0105],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0120],\n",
      "        [    0.0148],\n",
      "        [    0.0151],\n",
      "        [    0.0158],\n",
      "        [    0.0159],\n",
      "        [    0.0163],\n",
      "        [    0.0189],\n",
      "        [    0.0192],\n",
      "        [    0.0201],\n",
      "        [    0.0202],\n",
      "        [    0.0226],\n",
      "        [    0.0234],\n",
      "        [    0.0242],\n",
      "        [    0.0243],\n",
      "        [    0.0252],\n",
      "        [    0.0259],\n",
      "        [    0.0259],\n",
      "        [    0.0260],\n",
      "        [    0.0278],\n",
      "        [    0.0286],\n",
      "        [    0.0289],\n",
      "        [    0.0316],\n",
      "        [    0.0319],\n",
      "        [    0.0334],\n",
      "        [    0.0379],\n",
      "        [    0.0382],\n",
      "        [    0.0404],\n",
      "        [    0.0411],\n",
      "        [    0.0413],\n",
      "        [    0.0417],\n",
      "        [    0.0440],\n",
      "        [    0.0450],\n",
      "        [    0.0472],\n",
      "        [    0.0479],\n",
      "        [    0.0509],\n",
      "        [    0.0538],\n",
      "        [    0.0545],\n",
      "        [    0.0556],\n",
      "        [    0.0560],\n",
      "        [    0.0623],\n",
      "        [    0.0694],\n",
      "        [    0.0725],\n",
      "        [    0.0729],\n",
      "        [    0.0741],\n",
      "        [    0.0765],\n",
      "        [    0.0832]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0020],\n",
      "        [0.0015],\n",
      "        [0.0015],\n",
      "        [0.0004],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0031],\n",
      "        [0.0047],\n",
      "        [0.0013],\n",
      "        [0.0041],\n",
      "        [0.0060],\n",
      "        [0.0053],\n",
      "        [0.0085],\n",
      "        [0.0058],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0081],\n",
      "        [0.0059],\n",
      "        [0.0114],\n",
      "        [0.0087],\n",
      "        [0.0103],\n",
      "        [0.0128],\n",
      "        [0.0080],\n",
      "        [0.0145],\n",
      "        [0.0162],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0217],\n",
      "        [0.0211],\n",
      "        [0.0247],\n",
      "        [0.0279],\n",
      "        [0.0277],\n",
      "        [0.0257],\n",
      "        [0.0252],\n",
      "        [0.0261],\n",
      "        [0.0240],\n",
      "        [0.0295],\n",
      "        [0.0272],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0344],\n",
      "        [0.0323],\n",
      "        [0.0341],\n",
      "        [0.0373],\n",
      "        [0.0413],\n",
      "        [0.0392],\n",
      "        [0.0373],\n",
      "        [0.0420],\n",
      "        [0.0436],\n",
      "        [0.0401],\n",
      "        [0.0443],\n",
      "        [0.0453],\n",
      "        [0.0469],\n",
      "        [0.0534],\n",
      "        [0.0536],\n",
      "        [0.0526],\n",
      "        [0.0520],\n",
      "        [0.0657],\n",
      "        [0.0646],\n",
      "        [0.0766],\n",
      "        [0.0709],\n",
      "        [0.0722],\n",
      "        [0.0715],\n",
      "        [0.0776]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.794390678405762\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 70\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4194881714502117e-07, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 101, 25, 75, 93, 77, 99, 63, 34, 76, 98, 36, 28, 61, 62, 90, 94, 46, 56, 24, 100, 53, 89, 26, 55, 57, 91, 92, 95, 87, 88, 78, 49, 58, 73, 33, 97, 35, 32, 48, 67, 47, 96, 86, 52, 51, 23, 72, 68, 30, 85, 66, 27, 37, 54, 103, 102, 22, 65, 64, 29, 31, 21, 69, 59, 106, 60, 45, 108, 107] 數值 torch.Size([70, 1])\n",
      "目前模型的Data狀態 torch.Size([70, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7602],\n",
      "        [0.6487],\n",
      "        [0.8079],\n",
      "        [0.7449],\n",
      "        [0.7146],\n",
      "        [0.7233],\n",
      "        [0.6580],\n",
      "        [0.8637],\n",
      "        [0.8114],\n",
      "        [0.7545],\n",
      "        [0.6725],\n",
      "        [0.8372],\n",
      "        [0.8434],\n",
      "        [0.9144],\n",
      "        [0.8893],\n",
      "        [0.6967],\n",
      "        [0.7007],\n",
      "        [0.8353],\n",
      "        [0.9140],\n",
      "        [0.8013],\n",
      "        [0.6441],\n",
      "        [0.8777],\n",
      "        [0.7003],\n",
      "        [0.8443],\n",
      "        [0.8951],\n",
      "        [0.9361],\n",
      "        [0.6745],\n",
      "        [0.6884],\n",
      "        [0.6877],\n",
      "        [0.7152],\n",
      "        [0.7137],\n",
      "        [0.6841],\n",
      "        [0.8741],\n",
      "        [0.9115],\n",
      "        [0.7702],\n",
      "        [0.7948],\n",
      "        [0.6578],\n",
      "        [0.8139],\n",
      "        [0.7822],\n",
      "        [0.8740],\n",
      "        [0.8215],\n",
      "        [0.8592],\n",
      "        [0.6651],\n",
      "        [0.6862],\n",
      "        [0.9143],\n",
      "        [0.9301],\n",
      "        [0.8114],\n",
      "        [0.7805],\n",
      "        [0.7794],\n",
      "        [0.8869],\n",
      "        [0.7002],\n",
      "        [0.7953],\n",
      "        [0.8450],\n",
      "        [0.8215],\n",
      "        [0.9362],\n",
      "        [0.7034],\n",
      "        [0.6864],\n",
      "        [0.8078],\n",
      "        [0.7836],\n",
      "        [0.8218],\n",
      "        [0.8829],\n",
      "        [0.8579],\n",
      "        [0.7888],\n",
      "        [0.7695],\n",
      "        [0.8885],\n",
      "        [0.7681],\n",
      "        [0.8830],\n",
      "        [0.7845],\n",
      "        [0.7669],\n",
      "        [0.7698]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0031],\n",
      "        [0.0041],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0103],\n",
      "        [0.0114],\n",
      "        [0.0128],\n",
      "        [0.0145],\n",
      "        [0.0162],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0217],\n",
      "        [0.0240],\n",
      "        [0.0247],\n",
      "        [0.0252],\n",
      "        [0.0257],\n",
      "        [0.0261],\n",
      "        [0.0272],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0295],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0323],\n",
      "        [0.0341],\n",
      "        [0.0344],\n",
      "        [0.0373],\n",
      "        [0.0373],\n",
      "        [0.0392],\n",
      "        [0.0401],\n",
      "        [0.0413],\n",
      "        [0.0420],\n",
      "        [0.0436],\n",
      "        [0.0443],\n",
      "        [0.0453],\n",
      "        [0.0469],\n",
      "        [0.0520],\n",
      "        [0.0526],\n",
      "        [0.0534],\n",
      "        [0.0536],\n",
      "        [0.0646],\n",
      "        [0.0657],\n",
      "        [0.0709],\n",
      "        [0.0715],\n",
      "        [0.0722],\n",
      "        [0.0766],\n",
      "        [0.0776],\n",
      "        [0.0791]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0026],\n",
      "        [0.0024],\n",
      "        [0.0019],\n",
      "        [0.0037],\n",
      "        [0.0010],\n",
      "        [0.0051],\n",
      "        [0.0024],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0079],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0061],\n",
      "        [0.0059],\n",
      "        [0.0085],\n",
      "        [0.0095],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0099],\n",
      "        [0.0050],\n",
      "        [0.0080],\n",
      "        [0.0135],\n",
      "        [0.0123],\n",
      "        [0.0161],\n",
      "        [0.0150],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0197],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0211],\n",
      "        [0.0214],\n",
      "        [0.0242],\n",
      "        [0.0258],\n",
      "        [0.0269],\n",
      "        [0.0286],\n",
      "        [0.0280],\n",
      "        [0.0284],\n",
      "        [0.0293],\n",
      "        [0.0304],\n",
      "        [0.0310],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0280],\n",
      "        [0.0291],\n",
      "        [0.0324],\n",
      "        [0.0358],\n",
      "        [0.0351],\n",
      "        [0.0343],\n",
      "        [0.0360],\n",
      "        [0.0374],\n",
      "        [0.0399],\n",
      "        [0.0440],\n",
      "        [0.0413],\n",
      "        [0.0392],\n",
      "        [0.0407],\n",
      "        [0.0453],\n",
      "        [0.0500],\n",
      "        [0.0515],\n",
      "        [0.0507],\n",
      "        [0.0508],\n",
      "        [0.0621],\n",
      "        [0.0674],\n",
      "        [0.0711],\n",
      "        [0.0644],\n",
      "        [0.0724],\n",
      "        [0.0787],\n",
      "        [0.0698],\n",
      "        [0.0717]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.064230918884277\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 71\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.482014676592371e-07, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 77, 75, 63, 25, 101, 93, 100, 99, 34, 76, 62, 46, 61, 56, 36, 28, 98, 53, 90, 94, 24, 26, 89, 57, 55, 91, 95, 92, 78, 49, 87, 88, 58, 73, 33, 52, 35, 32, 97, 51, 48, 67, 47, 23, 96, 86, 30, 68, 72, 85, 66, 103, 27, 102, 54, 37, 22, 65, 29, 31, 64, 21, 106, 69, 108, 59, 107, 60, 45, 104] 數值 torch.Size([71, 1])\n",
      "目前模型的Data狀態 torch.Size([71, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7600],\n",
      "        [0.7228],\n",
      "        [0.7444],\n",
      "        [0.8640],\n",
      "        [0.8089],\n",
      "        [0.6449],\n",
      "        [0.7124],\n",
      "        [0.6403],\n",
      "        [0.6547],\n",
      "        [0.8092],\n",
      "        [0.7533],\n",
      "        [0.8892],\n",
      "        [0.8373],\n",
      "        [0.9142],\n",
      "        [0.9129],\n",
      "        [0.8350],\n",
      "        [0.8414],\n",
      "        [0.6693],\n",
      "        [0.8754],\n",
      "        [0.6948],\n",
      "        [0.6982],\n",
      "        [0.8027],\n",
      "        [0.8439],\n",
      "        [0.6982],\n",
      "        [0.9350],\n",
      "        [0.8935],\n",
      "        [0.6721],\n",
      "        [0.6853],\n",
      "        [0.6857],\n",
      "        [0.6841],\n",
      "        [0.8738],\n",
      "        [0.7128],\n",
      "        [0.7111],\n",
      "        [0.9112],\n",
      "        [0.7714],\n",
      "        [0.7931],\n",
      "        [0.9115],\n",
      "        [0.8120],\n",
      "        [0.7810],\n",
      "        [0.6550],\n",
      "        [0.9270],\n",
      "        [0.8755],\n",
      "        [0.8240],\n",
      "        [0.8606],\n",
      "        [0.8130],\n",
      "        [0.6624],\n",
      "        [0.6835],\n",
      "        [0.8839],\n",
      "        [0.7816],\n",
      "        [0.7820],\n",
      "        [0.6971],\n",
      "        [0.7981],\n",
      "        [0.6984],\n",
      "        [0.8435],\n",
      "        [0.6819],\n",
      "        [0.9338],\n",
      "        [0.8195],\n",
      "        [0.8095],\n",
      "        [0.7856],\n",
      "        [0.8803],\n",
      "        [0.8551],\n",
      "        [0.8229],\n",
      "        [0.7912],\n",
      "        [0.7610],\n",
      "        [0.7712],\n",
      "        [0.7590],\n",
      "        [0.8883],\n",
      "        [0.7624],\n",
      "        [0.8828],\n",
      "        [0.7866],\n",
      "        [0.7463]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0010],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0037],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0069],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0085],\n",
      "        [0.0095],\n",
      "        [0.0099],\n",
      "        [0.0123],\n",
      "        [0.0135],\n",
      "        [0.0150],\n",
      "        [0.0161],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0211],\n",
      "        [0.0214],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0242],\n",
      "        [0.0258],\n",
      "        [0.0269],\n",
      "        [0.0280],\n",
      "        [0.0280],\n",
      "        [0.0284],\n",
      "        [0.0286],\n",
      "        [0.0291],\n",
      "        [0.0293],\n",
      "        [0.0304],\n",
      "        [0.0310],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0343],\n",
      "        [0.0351],\n",
      "        [0.0358],\n",
      "        [0.0360],\n",
      "        [0.0374],\n",
      "        [0.0392],\n",
      "        [0.0399],\n",
      "        [0.0407],\n",
      "        [0.0413],\n",
      "        [0.0440],\n",
      "        [0.0453],\n",
      "        [0.0500],\n",
      "        [0.0507],\n",
      "        [0.0508],\n",
      "        [0.0515],\n",
      "        [0.0621],\n",
      "        [0.0644],\n",
      "        [0.0674],\n",
      "        [0.0698],\n",
      "        [0.0711],\n",
      "        [0.0717],\n",
      "        [0.0724],\n",
      "        [0.0787],\n",
      "        [0.0873]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0024],\n",
      "        [0.0008],\n",
      "        [0.0041],\n",
      "        [0.0007],\n",
      "        [0.0014],\n",
      "        [0.0079],\n",
      "        [0.0072],\n",
      "        [0.0003],\n",
      "        [0.0098],\n",
      "        [0.0083],\n",
      "        [0.0080],\n",
      "        [0.0038],\n",
      "        [0.0061],\n",
      "        [0.0082],\n",
      "        [0.0041],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0124],\n",
      "        [0.0044],\n",
      "        [0.0113],\n",
      "        [0.0134],\n",
      "        [0.0094],\n",
      "        [0.0097],\n",
      "        [0.0164],\n",
      "        [0.0119],\n",
      "        [0.0192],\n",
      "        [0.0225],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0198],\n",
      "        [0.0193],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0255],\n",
      "        [0.0297],\n",
      "        [0.0241],\n",
      "        [0.0311],\n",
      "        [0.0308],\n",
      "        [0.0327],\n",
      "        [0.0248],\n",
      "        [0.0289],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0328],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0303],\n",
      "        [0.0350],\n",
      "        [0.0357],\n",
      "        [0.0319],\n",
      "        [0.0371],\n",
      "        [0.0325],\n",
      "        [0.0366],\n",
      "        [0.0346],\n",
      "        [0.0376],\n",
      "        [0.0469],\n",
      "        [0.0458],\n",
      "        [0.0504],\n",
      "        [0.0468],\n",
      "        [0.0470],\n",
      "        [0.0526],\n",
      "        [0.0618],\n",
      "        [0.0561],\n",
      "        [0.0673],\n",
      "        [0.0609],\n",
      "        [0.0733],\n",
      "        [0.0632],\n",
      "        [0.0746],\n",
      "        [0.0790],\n",
      "        [0.0800]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.33453631401062\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 72\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.001726701588268e-07, 100)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [100, 63, 77, 25, 74, 62, 56, 75, 53, 46, 93, 101, 76, 61, 34, 24, 26, 99, 36, 90, 28, 57, 98, 94, 89, 55, 49, 78, 91, 95, 92, 52, 51, 73, 87, 88, 58, 48, 33, 30, 67, 47, 32, 35, 85, 103, 97, 23, 102, 68, 72, 96, 86, 27, 66, 54, 22, 29, 37, 31, 65, 64, 106, 108, 21, 107, 69, 59, 60, 45, 104, 50] 數值 torch.Size([72, 1])\n",
      "目前模型的Data狀態 torch.Size([72, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6350],\n",
      "        [0.8624],\n",
      "        [0.7210],\n",
      "        [0.8079],\n",
      "        [0.7582],\n",
      "        [0.8871],\n",
      "        [0.9100],\n",
      "        [0.7422],\n",
      "        [0.8718],\n",
      "        [0.8372],\n",
      "        [0.7089],\n",
      "        [0.6395],\n",
      "        [0.7506],\n",
      "        [0.9121],\n",
      "        [0.8062],\n",
      "        [0.8022],\n",
      "        [0.8413],\n",
      "        [0.6499],\n",
      "        [0.8318],\n",
      "        [0.6921],\n",
      "        [0.8379],\n",
      "        [0.9319],\n",
      "        [0.6648],\n",
      "        [0.6943],\n",
      "        [0.6954],\n",
      "        [0.8904],\n",
      "        [0.8717],\n",
      "        [0.6828],\n",
      "        [0.6689],\n",
      "        [0.6816],\n",
      "        [0.6819],\n",
      "        [0.9076],\n",
      "        [0.9227],\n",
      "        [0.7710],\n",
      "        [0.7096],\n",
      "        [0.7079],\n",
      "        [0.9088],\n",
      "        [0.8752],\n",
      "        [0.7903],\n",
      "        [0.8799],\n",
      "        [0.8240],\n",
      "        [0.8602],\n",
      "        [0.7786],\n",
      "        [0.8089],\n",
      "        [0.6930],\n",
      "        [0.6917],\n",
      "        [0.6508],\n",
      "        [0.8127],\n",
      "        [0.6757],\n",
      "        [0.7816],\n",
      "        [0.7818],\n",
      "        [0.6585],\n",
      "        [0.6802],\n",
      "        [0.8403],\n",
      "        [0.7984],\n",
      "        [0.9302],\n",
      "        [0.8090],\n",
      "        [0.8763],\n",
      "        [0.8166],\n",
      "        [0.8512],\n",
      "        [0.7852],\n",
      "        [0.8218],\n",
      "        [0.7527],\n",
      "        [0.7501],\n",
      "        [0.7916],\n",
      "        [0.7539],\n",
      "        [0.7711],\n",
      "        [0.8861],\n",
      "        [0.8807],\n",
      "        [0.7869],\n",
      "        [0.7390],\n",
      "        [0.9725]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0024],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0044],\n",
      "        [0.0061],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0124],\n",
      "        [0.0134],\n",
      "        [0.0164],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0198],\n",
      "        [0.0225],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0241],\n",
      "        [0.0248],\n",
      "        [0.0255],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0289],\n",
      "        [0.0297],\n",
      "        [0.0303],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0319],\n",
      "        [0.0325],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0357],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0371],\n",
      "        [0.0376],\n",
      "        [0.0458],\n",
      "        [0.0468],\n",
      "        [0.0469],\n",
      "        [0.0470],\n",
      "        [0.0504],\n",
      "        [0.0526],\n",
      "        [0.0561],\n",
      "        [0.0609],\n",
      "        [0.0618],\n",
      "        [0.0632],\n",
      "        [0.0673],\n",
      "        [0.0733],\n",
      "        [0.0746],\n",
      "        [0.0790],\n",
      "        [0.0800],\n",
      "        [0.0833]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0007],\n",
      "        [0.0020],\n",
      "        [0.0019],\n",
      "        [0.0004],\n",
      "        [0.0040],\n",
      "        [0.0004],\n",
      "        [0.0074],\n",
      "        [0.0064],\n",
      "        [0.0083],\n",
      "        [0.0070],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0093],\n",
      "        [0.0074],\n",
      "        [0.0097],\n",
      "        [0.0133],\n",
      "        [0.0095],\n",
      "        [0.0139],\n",
      "        [0.0083],\n",
      "        [0.0121],\n",
      "        [0.0131],\n",
      "        [0.0147],\n",
      "        [0.0228],\n",
      "        [0.0157],\n",
      "        [0.0220],\n",
      "        [0.0208],\n",
      "        [0.0231],\n",
      "        [0.0225],\n",
      "        [0.0196],\n",
      "        [0.0194],\n",
      "        [0.0269],\n",
      "        [0.0243],\n",
      "        [0.0248],\n",
      "        [0.0295],\n",
      "        [0.0269],\n",
      "        [0.0317],\n",
      "        [0.0270],\n",
      "        [0.0311],\n",
      "        [0.0290],\n",
      "        [0.0326],\n",
      "        [0.0336],\n",
      "        [0.0324],\n",
      "        [0.0304],\n",
      "        [0.0320],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0336],\n",
      "        [0.0371],\n",
      "        [0.0357],\n",
      "        [0.0347],\n",
      "        [0.0339],\n",
      "        [0.0362],\n",
      "        [0.0334],\n",
      "        [0.0461],\n",
      "        [0.0435],\n",
      "        [0.0493],\n",
      "        [0.0437],\n",
      "        [0.0502],\n",
      "        [0.0533],\n",
      "        [0.0524],\n",
      "        [0.0568],\n",
      "        [0.0614],\n",
      "        [0.0595],\n",
      "        [0.0686],\n",
      "        [0.0753],\n",
      "        [0.0763],\n",
      "        [0.0785],\n",
      "        [0.0771],\n",
      "        [0.0772]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.60496735572815\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 73\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.672380278705532e-07, 53)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [53, 56, 100, 25, 63, 77, 62, 74, 75, 93, 76, 46, 26, 57, 101, 24, 90, 99, 61, 34, 98, 94, 36, 28, 89, 49, 51, 52, 91, 78, 92, 55, 95, 87, 88, 73, 48, 30, 47, 58, 103, 67, 33, 97, 85, 32, 23, 102, 54, 35, 68, 27, 86, 96, 66, 72, 29, 31, 22, 37, 65, 106, 64, 108, 107, 21, 69, 59, 60, 104, 50, 45, 105] 數值 torch.Size([73, 1])\n",
      "目前模型的Data狀態 torch.Size([73, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8678],\n",
      "        [0.9063],\n",
      "        [0.6348],\n",
      "        [0.8073],\n",
      "        [0.8609],\n",
      "        [0.7229],\n",
      "        [0.8852],\n",
      "        [0.7586],\n",
      "        [0.7423],\n",
      "        [0.7097],\n",
      "        [0.7516],\n",
      "        [0.8359],\n",
      "        [0.8390],\n",
      "        [0.9282],\n",
      "        [0.6391],\n",
      "        [0.8022],\n",
      "        [0.6938],\n",
      "        [0.6501],\n",
      "        [0.9099],\n",
      "        [0.8040],\n",
      "        [0.6651],\n",
      "        [0.6946],\n",
      "        [0.8292],\n",
      "        [0.8353],\n",
      "        [0.6971],\n",
      "        [0.8681],\n",
      "        [0.9173],\n",
      "        [0.9031],\n",
      "        [0.6706],\n",
      "        [0.6850],\n",
      "        [0.6831],\n",
      "        [0.8868],\n",
      "        [0.6819],\n",
      "        [0.7111],\n",
      "        [0.7094],\n",
      "        [0.7724],\n",
      "        [0.8732],\n",
      "        [0.8766],\n",
      "        [0.8586],\n",
      "        [0.9059],\n",
      "        [0.6896],\n",
      "        [0.8247],\n",
      "        [0.7883],\n",
      "        [0.6515],\n",
      "        [0.6934],\n",
      "        [0.7769],\n",
      "        [0.8126],\n",
      "        [0.6745],\n",
      "        [0.9260],\n",
      "        [0.8064],\n",
      "        [0.7830],\n",
      "        [0.8376],\n",
      "        [0.6820],\n",
      "        [0.6593],\n",
      "        [0.7993],\n",
      "        [0.7832],\n",
      "        [0.8730],\n",
      "        [0.8479],\n",
      "        [0.8087],\n",
      "        [0.8142],\n",
      "        [0.7854],\n",
      "        [0.7490],\n",
      "        [0.8210],\n",
      "        [0.7461],\n",
      "        [0.7502],\n",
      "        [0.7919],\n",
      "        [0.7725],\n",
      "        [0.8841],\n",
      "        [0.8789],\n",
      "        [0.7361],\n",
      "        [0.9664],\n",
      "        [0.7864],\n",
      "        [0.7628]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0004],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0040],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0093],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0121],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0147],\n",
      "        [0.0157],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0208],\n",
      "        [0.0220],\n",
      "        [0.0225],\n",
      "        [0.0228],\n",
      "        [0.0231],\n",
      "        [0.0243],\n",
      "        [0.0248],\n",
      "        [0.0269],\n",
      "        [0.0269],\n",
      "        [0.0270],\n",
      "        [0.0290],\n",
      "        [0.0295],\n",
      "        [0.0304],\n",
      "        [0.0311],\n",
      "        [0.0317],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0347],\n",
      "        [0.0357],\n",
      "        [0.0362],\n",
      "        [0.0371],\n",
      "        [0.0435],\n",
      "        [0.0437],\n",
      "        [0.0461],\n",
      "        [0.0493],\n",
      "        [0.0502],\n",
      "        [0.0524],\n",
      "        [0.0533],\n",
      "        [0.0568],\n",
      "        [0.0595],\n",
      "        [0.0614],\n",
      "        [0.0686],\n",
      "        [0.0753],\n",
      "        [0.0763],\n",
      "        [0.0771],\n",
      "        [0.0772],\n",
      "        [0.0785],\n",
      "        [0.0834]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0005],\n",
      "        [0.0023],\n",
      "        [0.0030],\n",
      "        [0.0007],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0011],\n",
      "        [0.0037],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0043],\n",
      "        [0.0081],\n",
      "        [0.0081],\n",
      "        [0.0102],\n",
      "        [0.0123],\n",
      "        [0.0095],\n",
      "        [0.0110],\n",
      "        [0.0095],\n",
      "        [0.0101],\n",
      "        [0.0132],\n",
      "        [0.0139],\n",
      "        [0.0131],\n",
      "        [0.0138],\n",
      "        [0.0149],\n",
      "        [0.0163],\n",
      "        [0.0177],\n",
      "        [0.0184],\n",
      "        [0.0209],\n",
      "        [0.0237],\n",
      "        [0.0230],\n",
      "        [0.0229],\n",
      "        [0.0238],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0292],\n",
      "        [0.0294],\n",
      "        [0.0264],\n",
      "        [0.0317],\n",
      "        [0.0290],\n",
      "        [0.0270],\n",
      "        [0.0340],\n",
      "        [0.0310],\n",
      "        [0.0328],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0327],\n",
      "        [0.0332],\n",
      "        [0.0305],\n",
      "        [0.0341],\n",
      "        [0.0353],\n",
      "        [0.0363],\n",
      "        [0.0329],\n",
      "        [0.0396],\n",
      "        [0.0429],\n",
      "        [0.0432],\n",
      "        [0.0433],\n",
      "        [0.0488],\n",
      "        [0.0474],\n",
      "        [0.0473],\n",
      "        [0.0512],\n",
      "        [0.0513],\n",
      "        [0.0543],\n",
      "        [0.0578],\n",
      "        [0.0716],\n",
      "        [0.0744],\n",
      "        [0.0753],\n",
      "        [0.0729],\n",
      "        [0.0750],\n",
      "        [0.0822],\n",
      "        [0.0785]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.878023386001587\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 74\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.842655698666931e-08, 53)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [53, 56, 63, 74, 100, 77, 62, 25, 75, 46, 93, 76, 26, 57, 90, 61, 34, 101, 99, 24, 36, 98, 28, 94, 89, 49, 51, 52, 91, 55, 92, 78, 95, 87, 88, 30, 103, 58, 73, 48, 23, 68, 102, 85, 33, 32, 47, 54, 97, 66, 35, 67, 27, 86, 96, 72, 29, 31, 22, 106, 65, 37, 64, 108, 107, 21, 69, 104, 59, 50, 60, 17, 105, 45] 數值 torch.Size([74, 1])\n",
      "目前模型的Data狀態 torch.Size([74, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8672],\n",
      "        [0.9064],\n",
      "        [0.8623],\n",
      "        [0.7595],\n",
      "        [0.6330],\n",
      "        [0.7242],\n",
      "        [0.8862],\n",
      "        [0.8095],\n",
      "        [0.7426],\n",
      "        [0.8390],\n",
      "        [0.7093],\n",
      "        [0.7517],\n",
      "        [0.8396],\n",
      "        [0.9281],\n",
      "        [0.6939],\n",
      "        [0.9108],\n",
      "        [0.8044],\n",
      "        [0.6373],\n",
      "        [0.6487],\n",
      "        [0.8051],\n",
      "        [0.8295],\n",
      "        [0.6640],\n",
      "        [0.8354],\n",
      "        [0.6939],\n",
      "        [0.6969],\n",
      "        [0.8687],\n",
      "        [0.9155],\n",
      "        [0.9019],\n",
      "        [0.6705],\n",
      "        [0.8867],\n",
      "        [0.6826],\n",
      "        [0.6867],\n",
      "        [0.6812],\n",
      "        [0.7106],\n",
      "        [0.7090],\n",
      "        [0.8760],\n",
      "        [0.6861],\n",
      "        [0.9065],\n",
      "        [0.7748],\n",
      "        [0.8757],\n",
      "        [0.8157],\n",
      "        [0.7861],\n",
      "        [0.6718],\n",
      "        [0.6920],\n",
      "        [0.7889],\n",
      "        [0.7779],\n",
      "        [0.8614],\n",
      "        [0.9252],\n",
      "        [0.6508],\n",
      "        [0.8026],\n",
      "        [0.8068],\n",
      "        [0.8277],\n",
      "        [0.8378],\n",
      "        [0.6815],\n",
      "        [0.6587],\n",
      "        [0.7858],\n",
      "        [0.8724],\n",
      "        [0.8475],\n",
      "        [0.8115],\n",
      "        [0.7439],\n",
      "        [0.7882],\n",
      "        [0.8147],\n",
      "        [0.8232],\n",
      "        [0.7405],\n",
      "        [0.7450],\n",
      "        [0.7955],\n",
      "        [0.7754],\n",
      "        [0.7319],\n",
      "        [0.8850],\n",
      "        [0.9642],\n",
      "        [0.8799],\n",
      "        [0.8105],\n",
      "        [0.7580],\n",
      "        [0.7901]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0037],\n",
      "        [0.0043],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0081],\n",
      "        [0.0081],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0110],\n",
      "        [0.0123],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0149],\n",
      "        [0.0163],\n",
      "        [0.0177],\n",
      "        [0.0184],\n",
      "        [0.0209],\n",
      "        [0.0229],\n",
      "        [0.0230],\n",
      "        [0.0237],\n",
      "        [0.0238],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0264],\n",
      "        [0.0270],\n",
      "        [0.0290],\n",
      "        [0.0292],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0315],\n",
      "        [0.0317],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0329],\n",
      "        [0.0332],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0353],\n",
      "        [0.0363],\n",
      "        [0.0396],\n",
      "        [0.0429],\n",
      "        [0.0432],\n",
      "        [0.0433],\n",
      "        [0.0473],\n",
      "        [0.0474],\n",
      "        [0.0488],\n",
      "        [0.0512],\n",
      "        [0.0513],\n",
      "        [0.0543],\n",
      "        [0.0578],\n",
      "        [0.0716],\n",
      "        [0.0729],\n",
      "        [0.0744],\n",
      "        [0.0750],\n",
      "        [0.0753],\n",
      "        [0.0784],\n",
      "        [0.0785],\n",
      "        [0.0822]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0018],\n",
      "        [0.0001],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0044],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0055],\n",
      "        [0.0045],\n",
      "        [0.0017],\n",
      "        [0.0074],\n",
      "        [0.0078],\n",
      "        [0.0086],\n",
      "        [0.0075],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0107],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0154],\n",
      "        [0.0142],\n",
      "        [0.0148],\n",
      "        [0.0140],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0161],\n",
      "        [0.0152],\n",
      "        [0.0164],\n",
      "        [0.0213],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0254],\n",
      "        [0.0234],\n",
      "        [0.0289],\n",
      "        [0.0305],\n",
      "        [0.0314],\n",
      "        [0.0265],\n",
      "        [0.0276],\n",
      "        [0.0276],\n",
      "        [0.0293],\n",
      "        [0.0312],\n",
      "        [0.0311],\n",
      "        [0.0338],\n",
      "        [0.0310],\n",
      "        [0.0340],\n",
      "        [0.0297],\n",
      "        [0.0340],\n",
      "        [0.0369],\n",
      "        [0.0340],\n",
      "        [0.0360],\n",
      "        [0.0374],\n",
      "        [0.0411],\n",
      "        [0.0419],\n",
      "        [0.0421],\n",
      "        [0.0403],\n",
      "        [0.0420],\n",
      "        [0.0452],\n",
      "        [0.0496],\n",
      "        [0.0498],\n",
      "        [0.0454],\n",
      "        [0.0488],\n",
      "        [0.0537],\n",
      "        [0.0741],\n",
      "        [0.0685],\n",
      "        [0.0741],\n",
      "        [0.0720],\n",
      "        [0.0750],\n",
      "        [0.0716],\n",
      "        [0.0734],\n",
      "        [0.0854]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.1518771648407\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 75\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1377420722501483e-08, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 63, 74, 46, 53, 77, 62, 100, 75, 25, 93, 57, 76, 26, 61, 90, 34, 101, 99, 28, 36, 98, 94, 51, 89, 24, 49, 52, 91, 103, 55, 92, 78, 95, 30, 87, 88, 23, 102, 68, 58, 85, 66, 73, 54, 32, 33, 48, 47, 97, 35, 27, 86, 67, 96, 22, 72, 29, 106, 31, 65, 108, 107, 37, 64, 21, 104, 17, 50, 105, 59, 69, 60, 45, 20] 數值 torch.Size([75, 1])\n",
      "目前模型的Data狀態 torch.Size([75, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9058],\n",
      "        [0.8628],\n",
      "        [0.7590],\n",
      "        [0.8416],\n",
      "        [0.8656],\n",
      "        [0.7245],\n",
      "        [0.8864],\n",
      "        [0.6309],\n",
      "        [0.7418],\n",
      "        [0.8120],\n",
      "        [0.7087],\n",
      "        [0.9275],\n",
      "        [0.7508],\n",
      "        [0.8401],\n",
      "        [0.9108],\n",
      "        [0.6938],\n",
      "        [0.8038],\n",
      "        [0.6352],\n",
      "        [0.6470],\n",
      "        [0.8352],\n",
      "        [0.8284],\n",
      "        [0.6623],\n",
      "        [0.6929],\n",
      "        [0.9130],\n",
      "        [0.6964],\n",
      "        [0.8082],\n",
      "        [0.8686],\n",
      "        [0.8999],\n",
      "        [0.6702],\n",
      "        [0.6825],\n",
      "        [0.8858],\n",
      "        [0.6817],\n",
      "        [0.6874],\n",
      "        [0.6803],\n",
      "        [0.8750],\n",
      "        [0.7099],\n",
      "        [0.7083],\n",
      "        [0.8189],\n",
      "        [0.6687],\n",
      "        [0.7891],\n",
      "        [0.9066],\n",
      "        [0.6904],\n",
      "        [0.8058],\n",
      "        [0.7760],\n",
      "        [0.9236],\n",
      "        [0.7783],\n",
      "        [0.7887],\n",
      "        [0.8776],\n",
      "        [0.8634],\n",
      "        [0.6496],\n",
      "        [0.8061],\n",
      "        [0.8376],\n",
      "        [0.6807],\n",
      "        [0.8305],\n",
      "        [0.6577],\n",
      "        [0.8145],\n",
      "        [0.7872],\n",
      "        [0.8714],\n",
      "        [0.7386],\n",
      "        [0.8464],\n",
      "        [0.7905],\n",
      "        [0.7347],\n",
      "        [0.7395],\n",
      "        [0.8139],\n",
      "        [0.8245],\n",
      "        [0.7997],\n",
      "        [0.7275],\n",
      "        [0.8174],\n",
      "        [0.9613],\n",
      "        [0.7528],\n",
      "        [0.8853],\n",
      "        [0.7780],\n",
      "        [0.8803],\n",
      "        [0.7933],\n",
      "        [0.8268]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0044],\n",
      "        [0.0045],\n",
      "        [0.0055],\n",
      "        [0.0074],\n",
      "        [0.0075],\n",
      "        [0.0078],\n",
      "        [0.0086],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0107],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0140],\n",
      "        [0.0142],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0161],\n",
      "        [0.0164],\n",
      "        [0.0213],\n",
      "        [0.0234],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0265],\n",
      "        [0.0276],\n",
      "        [0.0276],\n",
      "        [0.0289],\n",
      "        [0.0293],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0360],\n",
      "        [0.0369],\n",
      "        [0.0374],\n",
      "        [0.0403],\n",
      "        [0.0411],\n",
      "        [0.0419],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0452],\n",
      "        [0.0454],\n",
      "        [0.0488],\n",
      "        [0.0496],\n",
      "        [0.0498],\n",
      "        [0.0537],\n",
      "        [0.0685],\n",
      "        [0.0716],\n",
      "        [0.0720],\n",
      "        [0.0734],\n",
      "        [0.0741],\n",
      "        [0.0741],\n",
      "        [0.0750],\n",
      "        [0.0854],\n",
      "        [0.0891]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0016],\n",
      "        [0.0024],\n",
      "        [0.0010],\n",
      "        [0.0032],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0058],\n",
      "        [0.0057],\n",
      "        [0.0091],\n",
      "        [0.0075],\n",
      "        [0.0071],\n",
      "        [0.0087],\n",
      "        [0.0100],\n",
      "        [0.0094],\n",
      "        [0.0088],\n",
      "        [0.0108],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0132],\n",
      "        [0.0151],\n",
      "        [0.0158],\n",
      "        [0.0152],\n",
      "        [0.0130],\n",
      "        [0.0149],\n",
      "        [0.0194],\n",
      "        [0.0162],\n",
      "        [0.0147],\n",
      "        [0.0208],\n",
      "        [0.0206],\n",
      "        [0.0247],\n",
      "        [0.0241],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0252],\n",
      "        [0.0255],\n",
      "        [0.0258],\n",
      "        [0.0223],\n",
      "        [0.0254],\n",
      "        [0.0241],\n",
      "        [0.0285],\n",
      "        [0.0284],\n",
      "        [0.0261],\n",
      "        [0.0312],\n",
      "        [0.0294],\n",
      "        [0.0304],\n",
      "        [0.0310],\n",
      "        [0.0334],\n",
      "        [0.0359],\n",
      "        [0.0346],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0361],\n",
      "        [0.0402],\n",
      "        [0.0379],\n",
      "        [0.0361],\n",
      "        [0.0421],\n",
      "        [0.0419],\n",
      "        [0.0378],\n",
      "        [0.0415],\n",
      "        [0.0429],\n",
      "        [0.0409],\n",
      "        [0.0446],\n",
      "        [0.0503],\n",
      "        [0.0487],\n",
      "        [0.0482],\n",
      "        [0.0651],\n",
      "        [0.0638],\n",
      "        [0.0694],\n",
      "        [0.0692],\n",
      "        [0.0735],\n",
      "        [0.0769],\n",
      "        [0.0744],\n",
      "        [0.0886],\n",
      "        [0.0829]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.426676034927368\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 76\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.53602660602337e-07, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 46, 63, 74, 77, 62, 53, 75, 100, 57, 93, 76, 90, 25, 61, 26, 34, 51, 28, 101, 99, 52, 89, 36, 94, 98, 49, 24, 103, 91, 23, 68, 92, 55, 78, 30, 95, 102, 87, 88, 66, 85, 58, 54, 32, 33, 73, 48, 35, 97, 27, 47, 86, 22, 106, 96, 67, 108, 31, 29, 72, 65, 107, 21, 64, 37, 17, 104, 105, 50, 59, 60, 69, 20, 45, 14] 數值 torch.Size([76, 1])\n",
      "目前模型的Data狀態 torch.Size([76, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9052],\n",
      "        [0.8443],\n",
      "        [0.8632],\n",
      "        [0.7582],\n",
      "        [0.7245],\n",
      "        [0.8865],\n",
      "        [0.8642],\n",
      "        [0.7406],\n",
      "        [0.6295],\n",
      "        [0.9271],\n",
      "        [0.7086],\n",
      "        [0.7499],\n",
      "        [0.6945],\n",
      "        [0.8156],\n",
      "        [0.9109],\n",
      "        [0.8416],\n",
      "        [0.8037],\n",
      "        [0.9109],\n",
      "        [0.8360],\n",
      "        [0.6338],\n",
      "        [0.6461],\n",
      "        [0.8981],\n",
      "        [0.6968],\n",
      "        [0.8274],\n",
      "        [0.6925],\n",
      "        [0.6614],\n",
      "        [0.8687],\n",
      "        [0.8123],\n",
      "        [0.6797],\n",
      "        [0.6706],\n",
      "        [0.8231],\n",
      "        [0.7926],\n",
      "        [0.6815],\n",
      "        [0.8849],\n",
      "        [0.6878],\n",
      "        [0.8748],\n",
      "        [0.6798],\n",
      "        [0.6666],\n",
      "        [0.7100],\n",
      "        [0.7085],\n",
      "        [0.8094],\n",
      "        [0.6895],\n",
      "        [0.9069],\n",
      "        [0.9220],\n",
      "        [0.7791],\n",
      "        [0.7889],\n",
      "        [0.7768],\n",
      "        [0.8797],\n",
      "        [0.8055],\n",
      "        [0.6490],\n",
      "        [0.8384],\n",
      "        [0.8656],\n",
      "        [0.6807],\n",
      "        [0.8186],\n",
      "        [0.7344],\n",
      "        [0.6571],\n",
      "        [0.8338],\n",
      "        [0.7302],\n",
      "        [0.8458],\n",
      "        [0.8714],\n",
      "        [0.7882],\n",
      "        [0.7927],\n",
      "        [0.7353],\n",
      "        [0.8052],\n",
      "        [0.8257],\n",
      "        [0.8133],\n",
      "        [0.8252],\n",
      "        [0.7241],\n",
      "        [0.7487],\n",
      "        [0.9587],\n",
      "        [0.8858],\n",
      "        [0.8809],\n",
      "        [0.7807],\n",
      "        [0.8330],\n",
      "        [0.7965],\n",
      "        [0.8910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0032],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0100],\n",
      "        [0.0108],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0194],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0223],\n",
      "        [0.0241],\n",
      "        [0.0241],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0255],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0284],\n",
      "        [0.0285],\n",
      "        [0.0294],\n",
      "        [0.0304],\n",
      "        [0.0310],\n",
      "        [0.0312],\n",
      "        [0.0334],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0359],\n",
      "        [0.0361],\n",
      "        [0.0361],\n",
      "        [0.0378],\n",
      "        [0.0379],\n",
      "        [0.0402],\n",
      "        [0.0409],\n",
      "        [0.0415],\n",
      "        [0.0419],\n",
      "        [0.0421],\n",
      "        [0.0429],\n",
      "        [0.0446],\n",
      "        [0.0482],\n",
      "        [0.0487],\n",
      "        [0.0503],\n",
      "        [0.0638],\n",
      "        [0.0651],\n",
      "        [0.0692],\n",
      "        [0.0694],\n",
      "        [0.0735],\n",
      "        [0.0744],\n",
      "        [0.0769],\n",
      "        [0.0829],\n",
      "        [0.0886],\n",
      "        [0.0890]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0046],\n",
      "        [0.0029],\n",
      "        [0.0026],\n",
      "        [0.0034],\n",
      "        [0.0044],\n",
      "        [0.0037],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0081],\n",
      "        [0.0067],\n",
      "        [0.0089],\n",
      "        [0.0073],\n",
      "        [0.0136],\n",
      "        [0.0082],\n",
      "        [0.0124],\n",
      "        [0.0101],\n",
      "        [0.0122],\n",
      "        [0.0115],\n",
      "        [0.0144],\n",
      "        [0.0141],\n",
      "        [0.0141],\n",
      "        [0.0138],\n",
      "        [0.0154],\n",
      "        [0.0148],\n",
      "        [0.0162],\n",
      "        [0.0174],\n",
      "        [0.0244],\n",
      "        [0.0187],\n",
      "        [0.0198],\n",
      "        [0.0172],\n",
      "        [0.0197],\n",
      "        [0.0237],\n",
      "        [0.0246],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0250],\n",
      "        [0.0239],\n",
      "        [0.0243],\n",
      "        [0.0248],\n",
      "        [0.0214],\n",
      "        [0.0286],\n",
      "        [0.0268],\n",
      "        [0.0289],\n",
      "        [0.0288],\n",
      "        [0.0300],\n",
      "        [0.0327],\n",
      "        [0.0366],\n",
      "        [0.0343],\n",
      "        [0.0347],\n",
      "        [0.0365],\n",
      "        [0.0390],\n",
      "        [0.0353],\n",
      "        [0.0310],\n",
      "        [0.0346],\n",
      "        [0.0380],\n",
      "        [0.0446],\n",
      "        [0.0373],\n",
      "        [0.0419],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0398],\n",
      "        [0.0413],\n",
      "        [0.0418],\n",
      "        [0.0467],\n",
      "        [0.0504],\n",
      "        [0.0547],\n",
      "        [0.0627],\n",
      "        [0.0663],\n",
      "        [0.0683],\n",
      "        [0.0718],\n",
      "        [0.0727],\n",
      "        [0.0804],\n",
      "        [0.0755],\n",
      "        [0.0924],\n",
      "        [0.0803]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.70018196105957\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 77\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.505097095375277e-08, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 74, 63, 77, 53, 62, 46, 75, 100, 93, 90, 57, 61, 76, 34, 28, 51, 26, 25, 89, 52, 99, 101, 94, 36, 98, 23, 49, 103, 68, 91, 66, 92, 102, 87, 24, 55, 88, 95, 78, 30, 58, 85, 32, 54, 33, 22, 73, 35, 106, 97, 86, 27, 48, 108, 96, 47, 65, 107, 21, 31, 29, 72, 67, 64, 37, 17, 104, 105, 50, 59, 60, 20, 14, 69, 11, 18] 數值 torch.Size([77, 1])\n",
      "目前模型的Data狀態 torch.Size([77, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9057],\n",
      "        [0.7579],\n",
      "        [0.8646],\n",
      "        [0.7251],\n",
      "        [0.8637],\n",
      "        [0.8877],\n",
      "        [0.8479],\n",
      "        [0.7402],\n",
      "        [0.6287],\n",
      "        [0.7094],\n",
      "        [0.6961],\n",
      "        [0.9281],\n",
      "        [0.9121],\n",
      "        [0.7497],\n",
      "        [0.8043],\n",
      "        [0.8377],\n",
      "        [0.9100],\n",
      "        [0.8440],\n",
      "        [0.8201],\n",
      "        [0.6980],\n",
      "        [0.8975],\n",
      "        [0.6457],\n",
      "        [0.6330],\n",
      "        [0.6930],\n",
      "        [0.8271],\n",
      "        [0.6610],\n",
      "        [0.8283],\n",
      "        [0.8698],\n",
      "        [0.6778],\n",
      "        [0.7969],\n",
      "        [0.6717],\n",
      "        [0.8141],\n",
      "        [0.6820],\n",
      "        [0.6651],\n",
      "        [0.7111],\n",
      "        [0.8173],\n",
      "        [0.8850],\n",
      "        [0.7095],\n",
      "        [0.6800],\n",
      "        [0.6887],\n",
      "        [0.8756],\n",
      "        [0.9086],\n",
      "        [0.6896],\n",
      "        [0.7807],\n",
      "        [0.9214],\n",
      "        [0.7900],\n",
      "        [0.8237],\n",
      "        [0.7782],\n",
      "        [0.8057],\n",
      "        [0.7312],\n",
      "        [0.6488],\n",
      "        [0.6815],\n",
      "        [0.8402],\n",
      "        [0.8828],\n",
      "        [0.7266],\n",
      "        [0.6570],\n",
      "        [0.8687],\n",
      "        [0.7958],\n",
      "        [0.7320],\n",
      "        [0.8116],\n",
      "        [0.8462],\n",
      "        [0.8725],\n",
      "        [0.7899],\n",
      "        [0.8382],\n",
      "        [0.8277],\n",
      "        [0.8132],\n",
      "        [0.8343],\n",
      "        [0.7217],\n",
      "        [0.7457],\n",
      "        [0.9575],\n",
      "        [0.8876],\n",
      "        [0.8825],\n",
      "        [0.8404],\n",
      "        [0.8997],\n",
      "        [0.7842],\n",
      "        [0.8933],\n",
      "        [0.8247]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0044],\n",
      "        [0.0046],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0067],\n",
      "        [0.0073],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0089],\n",
      "        [0.0101],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0124],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0141],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0187],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0214],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0243],\n",
      "        [0.0244],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0250],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0268],\n",
      "        [0.0286],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0300],\n",
      "        [0.0310],\n",
      "        [0.0327],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0347],\n",
      "        [0.0353],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0373],\n",
      "        [0.0380],\n",
      "        [0.0390],\n",
      "        [0.0398],\n",
      "        [0.0413],\n",
      "        [0.0418],\n",
      "        [0.0419],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0446],\n",
      "        [0.0467],\n",
      "        [0.0504],\n",
      "        [0.0547],\n",
      "        [0.0627],\n",
      "        [0.0663],\n",
      "        [0.0683],\n",
      "        [0.0718],\n",
      "        [0.0727],\n",
      "        [0.0755],\n",
      "        [0.0803],\n",
      "        [0.0804],\n",
      "        [0.0857],\n",
      "        [0.0894]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0038],\n",
      "        [0.0027],\n",
      "        [0.0072],\n",
      "        [0.0038],\n",
      "        [0.0013],\n",
      "        [0.0085],\n",
      "        [0.0130],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0128],\n",
      "        [0.0039],\n",
      "        [0.0095],\n",
      "        [0.0064],\n",
      "        [0.0061],\n",
      "        [0.0141],\n",
      "        [0.0188],\n",
      "        [0.0228],\n",
      "        [0.0128],\n",
      "        [0.0164],\n",
      "        [0.0144],\n",
      "        [0.0151],\n",
      "        [0.0140],\n",
      "        [0.0131],\n",
      "        [0.0166],\n",
      "        [0.0071],\n",
      "        [0.0223],\n",
      "        [0.0165],\n",
      "        [0.0120],\n",
      "        [0.0187],\n",
      "        [0.0129],\n",
      "        [0.0232],\n",
      "        [0.0222],\n",
      "        [0.0234],\n",
      "        [0.0343],\n",
      "        [0.0213],\n",
      "        [0.0240],\n",
      "        [0.0246],\n",
      "        [0.0266],\n",
      "        [0.0305],\n",
      "        [0.0214],\n",
      "        [0.0279],\n",
      "        [0.0237],\n",
      "        [0.0314],\n",
      "        [0.0259],\n",
      "        [0.0210],\n",
      "        [0.0350],\n",
      "        [0.0314],\n",
      "        [0.0309],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0420],\n",
      "        [0.0443],\n",
      "        [0.0332],\n",
      "        [0.0381],\n",
      "        [0.0467],\n",
      "        [0.0336],\n",
      "        [0.0375],\n",
      "        [0.0297],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0466],\n",
      "        [0.0526],\n",
      "        [0.0416],\n",
      "        [0.0479],\n",
      "        [0.0389],\n",
      "        [0.0600],\n",
      "        [0.0629],\n",
      "        [0.0702],\n",
      "        [0.0667],\n",
      "        [0.0679],\n",
      "        [0.0620],\n",
      "        [0.0652],\n",
      "        [0.0867],\n",
      "        [0.0691],\n",
      "        [0.0749]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.97368860244751\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 78\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6494298051838996e-06, 53)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [53, 74, 56, 77, 61, 93, 90, 28, 34, 75, 23, 63, 100, 62, 76, 68, 57, 89, 66, 46, 36, 94, 51, 99, 101, 52, 103, 98, 91, 26, 22, 55, 58, 102, 49, 25, 92, 87, 32, 88, 95, 33, 78, 85, 21, 30, 106, 54, 35, 108, 65, 24, 97, 73, 86, 107, 96, 17, 64, 27, 48, 31, 72, 47, 29, 37, 67, 104, 20, 105, 14, 59, 60, 11, 50, 18, 19, 13] 數值 torch.Size([78, 1])\n",
      "目前模型的Data狀態 torch.Size([78, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8661],\n",
      "        [0.7579],\n",
      "        [0.9097],\n",
      "        [0.7256],\n",
      "        [0.9164],\n",
      "        [0.7105],\n",
      "        [0.6978],\n",
      "        [0.8431],\n",
      "        [0.8081],\n",
      "        [0.7395],\n",
      "        [0.8383],\n",
      "        [0.8689],\n",
      "        [0.6278],\n",
      "        [0.8918],\n",
      "        [0.7491],\n",
      "        [0.8046],\n",
      "        [0.9327],\n",
      "        [0.6990],\n",
      "        [0.8226],\n",
      "        [0.8563],\n",
      "        [0.8294],\n",
      "        [0.6938],\n",
      "        [0.9120],\n",
      "        [0.6453],\n",
      "        [0.6323],\n",
      "        [0.8998],\n",
      "        [0.6757],\n",
      "        [0.6605],\n",
      "        [0.6728],\n",
      "        [0.8504],\n",
      "        [0.8338],\n",
      "        [0.8883],\n",
      "        [0.9141],\n",
      "        [0.6634],\n",
      "        [0.8747],\n",
      "        [0.8294],\n",
      "        [0.6824],\n",
      "        [0.7120],\n",
      "        [0.7857],\n",
      "        [0.7102],\n",
      "        [0.6804],\n",
      "        [0.7941],\n",
      "        [0.6897],\n",
      "        [0.6889],\n",
      "        [0.8236],\n",
      "        [0.8801],\n",
      "        [0.7275],\n",
      "        [0.9240],\n",
      "        [0.8086],\n",
      "        [0.7224],\n",
      "        [0.8021],\n",
      "        [0.8271],\n",
      "        [0.6487],\n",
      "        [0.7805],\n",
      "        [0.6817],\n",
      "        [0.7282],\n",
      "        [0.6570],\n",
      "        [0.8500],\n",
      "        [0.8327],\n",
      "        [0.8456],\n",
      "        [0.8906],\n",
      "        [0.8498],\n",
      "        [0.7928],\n",
      "        [0.8763],\n",
      "        [0.8771],\n",
      "        [0.8156],\n",
      "        [0.8463],\n",
      "        [0.7190],\n",
      "        [0.8539],\n",
      "        [0.7424],\n",
      "        [0.9148],\n",
      "        [0.8927],\n",
      "        [0.8873],\n",
      "        [0.9099],\n",
      "        [0.9595],\n",
      "        [0.8392],\n",
      "        [0.8598],\n",
      "        [0.9060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0027],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0075],\n",
      "        [0.0085],\n",
      "        [0.0095],\n",
      "        [0.0120],\n",
      "        [0.0128],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0140],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0151],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0210],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0222],\n",
      "        [0.0223],\n",
      "        [0.0228],\n",
      "        [0.0232],\n",
      "        [0.0234],\n",
      "        [0.0237],\n",
      "        [0.0240],\n",
      "        [0.0246],\n",
      "        [0.0259],\n",
      "        [0.0266],\n",
      "        [0.0279],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0314],\n",
      "        [0.0332],\n",
      "        [0.0336],\n",
      "        [0.0343],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0350],\n",
      "        [0.0375],\n",
      "        [0.0381],\n",
      "        [0.0389],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0443],\n",
      "        [0.0455],\n",
      "        [0.0466],\n",
      "        [0.0467],\n",
      "        [0.0476],\n",
      "        [0.0479],\n",
      "        [0.0526],\n",
      "        [0.0600],\n",
      "        [0.0620],\n",
      "        [0.0629],\n",
      "        [0.0652],\n",
      "        [0.0667],\n",
      "        [0.0679],\n",
      "        [0.0691],\n",
      "        [0.0702],\n",
      "        [0.0749],\n",
      "        [0.0768],\n",
      "        [0.0773]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0024],\n",
      "        [0.0055],\n",
      "        [0.0053],\n",
      "        [0.0013],\n",
      "        [0.0034],\n",
      "        [0.0025],\n",
      "        [0.0024],\n",
      "        [0.0043],\n",
      "        [0.0069],\n",
      "        [0.0010],\n",
      "        [0.0098],\n",
      "        [0.0074],\n",
      "        [0.0110],\n",
      "        [0.0090],\n",
      "        [0.0052],\n",
      "        [0.0154],\n",
      "        [0.0101],\n",
      "        [0.0057],\n",
      "        [0.0186],\n",
      "        [0.0126],\n",
      "        [0.0122],\n",
      "        [0.0140],\n",
      "        [0.0137],\n",
      "        [0.0150],\n",
      "        [0.0166],\n",
      "        [0.0153],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0233],\n",
      "        [0.0129],\n",
      "        [0.0201],\n",
      "        [0.0179],\n",
      "        [0.0215],\n",
      "        [0.0247],\n",
      "        [0.0302],\n",
      "        [0.0216],\n",
      "        [0.0207],\n",
      "        [0.0205],\n",
      "        [0.0214],\n",
      "        [0.0233],\n",
      "        [0.0235],\n",
      "        [0.0284],\n",
      "        [0.0290],\n",
      "        [0.0198],\n",
      "        [0.0332],\n",
      "        [0.0282],\n",
      "        [0.0320],\n",
      "        [0.0304],\n",
      "        [0.0300],\n",
      "        [0.0289],\n",
      "        [0.0422],\n",
      "        [0.0338],\n",
      "        [0.0373],\n",
      "        [0.0328],\n",
      "        [0.0347],\n",
      "        [0.0370],\n",
      "        [0.0255],\n",
      "        [0.0383],\n",
      "        [0.0456],\n",
      "        [0.0494],\n",
      "        [0.0474],\n",
      "        [0.0494],\n",
      "        [0.0515],\n",
      "        [0.0506],\n",
      "        [0.0473],\n",
      "        [0.0597],\n",
      "        [0.0582],\n",
      "        [0.0508],\n",
      "        [0.0606],\n",
      "        [0.0522],\n",
      "        [0.0633],\n",
      "        [0.0647],\n",
      "        [0.0541],\n",
      "        [0.0700],\n",
      "        [0.0626],\n",
      "        [0.0645],\n",
      "        [0.0634]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.24842619895935\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 79\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.947411748114973e-07, 53)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [53, 23, 61, 28, 74, 90, 93, 34, 68, 77, 56, 66, 75, 100, 76, 63, 89, 62, 94, 36, 22, 99, 51, 101, 103, 57, 98, 91, 52, 58, 46, 21, 55, 32, 87, 88, 102, 92, 26, 95, 33, 49, 17, 106, 78, 65, 85, 108, 25, 35, 54, 86, 30, 97, 107, 96, 73, 64, 24, 27, 37, 31, 48, 72, 29, 20, 47, 14, 11, 104, 67, 105, 18, 59, 13, 19, 60, 15, 16] 數值 torch.Size([79, 1])\n",
      "目前模型的Data狀態 torch.Size([79, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8666],\n",
      "        [0.8464],\n",
      "        [0.9190],\n",
      "        [0.8468],\n",
      "        [0.7582],\n",
      "        [0.7009],\n",
      "        [0.7126],\n",
      "        [0.8101],\n",
      "        [0.8115],\n",
      "        [0.7270],\n",
      "        [0.9114],\n",
      "        [0.8298],\n",
      "        [0.7394],\n",
      "        [0.6279],\n",
      "        [0.7496],\n",
      "        [0.8715],\n",
      "        [0.7016],\n",
      "        [0.8943],\n",
      "        [0.6955],\n",
      "        [0.8299],\n",
      "        [0.8419],\n",
      "        [0.6461],\n",
      "        [0.9119],\n",
      "        [0.6325],\n",
      "        [0.6744],\n",
      "        [0.9354],\n",
      "        [0.6612],\n",
      "        [0.6752],\n",
      "        [0.9001],\n",
      "        [0.9176],\n",
      "        [0.8618],\n",
      "        [0.8336],\n",
      "        [0.8895],\n",
      "        [0.7889],\n",
      "        [0.7148],\n",
      "        [0.7128],\n",
      "        [0.6627],\n",
      "        [0.6841],\n",
      "        [0.8549],\n",
      "        [0.6817],\n",
      "        [0.7965],\n",
      "        [0.8771],\n",
      "        [0.8634],\n",
      "        [0.7248],\n",
      "        [0.6914],\n",
      "        [0.8068],\n",
      "        [0.6900],\n",
      "        [0.7192],\n",
      "        [0.8368],\n",
      "        [0.8096],\n",
      "        [0.9246],\n",
      "        [0.6840],\n",
      "        [0.8828],\n",
      "        [0.6498],\n",
      "        [0.7254],\n",
      "        [0.6581],\n",
      "        [0.7828],\n",
      "        [0.8360],\n",
      "        [0.8351],\n",
      "        [0.8492],\n",
      "        [0.8162],\n",
      "        [0.8516],\n",
      "        [0.8956],\n",
      "        [0.7955],\n",
      "        [0.8801],\n",
      "        [0.8651],\n",
      "        [0.8812],\n",
      "        [0.9279],\n",
      "        [0.9250],\n",
      "        [0.7172],\n",
      "        [0.8533],\n",
      "        [0.7400],\n",
      "        [0.8515],\n",
      "        [0.8961],\n",
      "        [0.9200],\n",
      "        [0.8722],\n",
      "        [0.8906],\n",
      "        [0.8988],\n",
      "        [0.8742]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0010],\n",
      "        [0.0013],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0034],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0057],\n",
      "        [0.0069],\n",
      "        [0.0074],\n",
      "        [0.0090],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0110],\n",
      "        [0.0122],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0186],\n",
      "        [0.0198],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0233],\n",
      "        [0.0233],\n",
      "        [0.0235],\n",
      "        [0.0247],\n",
      "        [0.0255],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0300],\n",
      "        [0.0302],\n",
      "        [0.0304],\n",
      "        [0.0320],\n",
      "        [0.0328],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0347],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0383],\n",
      "        [0.0422],\n",
      "        [0.0456],\n",
      "        [0.0473],\n",
      "        [0.0474],\n",
      "        [0.0494],\n",
      "        [0.0494],\n",
      "        [0.0506],\n",
      "        [0.0508],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0541],\n",
      "        [0.0582],\n",
      "        [0.0597],\n",
      "        [0.0606],\n",
      "        [0.0626],\n",
      "        [0.0633],\n",
      "        [0.0634],\n",
      "        [0.0645],\n",
      "        [0.0647],\n",
      "        [0.0685],\n",
      "        [0.0696]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0078],\n",
      "        [0.0009],\n",
      "        [0.0002],\n",
      "        [0.0032],\n",
      "        [0.0010],\n",
      "        [0.0030],\n",
      "        [0.0029],\n",
      "        [0.0001],\n",
      "        [0.0050],\n",
      "        [0.0074],\n",
      "        [0.0003],\n",
      "        [0.0080],\n",
      "        [0.0093],\n",
      "        [0.0099],\n",
      "        [0.0120],\n",
      "        [0.0091],\n",
      "        [0.0131],\n",
      "        [0.0121],\n",
      "        [0.0127],\n",
      "        [0.0061],\n",
      "        [0.0149],\n",
      "        [0.0147],\n",
      "        [0.0168],\n",
      "        [0.0124],\n",
      "        [0.0180],\n",
      "        [0.0171],\n",
      "        [0.0156],\n",
      "        [0.0175],\n",
      "        [0.0146],\n",
      "        [0.0236],\n",
      "        [0.0113],\n",
      "        [0.0188],\n",
      "        [0.0181],\n",
      "        [0.0195],\n",
      "        [0.0205],\n",
      "        [0.0190],\n",
      "        [0.0217],\n",
      "        [0.0269],\n",
      "        [0.0237],\n",
      "        [0.0218],\n",
      "        [0.0273],\n",
      "        [0.0135],\n",
      "        [0.0244],\n",
      "        [0.0286],\n",
      "        [0.0253],\n",
      "        [0.0287],\n",
      "        [0.0255],\n",
      "        [0.0363],\n",
      "        [0.0300],\n",
      "        [0.0329],\n",
      "        [0.0322],\n",
      "        [0.0353],\n",
      "        [0.0347],\n",
      "        [0.0306],\n",
      "        [0.0377],\n",
      "        [0.0386],\n",
      "        [0.0358],\n",
      "        [0.0487],\n",
      "        [0.0482],\n",
      "        [0.0475],\n",
      "        [0.0487],\n",
      "        [0.0543],\n",
      "        [0.0511],\n",
      "        [0.0528],\n",
      "        [0.0410],\n",
      "        [0.0560],\n",
      "        [0.0400],\n",
      "        [0.0406],\n",
      "        [0.0551],\n",
      "        [0.0654],\n",
      "        [0.0572],\n",
      "        [0.0519],\n",
      "        [0.0603],\n",
      "        [0.0505],\n",
      "        [0.0538],\n",
      "        [0.0620],\n",
      "        [0.0573],\n",
      "        [0.0578]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.52311897277832\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 80\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6529529744957472e-08, 68)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [68, 28, 53, 66, 61, 90, 34, 93, 74, 77, 22, 56, 23, 75, 89, 100, 76, 21, 63, 94, 103, 36, 62, 17, 58, 51, 99, 91, 101, 98, 52, 57, 32, 55, 102, 87, 88, 92, 33, 46, 95, 106, 65, 108, 26, 49, 78, 85, 35, 107, 86, 54, 97, 30, 64, 25, 96, 73, 14, 11, 20, 37, 27, 31, 24, 13, 72, 18, 29, 19, 48, 104, 47, 105, 15, 16, 59, 9, 60, 10] 數值 torch.Size([80, 1])\n",
      "目前模型的Data狀態 torch.Size([80, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8168],\n",
      "        [0.8494],\n",
      "        [0.8672],\n",
      "        [0.8358],\n",
      "        [0.9212],\n",
      "        [0.7023],\n",
      "        [0.8115],\n",
      "        [0.7131],\n",
      "        [0.7574],\n",
      "        [0.7268],\n",
      "        [0.8487],\n",
      "        [0.9134],\n",
      "        [0.8533],\n",
      "        [0.7383],\n",
      "        [0.7026],\n",
      "        [0.6260],\n",
      "        [0.7487],\n",
      "        [0.8421],\n",
      "        [0.8737],\n",
      "        [0.6956],\n",
      "        [0.6716],\n",
      "        [0.8298],\n",
      "        [0.8964],\n",
      "        [0.8754],\n",
      "        [0.9208],\n",
      "        [0.9125],\n",
      "        [0.6448],\n",
      "        [0.6758],\n",
      "        [0.6306],\n",
      "        [0.6600],\n",
      "        [0.9009],\n",
      "        [0.9379],\n",
      "        [0.7913],\n",
      "        [0.8908],\n",
      "        [0.6601],\n",
      "        [0.7160],\n",
      "        [0.7137],\n",
      "        [0.6839],\n",
      "        [0.7982],\n",
      "        [0.8669],\n",
      "        [0.6814],\n",
      "        [0.7210],\n",
      "        [0.8104],\n",
      "        [0.7148],\n",
      "        [0.8584],\n",
      "        [0.8797],\n",
      "        [0.6916],\n",
      "        [0.6898],\n",
      "        [0.8100],\n",
      "        [0.7213],\n",
      "        [0.6846],\n",
      "        [0.9255],\n",
      "        [0.6489],\n",
      "        [0.8848],\n",
      "        [0.8386],\n",
      "        [0.8429],\n",
      "        [0.6573],\n",
      "        [0.7841],\n",
      "        [0.9401],\n",
      "        [0.9385],\n",
      "        [0.8749],\n",
      "        [0.8161],\n",
      "        [0.8518],\n",
      "        [0.8530],\n",
      "        [0.8416],\n",
      "        [0.9329],\n",
      "        [0.7973],\n",
      "        [0.8622],\n",
      "        [0.8824],\n",
      "        [0.8829],\n",
      "        [0.9006],\n",
      "        [0.7141],\n",
      "        [0.8857],\n",
      "        [0.7366],\n",
      "        [0.9100],\n",
      "        [0.8860],\n",
      "        [0.8991],\n",
      "        [0.9150],\n",
      "        [0.8933],\n",
      "        [0.9294]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0050],\n",
      "        [0.0061],\n",
      "        [0.0074],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0099],\n",
      "        [0.0113],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0124],\n",
      "        [0.0127],\n",
      "        [0.0131],\n",
      "        [0.0135],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0156],\n",
      "        [0.0168],\n",
      "        [0.0171],\n",
      "        [0.0175],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0195],\n",
      "        [0.0205],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0244],\n",
      "        [0.0253],\n",
      "        [0.0255],\n",
      "        [0.0269],\n",
      "        [0.0273],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0322],\n",
      "        [0.0329],\n",
      "        [0.0347],\n",
      "        [0.0353],\n",
      "        [0.0358],\n",
      "        [0.0363],\n",
      "        [0.0377],\n",
      "        [0.0386],\n",
      "        [0.0400],\n",
      "        [0.0406],\n",
      "        [0.0410],\n",
      "        [0.0475],\n",
      "        [0.0482],\n",
      "        [0.0487],\n",
      "        [0.0487],\n",
      "        [0.0505],\n",
      "        [0.0511],\n",
      "        [0.0519],\n",
      "        [0.0528],\n",
      "        [0.0538],\n",
      "        [0.0543],\n",
      "        [0.0551],\n",
      "        [0.0560],\n",
      "        [0.0572],\n",
      "        [0.0573],\n",
      "        [0.0578],\n",
      "        [0.0603],\n",
      "        [0.0605],\n",
      "        [0.0620],\n",
      "        [0.0634]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0048],\n",
      "        [0.0018],\n",
      "        [0.0011],\n",
      "        [0.0049],\n",
      "        [0.0019],\n",
      "        [0.0025],\n",
      "        [0.0023],\n",
      "        [0.0012],\n",
      "        [0.0036],\n",
      "        [0.0063],\n",
      "        [0.0013],\n",
      "        [0.0075],\n",
      "        [0.0130],\n",
      "        [0.0086],\n",
      "        [0.0059],\n",
      "        [0.0098],\n",
      "        [0.0092],\n",
      "        [0.0049],\n",
      "        [0.0128],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0141],\n",
      "        [0.0138],\n",
      "        [0.0042],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0145],\n",
      "        [0.0129],\n",
      "        [0.0174],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0190],\n",
      "        [0.0170],\n",
      "        [0.0192],\n",
      "        [0.0179],\n",
      "        [0.0156],\n",
      "        [0.0172],\n",
      "        [0.0200],\n",
      "        [0.0212],\n",
      "        [0.0255],\n",
      "        [0.0230],\n",
      "        [0.0224],\n",
      "        [0.0235],\n",
      "        [0.0230],\n",
      "        [0.0286],\n",
      "        [0.0271],\n",
      "        [0.0299],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.0324],\n",
      "        [0.0339],\n",
      "        [0.0367],\n",
      "        [0.0350],\n",
      "        [0.0408],\n",
      "        [0.0370],\n",
      "        [0.0398],\n",
      "        [0.0297],\n",
      "        [0.0275],\n",
      "        [0.0333],\n",
      "        [0.0489],\n",
      "        [0.0493],\n",
      "        [0.0491],\n",
      "        [0.0537],\n",
      "        [0.0392],\n",
      "        [0.0527],\n",
      "        [0.0434],\n",
      "        [0.0543],\n",
      "        [0.0452],\n",
      "        [0.0562],\n",
      "        [0.0535],\n",
      "        [0.0576],\n",
      "        [0.0556],\n",
      "        [0.0480],\n",
      "        [0.0483],\n",
      "        [0.0585],\n",
      "        [0.0485],\n",
      "        [0.0603],\n",
      "        [0.0504]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.797691583633423\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 81\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.2403561413520947e-06, 53)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [53, 93, 22, 28, 61, 34, 90, 74, 17, 68, 66, 21, 89, 77, 56, 75, 76, 100, 94, 103, 63, 91, 23, 58, 51, 62, 36, 99, 87, 98, 52, 32, 88, 101, 102, 57, 55, 92, 33, 106, 108, 95, 65, 46, 49, 11, 107, 26, 86, 14, 78, 85, 35, 54, 20, 97, 64, 30, 96, 13, 73, 25, 18, 19, 15, 16, 9, 37, 31, 27, 10, 72, 104, 24, 29, 105, 48, 47, 59, 60, 12] 數值 torch.Size([81, 1])\n",
      "目前模型的Data狀態 torch.Size([81, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8663],\n",
      "        [0.7149],\n",
      "        [0.8534],\n",
      "        [0.8510],\n",
      "        [0.9222],\n",
      "        [0.8121],\n",
      "        [0.7059],\n",
      "        [0.7570],\n",
      "        [0.8848],\n",
      "        [0.8214],\n",
      "        [0.8404],\n",
      "        [0.8484],\n",
      "        [0.7059],\n",
      "        [0.7281],\n",
      "        [0.9134],\n",
      "        [0.7377],\n",
      "        [0.7494],\n",
      "        [0.6256],\n",
      "        [0.6968],\n",
      "        [0.6702],\n",
      "        [0.8745],\n",
      "        [0.6785],\n",
      "        [0.8584],\n",
      "        [0.9225],\n",
      "        [0.9115],\n",
      "        [0.8971],\n",
      "        [0.8285],\n",
      "        [0.6452],\n",
      "        [0.7199],\n",
      "        [0.6606],\n",
      "        [0.9004],\n",
      "        [0.7924],\n",
      "        [0.7171],\n",
      "        [0.6301],\n",
      "        [0.6590],\n",
      "        [0.9390],\n",
      "        [0.8904],\n",
      "        [0.6857],\n",
      "        [0.7988],\n",
      "        [0.7190],\n",
      "        [0.7122],\n",
      "        [0.6820],\n",
      "        [0.8121],\n",
      "        [0.8688],\n",
      "        [0.8795],\n",
      "        [0.9516],\n",
      "        [0.7192],\n",
      "        [0.8601],\n",
      "        [0.6881],\n",
      "        [0.9503],\n",
      "        [0.6929],\n",
      "        [0.6918],\n",
      "        [0.8090],\n",
      "        [0.9250],\n",
      "        [0.8826],\n",
      "        [0.6496],\n",
      "        [0.8394],\n",
      "        [0.8862],\n",
      "        [0.6580],\n",
      "        [0.9442],\n",
      "        [0.7853],\n",
      "        [0.8473],\n",
      "        [0.8707],\n",
      "        [0.8915],\n",
      "        [0.9194],\n",
      "        [0.8955],\n",
      "        [0.9270],\n",
      "        [0.8146],\n",
      "        [0.8533],\n",
      "        [0.8530],\n",
      "        [0.9424],\n",
      "        [0.7988],\n",
      "        [0.7125],\n",
      "        [0.8465],\n",
      "        [0.8838],\n",
      "        [0.7350],\n",
      "        [0.9025],\n",
      "        [0.8873],\n",
      "        [0.9009],\n",
      "        [0.8949],\n",
      "        [0.9360]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0011],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0049],\n",
      "        [0.0059],\n",
      "        [0.0063],\n",
      "        [0.0075],\n",
      "        [0.0086],\n",
      "        [0.0092],\n",
      "        [0.0098],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0141],\n",
      "        [0.0145],\n",
      "        [0.0156],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0179],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0200],\n",
      "        [0.0212],\n",
      "        [0.0224],\n",
      "        [0.0230],\n",
      "        [0.0230],\n",
      "        [0.0235],\n",
      "        [0.0255],\n",
      "        [0.0271],\n",
      "        [0.0275],\n",
      "        [0.0285],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0324],\n",
      "        [0.0333],\n",
      "        [0.0339],\n",
      "        [0.0350],\n",
      "        [0.0367],\n",
      "        [0.0370],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0408],\n",
      "        [0.0434],\n",
      "        [0.0452],\n",
      "        [0.0480],\n",
      "        [0.0483],\n",
      "        [0.0485],\n",
      "        [0.0489],\n",
      "        [0.0491],\n",
      "        [0.0493],\n",
      "        [0.0504],\n",
      "        [0.0527],\n",
      "        [0.0535],\n",
      "        [0.0537],\n",
      "        [0.0543],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0576],\n",
      "        [0.0585],\n",
      "        [0.0603],\n",
      "        [0.0613]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0043],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0008],\n",
      "        [0.0039],\n",
      "        [0.0047],\n",
      "        [0.0055],\n",
      "        [0.0024],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0014],\n",
      "        [0.0039],\n",
      "        [0.0065],\n",
      "        [0.0051],\n",
      "        [0.0107],\n",
      "        [0.0096],\n",
      "        [0.0114],\n",
      "        [0.0112],\n",
      "        [0.0084],\n",
      "        [0.0114],\n",
      "        [0.0116],\n",
      "        [0.0155],\n",
      "        [0.0135],\n",
      "        [0.0103],\n",
      "        [0.0124],\n",
      "        [0.0175],\n",
      "        [0.0154],\n",
      "        [0.0128],\n",
      "        [0.0171],\n",
      "        [0.0140],\n",
      "        [0.0182],\n",
      "        [0.0150],\n",
      "        [0.0192],\n",
      "        [0.0156],\n",
      "        [0.0178],\n",
      "        [0.0220],\n",
      "        [0.0194],\n",
      "        [0.0226],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0238],\n",
      "        [0.0240],\n",
      "        [0.0247],\n",
      "        [0.0241],\n",
      "        [0.0164],\n",
      "        [0.0251],\n",
      "        [0.0279],\n",
      "        [0.0262],\n",
      "        [0.0219],\n",
      "        [0.0299],\n",
      "        [0.0316],\n",
      "        [0.0343],\n",
      "        [0.0297],\n",
      "        [0.0283],\n",
      "        [0.0343],\n",
      "        [0.0363],\n",
      "        [0.0359],\n",
      "        [0.0376],\n",
      "        [0.0302],\n",
      "        [0.0392],\n",
      "        [0.0426],\n",
      "        [0.0375],\n",
      "        [0.0393],\n",
      "        [0.0411],\n",
      "        [0.0414],\n",
      "        [0.0384],\n",
      "        [0.0527],\n",
      "        [0.0472],\n",
      "        [0.0481],\n",
      "        [0.0393],\n",
      "        [0.0525],\n",
      "        [0.0507],\n",
      "        [0.0561],\n",
      "        [0.0536],\n",
      "        [0.0528],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0588],\n",
      "        [0.0607],\n",
      "        [0.0504]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.06815505027771\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 82\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.241151939117117e-07, 93)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [93, 22, 61, 28, 21, 17, 34, 89, 53, 90, 56, 74, 77, 68, 66, 103, 76, 51, 75, 94, 100, 63, 91, 62, 87, 58, 52, 88, 99, 23, 102, 11, 98, 36, 57, 32, 106, 108, 101, 92, 14, 55, 33, 95, 65, 49, 46, 107, 86, 26, 20, 54, 78, 13, 85, 35, 97, 30, 64, 18, 96, 9, 73, 10, 19, 15, 16, 25, 31, 27, 12, 104, 72, 37, 105, 29, 48, 24, 47, 59, 60, 8] 數值 torch.Size([82, 1])\n",
      "目前模型的Data狀態 torch.Size([82, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7154],\n",
      "        [0.8555],\n",
      "        [0.9211],\n",
      "        [0.8503],\n",
      "        [0.8520],\n",
      "        [0.8914],\n",
      "        [0.8106],\n",
      "        [0.7078],\n",
      "        [0.8631],\n",
      "        [0.7081],\n",
      "        [0.9110],\n",
      "        [0.7551],\n",
      "        [0.7282],\n",
      "        [0.8240],\n",
      "        [0.8428],\n",
      "        [0.6675],\n",
      "        [0.7490],\n",
      "        [0.9081],\n",
      "        [0.7355],\n",
      "        [0.6966],\n",
      "        [0.6239],\n",
      "        [0.8731],\n",
      "        [0.6799],\n",
      "        [0.8957],\n",
      "        [0.7227],\n",
      "        [0.9219],\n",
      "        [0.8975],\n",
      "        [0.7192],\n",
      "        [0.6444],\n",
      "        [0.8610],\n",
      "        [0.6567],\n",
      "        [0.9626],\n",
      "        [0.6600],\n",
      "        [0.8250],\n",
      "        [0.9378],\n",
      "        [0.7913],\n",
      "        [0.7157],\n",
      "        [0.7084],\n",
      "        [0.6283],\n",
      "        [0.6862],\n",
      "        [0.9581],\n",
      "        [0.8876],\n",
      "        [0.7973],\n",
      "        [0.6812],\n",
      "        [0.8117],\n",
      "        [0.8766],\n",
      "        [0.8680],\n",
      "        [0.7158],\n",
      "        [0.6906],\n",
      "        [0.8594],\n",
      "        [0.8876],\n",
      "        [0.9222],\n",
      "        [0.6929],\n",
      "        [0.9532],\n",
      "        [0.6927],\n",
      "        [0.8058],\n",
      "        [0.6492],\n",
      "        [0.8855],\n",
      "        [0.8380],\n",
      "        [0.8766],\n",
      "        [0.6574],\n",
      "        [0.9372],\n",
      "        [0.7847],\n",
      "        [0.9534],\n",
      "        [0.8973],\n",
      "        [0.9262],\n",
      "        [0.9024],\n",
      "        [0.8492],\n",
      "        [0.8515],\n",
      "        [0.8518],\n",
      "        [0.9469],\n",
      "        [0.7097],\n",
      "        [0.7986],\n",
      "        [0.8109],\n",
      "        [0.7323],\n",
      "        [0.8831],\n",
      "        [0.9017],\n",
      "        [0.8489],\n",
      "        [0.8861],\n",
      "        [0.9006],\n",
      "        [0.8946],\n",
      "        [0.9132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0008],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0014],\n",
      "        [0.0024],\n",
      "        [0.0039],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0051],\n",
      "        [0.0055],\n",
      "        [0.0065],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0084],\n",
      "        [0.0096],\n",
      "        [0.0103],\n",
      "        [0.0107],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0116],\n",
      "        [0.0124],\n",
      "        [0.0128],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0156],\n",
      "        [0.0164],\n",
      "        [0.0171],\n",
      "        [0.0175],\n",
      "        [0.0178],\n",
      "        [0.0182],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0226],\n",
      "        [0.0238],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0262],\n",
      "        [0.0279],\n",
      "        [0.0283],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0316],\n",
      "        [0.0343],\n",
      "        [0.0343],\n",
      "        [0.0359],\n",
      "        [0.0363],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0384],\n",
      "        [0.0392],\n",
      "        [0.0393],\n",
      "        [0.0393],\n",
      "        [0.0411],\n",
      "        [0.0414],\n",
      "        [0.0426],\n",
      "        [0.0472],\n",
      "        [0.0481],\n",
      "        [0.0504],\n",
      "        [0.0507],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0528],\n",
      "        [0.0536],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0565],\n",
      "        [0.0588],\n",
      "        [0.0607],\n",
      "        [0.0635]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0018],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0067],\n",
      "        [    0.0046],\n",
      "        [    0.0016],\n",
      "        [    0.0067],\n",
      "        [    0.0072],\n",
      "        [    0.0031],\n",
      "        [    0.0069],\n",
      "        [    0.0071],\n",
      "        [    0.0091],\n",
      "        [    0.0087],\n",
      "        [    0.0068],\n",
      "        [    0.0093],\n",
      "        [    0.0078],\n",
      "        [    0.0123],\n",
      "        [    0.0110],\n",
      "        [    0.0122],\n",
      "        [    0.0103],\n",
      "        [    0.0097],\n",
      "        [    0.0112],\n",
      "        [    0.0097],\n",
      "        [    0.0142],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0155],\n",
      "        [    0.0171],\n",
      "        [    0.0143],\n",
      "        [    0.0079],\n",
      "        [    0.0170],\n",
      "        [    0.0200],\n",
      "        [    0.0167],\n",
      "        [    0.0189],\n",
      "        [    0.0174],\n",
      "        [    0.0171],\n",
      "        [    0.0201],\n",
      "        [    0.0183],\n",
      "        [    0.0164],\n",
      "        [    0.0241],\n",
      "        [    0.0235],\n",
      "        [    0.0241],\n",
      "        [    0.0248],\n",
      "        [    0.0214],\n",
      "        [    0.0234],\n",
      "        [    0.0234],\n",
      "        [    0.0231],\n",
      "        [    0.0269],\n",
      "        [    0.0250],\n",
      "        [    0.0277],\n",
      "        [    0.0304],\n",
      "        [    0.0237],\n",
      "        [    0.0331],\n",
      "        [    0.0366],\n",
      "        [    0.0341],\n",
      "        [    0.0356],\n",
      "        [    0.0377],\n",
      "        [    0.0337],\n",
      "        [    0.0375],\n",
      "        [    0.0301],\n",
      "        [    0.0388],\n",
      "        [    0.0306],\n",
      "        [    0.0355],\n",
      "        [    0.0362],\n",
      "        [    0.0368],\n",
      "        [    0.0436],\n",
      "        [    0.0460],\n",
      "        [    0.0470],\n",
      "        [    0.0424],\n",
      "        [    0.0491],\n",
      "        [    0.0524],\n",
      "        [    0.0554],\n",
      "        [    0.0514],\n",
      "        [    0.0531],\n",
      "        [    0.0543],\n",
      "        [    0.0576],\n",
      "        [    0.0551],\n",
      "        [    0.0591],\n",
      "        [    0.0611],\n",
      "        [    0.0559]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.338534355163574\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 83\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.381903171539307e-09, 93)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [93, 61, 28, 21, 89, 22, 56, 34, 17, 53, 103, 74, 77, 90, 51, 11, 66, 68, 76, 91, 87, 63, 94, 62, 52, 100, 75, 88, 58, 102, 99, 14, 57, 98, 108, 23, 106, 92, 32, 36, 101, 49, 86, 107, 46, 33, 13, 55, 95, 65, 20, 26, 54, 9, 78, 10, 85, 18, 97, 19, 30, 15, 35, 16, 96, 64, 73, 12, 25, 31, 27, 104, 105, 72, 29, 48, 47, 37, 8, 24, 59, 2, 60] 數值 torch.Size([83, 1])\n",
      "目前模型的Data狀態 torch.Size([83, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7162],\n",
      "        [0.9201],\n",
      "        [0.8498],\n",
      "        [0.8540],\n",
      "        [0.7102],\n",
      "        [0.8566],\n",
      "        [0.9091],\n",
      "        [0.8099],\n",
      "        [0.8956],\n",
      "        [0.8607],\n",
      "        [0.6659],\n",
      "        [0.7537],\n",
      "        [0.7288],\n",
      "        [0.7105],\n",
      "        [0.9057],\n",
      "        [0.9711],\n",
      "        [0.8442],\n",
      "        [0.8257],\n",
      "        [0.7493],\n",
      "        [0.6817],\n",
      "        [0.7258],\n",
      "        [0.8719],\n",
      "        [0.6967],\n",
      "        [0.8945],\n",
      "        [0.8955],\n",
      "        [0.6231],\n",
      "        [0.7340],\n",
      "        [0.7218],\n",
      "        [0.9212],\n",
      "        [0.6554],\n",
      "        [0.6443],\n",
      "        [0.9636],\n",
      "        [0.9367],\n",
      "        [0.6602],\n",
      "        [0.7063],\n",
      "        [0.8625],\n",
      "        [0.7140],\n",
      "        [0.6874],\n",
      "        [0.7905],\n",
      "        [0.8225],\n",
      "        [0.6273],\n",
      "        [0.8738],\n",
      "        [0.6937],\n",
      "        [0.7141],\n",
      "        [0.8667],\n",
      "        [0.7965],\n",
      "        [0.9597],\n",
      "        [0.8855],\n",
      "        [0.6809],\n",
      "        [0.8108],\n",
      "        [0.8909],\n",
      "        [0.8585],\n",
      "        [0.9203],\n",
      "        [0.9454],\n",
      "        [0.6934],\n",
      "        [0.9622],\n",
      "        [0.6942],\n",
      "        [0.8804],\n",
      "        [0.6495],\n",
      "        [0.9011],\n",
      "        [0.8852],\n",
      "        [0.9311],\n",
      "        [0.8034],\n",
      "        [0.9070],\n",
      "        [0.6575],\n",
      "        [0.8366],\n",
      "        [0.7843],\n",
      "        [0.9548],\n",
      "        [0.8502],\n",
      "        [0.8503],\n",
      "        [0.8506],\n",
      "        [0.7080],\n",
      "        [0.7309],\n",
      "        [0.7985],\n",
      "        [0.8826],\n",
      "        [0.9005],\n",
      "        [0.8847],\n",
      "        [0.8081],\n",
      "        [0.9207],\n",
      "        [0.8504],\n",
      "        [0.9003],\n",
      "        [0.7821],\n",
      "        [0.8942]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0031],\n",
      "        [    0.0046],\n",
      "        [    0.0067],\n",
      "        [    0.0067],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0078],\n",
      "        [    0.0079],\n",
      "        [    0.0087],\n",
      "        [    0.0091],\n",
      "        [    0.0093],\n",
      "        [    0.0097],\n",
      "        [    0.0097],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0112],\n",
      "        [    0.0121],\n",
      "        [    0.0122],\n",
      "        [    0.0123],\n",
      "        [    0.0124],\n",
      "        [    0.0142],\n",
      "        [    0.0143],\n",
      "        [    0.0155],\n",
      "        [    0.0164],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0171],\n",
      "        [    0.0174],\n",
      "        [    0.0183],\n",
      "        [    0.0189],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0214],\n",
      "        [    0.0231],\n",
      "        [    0.0234],\n",
      "        [    0.0234],\n",
      "        [    0.0235],\n",
      "        [    0.0237],\n",
      "        [    0.0241],\n",
      "        [    0.0241],\n",
      "        [    0.0248],\n",
      "        [    0.0250],\n",
      "        [    0.0269],\n",
      "        [    0.0277],\n",
      "        [    0.0301],\n",
      "        [    0.0304],\n",
      "        [    0.0306],\n",
      "        [    0.0331],\n",
      "        [    0.0337],\n",
      "        [    0.0341],\n",
      "        [    0.0355],\n",
      "        [    0.0356],\n",
      "        [    0.0362],\n",
      "        [    0.0366],\n",
      "        [    0.0368],\n",
      "        [    0.0375],\n",
      "        [    0.0377],\n",
      "        [    0.0388],\n",
      "        [    0.0424],\n",
      "        [    0.0436],\n",
      "        [    0.0460],\n",
      "        [    0.0470],\n",
      "        [    0.0491],\n",
      "        [    0.0514],\n",
      "        [    0.0524],\n",
      "        [    0.0531],\n",
      "        [    0.0543],\n",
      "        [    0.0551],\n",
      "        [    0.0554],\n",
      "        [    0.0559],\n",
      "        [    0.0576],\n",
      "        [    0.0591],\n",
      "        [    0.0601],\n",
      "        [    0.0611]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0030],\n",
      "        [0.0015],\n",
      "        [0.0024],\n",
      "        [0.0055],\n",
      "        [0.0025],\n",
      "        [0.0055],\n",
      "        [0.0043],\n",
      "        [0.0027],\n",
      "        [0.0136],\n",
      "        [0.0059],\n",
      "        [0.0077],\n",
      "        [0.0056],\n",
      "        [0.0099],\n",
      "        [0.0116],\n",
      "        [0.0085],\n",
      "        [0.0025],\n",
      "        [0.0129],\n",
      "        [0.0133],\n",
      "        [0.0068],\n",
      "        [0.0058],\n",
      "        [0.0049],\n",
      "        [0.0121],\n",
      "        [0.0085],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0105],\n",
      "        [0.0111],\n",
      "        [0.0082],\n",
      "        [0.0120],\n",
      "        [0.0153],\n",
      "        [0.0131],\n",
      "        [0.0086],\n",
      "        [0.0184],\n",
      "        [0.0144],\n",
      "        [0.0172],\n",
      "        [0.0213],\n",
      "        [0.0180],\n",
      "        [0.0151],\n",
      "        [0.0169],\n",
      "        [0.0200],\n",
      "        [0.0186],\n",
      "        [0.0218],\n",
      "        [0.0184],\n",
      "        [0.0237],\n",
      "        [0.0253],\n",
      "        [0.0216],\n",
      "        [0.0148],\n",
      "        [0.0231],\n",
      "        [0.0219],\n",
      "        [0.0226],\n",
      "        [0.0194],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0198],\n",
      "        [0.0333],\n",
      "        [0.0199],\n",
      "        [0.0368],\n",
      "        [0.0272],\n",
      "        [0.0314],\n",
      "        [0.0293],\n",
      "        [0.0376],\n",
      "        [0.0288],\n",
      "        [0.0364],\n",
      "        [0.0294],\n",
      "        [0.0350],\n",
      "        [0.0360],\n",
      "        [0.0409],\n",
      "        [0.0323],\n",
      "        [0.0473],\n",
      "        [0.0475],\n",
      "        [0.0484],\n",
      "        [0.0498],\n",
      "        [0.0523],\n",
      "        [0.0549],\n",
      "        [0.0549],\n",
      "        [0.0562],\n",
      "        [0.0567],\n",
      "        [0.0557],\n",
      "        [0.0463],\n",
      "        [0.0617],\n",
      "        [0.0567],\n",
      "        [0.0486],\n",
      "        [0.0587]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.61133050918579\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 84\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.147164423149661e-06, 61)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [61, 28, 11, 89, 34, 93, 56, 87, 21, 22, 74, 91, 53, 76, 103, 88, 51, 94, 14, 77, 100, 75, 90, 58, 63, 62, 66, 52, 99, 68, 17, 98, 13, 92, 102, 32, 108, 106, 86, 57, 101, 20, 9, 10, 36, 23, 33, 49, 95, 65, 55, 107, 46, 18, 26, 54, 15, 19, 16, 97, 12, 78, 96, 64, 35, 85, 30, 73, 8, 25, 31, 27, 2, 104, 105, 72, 29, 37, 48, 59, 47, 60, 24, 50] 數值 torch.Size([84, 1])\n",
      "目前模型的Data狀態 torch.Size([84, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9218],\n",
      "        [0.8516],\n",
      "        [0.9815],\n",
      "        [0.7142],\n",
      "        [0.8117],\n",
      "        [0.7191],\n",
      "        [0.9102],\n",
      "        [0.7305],\n",
      "        [0.8588],\n",
      "        [0.8603],\n",
      "        [0.7550],\n",
      "        [0.6856],\n",
      "        [0.8615],\n",
      "        [0.7518],\n",
      "        [0.6669],\n",
      "        [0.7261],\n",
      "        [0.9064],\n",
      "        [0.6992],\n",
      "        [0.9715],\n",
      "        [0.7317],\n",
      "        [0.6249],\n",
      "        [0.7352],\n",
      "        [0.7149],\n",
      "        [0.9234],\n",
      "        [0.8738],\n",
      "        [0.8961],\n",
      "        [0.8484],\n",
      "        [0.8965],\n",
      "        [0.6466],\n",
      "        [0.8300],\n",
      "        [0.9026],\n",
      "        [0.6627],\n",
      "        [0.9685],\n",
      "        [0.6905],\n",
      "        [0.6565],\n",
      "        [0.7926],\n",
      "        [0.7064],\n",
      "        [0.7146],\n",
      "        [0.6984],\n",
      "        [0.9384],\n",
      "        [0.6288],\n",
      "        [0.8965],\n",
      "        [0.9557],\n",
      "        [0.9728],\n",
      "        [0.8225],\n",
      "        [0.8667],\n",
      "        [0.7983],\n",
      "        [0.8742],\n",
      "        [0.6831],\n",
      "        [0.8130],\n",
      "        [0.8865],\n",
      "        [0.7145],\n",
      "        [0.8686],\n",
      "        [0.8869],\n",
      "        [0.8602],\n",
      "        [0.9213],\n",
      "        [0.9385],\n",
      "        [0.9074],\n",
      "        [0.9144],\n",
      "        [0.6521],\n",
      "        [0.9650],\n",
      "        [0.6963],\n",
      "        [0.6601],\n",
      "        [0.8383],\n",
      "        [0.8037],\n",
      "        [0.6978],\n",
      "        [0.8872],\n",
      "        [0.7864],\n",
      "        [0.9304],\n",
      "        [0.8538],\n",
      "        [0.8517],\n",
      "        [0.8520],\n",
      "        [0.7936],\n",
      "        [0.7088],\n",
      "        [0.7318],\n",
      "        [0.8010],\n",
      "        [0.8845],\n",
      "        [0.8079],\n",
      "        [0.9025],\n",
      "        [0.9027],\n",
      "        [0.8864],\n",
      "        [0.8966],\n",
      "        [0.8546],\n",
      "        [0.9530]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0015],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0043],\n",
      "        [0.0049],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0068],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0099],\n",
      "        [0.0105],\n",
      "        [0.0111],\n",
      "        [0.0116],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0131],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0136],\n",
      "        [0.0144],\n",
      "        [0.0148],\n",
      "        [0.0151],\n",
      "        [0.0153],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0226],\n",
      "        [0.0231],\n",
      "        [0.0237],\n",
      "        [0.0253],\n",
      "        [0.0272],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0288],\n",
      "        [0.0293],\n",
      "        [0.0294],\n",
      "        [0.0314],\n",
      "        [0.0323],\n",
      "        [0.0333],\n",
      "        [0.0350],\n",
      "        [0.0360],\n",
      "        [0.0364],\n",
      "        [0.0368],\n",
      "        [0.0376],\n",
      "        [0.0409],\n",
      "        [0.0463],\n",
      "        [0.0473],\n",
      "        [0.0475],\n",
      "        [0.0484],\n",
      "        [0.0486],\n",
      "        [0.0498],\n",
      "        [0.0523],\n",
      "        [0.0549],\n",
      "        [0.0549],\n",
      "        [0.0557],\n",
      "        [0.0562],\n",
      "        [0.0567],\n",
      "        [0.0567],\n",
      "        [0.0587],\n",
      "        [0.0617],\n",
      "        [0.0638]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0018],\n",
      "        [0.0004],\n",
      "        [0.0058],\n",
      "        [0.0033],\n",
      "        [0.0051],\n",
      "        [0.0028],\n",
      "        [0.0003],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0035],\n",
      "        [0.0078],\n",
      "        [0.0048],\n",
      "        [0.0098],\n",
      "        [0.0074],\n",
      "        [0.0062],\n",
      "        [0.0071],\n",
      "        [0.0040],\n",
      "        [0.0091],\n",
      "        [0.0078],\n",
      "        [0.0098],\n",
      "        [0.0111],\n",
      "        [0.0132],\n",
      "        [0.0126],\n",
      "        [0.0151],\n",
      "        [0.0092],\n",
      "        [0.0096],\n",
      "        [0.0119],\n",
      "        [0.0091],\n",
      "        [0.0133],\n",
      "        [0.0127],\n",
      "        [0.0139],\n",
      "        [0.0145],\n",
      "        [0.0131],\n",
      "        [0.0147],\n",
      "        [0.0140],\n",
      "        [0.0192],\n",
      "        [0.0149],\n",
      "        [0.0159],\n",
      "        [0.0165],\n",
      "        [0.0149],\n",
      "        [0.0194],\n",
      "        [0.0200],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        [0.0240],\n",
      "        [0.0198],\n",
      "        [0.0240],\n",
      "        [0.0168],\n",
      "        [0.0228],\n",
      "        [0.0251],\n",
      "        [0.0270],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0272],\n",
      "        [0.0253],\n",
      "        [0.0247],\n",
      "        [0.0282],\n",
      "        [0.0295],\n",
      "        [0.0289],\n",
      "        [0.0314],\n",
      "        [0.0293],\n",
      "        [0.0333],\n",
      "        [0.0352],\n",
      "        [0.0389],\n",
      "        [0.0400],\n",
      "        [0.0373],\n",
      "        [0.0349],\n",
      "        [0.0391],\n",
      "        [0.0431],\n",
      "        [0.0454],\n",
      "        [0.0444],\n",
      "        [0.0450],\n",
      "        [0.0439],\n",
      "        [0.0478],\n",
      "        [0.0504],\n",
      "        [0.0533],\n",
      "        [0.0520],\n",
      "        [0.0598],\n",
      "        [0.0524],\n",
      "        [0.0593],\n",
      "        [0.0530],\n",
      "        [0.0612],\n",
      "        [0.0604],\n",
      "        [0.0583]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.882877111434937\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 85\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.432857612206135e-08, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 28, 61, 93, 89, 87, 22, 51, 21, 91, 34, 11, 103, 88, 76, 14, 74, 94, 52, 63, 62, 53, 77, 100, 66, 90, 68, 13, 75, 99, 17, 102, 98, 92, 108, 57, 58, 106, 10, 9, 86, 49, 32, 101, 23, 20, 107, 46, 95, 36, 33, 54, 65, 26, 55, 18, 15, 16, 12, 19, 97, 78, 30, 96, 85, 64, 73, 35, 8, 2, 31, 27, 25, 104, 105, 29, 48, 47, 72, 50, 59, 37, 1, 24, 60] 數值 torch.Size([85, 1])\n",
      "目前模型的Data狀態 torch.Size([85, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9062],\n",
      "        [0.8488],\n",
      "        [0.9185],\n",
      "        [0.7189],\n",
      "        [0.7151],\n",
      "        [0.7321],\n",
      "        [0.8582],\n",
      "        [0.9019],\n",
      "        [0.8576],\n",
      "        [0.6866],\n",
      "        [0.8093],\n",
      "        [0.9849],\n",
      "        [0.6653],\n",
      "        [0.7271],\n",
      "        [0.7512],\n",
      "        [0.9722],\n",
      "        [0.7528],\n",
      "        [0.6986],\n",
      "        [0.8926],\n",
      "        [0.8709],\n",
      "        [0.8929],\n",
      "        [0.8576],\n",
      "        [0.7316],\n",
      "        [0.6243],\n",
      "        [0.8473],\n",
      "        [0.7160],\n",
      "        [0.8293],\n",
      "        [0.9702],\n",
      "        [0.7331],\n",
      "        [0.6464],\n",
      "        [0.9028],\n",
      "        [0.6552],\n",
      "        [0.6626],\n",
      "        [0.6909],\n",
      "        [0.7041],\n",
      "        [0.9349],\n",
      "        [0.9203],\n",
      "        [0.7125],\n",
      "        [0.9766],\n",
      "        [0.9594],\n",
      "        [0.7002],\n",
      "        [0.8693],\n",
      "        [0.7902],\n",
      "        [0.6280],\n",
      "        [0.8652],\n",
      "        [0.8959],\n",
      "        [0.7123],\n",
      "        [0.8651],\n",
      "        [0.6822],\n",
      "        [0.8186],\n",
      "        [0.7960],\n",
      "        [0.9173],\n",
      "        [0.8105],\n",
      "        [0.8568],\n",
      "        [0.8826],\n",
      "        [0.8869],\n",
      "        [0.9392],\n",
      "        [0.9149],\n",
      "        [0.9679],\n",
      "        [0.9072],\n",
      "        [0.6522],\n",
      "        [0.6963],\n",
      "        [0.8845],\n",
      "        [0.6599],\n",
      "        [0.6984],\n",
      "        [0.8354],\n",
      "        [0.7846],\n",
      "        [0.8000],\n",
      "        [0.9336],\n",
      "        [0.7983],\n",
      "        [0.8487],\n",
      "        [0.8486],\n",
      "        [0.8520],\n",
      "        [0.7068],\n",
      "        [0.7298],\n",
      "        [0.8815],\n",
      "        [0.8987],\n",
      "        [0.8826],\n",
      "        [0.7994],\n",
      "        [0.9476],\n",
      "        [0.9001],\n",
      "        [0.8037],\n",
      "        [0.7532],\n",
      "        [0.8532],\n",
      "        [0.8941]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0004],\n",
      "        [0.0018],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0048],\n",
      "        [0.0051],\n",
      "        [0.0058],\n",
      "        [0.0062],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0078],\n",
      "        [0.0078],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0111],\n",
      "        [0.0119],\n",
      "        [0.0126],\n",
      "        [0.0127],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0140],\n",
      "        [0.0145],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        [0.0165],\n",
      "        [0.0168],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0228],\n",
      "        [0.0240],\n",
      "        [0.0240],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0253],\n",
      "        [0.0270],\n",
      "        [0.0272],\n",
      "        [0.0282],\n",
      "        [0.0289],\n",
      "        [0.0293],\n",
      "        [0.0295],\n",
      "        [0.0314],\n",
      "        [0.0333],\n",
      "        [0.0349],\n",
      "        [0.0352],\n",
      "        [0.0373],\n",
      "        [0.0389],\n",
      "        [0.0391],\n",
      "        [0.0400],\n",
      "        [0.0431],\n",
      "        [0.0439],\n",
      "        [0.0444],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0478],\n",
      "        [0.0504],\n",
      "        [0.0520],\n",
      "        [0.0524],\n",
      "        [0.0530],\n",
      "        [0.0533],\n",
      "        [0.0583],\n",
      "        [0.0593],\n",
      "        [0.0598],\n",
      "        [0.0604],\n",
      "        [0.0604],\n",
      "        [0.0612]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0019],\n",
      "        [0.0023],\n",
      "        [0.0039],\n",
      "        [0.0021],\n",
      "        [0.0033],\n",
      "        [0.0030],\n",
      "        [0.0028],\n",
      "        [0.0013],\n",
      "        [0.0044],\n",
      "        [0.0046],\n",
      "        [0.0066],\n",
      "        [0.0096],\n",
      "        [0.0047],\n",
      "        [0.0071],\n",
      "        [0.0084],\n",
      "        [0.0058],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0068],\n",
      "        [0.0076],\n",
      "        [0.0075],\n",
      "        [0.0121],\n",
      "        [0.0095],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0129],\n",
      "        [0.0127],\n",
      "        [0.0103],\n",
      "        [0.0149],\n",
      "        [0.0138],\n",
      "        [0.0157],\n",
      "        [0.0125],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0122],\n",
      "        [0.0128],\n",
      "        [0.0167],\n",
      "        [0.0136],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0158],\n",
      "        [0.0140],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0196],\n",
      "        [0.0193],\n",
      "        [0.0191],\n",
      "        [0.0204],\n",
      "        [0.0236],\n",
      "        [0.0270],\n",
      "        [0.0253],\n",
      "        [0.0223],\n",
      "        [0.0262],\n",
      "        [0.0233],\n",
      "        [0.0292],\n",
      "        [0.0257],\n",
      "        [0.0262],\n",
      "        [0.0267],\n",
      "        [0.0255],\n",
      "        [0.0285],\n",
      "        [0.0316],\n",
      "        [0.0332],\n",
      "        [0.0329],\n",
      "        [0.0355],\n",
      "        [0.0373],\n",
      "        [0.0404],\n",
      "        [0.0378],\n",
      "        [0.0426],\n",
      "        [0.0396],\n",
      "        [0.0376],\n",
      "        [0.0425],\n",
      "        [0.0427],\n",
      "        [0.0449],\n",
      "        [0.0458],\n",
      "        [0.0483],\n",
      "        [0.0499],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0523],\n",
      "        [0.0548],\n",
      "        [0.0606],\n",
      "        [0.0629],\n",
      "        [0.0539],\n",
      "        [0.0603],\n",
      "        [0.0626]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.15606951713562\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 86\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.710146989353234e-06, 51)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [51, 56, 93, 28, 22, 87, 89, 61, 21, 91, 103, 14, 34, 52, 88, 62, 63, 76, 77, 74, 11, 94, 13, 100, 66, 10, 53, 108, 9, 102, 68, 57, 90, 106, 99, 49, 75, 98, 92, 17, 86, 58, 107, 20, 23, 32, 101, 46, 54, 26, 95, 33, 12, 18, 65, 15, 16, 36, 19, 55, 97, 30, 78, 96, 85, 2, 73, 8, 64, 31, 35, 27, 25, 104, 105, 29, 48, 47, 72, 1, 50, 24, 59, 0, 60, 37] 數值 torch.Size([86, 1])\n",
      "目前模型的Data狀態 torch.Size([86, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8992],\n",
      "        [0.9040],\n",
      "        [0.7182],\n",
      "        [0.8469],\n",
      "        [0.8575],\n",
      "        [0.7325],\n",
      "        [0.7150],\n",
      "        [0.9164],\n",
      "        [0.8578],\n",
      "        [0.6868],\n",
      "        [0.6638],\n",
      "        [0.9742],\n",
      "        [0.8079],\n",
      "        [0.8902],\n",
      "        [0.7272],\n",
      "        [0.8908],\n",
      "        [0.8693],\n",
      "        [0.7502],\n",
      "        [0.7312],\n",
      "        [0.7510],\n",
      "        [0.9887],\n",
      "        [0.6979],\n",
      "        [0.9730],\n",
      "        [0.6235],\n",
      "        [0.8475],\n",
      "        [0.9808],\n",
      "        [0.8554],\n",
      "        [0.7015],\n",
      "        [0.9633],\n",
      "        [0.6537],\n",
      "        [0.8294],\n",
      "        [0.9328],\n",
      "        [0.7163],\n",
      "        [0.7102],\n",
      "        [0.6460],\n",
      "        [0.8664],\n",
      "        [0.7314],\n",
      "        [0.6623],\n",
      "        [0.6906],\n",
      "        [0.9046],\n",
      "        [0.7010],\n",
      "        [0.9187],\n",
      "        [0.7098],\n",
      "        [0.8966],\n",
      "        [0.8650],\n",
      "        [0.7891],\n",
      "        [0.6271],\n",
      "        [0.8636],\n",
      "        [0.9148],\n",
      "        [0.8548],\n",
      "        [0.6814],\n",
      "        [0.7947],\n",
      "        [0.9717],\n",
      "        [0.8884],\n",
      "        [0.8095],\n",
      "        [0.9411],\n",
      "        [0.9170],\n",
      "        [0.8156],\n",
      "        [0.9082],\n",
      "        [0.8804],\n",
      "        [0.6520],\n",
      "        [0.8825],\n",
      "        [0.6963],\n",
      "        [0.6595],\n",
      "        [0.6984],\n",
      "        [0.8046],\n",
      "        [0.7833],\n",
      "        [0.9371],\n",
      "        [0.8340],\n",
      "        [0.8468],\n",
      "        [0.7974],\n",
      "        [0.8464],\n",
      "        [0.8514],\n",
      "        [0.7048],\n",
      "        [0.7277],\n",
      "        [0.8794],\n",
      "        [0.8970],\n",
      "        [0.8808],\n",
      "        [0.7984],\n",
      "        [0.7596],\n",
      "        [0.9440],\n",
      "        [0.8531],\n",
      "        [0.8988],\n",
      "        [0.7430],\n",
      "        [0.8926],\n",
      "        [0.8006]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0033],\n",
      "        [0.0039],\n",
      "        [0.0044],\n",
      "        [0.0046],\n",
      "        [0.0047],\n",
      "        [0.0058],\n",
      "        [0.0066],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0084],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0103],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0127],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0158],\n",
      "        [0.0167],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0223],\n",
      "        [0.0233],\n",
      "        [0.0236],\n",
      "        [0.0253],\n",
      "        [0.0255],\n",
      "        [0.0257],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0267],\n",
      "        [0.0270],\n",
      "        [0.0285],\n",
      "        [0.0292],\n",
      "        [0.0316],\n",
      "        [0.0329],\n",
      "        [0.0332],\n",
      "        [0.0355],\n",
      "        [0.0373],\n",
      "        [0.0376],\n",
      "        [0.0378],\n",
      "        [0.0396],\n",
      "        [0.0404],\n",
      "        [0.0425],\n",
      "        [0.0426],\n",
      "        [0.0427],\n",
      "        [0.0449],\n",
      "        [0.0458],\n",
      "        [0.0483],\n",
      "        [0.0499],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0523],\n",
      "        [0.0539],\n",
      "        [0.0548],\n",
      "        [0.0603],\n",
      "        [0.0606],\n",
      "        [0.0610],\n",
      "        [0.0626],\n",
      "        [0.0629]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0037],\n",
      "        [0.0017],\n",
      "        [0.0040],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0033],\n",
      "        [0.0059],\n",
      "        [0.0053],\n",
      "        [0.0041],\n",
      "        [0.0038],\n",
      "        [0.0034],\n",
      "        [0.0077],\n",
      "        [0.0048],\n",
      "        [0.0070],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0108],\n",
      "        [0.0137],\n",
      "        [0.0102],\n",
      "        [0.0070],\n",
      "        [0.0119],\n",
      "        [0.0128],\n",
      "        [0.0077],\n",
      "        [0.0138],\n",
      "        [0.0097],\n",
      "        [0.0081],\n",
      "        [0.0115],\n",
      "        [0.0133],\n",
      "        [0.0111],\n",
      "        [0.0134],\n",
      "        [0.0115],\n",
      "        [0.0136],\n",
      "        [0.0119],\n",
      "        [0.0159],\n",
      "        [0.0147],\n",
      "        [0.0152],\n",
      "        [0.0182],\n",
      "        [0.0150],\n",
      "        [0.0179],\n",
      "        [0.0167],\n",
      "        [0.0183],\n",
      "        [0.0198],\n",
      "        [0.0209],\n",
      "        [0.0206],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0217],\n",
      "        [0.0240],\n",
      "        [0.0261],\n",
      "        [0.0212],\n",
      "        [0.0236],\n",
      "        [0.0265],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0299],\n",
      "        [0.0271],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0310],\n",
      "        [0.0337],\n",
      "        [0.0353],\n",
      "        [0.0377],\n",
      "        [0.0297],\n",
      "        [0.0370],\n",
      "        [0.0358],\n",
      "        [0.0412],\n",
      "        [0.0408],\n",
      "        [0.0448],\n",
      "        [0.0408],\n",
      "        [0.0449],\n",
      "        [0.0443],\n",
      "        [0.0465],\n",
      "        [0.0478],\n",
      "        [0.0495],\n",
      "        [0.0498],\n",
      "        [0.0516],\n",
      "        [0.0457],\n",
      "        [0.0516],\n",
      "        [0.0607],\n",
      "        [0.0617],\n",
      "        [0.0523],\n",
      "        [0.0637],\n",
      "        [0.0659]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.432887315750122\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 87\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.450801942672115e-07, 51)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [51, 93, 87, 22, 89, 14, 56, 103, 28, 91, 52, 21, 62, 61, 63, 88, 13, 10, 34, 9, 76, 77, 108, 94, 74, 57, 106, 102, 100, 49, 66, 68, 90, 99, 11, 53, 98, 86, 92, 75, 107, 58, 17, 20, 46, 23, 54, 101, 32, 12, 26, 18, 15, 16, 95, 33, 65, 19, 2, 36, 55, 30, 97, 78, 96, 8, 73, 85, 31, 27, 64, 104, 35, 25, 1, 105, 29, 48, 47, 50, 72, 0, 24, 59, 60, 37, 109] 數值 torch.Size([87, 1])\n",
      "目前模型的Data狀態 torch.Size([87, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8969],\n",
      "        [0.7178],\n",
      "        [0.7329],\n",
      "        [0.8573],\n",
      "        [0.7151],\n",
      "        [0.9766],\n",
      "        [0.9022],\n",
      "        [0.6629],\n",
      "        [0.8452],\n",
      "        [0.6873],\n",
      "        [0.8882],\n",
      "        [0.8587],\n",
      "        [0.8891],\n",
      "        [0.9144],\n",
      "        [0.8682],\n",
      "        [0.7273],\n",
      "        [0.9763],\n",
      "        [0.9851],\n",
      "        [0.8068],\n",
      "        [0.9674],\n",
      "        [0.7493],\n",
      "        [0.7311],\n",
      "        [0.6990],\n",
      "        [0.6976],\n",
      "        [0.7498],\n",
      "        [0.9311],\n",
      "        [0.7081],\n",
      "        [0.6527],\n",
      "        [0.6235],\n",
      "        [0.8644],\n",
      "        [0.8483],\n",
      "        [0.8300],\n",
      "        [0.7168],\n",
      "        [0.6462],\n",
      "        [0.9927],\n",
      "        [0.8537],\n",
      "        [0.6625],\n",
      "        [0.7018],\n",
      "        [0.6905],\n",
      "        [0.7304],\n",
      "        [0.7074],\n",
      "        [0.9175],\n",
      "        [0.9072],\n",
      "        [0.8976],\n",
      "        [0.8629],\n",
      "        [0.8652],\n",
      "        [0.9126],\n",
      "        [0.6268],\n",
      "        [0.7885],\n",
      "        [0.9760],\n",
      "        [0.8532],\n",
      "        [0.8905],\n",
      "        [0.9435],\n",
      "        [0.9199],\n",
      "        [0.6811],\n",
      "        [0.7939],\n",
      "        [0.8091],\n",
      "        [0.9096],\n",
      "        [0.8124],\n",
      "        [0.8127],\n",
      "        [0.8787],\n",
      "        [0.8806],\n",
      "        [0.6524],\n",
      "        [0.6967],\n",
      "        [0.6598],\n",
      "        [0.9409],\n",
      "        [0.7825],\n",
      "        [0.6987],\n",
      "        [0.8451],\n",
      "        [0.8445],\n",
      "        [0.8332],\n",
      "        [0.7032],\n",
      "        [0.7952],\n",
      "        [0.8514],\n",
      "        [0.7679],\n",
      "        [0.7259],\n",
      "        [0.8773],\n",
      "        [0.8958],\n",
      "        [0.8795],\n",
      "        [0.9408],\n",
      "        [0.7978],\n",
      "        [0.7517],\n",
      "        [0.8535],\n",
      "        [0.8977],\n",
      "        [0.8915],\n",
      "        [0.7976],\n",
      "        [0.7399]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0017],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0048],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0070],\n",
      "        [0.0070],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0115],\n",
      "        [0.0115],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0128],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0152],\n",
      "        [0.0159],\n",
      "        [0.0167],\n",
      "        [0.0179],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0217],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0240],\n",
      "        [0.0261],\n",
      "        [0.0265],\n",
      "        [0.0271],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0312],\n",
      "        [0.0337],\n",
      "        [0.0353],\n",
      "        [0.0358],\n",
      "        [0.0370],\n",
      "        [0.0377],\n",
      "        [0.0408],\n",
      "        [0.0408],\n",
      "        [0.0412],\n",
      "        [0.0443],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0457],\n",
      "        [0.0465],\n",
      "        [0.0478],\n",
      "        [0.0495],\n",
      "        [0.0498],\n",
      "        [0.0516],\n",
      "        [0.0516],\n",
      "        [0.0523],\n",
      "        [0.0607],\n",
      "        [0.0617],\n",
      "        [0.0637],\n",
      "        [0.0659],\n",
      "        [0.0679]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0016],\n",
      "        [0.0026],\n",
      "        [0.0040],\n",
      "        [0.0030],\n",
      "        [0.0005],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0041],\n",
      "        [0.0039],\n",
      "        [0.0049],\n",
      "        [0.0076],\n",
      "        [0.0064],\n",
      "        [0.0054],\n",
      "        [0.0078],\n",
      "        [0.0073],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0071],\n",
      "        [0.0037],\n",
      "        [0.0091],\n",
      "        [0.0103],\n",
      "        [0.0070],\n",
      "        [0.0102],\n",
      "        [0.0101],\n",
      "        [0.0118],\n",
      "        [0.0093],\n",
      "        [0.0107],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0153],\n",
      "        [0.0153],\n",
      "        [0.0136],\n",
      "        [0.0133],\n",
      "        [0.0185],\n",
      "        [0.0132],\n",
      "        [0.0144],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0152],\n",
      "        [0.0142],\n",
      "        [0.0167],\n",
      "        [0.0224],\n",
      "        [0.0158],\n",
      "        [0.0216],\n",
      "        [0.0215],\n",
      "        [0.0203],\n",
      "        [0.0208],\n",
      "        [0.0197],\n",
      "        [0.0160],\n",
      "        [0.0220],\n",
      "        [0.0200],\n",
      "        [0.0199],\n",
      "        [0.0196],\n",
      "        [0.0239],\n",
      "        [0.0251],\n",
      "        [0.0247],\n",
      "        [0.0241],\n",
      "        [0.0218],\n",
      "        [0.0304],\n",
      "        [0.0302],\n",
      "        [0.0308],\n",
      "        [0.0306],\n",
      "        [0.0351],\n",
      "        [0.0347],\n",
      "        [0.0319],\n",
      "        [0.0382],\n",
      "        [0.0377],\n",
      "        [0.0409],\n",
      "        [0.0407],\n",
      "        [0.0396],\n",
      "        [0.0429],\n",
      "        [0.0448],\n",
      "        [0.0463],\n",
      "        [0.0373],\n",
      "        [0.0447],\n",
      "        [0.0474],\n",
      "        [0.0511],\n",
      "        [0.0513],\n",
      "        [0.0509],\n",
      "        [0.0530],\n",
      "        [0.0436],\n",
      "        [0.0626],\n",
      "        [0.0606],\n",
      "        [0.0628],\n",
      "        [0.0665],\n",
      "        [0.0648]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.709242820739746\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 88\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.4233443696175527e-07, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [14, 51, 93, 13, 87, 10, 89, 56, 103, 9, 91, 22, 28, 52, 61, 62, 108, 34, 88, 21, 63, 76, 106, 74, 94, 77, 102, 57, 100, 49, 53, 99, 90, 107, 98, 86, 75, 68, 66, 92, 20, 12, 58, 11, 16, 32, 15, 18, 54, 101, 23, 46, 2, 26, 17, 95, 19, 65, 33, 55, 36, 97, 30, 8, 96, 78, 1, 85, 73, 64, 27, 31, 104, 0, 105, 35, 25, 29, 50, 48, 47, 72, 59, 3, 24, 60, 109, 37] 數值 torch.Size([88, 1])\n",
      "目前模型的Data狀態 torch.Size([88, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9805],\n",
      "        [0.8968],\n",
      "        [0.7177],\n",
      "        [0.9810],\n",
      "        [0.7329],\n",
      "        [0.9899],\n",
      "        [0.7147],\n",
      "        [0.9029],\n",
      "        [0.6622],\n",
      "        [0.9718],\n",
      "        [0.6875],\n",
      "        [0.8588],\n",
      "        [0.8451],\n",
      "        [0.8883],\n",
      "        [0.9149],\n",
      "        [0.8897],\n",
      "        [0.6963],\n",
      "        [0.8074],\n",
      "        [0.7269],\n",
      "        [0.8609],\n",
      "        [0.8695],\n",
      "        [0.7495],\n",
      "        [0.7059],\n",
      "        [0.7505],\n",
      "        [0.6975],\n",
      "        [0.7320],\n",
      "        [0.6519],\n",
      "        [0.9318],\n",
      "        [0.6235],\n",
      "        [0.8649],\n",
      "        [0.8542],\n",
      "        [0.6464],\n",
      "        [0.7170],\n",
      "        [0.7049],\n",
      "        [0.6628],\n",
      "        [0.7020],\n",
      "        [0.7311],\n",
      "        [0.8319],\n",
      "        [0.8508],\n",
      "        [0.6903],\n",
      "        [0.9001],\n",
      "        [0.9813],\n",
      "        [0.9187],\n",
      "        [0.9975],\n",
      "        [0.9242],\n",
      "        [0.7898],\n",
      "        [0.9474],\n",
      "        [0.8941],\n",
      "        [0.9129],\n",
      "        [0.6267],\n",
      "        [0.8670],\n",
      "        [0.8649],\n",
      "        [0.8204],\n",
      "        [0.8536],\n",
      "        [0.9114],\n",
      "        [0.6811],\n",
      "        [0.9125],\n",
      "        [0.8109],\n",
      "        [0.7949],\n",
      "        [0.8794],\n",
      "        [0.8121],\n",
      "        [0.6530],\n",
      "        [0.8804],\n",
      "        [0.9448],\n",
      "        [0.6604],\n",
      "        [0.6981],\n",
      "        [0.7762],\n",
      "        [0.6988],\n",
      "        [0.7837],\n",
      "        [0.8347],\n",
      "        [0.8444],\n",
      "        [0.8452],\n",
      "        [0.7019],\n",
      "        [0.7604],\n",
      "        [0.7241],\n",
      "        [0.7952],\n",
      "        [0.8529],\n",
      "        [0.8770],\n",
      "        [0.9401],\n",
      "        [0.8974],\n",
      "        [0.8809],\n",
      "        [0.7991],\n",
      "        [0.8988],\n",
      "        [0.7827],\n",
      "        [0.8554],\n",
      "        [0.8924],\n",
      "        [0.7368],\n",
      "        [0.7970]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0107],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0136],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0148],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0167],\n",
      "        [0.0185],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0208],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0239],\n",
      "        [0.0241],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0302],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0319],\n",
      "        [0.0347],\n",
      "        [0.0351],\n",
      "        [0.0373],\n",
      "        [0.0377],\n",
      "        [0.0382],\n",
      "        [0.0396],\n",
      "        [0.0407],\n",
      "        [0.0409],\n",
      "        [0.0429],\n",
      "        [0.0436],\n",
      "        [0.0447],\n",
      "        [0.0448],\n",
      "        [0.0463],\n",
      "        [0.0474],\n",
      "        [0.0509],\n",
      "        [0.0511],\n",
      "        [0.0513],\n",
      "        [0.0530],\n",
      "        [0.0606],\n",
      "        [0.0620],\n",
      "        [0.0626],\n",
      "        [0.0628],\n",
      "        [0.0648],\n",
      "        [0.0665]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0037],\n",
      "        [    0.0001],\n",
      "        [    0.0017],\n",
      "        [    0.0036],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0052],\n",
      "        [    0.0013],\n",
      "        [    0.0023],\n",
      "        [    0.0047],\n",
      "        [    0.0023],\n",
      "        [    0.0067],\n",
      "        [    0.0025],\n",
      "        [    0.0081],\n",
      "        [    0.0040],\n",
      "        [    0.0038],\n",
      "        [    0.0086],\n",
      "        [    0.0085],\n",
      "        [    0.0066],\n",
      "        [    0.0061],\n",
      "        [    0.0106],\n",
      "        [    0.0064],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0095],\n",
      "        [    0.0089],\n",
      "        [    0.0094],\n",
      "        [    0.0127],\n",
      "        [    0.0101],\n",
      "        [    0.0151],\n",
      "        [    0.0140],\n",
      "        [    0.0127],\n",
      "        [    0.0110],\n",
      "        [    0.0151],\n",
      "        [    0.0152],\n",
      "        [    0.0165],\n",
      "        [    0.0145],\n",
      "        [    0.0147],\n",
      "        [    0.0166],\n",
      "        [    0.0168],\n",
      "        [    0.0146],\n",
      "        [    0.0188],\n",
      "        [    0.0197],\n",
      "        [    0.0190],\n",
      "        [    0.0208],\n",
      "        [    0.0197],\n",
      "        [    0.0200],\n",
      "        [    0.0179],\n",
      "        [    0.0219],\n",
      "        [    0.0202],\n",
      "        [    0.0201],\n",
      "        [    0.0167],\n",
      "        [    0.0194],\n",
      "        [    0.0227],\n",
      "        [    0.0250],\n",
      "        [    0.0248],\n",
      "        [    0.0259],\n",
      "        [    0.0263],\n",
      "        [    0.0322],\n",
      "        [    0.0333],\n",
      "        [    0.0310],\n",
      "        [    0.0281],\n",
      "        [    0.0307],\n",
      "        [    0.0352],\n",
      "        [    0.0350],\n",
      "        [    0.0318],\n",
      "        [    0.0369],\n",
      "        [    0.0370],\n",
      "        [    0.0411],\n",
      "        [    0.0379],\n",
      "        [    0.0387],\n",
      "        [    0.0404],\n",
      "        [    0.0378],\n",
      "        [    0.0420],\n",
      "        [    0.0472],\n",
      "        [    0.0449],\n",
      "        [    0.0446],\n",
      "        [    0.0472],\n",
      "        [    0.0493],\n",
      "        [    0.0493],\n",
      "        [    0.0519],\n",
      "        [    0.0625],\n",
      "        [    0.0566],\n",
      "        [    0.0615],\n",
      "        [    0.0648],\n",
      "        [    0.0610],\n",
      "        [    0.0695]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.985358476638794\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 89\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.546411578436164e-09, 93)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [93, 14, 103, 10, 13, 89, 22, 9, 52, 87, 51, 108, 62, 91, 56, 63, 106, 21, 28, 61, 88, 34, 102, 57, 77, 49, 76, 107, 74, 94, 100, 90, 99, 68, 12, 66, 98, 53, 86, 75, 92, 2, 20, 54, 58, 16, 26, 11, 15, 18, 46, 23, 32, 101, 17, 19, 95, 65, 33, 30, 8, 97, 1, 55, 36, 78, 96, 85, 73, 0, 27, 31, 104, 64, 105, 29, 25, 35, 50, 48, 47, 72, 3, 109, 24, 59, 60, 37, 7] 數值 torch.Size([89, 1])\n",
      "目前模型的Data狀態 torch.Size([89, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7162],\n",
      "        [0.9805],\n",
      "        [0.6604],\n",
      "        [0.9914],\n",
      "        [0.9817],\n",
      "        [0.7135],\n",
      "        [0.8570],\n",
      "        [0.9732],\n",
      "        [0.8860],\n",
      "        [0.7319],\n",
      "        [0.8942],\n",
      "        [0.6930],\n",
      "        [0.8873],\n",
      "        [0.6868],\n",
      "        [0.9007],\n",
      "        [0.8678],\n",
      "        [0.7030],\n",
      "        [0.8600],\n",
      "        [0.8425],\n",
      "        [0.9122],\n",
      "        [0.7257],\n",
      "        [0.8058],\n",
      "        [0.6500],\n",
      "        [0.9294],\n",
      "        [0.7313],\n",
      "        [0.8625],\n",
      "        [0.7480],\n",
      "        [0.7017],\n",
      "        [0.7491],\n",
      "        [0.6962],\n",
      "        [0.6226],\n",
      "        [0.7161],\n",
      "        [0.6457],\n",
      "        [0.8311],\n",
      "        [0.9826],\n",
      "        [0.8502],\n",
      "        [0.6621],\n",
      "        [0.8523],\n",
      "        [0.7015],\n",
      "        [0.7298],\n",
      "        [0.6891],\n",
      "        [0.8254],\n",
      "        [0.8991],\n",
      "        [0.9104],\n",
      "        [0.9167],\n",
      "        [0.9248],\n",
      "        [0.8510],\n",
      "        [0.9987],\n",
      "        [0.9477],\n",
      "        [0.8941],\n",
      "        [0.8634],\n",
      "        [0.8657],\n",
      "        [0.7886],\n",
      "        [0.6256],\n",
      "        [0.9117],\n",
      "        [0.9118],\n",
      "        [0.6800],\n",
      "        [0.8097],\n",
      "        [0.7937],\n",
      "        [0.8777],\n",
      "        [0.9460],\n",
      "        [0.6525],\n",
      "        [0.7817],\n",
      "        [0.8774],\n",
      "        [0.8093],\n",
      "        [0.6980],\n",
      "        [0.6599],\n",
      "        [0.6979],\n",
      "        [0.7825],\n",
      "        [0.7661],\n",
      "        [0.8415],\n",
      "        [0.8430],\n",
      "        [0.6994],\n",
      "        [0.8333],\n",
      "        [0.7214],\n",
      "        [0.8741],\n",
      "        [0.8514],\n",
      "        [0.7929],\n",
      "        [0.9364],\n",
      "        [0.8956],\n",
      "        [0.8790],\n",
      "        [0.7981],\n",
      "        [0.7881],\n",
      "        [0.7331],\n",
      "        [0.8544],\n",
      "        [0.8969],\n",
      "        [0.8904],\n",
      "        [0.7941],\n",
      "        [0.9245]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0013],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0017],\n",
      "        [    0.0023],\n",
      "        [    0.0023],\n",
      "        [    0.0025],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0038],\n",
      "        [    0.0040],\n",
      "        [    0.0047],\n",
      "        [    0.0052],\n",
      "        [    0.0061],\n",
      "        [    0.0064],\n",
      "        [    0.0066],\n",
      "        [    0.0067],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0086],\n",
      "        [    0.0089],\n",
      "        [    0.0094],\n",
      "        [    0.0095],\n",
      "        [    0.0101],\n",
      "        [    0.0106],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0127],\n",
      "        [    0.0127],\n",
      "        [    0.0140],\n",
      "        [    0.0145],\n",
      "        [    0.0146],\n",
      "        [    0.0147],\n",
      "        [    0.0151],\n",
      "        [    0.0151],\n",
      "        [    0.0152],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0167],\n",
      "        [    0.0168],\n",
      "        [    0.0179],\n",
      "        [    0.0188],\n",
      "        [    0.0190],\n",
      "        [    0.0194],\n",
      "        [    0.0197],\n",
      "        [    0.0197],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0202],\n",
      "        [    0.0208],\n",
      "        [    0.0219],\n",
      "        [    0.0227],\n",
      "        [    0.0248],\n",
      "        [    0.0250],\n",
      "        [    0.0259],\n",
      "        [    0.0263],\n",
      "        [    0.0281],\n",
      "        [    0.0307],\n",
      "        [    0.0310],\n",
      "        [    0.0318],\n",
      "        [    0.0322],\n",
      "        [    0.0333],\n",
      "        [    0.0350],\n",
      "        [    0.0352],\n",
      "        [    0.0369],\n",
      "        [    0.0370],\n",
      "        [    0.0378],\n",
      "        [    0.0379],\n",
      "        [    0.0387],\n",
      "        [    0.0404],\n",
      "        [    0.0411],\n",
      "        [    0.0420],\n",
      "        [    0.0446],\n",
      "        [    0.0449],\n",
      "        [    0.0472],\n",
      "        [    0.0472],\n",
      "        [    0.0493],\n",
      "        [    0.0493],\n",
      "        [    0.0519],\n",
      "        [    0.0566],\n",
      "        [    0.0610],\n",
      "        [    0.0615],\n",
      "        [    0.0625],\n",
      "        [    0.0648],\n",
      "        [    0.0695],\n",
      "        [    0.0749]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0021],\n",
      "        [0.0050],\n",
      "        [0.0028],\n",
      "        [0.0051],\n",
      "        [0.0037],\n",
      "        [0.0041],\n",
      "        [0.0046],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0008],\n",
      "        [0.0020],\n",
      "        [0.0041],\n",
      "        [0.0056],\n",
      "        [0.0019],\n",
      "        [0.0033],\n",
      "        [0.0083],\n",
      "        [0.0072],\n",
      "        [0.0098],\n",
      "        [0.0051],\n",
      "        [0.0066],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0122],\n",
      "        [0.0117],\n",
      "        [0.0085],\n",
      "        [0.0115],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0106],\n",
      "        [0.0155],\n",
      "        [0.0116],\n",
      "        [0.0175],\n",
      "        [0.0086],\n",
      "        [0.0179],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0121],\n",
      "        [0.0145],\n",
      "        [0.0143],\n",
      "        [0.0078],\n",
      "        [0.0135],\n",
      "        [0.0197],\n",
      "        [0.0167],\n",
      "        [0.0141],\n",
      "        [0.0209],\n",
      "        [0.0257],\n",
      "        [0.0150],\n",
      "        [0.0158],\n",
      "        [0.0225],\n",
      "        [0.0231],\n",
      "        [0.0181],\n",
      "        [0.0199],\n",
      "        [0.0272],\n",
      "        [0.0212],\n",
      "        [0.0229],\n",
      "        [0.0236],\n",
      "        [0.0236],\n",
      "        [0.0299],\n",
      "        [0.0247],\n",
      "        [0.0283],\n",
      "        [0.0228],\n",
      "        [0.0301],\n",
      "        [0.0321],\n",
      "        [0.0381],\n",
      "        [0.0325],\n",
      "        [0.0395],\n",
      "        [0.0394],\n",
      "        [0.0286],\n",
      "        [0.0392],\n",
      "        [0.0408],\n",
      "        [0.0415],\n",
      "        [0.0388],\n",
      "        [0.0430],\n",
      "        [0.0461],\n",
      "        [0.0475],\n",
      "        [0.0456],\n",
      "        [0.0480],\n",
      "        [0.0515],\n",
      "        [0.0514],\n",
      "        [0.0544],\n",
      "        [0.0474],\n",
      "        [0.0612],\n",
      "        [0.0646],\n",
      "        [0.0603],\n",
      "        [0.0627],\n",
      "        [0.0685],\n",
      "        [0.0685]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.26156210899353\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 90\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.261735165935534e-07, 87)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [87, 91, 51, 93, 103, 56, 13, 9, 89, 108, 52, 22, 14, 10, 28, 62, 88, 34, 61, 106, 2, 63, 76, 12, 74, 94, 21, 102, 100, 57, 107, 99, 49, 86, 77, 98, 53, 20, 16, 92, 75, 15, 90, 18, 58, 68, 66, 32, 54, 101, 26, 19, 46, 1, 95, 23, 65, 33, 8, 11, 17, 97, 0, 30, 55, 36, 96, 78, 64, 27, 73, 85, 31, 104, 105, 35, 29, 3, 25, 50, 47, 48, 72, 59, 109, 60, 24, 7, 37, 67] 數值 torch.Size([90, 1])\n",
      "目前模型的Data狀態 torch.Size([90, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7347],\n",
      "        [0.6895],\n",
      "        [0.8958],\n",
      "        [0.7182],\n",
      "        [0.6619],\n",
      "        [0.9026],\n",
      "        [0.9870],\n",
      "        [0.9795],\n",
      "        [0.7159],\n",
      "        [0.6934],\n",
      "        [0.8879],\n",
      "        [0.8594],\n",
      "        [0.9850],\n",
      "        [0.9978],\n",
      "        [0.8441],\n",
      "        [0.8889],\n",
      "        [0.7283],\n",
      "        [0.8084],\n",
      "        [0.9137],\n",
      "        [0.7038],\n",
      "        [0.8343],\n",
      "        [0.8700],\n",
      "        [0.7501],\n",
      "        [0.9887],\n",
      "        [0.7512],\n",
      "        [0.6982],\n",
      "        [0.8631],\n",
      "        [0.6514],\n",
      "        [0.6247],\n",
      "        [0.9312],\n",
      "        [0.7022],\n",
      "        [0.6481],\n",
      "        [0.8641],\n",
      "        [0.7047],\n",
      "        [0.7340],\n",
      "        [0.6647],\n",
      "        [0.8544],\n",
      "        [0.9024],\n",
      "        [0.9297],\n",
      "        [0.6913],\n",
      "        [0.7318],\n",
      "        [0.9524],\n",
      "        [0.7189],\n",
      "        [0.8983],\n",
      "        [0.9188],\n",
      "        [0.8341],\n",
      "        [0.8534],\n",
      "        [0.7914],\n",
      "        [0.9122],\n",
      "        [0.6275],\n",
      "        [0.8524],\n",
      "        [0.9154],\n",
      "        [0.8658],\n",
      "        [0.7907],\n",
      "        [0.6821],\n",
      "        [0.8685],\n",
      "        [0.8120],\n",
      "        [0.7964],\n",
      "        [0.9520],\n",
      "        [1.0047],\n",
      "        [0.9161],\n",
      "        [0.6552],\n",
      "        [0.7754],\n",
      "        [0.8795],\n",
      "        [0.8795],\n",
      "        [0.8104],\n",
      "        [0.6625],\n",
      "        [0.7011],\n",
      "        [0.8355],\n",
      "        [0.8428],\n",
      "        [0.7849],\n",
      "        [0.7006],\n",
      "        [0.8450],\n",
      "        [0.7005],\n",
      "        [0.7224],\n",
      "        [0.7944],\n",
      "        [0.8756],\n",
      "        [0.7973],\n",
      "        [0.8540],\n",
      "        [0.9372],\n",
      "        [0.8810],\n",
      "        [0.8978],\n",
      "        [0.8006],\n",
      "        [0.8991],\n",
      "        [0.7332],\n",
      "        [0.8926],\n",
      "        [0.8575],\n",
      "        [0.9309],\n",
      "        [0.7950],\n",
      "        [0.8745]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0044],\n",
      "        [0.0046],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0051],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0072],\n",
      "        [0.0078],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0102],\n",
      "        [0.0106],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0117],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0145],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0167],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0225],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0236],\n",
      "        [0.0236],\n",
      "        [0.0247],\n",
      "        [0.0257],\n",
      "        [0.0272],\n",
      "        [0.0283],\n",
      "        [0.0286],\n",
      "        [0.0299],\n",
      "        [0.0301],\n",
      "        [0.0321],\n",
      "        [0.0325],\n",
      "        [0.0381],\n",
      "        [0.0388],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0395],\n",
      "        [0.0408],\n",
      "        [0.0415],\n",
      "        [0.0430],\n",
      "        [0.0456],\n",
      "        [0.0461],\n",
      "        [0.0474],\n",
      "        [0.0475],\n",
      "        [0.0480],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0544],\n",
      "        [0.0603],\n",
      "        [0.0612],\n",
      "        [0.0627],\n",
      "        [0.0646],\n",
      "        [0.0685],\n",
      "        [0.0685],\n",
      "        [0.0809]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0020],\n",
      "        [0.0040],\n",
      "        [0.0010],\n",
      "        [0.0015],\n",
      "        [0.0054],\n",
      "        [0.0032],\n",
      "        [0.0050],\n",
      "        [0.0035],\n",
      "        [0.0023],\n",
      "        [0.0029],\n",
      "        [0.0023],\n",
      "        [0.0038],\n",
      "        [0.0060],\n",
      "        [0.0070],\n",
      "        [0.0033],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0091],\n",
      "        [0.0056],\n",
      "        [0.0047],\n",
      "        [0.0065],\n",
      "        [0.0096],\n",
      "        [0.0087],\n",
      "        [0.0108],\n",
      "        [0.0106],\n",
      "        [0.0080],\n",
      "        [0.0090],\n",
      "        [0.0112],\n",
      "        [0.0089],\n",
      "        [0.0097],\n",
      "        [0.0120],\n",
      "        [0.0092],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0128],\n",
      "        [0.0143],\n",
      "        [0.0153],\n",
      "        [0.0149],\n",
      "        [0.0148],\n",
      "        [0.0159],\n",
      "        [0.0157],\n",
      "        [0.0152],\n",
      "        [0.0170],\n",
      "        [0.0189],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0186],\n",
      "        [0.0179],\n",
      "        [0.0207],\n",
      "        [0.0181],\n",
      "        [0.0229],\n",
      "        [0.0205],\n",
      "        [0.0197],\n",
      "        [0.0238],\n",
      "        [0.0213],\n",
      "        [0.0256],\n",
      "        [0.0241],\n",
      "        [0.0236],\n",
      "        [0.0260],\n",
      "        [0.0259],\n",
      "        [0.0284],\n",
      "        [0.0254],\n",
      "        [0.0283],\n",
      "        [0.0318],\n",
      "        [0.0338],\n",
      "        [0.0328],\n",
      "        [0.0381],\n",
      "        [0.0407],\n",
      "        [0.0366],\n",
      "        [0.0381],\n",
      "        [0.0392],\n",
      "        [0.0395],\n",
      "        [0.0397],\n",
      "        [0.0414],\n",
      "        [0.0471],\n",
      "        [0.0440],\n",
      "        [0.0437],\n",
      "        [0.0456],\n",
      "        [0.0450],\n",
      "        [0.0492],\n",
      "        [0.0494],\n",
      "        [0.0532],\n",
      "        [0.0623],\n",
      "        [0.0592],\n",
      "        [0.0646],\n",
      "        [0.0631],\n",
      "        [0.0671],\n",
      "        [0.0704],\n",
      "        [0.0788]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.53733992576599\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 91\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.174701744996128e-07, 93)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [93, 87, 103, 91, 22, 108, 52, 13, 62, 89, 14, 51, 2, 9, 56, 106, 10, 88, 63, 34, 28, 21, 12, 57, 102, 61, 49, 76, 107, 94, 74, 100, 86, 77, 99, 98, 53, 92, 16, 90, 20, 15, 75, 68, 66, 18, 54, 26, 32, 58, 1, 46, 101, 23, 19, 8, 95, 33, 0, 65, 17, 11, 30, 97, 55, 96, 36, 27, 78, 73, 85, 31, 104, 64, 105, 3, 29, 50, 25, 35, 47, 48, 72, 109, 59, 24, 60, 7, 37, 67, 125] 數值 torch.Size([91, 1])\n",
      "目前模型的Data狀態 torch.Size([91, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7170],\n",
      "        [0.7345],\n",
      "        [0.6606],\n",
      "        [0.6894],\n",
      "        [0.8571],\n",
      "        [0.6916],\n",
      "        [0.8863],\n",
      "        [0.9865],\n",
      "        [0.8866],\n",
      "        [0.7153],\n",
      "        [0.9838],\n",
      "        [0.8939],\n",
      "        [0.8374],\n",
      "        [0.9806],\n",
      "        [0.9005],\n",
      "        [0.7022],\n",
      "        [0.9987],\n",
      "        [0.7278],\n",
      "        [0.8681],\n",
      "        [0.8078],\n",
      "        [0.8422],\n",
      "        [0.8613],\n",
      "        [0.9886],\n",
      "        [0.9289],\n",
      "        [0.6501],\n",
      "        [0.9112],\n",
      "        [0.8616],\n",
      "        [0.7490],\n",
      "        [0.7005],\n",
      "        [0.6972],\n",
      "        [0.7498],\n",
      "        [0.6242],\n",
      "        [0.7051],\n",
      "        [0.7336],\n",
      "        [0.6477],\n",
      "        [0.6644],\n",
      "        [0.8531],\n",
      "        [0.6908],\n",
      "        [0.9289],\n",
      "        [0.7185],\n",
      "        [0.9006],\n",
      "        [0.9516],\n",
      "        [0.7304],\n",
      "        [0.8326],\n",
      "        [0.8517],\n",
      "        [0.8971],\n",
      "        [0.9105],\n",
      "        [0.8497],\n",
      "        [0.7908],\n",
      "        [0.9166],\n",
      "        [0.7938],\n",
      "        [0.8638],\n",
      "        [0.6268],\n",
      "        [0.8668],\n",
      "        [0.9137],\n",
      "        [0.9531],\n",
      "        [0.6812],\n",
      "        [0.7959],\n",
      "        [0.7785],\n",
      "        [0.8100],\n",
      "        [0.9149],\n",
      "        [1.0051],\n",
      "        [0.8779],\n",
      "        [0.6551],\n",
      "        [0.8778],\n",
      "        [0.6622],\n",
      "        [0.8087],\n",
      "        [0.8402],\n",
      "        [0.7011],\n",
      "        [0.7836],\n",
      "        [0.7003],\n",
      "        [0.8438],\n",
      "        [0.6987],\n",
      "        [0.8337],\n",
      "        [0.7209],\n",
      "        [0.8010],\n",
      "        [0.8735],\n",
      "        [0.9342],\n",
      "        [0.8521],\n",
      "        [0.7929],\n",
      "        [0.8789],\n",
      "        [0.8957],\n",
      "        [0.7993],\n",
      "        [0.7312],\n",
      "        [0.8971],\n",
      "        [0.8560],\n",
      "        [0.8906],\n",
      "        [0.9323],\n",
      "        [0.7931],\n",
      "        [0.8724],\n",
      "        [0.7038]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0010],\n",
      "        [0.0015],\n",
      "        [0.0020],\n",
      "        [0.0023],\n",
      "        [0.0023],\n",
      "        [0.0029],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0047],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0070],\n",
      "        [0.0080],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0112],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0128],\n",
      "        [0.0143],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0157],\n",
      "        [0.0159],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0170],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0229],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0241],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0283],\n",
      "        [0.0284],\n",
      "        [0.0318],\n",
      "        [0.0328],\n",
      "        [0.0338],\n",
      "        [0.0366],\n",
      "        [0.0381],\n",
      "        [0.0381],\n",
      "        [0.0392],\n",
      "        [0.0395],\n",
      "        [0.0397],\n",
      "        [0.0407],\n",
      "        [0.0414],\n",
      "        [0.0437],\n",
      "        [0.0440],\n",
      "        [0.0450],\n",
      "        [0.0456],\n",
      "        [0.0471],\n",
      "        [0.0492],\n",
      "        [0.0494],\n",
      "        [0.0532],\n",
      "        [0.0592],\n",
      "        [0.0623],\n",
      "        [0.0631],\n",
      "        [0.0646],\n",
      "        [0.0671],\n",
      "        [0.0704],\n",
      "        [0.0788],\n",
      "        [0.0799]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0025],\n",
      "        [0.0011],\n",
      "        [0.0035],\n",
      "        [0.0007],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0039],\n",
      "        [0.0024],\n",
      "        [0.0015],\n",
      "        [0.0038],\n",
      "        [0.0052],\n",
      "        [0.0014],\n",
      "        [0.0063],\n",
      "        [0.0061],\n",
      "        [0.0022],\n",
      "        [0.0075],\n",
      "        [0.0082],\n",
      "        [0.0062],\n",
      "        [0.0068],\n",
      "        [0.0088],\n",
      "        [0.0068],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0065],\n",
      "        [0.0101],\n",
      "        [0.0081],\n",
      "        [0.0107],\n",
      "        [0.0061],\n",
      "        [0.0125],\n",
      "        [0.0114],\n",
      "        [0.0127],\n",
      "        [0.0129],\n",
      "        [0.0114],\n",
      "        [0.0134],\n",
      "        [0.0139],\n",
      "        [0.0147],\n",
      "        [0.0164],\n",
      "        [0.0144],\n",
      "        [0.0135],\n",
      "        [0.0163],\n",
      "        [0.0152],\n",
      "        [0.0168],\n",
      "        [0.0152],\n",
      "        [0.0157],\n",
      "        [0.0170],\n",
      "        [0.0174],\n",
      "        [0.0161],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0165],\n",
      "        [0.0204],\n",
      "        [0.0224],\n",
      "        [0.0202],\n",
      "        [0.0236],\n",
      "        [0.0227],\n",
      "        [0.0254],\n",
      "        [0.0243],\n",
      "        [0.0221],\n",
      "        [0.0264],\n",
      "        [0.0260],\n",
      "        [0.0274],\n",
      "        [0.0270],\n",
      "        [0.0293],\n",
      "        [0.0322],\n",
      "        [0.0336],\n",
      "        [0.0346],\n",
      "        [0.0345],\n",
      "        [0.0382],\n",
      "        [0.0379],\n",
      "        [0.0373],\n",
      "        [0.0387],\n",
      "        [0.0366],\n",
      "        [0.0410],\n",
      "        [0.0384],\n",
      "        [0.0402],\n",
      "        [0.0422],\n",
      "        [0.0430],\n",
      "        [0.0442],\n",
      "        [0.0479],\n",
      "        [0.0488],\n",
      "        [0.0490],\n",
      "        [0.0531],\n",
      "        [0.0550],\n",
      "        [0.0632],\n",
      "        [0.0623],\n",
      "        [0.0655],\n",
      "        [0.0660],\n",
      "        [0.0715],\n",
      "        [0.0777],\n",
      "        [0.0732]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.81330370903015\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 92\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.711546921498666e-07, 22)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [22, 93, 103, 2, 108, 89, 52, 106, 62, 87, 91, 14, 13, 51, 56, 107, 63, 9, 102, 21, 34, 10, 12, 57, 49, 88, 28, 61, 76, 74, 77, 94, 100, 86, 99, 90, 98, 16, 53, 68, 15, 66, 26, 20, 92, 1, 75, 18, 54, 32, 58, 23, 46, 0, 101, 8, 19, 33, 95, 17, 65, 30, 11, 97, 55, 96, 27, 36, 104, 85, 73, 78, 105, 31, 3, 64, 29, 50, 25, 35, 47, 48, 72, 109, 24, 59, 60, 7, 37, 125, 67, 6] 數值 torch.Size([92, 1])\n",
      "目前模型的Data狀態 torch.Size([92, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8554],\n",
      "        [0.7152],\n",
      "        [0.6580],\n",
      "        [0.8407],\n",
      "        [0.6878],\n",
      "        [0.7133],\n",
      "        [0.8854],\n",
      "        [0.6988],\n",
      "        [0.8858],\n",
      "        [0.7330],\n",
      "        [0.6879],\n",
      "        [0.9838],\n",
      "        [0.9872],\n",
      "        [0.8926],\n",
      "        [0.8998],\n",
      "        [0.6968],\n",
      "        [0.8679],\n",
      "        [0.9818],\n",
      "        [0.6477],\n",
      "        [0.8601],\n",
      "        [0.8077],\n",
      "        [1.0002],\n",
      "        [0.9896],\n",
      "        [0.9280],\n",
      "        [0.8605],\n",
      "        [0.7260],\n",
      "        [0.8404],\n",
      "        [0.9101],\n",
      "        [0.7479],\n",
      "        [0.7492],\n",
      "        [0.7332],\n",
      "        [0.6953],\n",
      "        [0.6226],\n",
      "        [0.7038],\n",
      "        [0.6464],\n",
      "        [0.7169],\n",
      "        [0.6632],\n",
      "        [0.9294],\n",
      "        [0.8528],\n",
      "        [0.8318],\n",
      "        [0.9521],\n",
      "        [0.8512],\n",
      "        [0.8476],\n",
      "        [0.8996],\n",
      "        [0.6892],\n",
      "        [0.7970],\n",
      "        [0.7295],\n",
      "        [0.8971],\n",
      "        [0.9100],\n",
      "        [0.7906],\n",
      "        [0.9158],\n",
      "        [0.8656],\n",
      "        [0.8637],\n",
      "        [0.7818],\n",
      "        [0.6250],\n",
      "        [0.9540],\n",
      "        [0.9130],\n",
      "        [0.7957],\n",
      "        [0.6796],\n",
      "        [0.9150],\n",
      "        [0.8093],\n",
      "        [0.8766],\n",
      "        [1.0064],\n",
      "        [0.6542],\n",
      "        [0.8774],\n",
      "        [0.6614],\n",
      "        [0.8382],\n",
      "        [0.8079],\n",
      "        [0.6956],\n",
      "        [0.6984],\n",
      "        [0.7834],\n",
      "        [0.7012],\n",
      "        [0.7178],\n",
      "        [0.8430],\n",
      "        [0.8045],\n",
      "        [0.8334],\n",
      "        [0.8717],\n",
      "        [0.9322],\n",
      "        [0.8507],\n",
      "        [0.7921],\n",
      "        [0.8784],\n",
      "        [0.8953],\n",
      "        [0.7993],\n",
      "        [0.7270],\n",
      "        [0.8551],\n",
      "        [0.8962],\n",
      "        [0.8898],\n",
      "        [0.9334],\n",
      "        [0.7921],\n",
      "        [0.6970],\n",
      "        [0.8714],\n",
      "        [0.8949]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0035],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0052],\n",
      "        [0.0061],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0125],\n",
      "        [0.0127],\n",
      "        [0.0129],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0139],\n",
      "        [0.0144],\n",
      "        [0.0147],\n",
      "        [0.0152],\n",
      "        [0.0152],\n",
      "        [0.0157],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0168],\n",
      "        [0.0170],\n",
      "        [0.0174],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0204],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0227],\n",
      "        [0.0236],\n",
      "        [0.0243],\n",
      "        [0.0254],\n",
      "        [0.0260],\n",
      "        [0.0264],\n",
      "        [0.0270],\n",
      "        [0.0274],\n",
      "        [0.0293],\n",
      "        [0.0322],\n",
      "        [0.0336],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0366],\n",
      "        [0.0373],\n",
      "        [0.0379],\n",
      "        [0.0382],\n",
      "        [0.0384],\n",
      "        [0.0387],\n",
      "        [0.0402],\n",
      "        [0.0410],\n",
      "        [0.0422],\n",
      "        [0.0430],\n",
      "        [0.0442],\n",
      "        [0.0479],\n",
      "        [0.0488],\n",
      "        [0.0490],\n",
      "        [0.0531],\n",
      "        [0.0550],\n",
      "        [0.0623],\n",
      "        [0.0632],\n",
      "        [0.0655],\n",
      "        [0.0660],\n",
      "        [0.0715],\n",
      "        [0.0732],\n",
      "        [0.0777],\n",
      "        [0.0799]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0021],\n",
      "        [0.0032],\n",
      "        [0.0020],\n",
      "        [0.0043],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0009],\n",
      "        [0.0028],\n",
      "        [0.0041],\n",
      "        [0.0035],\n",
      "        [0.0043],\n",
      "        [0.0066],\n",
      "        [0.0074],\n",
      "        [0.0035],\n",
      "        [0.0052],\n",
      "        [0.0082],\n",
      "        [0.0046],\n",
      "        [0.0052],\n",
      "        [0.0070],\n",
      "        [0.0094],\n",
      "        [0.0067],\n",
      "        [0.0066],\n",
      "        [0.0062],\n",
      "        [0.0089],\n",
      "        [0.0105],\n",
      "        [0.0118],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0112],\n",
      "        [0.0139],\n",
      "        [0.0139],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0128],\n",
      "        [0.0145],\n",
      "        [0.0145],\n",
      "        [0.0154],\n",
      "        [0.0142],\n",
      "        [0.0151],\n",
      "        [0.0146],\n",
      "        [0.0136],\n",
      "        [0.0177],\n",
      "        [0.0173],\n",
      "        [0.0134],\n",
      "        [0.0178],\n",
      "        [0.0176],\n",
      "        [0.0165],\n",
      "        [0.0193],\n",
      "        [0.0210],\n",
      "        [0.0187],\n",
      "        [0.0192],\n",
      "        [0.0189],\n",
      "        [0.0237],\n",
      "        [0.0211],\n",
      "        [0.0247],\n",
      "        [0.0246],\n",
      "        [0.0266],\n",
      "        [0.0254],\n",
      "        [0.0280],\n",
      "        [0.0258],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0331],\n",
      "        [0.0340],\n",
      "        [0.0322],\n",
      "        [0.0357],\n",
      "        [0.0342],\n",
      "        [0.0366],\n",
      "        [0.0374],\n",
      "        [0.0383],\n",
      "        [0.0362],\n",
      "        [0.0378],\n",
      "        [0.0365],\n",
      "        [0.0422],\n",
      "        [0.0405],\n",
      "        [0.0408],\n",
      "        [0.0425],\n",
      "        [0.0491],\n",
      "        [0.0473],\n",
      "        [0.0477],\n",
      "        [0.0527],\n",
      "        [0.0520],\n",
      "        [0.0611],\n",
      "        [0.0645],\n",
      "        [0.0668],\n",
      "        [0.0642],\n",
      "        [0.0730],\n",
      "        [0.0681],\n",
      "        [0.0763],\n",
      "        [0.0776]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.08950114250183\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 93\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.5995294805625235e-08, 106)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [106, 89, 52, 62, 22, 2, 93, 87, 103, 107, 14, 91, 108, 13, 102, 63, 21, 49, 57, 51, 12, 34, 56, 9, 88, 10, 28, 77, 76, 61, 74, 90, 86, 1, 26, 94, 100, 68, 99, 98, 16, 66, 15, 53, 54, 92, 18, 20, 75, 23, 0, 46, 32, 58, 8, 101, 33, 19, 17, 30, 95, 65, 11, 97, 27, 55, 96, 104, 36, 105, 3, 85, 73, 31, 78, 29, 50, 64, 25, 47, 48, 35, 109, 72, 24, 7, 59, 60, 125, 37, 67, 6, 4] 數值 torch.Size([93, 1])\n",
      "目前模型的Data狀態 torch.Size([93, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6964],\n",
      "        [0.7124],\n",
      "        [0.8842],\n",
      "        [0.8842],\n",
      "        [0.8535],\n",
      "        [0.8441],\n",
      "        [0.7139],\n",
      "        [0.7327],\n",
      "        [0.6559],\n",
      "        [0.6942],\n",
      "        [0.9835],\n",
      "        [0.6873],\n",
      "        [0.6850],\n",
      "        [0.9877],\n",
      "        [0.6458],\n",
      "        [0.8668],\n",
      "        [0.8586],\n",
      "        [0.8586],\n",
      "        [0.9265],\n",
      "        [0.8912],\n",
      "        [0.9905],\n",
      "        [0.8074],\n",
      "        [0.8985],\n",
      "        [0.9837],\n",
      "        [0.7254],\n",
      "        [1.0021],\n",
      "        [0.8387],\n",
      "        [0.7330],\n",
      "        [0.7471],\n",
      "        [0.9085],\n",
      "        [0.7484],\n",
      "        [0.7162],\n",
      "        [0.7038],\n",
      "        [0.8002],\n",
      "        [0.8451],\n",
      "        [0.6939],\n",
      "        [0.6215],\n",
      "        [0.8308],\n",
      "        [0.6456],\n",
      "        [0.6626],\n",
      "        [0.9293],\n",
      "        [0.8501],\n",
      "        [0.9523],\n",
      "        [0.8520],\n",
      "        [0.9091],\n",
      "        [0.6884],\n",
      "        [0.8965],\n",
      "        [0.8982],\n",
      "        [0.7285],\n",
      "        [0.8642],\n",
      "        [0.7851],\n",
      "        [0.8625],\n",
      "        [0.7901],\n",
      "        [0.9144],\n",
      "        [0.9556],\n",
      "        [0.6237],\n",
      "        [0.7954],\n",
      "        [0.9120],\n",
      "        [0.9144],\n",
      "        [0.8754],\n",
      "        [0.6784],\n",
      "        [0.8077],\n",
      "        [1.0080],\n",
      "        [0.6539],\n",
      "        [0.8359],\n",
      "        [0.8764],\n",
      "        [0.6610],\n",
      "        [0.6931],\n",
      "        [0.8068],\n",
      "        [0.7157],\n",
      "        [0.8082],\n",
      "        [0.6976],\n",
      "        [0.7829],\n",
      "        [0.8420],\n",
      "        [0.7014],\n",
      "        [0.8700],\n",
      "        [0.9300],\n",
      "        [0.8322],\n",
      "        [0.8491],\n",
      "        [0.8770],\n",
      "        [0.8939],\n",
      "        [0.7910],\n",
      "        [0.7240],\n",
      "        [0.7988],\n",
      "        [0.8540],\n",
      "        [0.9352],\n",
      "        [0.8949],\n",
      "        [0.8884],\n",
      "        [0.6920],\n",
      "        [0.7906],\n",
      "        [0.8699],\n",
      "        [0.8971],\n",
      "        [0.8376]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0013],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0046],\n",
      "        [0.0052],\n",
      "        [0.0052],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0067],\n",
      "        [0.0070],\n",
      "        [0.0074],\n",
      "        [0.0082],\n",
      "        [0.0089],\n",
      "        [0.0094],\n",
      "        [0.0105],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0134],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0142],\n",
      "        [0.0145],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0165],\n",
      "        [0.0173],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0178],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0237],\n",
      "        [0.0246],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0258],\n",
      "        [0.0266],\n",
      "        [0.0280],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0322],\n",
      "        [0.0331],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0357],\n",
      "        [0.0362],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0374],\n",
      "        [0.0378],\n",
      "        [0.0383],\n",
      "        [0.0405],\n",
      "        [0.0408],\n",
      "        [0.0422],\n",
      "        [0.0425],\n",
      "        [0.0473],\n",
      "        [0.0477],\n",
      "        [0.0491],\n",
      "        [0.0520],\n",
      "        [0.0527],\n",
      "        [0.0611],\n",
      "        [0.0642],\n",
      "        [0.0645],\n",
      "        [0.0668],\n",
      "        [0.0681],\n",
      "        [0.0730],\n",
      "        [0.0763],\n",
      "        [0.0776],\n",
      "        [0.0913]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0025],\n",
      "        [0.0005],\n",
      "        [0.0099],\n",
      "        [0.0005],\n",
      "        [0.0002],\n",
      "        [0.0023],\n",
      "        [0.0037],\n",
      "        [0.0071],\n",
      "        [0.0018],\n",
      "        [0.0042],\n",
      "        [0.0088],\n",
      "        [0.0056],\n",
      "        [0.0073],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0085],\n",
      "        [0.0047],\n",
      "        [0.0017],\n",
      "        [0.0043],\n",
      "        [0.0054],\n",
      "        [0.0138],\n",
      "        [0.0066],\n",
      "        [0.0151],\n",
      "        [0.0091],\n",
      "        [0.0136],\n",
      "        [0.0098],\n",
      "        [0.0103],\n",
      "        [0.0100],\n",
      "        [0.0151],\n",
      "        [0.0102],\n",
      "        [0.0055],\n",
      "        [0.0147],\n",
      "        [0.0122],\n",
      "        [0.0121],\n",
      "        [0.0166],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0106],\n",
      "        [0.0172],\n",
      "        [0.0110],\n",
      "        [0.0129],\n",
      "        [0.0188],\n",
      "        [0.0154],\n",
      "        [0.0142],\n",
      "        [0.0151],\n",
      "        [0.0158],\n",
      "        [0.0209],\n",
      "        [0.0109],\n",
      "        [0.0215],\n",
      "        [0.0167],\n",
      "        [0.0189],\n",
      "        [0.0156],\n",
      "        [0.0221],\n",
      "        [0.0219],\n",
      "        [0.0219],\n",
      "        [0.0290],\n",
      "        [0.0278],\n",
      "        [0.0248],\n",
      "        [0.0262],\n",
      "        [0.0344],\n",
      "        [0.0272],\n",
      "        [0.0332],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0348],\n",
      "        [0.0342],\n",
      "        [0.0371],\n",
      "        [0.0282],\n",
      "        [0.0387],\n",
      "        [0.0397],\n",
      "        [0.0399],\n",
      "        [0.0411],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0402],\n",
      "        [0.0446],\n",
      "        [0.0493],\n",
      "        [0.0499],\n",
      "        [0.0474],\n",
      "        [0.0519],\n",
      "        [0.0551],\n",
      "        [0.0636],\n",
      "        [0.0585],\n",
      "        [0.0625],\n",
      "        [0.0648],\n",
      "        [0.0662],\n",
      "        [0.0717],\n",
      "        [0.0785],\n",
      "        [0.0714],\n",
      "        [0.0841]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.367145776748657\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 94\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.8734727303999534e-08, 87)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [87, 106, 22, 93, 12, 91, 103, 62, 89, 52, 107, 108, 34, 51, 56, 1, 102, 88, 14, 63, 21, 49, 57, 13, 28, 76, 2, 74, 86, 61, 16, 0, 15, 99, 100, 94, 98, 53, 77, 9, 18, 26, 10, 90, 20, 92, 8, 75, 68, 32, 66, 54, 58, 23, 46, 33, 19, 101, 95, 65, 97, 30, 3, 17, 55, 96, 27, 36, 11, 104, 105, 85, 73, 31, 64, 78, 29, 50, 25, 35, 47, 48, 109, 72, 7, 59, 24, 60, 125, 6, 37, 67, 4, 44] 數值 torch.Size([94, 1])\n",
      "目前模型的Data狀態 torch.Size([94, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7352],\n",
      "        [0.6970],\n",
      "        [0.8552],\n",
      "        [0.7156],\n",
      "        [0.9956],\n",
      "        [0.6896],\n",
      "        [0.6569],\n",
      "        [0.8858],\n",
      "        [0.7144],\n",
      "        [0.8864],\n",
      "        [0.6944],\n",
      "        [0.6851],\n",
      "        [0.8101],\n",
      "        [0.8931],\n",
      "        [0.9005],\n",
      "        [0.8080],\n",
      "        [0.6467],\n",
      "        [0.7276],\n",
      "        [0.9871],\n",
      "        [0.8689],\n",
      "        [0.8611],\n",
      "        [0.8603],\n",
      "        [0.9285],\n",
      "        [0.9921],\n",
      "        [0.8401],\n",
      "        [0.7488],\n",
      "        [0.8520],\n",
      "        [0.7506],\n",
      "        [0.7065],\n",
      "        [0.9100],\n",
      "        [0.9332],\n",
      "        [0.7931],\n",
      "        [0.9563],\n",
      "        [0.6477],\n",
      "        [0.6232],\n",
      "        [0.6956],\n",
      "        [0.6649],\n",
      "        [0.8545],\n",
      "        [0.7353],\n",
      "        [0.9893],\n",
      "        [0.8999],\n",
      "        [0.8463],\n",
      "        [1.0078],\n",
      "        [0.7185],\n",
      "        [0.9008],\n",
      "        [0.6902],\n",
      "        [0.9610],\n",
      "        [0.7305],\n",
      "        [0.8333],\n",
      "        [0.7927],\n",
      "        [0.8526],\n",
      "        [0.9114],\n",
      "        [0.9165],\n",
      "        [0.8664],\n",
      "        [0.8647],\n",
      "        [0.7981],\n",
      "        [0.9148],\n",
      "        [0.6253],\n",
      "        [0.6803],\n",
      "        [0.8094],\n",
      "        [0.6564],\n",
      "        [0.8774],\n",
      "        [0.8165],\n",
      "        [0.9179],\n",
      "        [0.8787],\n",
      "        [0.6636],\n",
      "        [0.8369],\n",
      "        [0.8084],\n",
      "        [1.0134],\n",
      "        [0.6937],\n",
      "        [0.7165],\n",
      "        [0.6998],\n",
      "        [0.7853],\n",
      "        [0.8442],\n",
      "        [0.8342],\n",
      "        [0.7042],\n",
      "        [0.8715],\n",
      "        [0.9313],\n",
      "        [0.8512],\n",
      "        [0.7927],\n",
      "        [0.8789],\n",
      "        [0.8961],\n",
      "        [0.7239],\n",
      "        [0.8013],\n",
      "        [0.9409],\n",
      "        [0.8969],\n",
      "        [0.8565],\n",
      "        [0.8904],\n",
      "        [0.6900],\n",
      "        [0.9034],\n",
      "        [0.7918],\n",
      "        [0.8721],\n",
      "        [0.8448],\n",
      "        [0.8043]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0005],\n",
      "        [0.0005],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0037],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0056],\n",
      "        [0.0066],\n",
      "        [0.0071],\n",
      "        [0.0073],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0129],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0209],\n",
      "        [0.0215],\n",
      "        [0.0219],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0248],\n",
      "        [0.0262],\n",
      "        [0.0272],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0290],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0332],\n",
      "        [0.0342],\n",
      "        [0.0344],\n",
      "        [0.0348],\n",
      "        [0.0371],\n",
      "        [0.0387],\n",
      "        [0.0397],\n",
      "        [0.0399],\n",
      "        [0.0402],\n",
      "        [0.0411],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0446],\n",
      "        [0.0474],\n",
      "        [0.0493],\n",
      "        [0.0499],\n",
      "        [0.0519],\n",
      "        [0.0551],\n",
      "        [0.0585],\n",
      "        [0.0625],\n",
      "        [0.0636],\n",
      "        [0.0648],\n",
      "        [0.0662],\n",
      "        [0.0714],\n",
      "        [0.0717],\n",
      "        [0.0785],\n",
      "        [0.0841],\n",
      "        [0.0976]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0001],\n",
      "        [0.0017],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0021],\n",
      "        [0.0045],\n",
      "        [0.0023],\n",
      "        [0.0056],\n",
      "        [0.0021],\n",
      "        [0.0034],\n",
      "        [0.0039],\n",
      "        [0.0009],\n",
      "        [0.0049],\n",
      "        [0.0070],\n",
      "        [0.0081],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0105],\n",
      "        [0.0085],\n",
      "        [0.0101],\n",
      "        [0.0145],\n",
      "        [0.0095],\n",
      "        [0.0102],\n",
      "        [0.0097],\n",
      "        [0.0087],\n",
      "        [0.0064],\n",
      "        [0.0092],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0125],\n",
      "        [0.0118],\n",
      "        [0.0108],\n",
      "        [0.0140],\n",
      "        [0.0162],\n",
      "        [0.0126],\n",
      "        [0.0148],\n",
      "        [0.0175],\n",
      "        [0.0148],\n",
      "        [0.0143],\n",
      "        [0.0155],\n",
      "        [0.0134],\n",
      "        [0.0157],\n",
      "        [0.0173],\n",
      "        [0.0146],\n",
      "        [0.0179],\n",
      "        [0.0206],\n",
      "        [0.0181],\n",
      "        [0.0217],\n",
      "        [0.0234],\n",
      "        [0.0196],\n",
      "        [0.0209],\n",
      "        [0.0221],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0265],\n",
      "        [0.0289],\n",
      "        [0.0230],\n",
      "        [0.0304],\n",
      "        [0.0290],\n",
      "        [0.0308],\n",
      "        [0.0334],\n",
      "        [0.0325],\n",
      "        [0.0366],\n",
      "        [0.0334],\n",
      "        [0.0360],\n",
      "        [0.0381],\n",
      "        [0.0406],\n",
      "        [0.0415],\n",
      "        [0.0388],\n",
      "        [0.0423],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0453],\n",
      "        [0.0458],\n",
      "        [0.0510],\n",
      "        [0.0516],\n",
      "        [0.0502],\n",
      "        [0.0562],\n",
      "        [0.0561],\n",
      "        [0.0617],\n",
      "        [0.0648],\n",
      "        [0.0640],\n",
      "        [0.0627],\n",
      "        [0.0683],\n",
      "        [0.0704],\n",
      "        [0.0787],\n",
      "        [0.0801],\n",
      "        [0.0941]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.644274473190308\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 95\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7698994270176627e-08, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [12, 87, 106, 22, 1, 93, 91, 34, 89, 107, 103, 62, 51, 56, 52, 102, 108, 0, 88, 14, 28, 63, 21, 16, 15, 49, 57, 74, 61, 76, 86, 13, 53, 99, 98, 100, 94, 18, 8, 77, 20, 2, 32, 26, 90, 92, 75, 9, 68, 10, 66, 58, 33, 54, 19, 23, 101, 3, 46, 95, 65, 97, 30, 55, 17, 96, 36, 27, 104, 105, 11, 85, 64, 73, 31, 78, 29, 50, 25, 35, 109, 47, 48, 7, 72, 59, 125, 60, 24, 6, 37, 67, 4, 44, 110] 數值 torch.Size([95, 1])\n",
      "目前模型的Data狀態 torch.Size([95, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9974],\n",
      "        [0.7350],\n",
      "        [0.6958],\n",
      "        [0.8556],\n",
      "        [0.8126],\n",
      "        [0.7151],\n",
      "        [0.6897],\n",
      "        [0.8124],\n",
      "        [0.7138],\n",
      "        [0.6930],\n",
      "        [0.6560],\n",
      "        [0.8866],\n",
      "        [0.8944],\n",
      "        [0.9020],\n",
      "        [0.8880],\n",
      "        [0.6460],\n",
      "        [0.6837],\n",
      "        [0.7976],\n",
      "        [0.7272],\n",
      "        [0.9882],\n",
      "        [0.8407],\n",
      "        [0.8703],\n",
      "        [0.8621],\n",
      "        [0.9350],\n",
      "        [0.9581],\n",
      "        [0.8616],\n",
      "        [0.9294],\n",
      "        [0.7511],\n",
      "        [0.9106],\n",
      "        [0.7485],\n",
      "        [0.7065],\n",
      "        [0.9938],\n",
      "        [0.8566],\n",
      "        [0.6480],\n",
      "        [0.6653],\n",
      "        [0.6234],\n",
      "        [0.6953],\n",
      "        [0.9015],\n",
      "        [0.9633],\n",
      "        [0.7358],\n",
      "        [0.9016],\n",
      "        [0.8567],\n",
      "        [0.7949],\n",
      "        [0.8464],\n",
      "        [0.7182],\n",
      "        [0.6902],\n",
      "        [0.7306],\n",
      "        [0.9917],\n",
      "        [0.8339],\n",
      "        [1.0103],\n",
      "        [0.8534],\n",
      "        [0.9174],\n",
      "        [0.8003],\n",
      "        [0.9132],\n",
      "        [0.9157],\n",
      "        [0.8672],\n",
      "        [0.6253],\n",
      "        [0.8217],\n",
      "        [0.8667],\n",
      "        [0.6803],\n",
      "        [0.8102],\n",
      "        [0.6570],\n",
      "        [0.8784],\n",
      "        [0.8806],\n",
      "        [0.9194],\n",
      "        [0.6642],\n",
      "        [0.8100],\n",
      "        [0.8370],\n",
      "        [0.6924],\n",
      "        [0.7154],\n",
      "        [1.0157],\n",
      "        [0.6992],\n",
      "        [0.8356],\n",
      "        [0.7861],\n",
      "        [0.8458],\n",
      "        [0.7053],\n",
      "        [0.8720],\n",
      "        [0.9318],\n",
      "        [0.8519],\n",
      "        [0.7942],\n",
      "        [0.7223],\n",
      "        [0.8806],\n",
      "        [0.8978],\n",
      "        [0.9433],\n",
      "        [0.8023],\n",
      "        [0.8977],\n",
      "        [0.6866],\n",
      "        [0.8912],\n",
      "        [0.8577],\n",
      "        [0.9064],\n",
      "        [0.7932],\n",
      "        [0.8723],\n",
      "        [0.8489],\n",
      "        [0.8077],\n",
      "        [0.7491]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0005],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0017],\n",
      "        [0.0021],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0056],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0092],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0134],\n",
      "        [0.0140],\n",
      "        [0.0143],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0148],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0196],\n",
      "        [0.0206],\n",
      "        [0.0209],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0230],\n",
      "        [0.0234],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0265],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0304],\n",
      "        [0.0308],\n",
      "        [0.0325],\n",
      "        [0.0334],\n",
      "        [0.0334],\n",
      "        [0.0360],\n",
      "        [0.0366],\n",
      "        [0.0381],\n",
      "        [0.0388],\n",
      "        [0.0406],\n",
      "        [0.0415],\n",
      "        [0.0423],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0453],\n",
      "        [0.0458],\n",
      "        [0.0502],\n",
      "        [0.0510],\n",
      "        [0.0516],\n",
      "        [0.0561],\n",
      "        [0.0562],\n",
      "        [0.0617],\n",
      "        [0.0627],\n",
      "        [0.0640],\n",
      "        [0.0648],\n",
      "        [0.0683],\n",
      "        [0.0704],\n",
      "        [0.0787],\n",
      "        [0.0801],\n",
      "        [0.0941],\n",
      "        [0.1075]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0017],\n",
      "        [    0.0046],\n",
      "        [    0.0062],\n",
      "        [    0.0022],\n",
      "        [    0.0001],\n",
      "        [    0.0051],\n",
      "        [    0.0056],\n",
      "        [    0.0031],\n",
      "        [    0.0023],\n",
      "        [    0.0033],\n",
      "        [    0.0079],\n",
      "        [    0.0007],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0027],\n",
      "        [    0.0003],\n",
      "        [    0.0114],\n",
      "        [    0.0055],\n",
      "        [    0.0112],\n",
      "        [    0.0056],\n",
      "        [    0.0113],\n",
      "        [    0.0067],\n",
      "        [    0.0060],\n",
      "        [    0.0104],\n",
      "        [    0.0109],\n",
      "        [    0.0075],\n",
      "        [    0.0072],\n",
      "        [    0.0116],\n",
      "        [    0.0124],\n",
      "        [    0.0135],\n",
      "        [    0.0140],\n",
      "        [    0.0086],\n",
      "        [    0.0119],\n",
      "        [    0.0153],\n",
      "        [    0.0152],\n",
      "        [    0.0157],\n",
      "        [    0.0165],\n",
      "        [    0.0145],\n",
      "        [    0.0152],\n",
      "        [    0.0115],\n",
      "        [    0.0171],\n",
      "        [    0.0154],\n",
      "        [    0.0156],\n",
      "        [    0.0115],\n",
      "        [    0.0108],\n",
      "        [    0.0194],\n",
      "        [    0.0183],\n",
      "        [    0.0146],\n",
      "        [    0.0145],\n",
      "        [    0.0162],\n",
      "        [    0.0153],\n",
      "        [    0.0204],\n",
      "        [    0.0207],\n",
      "        [    0.0192],\n",
      "        [    0.0235],\n",
      "        [    0.0191],\n",
      "        [    0.0259],\n",
      "        [    0.0217],\n",
      "        [    0.0224],\n",
      "        [    0.0282],\n",
      "        [    0.0279],\n",
      "        [    0.0295],\n",
      "        [    0.0265],\n",
      "        [    0.0303],\n",
      "        [    0.0285],\n",
      "        [    0.0336],\n",
      "        [    0.0339],\n",
      "        [    0.0301],\n",
      "        [    0.0281],\n",
      "        [    0.0308],\n",
      "        [    0.0352],\n",
      "        [    0.0338],\n",
      "        [    0.0407],\n",
      "        [    0.0388],\n",
      "        [    0.0398],\n",
      "        [    0.0404],\n",
      "        [    0.0396],\n",
      "        [    0.0397],\n",
      "        [    0.0425],\n",
      "        [    0.0473],\n",
      "        [    0.0440],\n",
      "        [    0.0496],\n",
      "        [    0.0502],\n",
      "        [    0.0575],\n",
      "        [    0.0544],\n",
      "        [    0.0643],\n",
      "        [    0.0547],\n",
      "        [    0.0666],\n",
      "        [    0.0625],\n",
      "        [    0.0691],\n",
      "        [    0.0720],\n",
      "        [    0.0755],\n",
      "        [    0.0797],\n",
      "        [    0.0937],\n",
      "        [    0.1007]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.921613693237305\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 96\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.2946765315955417e-09, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [1, 102, 62, 12, 22, 89, 52, 34, 107, 87, 93, 0, 91, 51, 14, 56, 21, 106, 63, 57, 49, 103, 13, 16, 90, 15, 88, 28, 108, 26, 77, 74, 53, 61, 76, 86, 68, 18, 9, 98, 8, 99, 66, 2, 32, 100, 10, 94, 20, 75, 23, 54, 92, 58, 33, 3, 46, 19, 101, 30, 65, 104, 95, 17, 97, 27, 55, 105, 96, 85, 36, 11, 73, 29, 50, 31, 78, 64, 25, 109, 35, 47, 48, 72, 125, 7, 24, 59, 60, 6, 37, 67, 4, 44, 110, 126] 數值 torch.Size([96, 1])\n",
      "目前模型的Data狀態 torch.Size([96, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8136],\n",
      "        [0.6415],\n",
      "        [0.8841],\n",
      "        [0.9955],\n",
      "        [0.8525],\n",
      "        [0.7095],\n",
      "        [0.8862],\n",
      "        [0.8114],\n",
      "        [0.6874],\n",
      "        [0.7308],\n",
      "        [0.7110],\n",
      "        [0.7984],\n",
      "        [0.6859],\n",
      "        [0.8922],\n",
      "        [0.9857],\n",
      "        [0.9002],\n",
      "        [0.8594],\n",
      "        [0.6904],\n",
      "        [0.8684],\n",
      "        [0.9271],\n",
      "        [0.8599],\n",
      "        [0.6512],\n",
      "        [0.9920],\n",
      "        [0.9334],\n",
      "        [0.7141],\n",
      "        [0.9564],\n",
      "        [0.7230],\n",
      "        [0.8379],\n",
      "        [0.6779],\n",
      "        [0.8430],\n",
      "        [0.7333],\n",
      "        [0.7490],\n",
      "        [0.8555],\n",
      "        [0.9079],\n",
      "        [0.7451],\n",
      "        [0.7028],\n",
      "        [0.8311],\n",
      "        [0.8996],\n",
      "        [0.9901],\n",
      "        [0.6620],\n",
      "        [0.9615],\n",
      "        [0.6444],\n",
      "        [0.8508],\n",
      "        [0.8576],\n",
      "        [0.7938],\n",
      "        [0.6196],\n",
      "        [1.0089],\n",
      "        [0.6913],\n",
      "        [0.8988],\n",
      "        [0.7280],\n",
      "        [0.8645],\n",
      "        [0.9118],\n",
      "        [0.6862],\n",
      "        [0.9150],\n",
      "        [0.7993],\n",
      "        [0.8230],\n",
      "        [0.8656],\n",
      "        [0.9131],\n",
      "        [0.6215],\n",
      "        [0.8761],\n",
      "        [0.8077],\n",
      "        [0.6871],\n",
      "        [0.6768],\n",
      "        [0.9174],\n",
      "        [0.6540],\n",
      "        [0.8337],\n",
      "        [0.8793],\n",
      "        [0.7103],\n",
      "        [0.6614],\n",
      "        [0.6949],\n",
      "        [0.8087],\n",
      "        [1.0142],\n",
      "        [0.7843],\n",
      "        [0.8691],\n",
      "        [0.9289],\n",
      "        [0.8441],\n",
      "        [0.7034],\n",
      "        [0.8337],\n",
      "        [0.8490],\n",
      "        [0.7160],\n",
      "        [0.7927],\n",
      "        [0.8792],\n",
      "        [0.8965],\n",
      "        [0.8006],\n",
      "        [0.6786],\n",
      "        [0.9419],\n",
      "        [0.8554],\n",
      "        [0.8951],\n",
      "        [0.8886],\n",
      "        [0.9057],\n",
      "        [0.7916],\n",
      "        [0.8691],\n",
      "        [0.8492],\n",
      "        [0.8082],\n",
      "        [0.7422],\n",
      "        [0.7209]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0007],\n",
      "        [    0.0017],\n",
      "        [    0.0022],\n",
      "        [    0.0023],\n",
      "        [    0.0027],\n",
      "        [    0.0031],\n",
      "        [    0.0033],\n",
      "        [    0.0046],\n",
      "        [    0.0051],\n",
      "        [    0.0055],\n",
      "        [    0.0056],\n",
      "        [    0.0056],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0060],\n",
      "        [    0.0062],\n",
      "        [    0.0067],\n",
      "        [    0.0072],\n",
      "        [    0.0075],\n",
      "        [    0.0079],\n",
      "        [    0.0086],\n",
      "        [    0.0104],\n",
      "        [    0.0108],\n",
      "        [    0.0109],\n",
      "        [    0.0112],\n",
      "        [    0.0113],\n",
      "        [    0.0114],\n",
      "        [    0.0115],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0119],\n",
      "        [    0.0124],\n",
      "        [    0.0135],\n",
      "        [    0.0140],\n",
      "        [    0.0145],\n",
      "        [    0.0145],\n",
      "        [    0.0146],\n",
      "        [    0.0152],\n",
      "        [    0.0152],\n",
      "        [    0.0153],\n",
      "        [    0.0153],\n",
      "        [    0.0154],\n",
      "        [    0.0156],\n",
      "        [    0.0157],\n",
      "        [    0.0162],\n",
      "        [    0.0165],\n",
      "        [    0.0171],\n",
      "        [    0.0183],\n",
      "        [    0.0191],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0204],\n",
      "        [    0.0207],\n",
      "        [    0.0217],\n",
      "        [    0.0224],\n",
      "        [    0.0235],\n",
      "        [    0.0259],\n",
      "        [    0.0265],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0282],\n",
      "        [    0.0285],\n",
      "        [    0.0295],\n",
      "        [    0.0301],\n",
      "        [    0.0303],\n",
      "        [    0.0308],\n",
      "        [    0.0336],\n",
      "        [    0.0338],\n",
      "        [    0.0339],\n",
      "        [    0.0352],\n",
      "        [    0.0388],\n",
      "        [    0.0396],\n",
      "        [    0.0397],\n",
      "        [    0.0398],\n",
      "        [    0.0404],\n",
      "        [    0.0407],\n",
      "        [    0.0425],\n",
      "        [    0.0440],\n",
      "        [    0.0473],\n",
      "        [    0.0496],\n",
      "        [    0.0502],\n",
      "        [    0.0544],\n",
      "        [    0.0547],\n",
      "        [    0.0575],\n",
      "        [    0.0625],\n",
      "        [    0.0643],\n",
      "        [    0.0666],\n",
      "        [    0.0691],\n",
      "        [    0.0720],\n",
      "        [    0.0755],\n",
      "        [    0.0797],\n",
      "        [    0.0937],\n",
      "        [    0.1007],\n",
      "        [    0.1038]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0043],\n",
      "        [0.0008],\n",
      "        [0.0022],\n",
      "        [0.0003],\n",
      "        [0.0019],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0012],\n",
      "        [0.0063],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0012],\n",
      "        [0.0059],\n",
      "        [0.0050],\n",
      "        [0.0067],\n",
      "        [0.0040],\n",
      "        [0.0067],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0085],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0105],\n",
      "        [0.0085],\n",
      "        [0.0101],\n",
      "        [0.0089],\n",
      "        [0.0120],\n",
      "        [0.0112],\n",
      "        [0.0146],\n",
      "        [0.0115],\n",
      "        [0.0133],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0112],\n",
      "        [0.0127],\n",
      "        [0.0145],\n",
      "        [0.0158],\n",
      "        [0.0127],\n",
      "        [0.0165],\n",
      "        [0.0145],\n",
      "        [0.0140],\n",
      "        [0.0150],\n",
      "        [0.0169],\n",
      "        [0.0196],\n",
      "        [0.0139],\n",
      "        [0.0157],\n",
      "        [0.0185],\n",
      "        [0.0167],\n",
      "        [0.0165],\n",
      "        [0.0163],\n",
      "        [0.0198],\n",
      "        [0.0212],\n",
      "        [0.0197],\n",
      "        [0.0191],\n",
      "        [0.0189],\n",
      "        [0.0174],\n",
      "        [0.0249],\n",
      "        [0.0225],\n",
      "        [0.0260],\n",
      "        [0.0271],\n",
      "        [0.0263],\n",
      "        [0.0260],\n",
      "        [0.0279],\n",
      "        [0.0302],\n",
      "        [0.0284],\n",
      "        [0.0300],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0322],\n",
      "        [0.0327],\n",
      "        [0.0320],\n",
      "        [0.0377],\n",
      "        [0.0417],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0409],\n",
      "        [0.0428],\n",
      "        [0.0384],\n",
      "        [0.0429],\n",
      "        [0.0400],\n",
      "        [0.0456],\n",
      "        [0.0518],\n",
      "        [0.0523],\n",
      "        [0.0574],\n",
      "        [0.0467],\n",
      "        [0.0561],\n",
      "        [0.0636],\n",
      "        [0.0632],\n",
      "        [0.0654],\n",
      "        [0.0669],\n",
      "        [0.0704],\n",
      "        [0.0764],\n",
      "        [0.0762],\n",
      "        [0.0898],\n",
      "        [0.0961],\n",
      "        [0.0953]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.199242115020752\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 97\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.994641914090607e-08, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [12, 102, 34, 0, 22, 62, 89, 52, 56, 1, 51, 87, 93, 91, 107, 21, 14, 16, 57, 106, 63, 15, 49, 74, 103, 53, 90, 13, 61, 28, 26, 88, 76, 18, 77, 32, 8, 86, 98, 108, 99, 100, 68, 75, 9, 20, 94, 66, 3, 10, 33, 58, 2, 92, 23, 54, 19, 46, 104, 101, 65, 30, 95, 55, 97, 105, 27, 17, 36, 96, 85, 11, 64, 29, 50, 109, 31, 73, 78, 25, 35, 125, 47, 48, 7, 72, 59, 24, 60, 6, 37, 4, 67, 44, 126, 110, 112] 數值 torch.Size([97, 1])\n",
      "目前模型的Data狀態 torch.Size([97, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9976],\n",
      "        [0.6404],\n",
      "        [0.8133],\n",
      "        [0.8028],\n",
      "        [0.8529],\n",
      "        [0.8855],\n",
      "        [0.7086],\n",
      "        [0.8873],\n",
      "        [0.9019],\n",
      "        [0.8178],\n",
      "        [0.8929],\n",
      "        [0.7303],\n",
      "        [0.7107],\n",
      "        [0.6855],\n",
      "        [0.6844],\n",
      "        [0.8601],\n",
      "        [0.9868],\n",
      "        [0.9353],\n",
      "        [0.9285],\n",
      "        [0.6879],\n",
      "        [0.8706],\n",
      "        [0.9584],\n",
      "        [0.8614],\n",
      "        [0.7516],\n",
      "        [0.6497],\n",
      "        [0.8577],\n",
      "        [0.7135],\n",
      "        [0.9938],\n",
      "        [0.9091],\n",
      "        [0.8380],\n",
      "        [0.8430],\n",
      "        [0.7222],\n",
      "        [0.7459],\n",
      "        [0.9014],\n",
      "        [0.7350],\n",
      "        [0.7955],\n",
      "        [0.9627],\n",
      "        [0.7023],\n",
      "        [0.6626],\n",
      "        [0.6747],\n",
      "        [0.6447],\n",
      "        [0.6196],\n",
      "        [0.8324],\n",
      "        [0.7299],\n",
      "        [0.9920],\n",
      "        [0.8994],\n",
      "        [0.6910],\n",
      "        [0.8524],\n",
      "        [0.8273],\n",
      "        [1.0113],\n",
      "        [0.8011],\n",
      "        [0.9164],\n",
      "        [0.8617],\n",
      "        [0.6860],\n",
      "        [0.8652],\n",
      "        [0.9138],\n",
      "        [0.9142],\n",
      "        [0.8682],\n",
      "        [0.6850],\n",
      "        [0.6214],\n",
      "        [0.8093],\n",
      "        [0.8767],\n",
      "        [0.6771],\n",
      "        [0.8814],\n",
      "        [0.6551],\n",
      "        [0.7081],\n",
      "        [0.8337],\n",
      "        [0.9192],\n",
      "        [0.8106],\n",
      "        [0.6629],\n",
      "        [0.6938],\n",
      "        [1.0168],\n",
      "        [0.8359],\n",
      "        [0.8691],\n",
      "        [0.9291],\n",
      "        [0.7120],\n",
      "        [0.8451],\n",
      "        [0.7872],\n",
      "        [0.7059],\n",
      "        [0.8495],\n",
      "        [0.7944],\n",
      "        [0.6705],\n",
      "        [0.8814],\n",
      "        [0.8986],\n",
      "        [0.9433],\n",
      "        [0.8035],\n",
      "        [0.8962],\n",
      "        [0.8564],\n",
      "        [0.8898],\n",
      "        [0.9078],\n",
      "        [0.7932],\n",
      "        [0.8528],\n",
      "        [0.8701],\n",
      "        [0.8121],\n",
      "        [0.7124],\n",
      "        [0.7376],\n",
      "        [0.7399]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0050],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0059],\n",
      "        [0.0063],\n",
      "        [0.0067],\n",
      "        [0.0067],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0089],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0097],\n",
      "        [0.0101],\n",
      "        [0.0105],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0120],\n",
      "        [0.0127],\n",
      "        [0.0127],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0140],\n",
      "        [0.0145],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0157],\n",
      "        [0.0158],\n",
      "        [0.0163],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0174],\n",
      "        [0.0185],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0212],\n",
      "        [0.0225],\n",
      "        [0.0249],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0271],\n",
      "        [0.0279],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0287],\n",
      "        [0.0300],\n",
      "        [0.0302],\n",
      "        [0.0320],\n",
      "        [0.0322],\n",
      "        [0.0327],\n",
      "        [0.0377],\n",
      "        [0.0384],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0400],\n",
      "        [0.0409],\n",
      "        [0.0417],\n",
      "        [0.0428],\n",
      "        [0.0429],\n",
      "        [0.0456],\n",
      "        [0.0467],\n",
      "        [0.0518],\n",
      "        [0.0523],\n",
      "        [0.0561],\n",
      "        [0.0574],\n",
      "        [0.0632],\n",
      "        [0.0636],\n",
      "        [0.0654],\n",
      "        [0.0669],\n",
      "        [0.0704],\n",
      "        [0.0762],\n",
      "        [0.0764],\n",
      "        [0.0898],\n",
      "        [0.0953],\n",
      "        [0.0961],\n",
      "        [0.1035]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0037],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0034],\n",
      "        [0.0019],\n",
      "        [0.0057],\n",
      "        [0.0032],\n",
      "        [0.0040],\n",
      "        [0.0070],\n",
      "        [0.0061],\n",
      "        [0.0075],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0112],\n",
      "        [0.0055],\n",
      "        [0.0060],\n",
      "        [0.0083],\n",
      "        [0.0081],\n",
      "        [0.0132],\n",
      "        [0.0093],\n",
      "        [0.0088],\n",
      "        [0.0087],\n",
      "        [0.0076],\n",
      "        [0.0128],\n",
      "        [0.0092],\n",
      "        [0.0079],\n",
      "        [0.0106],\n",
      "        [0.0117],\n",
      "        [0.0129],\n",
      "        [0.0096],\n",
      "        [0.0144],\n",
      "        [0.0133],\n",
      "        [0.0128],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0145],\n",
      "        [0.0164],\n",
      "        [0.0156],\n",
      "        [0.0199],\n",
      "        [0.0165],\n",
      "        [0.0175],\n",
      "        [0.0153],\n",
      "        [0.0156],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0188],\n",
      "        [0.0167],\n",
      "        [0.0147],\n",
      "        [0.0191],\n",
      "        [0.0187],\n",
      "        [0.0195],\n",
      "        [0.0222],\n",
      "        [0.0217],\n",
      "        [0.0186],\n",
      "        [0.0216],\n",
      "        [0.0234],\n",
      "        [0.0257],\n",
      "        [0.0220],\n",
      "        [0.0279],\n",
      "        [0.0266],\n",
      "        [0.0260],\n",
      "        [0.0292],\n",
      "        [0.0277],\n",
      "        [0.0290],\n",
      "        [0.0246],\n",
      "        [0.0281],\n",
      "        [0.0302],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0303],\n",
      "        [0.0385],\n",
      "        [0.0380],\n",
      "        [0.0379],\n",
      "        [0.0384],\n",
      "        [0.0338],\n",
      "        [0.0403],\n",
      "        [0.0433],\n",
      "        [0.0439],\n",
      "        [0.0415],\n",
      "        [0.0457],\n",
      "        [0.0367],\n",
      "        [0.0522],\n",
      "        [0.0526],\n",
      "        [0.0563],\n",
      "        [0.0590],\n",
      "        [0.0638],\n",
      "        [0.0627],\n",
      "        [0.0661],\n",
      "        [0.0663],\n",
      "        [0.0704],\n",
      "        [0.0743],\n",
      "        [0.0756],\n",
      "        [0.0876],\n",
      "        [0.0848],\n",
      "        [0.0892],\n",
      "        [0.0957]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.476029634475708\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 98\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.1305009429161146e-07, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [12, 34, 0, 62, 52, 22, 102, 56, 21, 89, 14, 51, 1, 87, 93, 74, 90, 91, 57, 16, 49, 15, 53, 63, 26, 13, 107, 61, 103, 18, 28, 106, 76, 77, 32, 88, 8, 3, 68, 75, 98, 86, 99, 9, 66, 100, 20, 23, 33, 94, 10, 58, 108, 54, 92, 104, 2, 19, 105, 46, 30, 65, 55, 101, 27, 97, 95, 17, 85, 36, 96, 109, 125, 29, 64, 50, 11, 31, 25, 73, 78, 35, 47, 48, 7, 72, 24, 59, 60, 6, 37, 4, 67, 126, 44, 110, 112, 111] 數值 torch.Size([98, 1])\n",
      "目前模型的Data狀態 torch.Size([98, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9978],\n",
      "        [0.8134],\n",
      "        [0.8056],\n",
      "        [0.8852],\n",
      "        [0.8867],\n",
      "        [0.8514],\n",
      "        [0.6375],\n",
      "        [0.9019],\n",
      "        [0.8588],\n",
      "        [0.7061],\n",
      "        [0.9860],\n",
      "        [0.8918],\n",
      "        [0.8206],\n",
      "        [0.7280],\n",
      "        [0.7086],\n",
      "        [0.7530],\n",
      "        [0.7113],\n",
      "        [0.6834],\n",
      "        [0.9281],\n",
      "        [0.9355],\n",
      "        [0.8611],\n",
      "        [0.9585],\n",
      "        [0.8582],\n",
      "        [0.8710],\n",
      "        [0.8412],\n",
      "        [0.9940],\n",
      "        [0.6795],\n",
      "        [0.9085],\n",
      "        [0.6464],\n",
      "        [0.9013],\n",
      "        [0.8363],\n",
      "        [0.6834],\n",
      "        [0.7453],\n",
      "        [0.7355],\n",
      "        [0.7956],\n",
      "        [0.7198],\n",
      "        [0.9622],\n",
      "        [0.8300],\n",
      "        [0.8319],\n",
      "        [0.7307],\n",
      "        [0.6615],\n",
      "        [0.7003],\n",
      "        [0.6432],\n",
      "        [0.9922],\n",
      "        [0.8522],\n",
      "        [0.6178],\n",
      "        [0.8980],\n",
      "        [0.8641],\n",
      "        [0.8013],\n",
      "        [0.6890],\n",
      "        [1.0119],\n",
      "        [0.9160],\n",
      "        [0.6694],\n",
      "        [0.9142],\n",
      "        [0.6839],\n",
      "        [0.6810],\n",
      "        [0.8644],\n",
      "        [0.9133],\n",
      "        [0.7041],\n",
      "        [0.8690],\n",
      "        [0.8756],\n",
      "        [0.8090],\n",
      "        [0.8819],\n",
      "        [0.6195],\n",
      "        [0.8317],\n",
      "        [0.6546],\n",
      "        [0.6758],\n",
      "        [0.9192],\n",
      "        [0.6913],\n",
      "        [0.8108],\n",
      "        [0.6627],\n",
      "        [0.7059],\n",
      "        [0.6606],\n",
      "        [0.8674],\n",
      "        [0.8364],\n",
      "        [0.9276],\n",
      "        [1.0175],\n",
      "        [0.8445],\n",
      "        [0.8480],\n",
      "        [0.7888],\n",
      "        [0.7069],\n",
      "        [0.7944],\n",
      "        [0.8819],\n",
      "        [0.8989],\n",
      "        [0.9431],\n",
      "        [0.8051],\n",
      "        [0.8556],\n",
      "        [0.8956],\n",
      "        [0.8892],\n",
      "        [0.9085],\n",
      "        [0.7931],\n",
      "        [0.8547],\n",
      "        [0.8692],\n",
      "        [0.7019],\n",
      "        [0.8143],\n",
      "        [0.7307],\n",
      "        [0.7321],\n",
      "        [0.7511]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0019],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0055],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0070],\n",
      "        [0.0075],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0106],\n",
      "        [0.0112],\n",
      "        [0.0117],\n",
      "        [0.0128],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0147],\n",
      "        [0.0153],\n",
      "        [0.0156],\n",
      "        [0.0156],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0186],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0199],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0234],\n",
      "        [0.0246],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0266],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0290],\n",
      "        [0.0292],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0338],\n",
      "        [0.0367],\n",
      "        [0.0379],\n",
      "        [0.0380],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0403],\n",
      "        [0.0415],\n",
      "        [0.0433],\n",
      "        [0.0439],\n",
      "        [0.0457],\n",
      "        [0.0522],\n",
      "        [0.0526],\n",
      "        [0.0563],\n",
      "        [0.0590],\n",
      "        [0.0627],\n",
      "        [0.0638],\n",
      "        [0.0661],\n",
      "        [0.0663],\n",
      "        [0.0704],\n",
      "        [0.0743],\n",
      "        [0.0756],\n",
      "        [0.0848],\n",
      "        [0.0876],\n",
      "        [0.0892],\n",
      "        [0.0957],\n",
      "        [0.1057]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0010],\n",
      "        [0.0044],\n",
      "        [0.0014],\n",
      "        [0.0025],\n",
      "        [0.0049],\n",
      "        [0.0066],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0080],\n",
      "        [0.0052],\n",
      "        [0.0073],\n",
      "        [0.0099],\n",
      "        [0.0096],\n",
      "        [0.0094],\n",
      "        [0.0059],\n",
      "        [0.0058],\n",
      "        [0.0100],\n",
      "        [0.0076],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0095],\n",
      "        [0.0078],\n",
      "        [0.0106],\n",
      "        [0.0162],\n",
      "        [0.0125],\n",
      "        [0.0161],\n",
      "        [0.0130],\n",
      "        [0.0146],\n",
      "        [0.0176],\n",
      "        [0.0137],\n",
      "        [0.0144],\n",
      "        [0.0137],\n",
      "        [0.0167],\n",
      "        [0.0153],\n",
      "        [0.0122],\n",
      "        [0.0147],\n",
      "        [0.0145],\n",
      "        [0.0167],\n",
      "        [0.0183],\n",
      "        [0.0180],\n",
      "        [0.0166],\n",
      "        [0.0164],\n",
      "        [0.0193],\n",
      "        [0.0193],\n",
      "        [0.0175],\n",
      "        [0.0186],\n",
      "        [0.0207],\n",
      "        [0.0195],\n",
      "        [0.0199],\n",
      "        [0.0252],\n",
      "        [0.0219],\n",
      "        [0.0237],\n",
      "        [0.0181],\n",
      "        [0.0248],\n",
      "        [0.0244],\n",
      "        [0.0206],\n",
      "        [0.0264],\n",
      "        [0.0249],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0298],\n",
      "        [0.0261],\n",
      "        [0.0295],\n",
      "        [0.0303],\n",
      "        [0.0302],\n",
      "        [0.0280],\n",
      "        [0.0314],\n",
      "        [0.0323],\n",
      "        [0.0275],\n",
      "        [0.0270],\n",
      "        [0.0361],\n",
      "        [0.0377],\n",
      "        [0.0367],\n",
      "        [0.0389],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0452],\n",
      "        [0.0451],\n",
      "        [0.0457],\n",
      "        [0.0525],\n",
      "        [0.0529],\n",
      "        [0.0567],\n",
      "        [0.0608],\n",
      "        [0.0619],\n",
      "        [0.0646],\n",
      "        [0.0669],\n",
      "        [0.0658],\n",
      "        [0.0704],\n",
      "        [0.0724],\n",
      "        [0.0747],\n",
      "        [0.0745],\n",
      "        [0.0855],\n",
      "        [0.0821],\n",
      "        [0.0877],\n",
      "        [0.0981]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.75384211540222\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 99\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.8833655935377465e-07, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [12, 34, 62, 52, 56, 21, 0, 22, 14, 90, 74, 102, 51, 57, 26, 89, 16, 49, 53, 15, 93, 63, 87, 1, 91, 13, 3, 61, 18, 76, 32, 77, 75, 28, 68, 8, 103, 107, 66, 9, 88, 98, 23, 106, 99, 104, 86, 33, 20, 100, 10, 58, 105, 94, 54, 92, 19, 2, 30, 108, 27, 46, 65, 125, 55, 109, 85, 97, 101, 17, 95, 36, 96, 29, 50, 64, 11, 31, 25, 78, 73, 35, 47, 48, 7, 72, 24, 59, 6, 60, 37, 4, 126, 67, 110, 44, 112, 111, 124] 數值 torch.Size([99, 1])\n",
      "目前模型的Data狀態 torch.Size([99, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9979],\n",
      "        [0.8135],\n",
      "        [0.8847],\n",
      "        [0.8860],\n",
      "        [0.9018],\n",
      "        [0.8575],\n",
      "        [0.8084],\n",
      "        [0.8499],\n",
      "        [0.9852],\n",
      "        [0.7092],\n",
      "        [0.7547],\n",
      "        [0.6346],\n",
      "        [0.8906],\n",
      "        [0.9276],\n",
      "        [0.8393],\n",
      "        [0.7037],\n",
      "        [0.9356],\n",
      "        [0.8609],\n",
      "        [0.8586],\n",
      "        [0.9585],\n",
      "        [0.7067],\n",
      "        [0.8712],\n",
      "        [0.7259],\n",
      "        [0.8234],\n",
      "        [0.6814],\n",
      "        [0.9939],\n",
      "        [0.8325],\n",
      "        [0.9078],\n",
      "        [0.9011],\n",
      "        [0.7449],\n",
      "        [0.7958],\n",
      "        [0.7362],\n",
      "        [0.7318],\n",
      "        [0.8346],\n",
      "        [0.8314],\n",
      "        [0.9614],\n",
      "        [0.6431],\n",
      "        [0.6745],\n",
      "        [0.8519],\n",
      "        [0.9921],\n",
      "        [0.7176],\n",
      "        [0.6605],\n",
      "        [0.8630],\n",
      "        [0.6790],\n",
      "        [0.6417],\n",
      "        [0.6771],\n",
      "        [0.6985],\n",
      "        [0.8013],\n",
      "        [0.8966],\n",
      "        [0.6160],\n",
      "        [1.0122],\n",
      "        [0.9155],\n",
      "        [0.7000],\n",
      "        [0.6871],\n",
      "        [0.9145],\n",
      "        [0.6819],\n",
      "        [0.9123],\n",
      "        [0.8669],\n",
      "        [0.8745],\n",
      "        [0.6641],\n",
      "        [0.8297],\n",
      "        [0.8697],\n",
      "        [0.8087],\n",
      "        [0.6509],\n",
      "        [0.8823],\n",
      "        [0.6995],\n",
      "        [0.6891],\n",
      "        [0.6541],\n",
      "        [0.6176],\n",
      "        [0.9191],\n",
      "        [0.6747],\n",
      "        [0.8111],\n",
      "        [0.6628],\n",
      "        [0.8657],\n",
      "        [0.9259],\n",
      "        [0.8366],\n",
      "        [1.0180],\n",
      "        [0.8439],\n",
      "        [0.8465],\n",
      "        [0.7081],\n",
      "        [0.7907],\n",
      "        [0.7944],\n",
      "        [0.8822],\n",
      "        [0.8991],\n",
      "        [0.9428],\n",
      "        [0.8069],\n",
      "        [0.8547],\n",
      "        [0.8948],\n",
      "        [0.9090],\n",
      "        [0.8884],\n",
      "        [0.7932],\n",
      "        [0.8565],\n",
      "        [0.6916],\n",
      "        [0.8683],\n",
      "        [0.7236],\n",
      "        [0.8163],\n",
      "        [0.7241],\n",
      "        [0.7435],\n",
      "        [0.6849]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0010],\n",
      "        [0.0014],\n",
      "        [0.0025],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0044],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0066],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0106],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0137],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.0153],\n",
      "        [0.0161],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0167],\n",
      "        [0.0175],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0186],\n",
      "        [0.0193],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0199],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0219],\n",
      "        [0.0237],\n",
      "        [0.0244],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0252],\n",
      "        [0.0261],\n",
      "        [0.0264],\n",
      "        [0.0270],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0275],\n",
      "        [0.0280],\n",
      "        [0.0295],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0314],\n",
      "        [0.0323],\n",
      "        [0.0361],\n",
      "        [0.0367],\n",
      "        [0.0377],\n",
      "        [0.0389],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0457],\n",
      "        [0.0525],\n",
      "        [0.0529],\n",
      "        [0.0567],\n",
      "        [0.0608],\n",
      "        [0.0619],\n",
      "        [0.0646],\n",
      "        [0.0658],\n",
      "        [0.0669],\n",
      "        [0.0704],\n",
      "        [0.0724],\n",
      "        [0.0745],\n",
      "        [0.0747],\n",
      "        [0.0821],\n",
      "        [0.0855],\n",
      "        [0.0877],\n",
      "        [0.0981],\n",
      "        [0.0998]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0008],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0041],\n",
      "        [0.0028],\n",
      "        [0.0067],\n",
      "        [0.0064],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0035],\n",
      "        [0.0088],\n",
      "        [0.0086],\n",
      "        [0.0073],\n",
      "        [0.0060],\n",
      "        [0.0096],\n",
      "        [0.0082],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0088],\n",
      "        [0.0105],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0120],\n",
      "        [0.0113],\n",
      "        [0.0106],\n",
      "        [0.0101],\n",
      "        [0.0126],\n",
      "        [0.0131],\n",
      "        [0.0131],\n",
      "        [0.0138],\n",
      "        [0.0160],\n",
      "        [0.0128],\n",
      "        [0.0163],\n",
      "        [0.0148],\n",
      "        [0.0161],\n",
      "        [0.0189],\n",
      "        [0.0207],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0181],\n",
      "        [0.0169],\n",
      "        [0.0163],\n",
      "        [0.0216],\n",
      "        [0.0186],\n",
      "        [0.0146],\n",
      "        [0.0194],\n",
      "        [0.0186],\n",
      "        [0.0206],\n",
      "        [0.0203],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0170],\n",
      "        [0.0218],\n",
      "        [0.0223],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0269],\n",
      "        [0.0239],\n",
      "        [0.0301],\n",
      "        [0.0242],\n",
      "        [0.0272],\n",
      "        [0.0270],\n",
      "        [0.0159],\n",
      "        [0.0269],\n",
      "        [0.0217],\n",
      "        [0.0263],\n",
      "        [0.0291],\n",
      "        [0.0309],\n",
      "        [0.0301],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0344],\n",
      "        [0.0351],\n",
      "        [0.0369],\n",
      "        [0.0398],\n",
      "        [0.0389],\n",
      "        [0.0384],\n",
      "        [0.0471],\n",
      "        [0.0477],\n",
      "        [0.0455],\n",
      "        [0.0529],\n",
      "        [0.0531],\n",
      "        [0.0573],\n",
      "        [0.0633],\n",
      "        [0.0610],\n",
      "        [0.0650],\n",
      "        [0.0655],\n",
      "        [0.0673],\n",
      "        [0.0701],\n",
      "        [0.0710],\n",
      "        [0.0629],\n",
      "        [0.0743],\n",
      "        [0.0754],\n",
      "        [0.0834],\n",
      "        [0.0802],\n",
      "        [0.0910],\n",
      "        [0.0882]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.0313777923584\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 100\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.595279842258606e-07, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 12, 62, 52, 21, 74, 56, 14, 90, 26, 22, 0, 57, 49, 16, 53, 51, 102, 15, 89, 3, 63, 93, 13, 87, 91, 1, 61, 75, 18, 76, 32, 104, 68, 125, 77, 8, 28, 23, 66, 9, 98, 105, 88, 33, 99, 103, 86, 10, 58, 100, 20, 107, 106, 109, 94, 54, 30, 27, 92, 19, 85, 2, 55, 65, 46, 97, 108, 17, 95, 101, 36, 96, 29, 50, 64, 25, 31, 11, 35, 78, 73, 47, 48, 7, 24, 126, 72, 59, 6, 60, 37, 4, 67, 110, 112, 44, 124, 111, 113] 數值 torch.Size([100, 1])\n",
      "目前模型的Data狀態 torch.Size([100, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8136],\n",
      "        [0.9981],\n",
      "        [0.8848],\n",
      "        [0.8851],\n",
      "        [0.8562],\n",
      "        [0.7571],\n",
      "        [0.9018],\n",
      "        [0.9844],\n",
      "        [0.7078],\n",
      "        [0.8375],\n",
      "        [0.8484],\n",
      "        [0.8107],\n",
      "        [0.9273],\n",
      "        [0.8604],\n",
      "        [0.9356],\n",
      "        [0.8591],\n",
      "        [0.8893],\n",
      "        [0.6324],\n",
      "        [0.9586],\n",
      "        [0.7022],\n",
      "        [0.8346],\n",
      "        [0.8719],\n",
      "        [0.7056],\n",
      "        [0.9939],\n",
      "        [0.7247],\n",
      "        [0.6801],\n",
      "        [0.8256],\n",
      "        [0.9077],\n",
      "        [0.7335],\n",
      "        [0.9010],\n",
      "        [0.7455],\n",
      "        [0.7957],\n",
      "        [0.6736],\n",
      "        [0.8314],\n",
      "        [0.6398],\n",
      "        [0.7378],\n",
      "        [0.9606],\n",
      "        [0.8329],\n",
      "        [0.8618],\n",
      "        [0.8519],\n",
      "        [0.9921],\n",
      "        [0.6602],\n",
      "        [0.6965],\n",
      "        [0.7162],\n",
      "        [0.8014],\n",
      "        [0.6411],\n",
      "        [0.6403],\n",
      "        [0.6973],\n",
      "        [1.0128],\n",
      "        [0.9152],\n",
      "        [0.6150],\n",
      "        [0.8953],\n",
      "        [0.6701],\n",
      "        [0.6750],\n",
      "        [0.6937],\n",
      "        [0.6859],\n",
      "        [0.9149],\n",
      "        [0.8735],\n",
      "        [0.8279],\n",
      "        [0.6809],\n",
      "        [0.9115],\n",
      "        [0.6874],\n",
      "        [0.8690],\n",
      "        [0.8827],\n",
      "        [0.8087],\n",
      "        [0.8705],\n",
      "        [0.6545],\n",
      "        [0.6592],\n",
      "        [0.9191],\n",
      "        [0.6742],\n",
      "        [0.6166],\n",
      "        [0.8117],\n",
      "        [0.6636],\n",
      "        [0.8640],\n",
      "        [0.9243],\n",
      "        [0.8374],\n",
      "        [0.8450],\n",
      "        [0.8432],\n",
      "        [1.0189],\n",
      "        [0.7945],\n",
      "        [0.7102],\n",
      "        [0.7932],\n",
      "        [0.8826],\n",
      "        [0.8993],\n",
      "        [0.9421],\n",
      "        [0.8538],\n",
      "        [0.6800],\n",
      "        [0.8095],\n",
      "        [0.8944],\n",
      "        [0.9092],\n",
      "        [0.8880],\n",
      "        [0.7934],\n",
      "        [0.8580],\n",
      "        [0.8679],\n",
      "        [0.7170],\n",
      "        [0.7166],\n",
      "        [0.8185],\n",
      "        [0.6734],\n",
      "        [0.7364],\n",
      "        [0.7356]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0008],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0028],\n",
      "        [0.0035],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0060],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0086],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0096],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0105],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0113],\n",
      "        [0.0120],\n",
      "        [0.0126],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0131],\n",
      "        [0.0138],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0181],\n",
      "        [0.0186],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0194],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0223],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0263],\n",
      "        [0.0269],\n",
      "        [0.0269],\n",
      "        [0.0270],\n",
      "        [0.0272],\n",
      "        [0.0291],\n",
      "        [0.0301],\n",
      "        [0.0301],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0344],\n",
      "        [0.0351],\n",
      "        [0.0369],\n",
      "        [0.0384],\n",
      "        [0.0389],\n",
      "        [0.0398],\n",
      "        [0.0455],\n",
      "        [0.0471],\n",
      "        [0.0477],\n",
      "        [0.0529],\n",
      "        [0.0531],\n",
      "        [0.0573],\n",
      "        [0.0610],\n",
      "        [0.0629],\n",
      "        [0.0633],\n",
      "        [0.0650],\n",
      "        [0.0655],\n",
      "        [0.0673],\n",
      "        [0.0701],\n",
      "        [0.0710],\n",
      "        [0.0743],\n",
      "        [0.0754],\n",
      "        [0.0802],\n",
      "        [0.0834],\n",
      "        [0.0882],\n",
      "        [0.0910],\n",
      "        [0.0996]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0009],\n",
      "        [0.0013],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0012],\n",
      "        [0.0043],\n",
      "        [0.0034],\n",
      "        [0.0030],\n",
      "        [0.0042],\n",
      "        [0.0079],\n",
      "        [0.0090],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0082],\n",
      "        [0.0080],\n",
      "        [0.0099],\n",
      "        [0.0110],\n",
      "        [0.0089],\n",
      "        [0.0111],\n",
      "        [0.0081],\n",
      "        [0.0107],\n",
      "        [0.0117],\n",
      "        [0.0105],\n",
      "        [0.0121],\n",
      "        [0.0126],\n",
      "        [0.0142],\n",
      "        [0.0130],\n",
      "        [0.0111],\n",
      "        [0.0133],\n",
      "        [0.0127],\n",
      "        [0.0138],\n",
      "        [0.0111],\n",
      "        [0.0146],\n",
      "        [0.0057],\n",
      "        [0.0175],\n",
      "        [0.0168],\n",
      "        [0.0180],\n",
      "        [0.0151],\n",
      "        [0.0162],\n",
      "        [0.0167],\n",
      "        [0.0173],\n",
      "        [0.0135],\n",
      "        [0.0195],\n",
      "        [0.0186],\n",
      "        [0.0195],\n",
      "        [0.0218],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0251],\n",
      "        [0.0256],\n",
      "        [0.0158],\n",
      "        [0.0231],\n",
      "        [0.0226],\n",
      "        [0.0229],\n",
      "        [0.0223],\n",
      "        [0.0260],\n",
      "        [0.0262],\n",
      "        [0.0247],\n",
      "        [0.0290],\n",
      "        [0.0266],\n",
      "        [0.0272],\n",
      "        [0.0278],\n",
      "        [0.0289],\n",
      "        [0.0349],\n",
      "        [0.0299],\n",
      "        [0.0314],\n",
      "        [0.0321],\n",
      "        [0.0304],\n",
      "        [0.0307],\n",
      "        [0.0328],\n",
      "        [0.0334],\n",
      "        [0.0365],\n",
      "        [0.0368],\n",
      "        [0.0383],\n",
      "        [0.0405],\n",
      "        [0.0454],\n",
      "        [0.0491],\n",
      "        [0.0501],\n",
      "        [0.0532],\n",
      "        [0.0531],\n",
      "        [0.0578],\n",
      "        [0.0600],\n",
      "        [0.0521],\n",
      "        [0.0657],\n",
      "        [0.0656],\n",
      "        [0.0652],\n",
      "        [0.0679],\n",
      "        [0.0698],\n",
      "        [0.0696],\n",
      "        [0.0737],\n",
      "        [0.0687],\n",
      "        [0.0726],\n",
      "        [0.0814],\n",
      "        [0.0777],\n",
      "        [0.0839],\n",
      "        [0.0919]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.308778762817383\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 101\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.4980021129958914e-07, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 52, 12, 74, 62, 21, 90, 14, 26, 56, 125, 57, 49, 22, 53, 3, 16, 15, 0, 51, 13, 63, 102, 75, 104, 89, 93, 87, 91, 76, 61, 18, 105, 32, 1, 68, 23, 109, 66, 9, 8, 98, 77, 28, 33, 99, 88, 86, 10, 58, 100, 103, 20, 27, 54, 30, 94, 85, 107, 106, 92, 19, 55, 65, 46, 97, 2, 17, 36, 96, 95, 101, 29, 50, 108, 64, 25, 31, 11, 35, 78, 73, 126, 48, 47, 7, 24, 6, 59, 72, 60, 110, 4, 37, 112, 67, 124, 44, 111, 133, 127] 數值 torch.Size([101, 1])\n",
      "目前模型的Data狀態 torch.Size([101, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8138],\n",
      "        [0.8842],\n",
      "        [0.9981],\n",
      "        [0.7594],\n",
      "        [0.8846],\n",
      "        [0.8548],\n",
      "        [0.7064],\n",
      "        [0.9834],\n",
      "        [0.8357],\n",
      "        [0.9016],\n",
      "        [0.6295],\n",
      "        [0.9269],\n",
      "        [0.8599],\n",
      "        [0.8468],\n",
      "        [0.8595],\n",
      "        [0.8366],\n",
      "        [0.9356],\n",
      "        [0.9585],\n",
      "        [0.8129],\n",
      "        [0.8879],\n",
      "        [0.9938],\n",
      "        [0.8724],\n",
      "        [0.6301],\n",
      "        [0.7352],\n",
      "        [0.6701],\n",
      "        [0.7006],\n",
      "        [0.7044],\n",
      "        [0.7234],\n",
      "        [0.6788],\n",
      "        [0.7459],\n",
      "        [0.9072],\n",
      "        [0.9008],\n",
      "        [0.6929],\n",
      "        [0.7957],\n",
      "        [0.8278],\n",
      "        [0.8312],\n",
      "        [0.8606],\n",
      "        [0.6878],\n",
      "        [0.8517],\n",
      "        [0.9922],\n",
      "        [0.9599],\n",
      "        [0.6598],\n",
      "        [0.7392],\n",
      "        [0.8312],\n",
      "        [0.8014],\n",
      "        [0.6402],\n",
      "        [0.7147],\n",
      "        [0.6963],\n",
      "        [1.0133],\n",
      "        [0.9148],\n",
      "        [0.6138],\n",
      "        [0.6374],\n",
      "        [0.8938],\n",
      "        [0.8260],\n",
      "        [0.9152],\n",
      "        [0.8725],\n",
      "        [0.6846],\n",
      "        [0.6857],\n",
      "        [0.6656],\n",
      "        [0.6710],\n",
      "        [0.6797],\n",
      "        [0.9105],\n",
      "        [0.8830],\n",
      "        [0.8084],\n",
      "        [0.8711],\n",
      "        [0.6547],\n",
      "        [0.8711],\n",
      "        [0.9189],\n",
      "        [0.8122],\n",
      "        [0.6643],\n",
      "        [0.6736],\n",
      "        [0.6153],\n",
      "        [0.8623],\n",
      "        [0.9226],\n",
      "        [0.6543],\n",
      "        [0.8379],\n",
      "        [0.8433],\n",
      "        [0.8426],\n",
      "        [1.0196],\n",
      "        [0.7946],\n",
      "        [0.7121],\n",
      "        [0.7956],\n",
      "        [0.6692],\n",
      "        [0.8994],\n",
      "        [0.8828],\n",
      "        [0.9416],\n",
      "        [0.8529],\n",
      "        [0.9095],\n",
      "        [0.8938],\n",
      "        [0.8119],\n",
      "        [0.8874],\n",
      "        [0.7103],\n",
      "        [0.8594],\n",
      "        [0.7937],\n",
      "        [0.7090],\n",
      "        [0.8673],\n",
      "        [0.6628],\n",
      "        [0.8204],\n",
      "        [0.7293],\n",
      "        [0.7618],\n",
      "        [0.7100]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0030],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0057],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0089],\n",
      "        [0.0090],\n",
      "        [0.0099],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0110],\n",
      "        [0.0111],\n",
      "        [0.0111],\n",
      "        [0.0111],\n",
      "        [0.0117],\n",
      "        [0.0121],\n",
      "        [0.0126],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0142],\n",
      "        [0.0146],\n",
      "        [0.0151],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0167],\n",
      "        [0.0168],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0180],\n",
      "        [0.0186],\n",
      "        [0.0195],\n",
      "        [0.0195],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0215],\n",
      "        [0.0218],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0226],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0256],\n",
      "        [0.0260],\n",
      "        [0.0262],\n",
      "        [0.0266],\n",
      "        [0.0272],\n",
      "        [0.0278],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0299],\n",
      "        [0.0304],\n",
      "        [0.0307],\n",
      "        [0.0314],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0334],\n",
      "        [0.0349],\n",
      "        [0.0365],\n",
      "        [0.0368],\n",
      "        [0.0383],\n",
      "        [0.0405],\n",
      "        [0.0454],\n",
      "        [0.0491],\n",
      "        [0.0501],\n",
      "        [0.0521],\n",
      "        [0.0531],\n",
      "        [0.0532],\n",
      "        [0.0578],\n",
      "        [0.0600],\n",
      "        [0.0652],\n",
      "        [0.0656],\n",
      "        [0.0657],\n",
      "        [0.0679],\n",
      "        [0.0687],\n",
      "        [0.0696],\n",
      "        [0.0698],\n",
      "        [0.0726],\n",
      "        [0.0737],\n",
      "        [0.0777],\n",
      "        [0.0814],\n",
      "        [0.0839],\n",
      "        [0.0901],\n",
      "        [0.0912]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0001],\n",
      "        [0.0007],\n",
      "        [0.0029],\n",
      "        [0.0021],\n",
      "        [0.0002],\n",
      "        [0.0035],\n",
      "        [0.0019],\n",
      "        [0.0028],\n",
      "        [0.0040],\n",
      "        [0.0055],\n",
      "        [0.0071],\n",
      "        [0.0068],\n",
      "        [0.0093],\n",
      "        [0.0071],\n",
      "        [0.0063],\n",
      "        [0.0087],\n",
      "        [0.0092],\n",
      "        [0.0110],\n",
      "        [0.0112],\n",
      "        [0.0100],\n",
      "        [0.0122],\n",
      "        [0.0108],\n",
      "        [0.0076],\n",
      "        [0.0097],\n",
      "        [0.0108],\n",
      "        [0.0107],\n",
      "        [0.0112],\n",
      "        [0.0119],\n",
      "        [0.0102],\n",
      "        [0.0125],\n",
      "        [0.0136],\n",
      "        [0.0120],\n",
      "        [0.0139],\n",
      "        [0.0161],\n",
      "        [0.0155],\n",
      "        [0.0140],\n",
      "        [0.0117],\n",
      "        [0.0169],\n",
      "        [0.0168],\n",
      "        [0.0176],\n",
      "        [0.0151],\n",
      "        [0.0210],\n",
      "        [0.0194],\n",
      "        [0.0185],\n",
      "        [0.0177],\n",
      "        [0.0189],\n",
      "        [0.0196],\n",
      "        [0.0210],\n",
      "        [0.0205],\n",
      "        [0.0200],\n",
      "        [0.0224],\n",
      "        [0.0235],\n",
      "        [0.0208],\n",
      "        [0.0236],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0248],\n",
      "        [0.0275],\n",
      "        [0.0275],\n",
      "        [0.0248],\n",
      "        [0.0273],\n",
      "        [0.0258],\n",
      "        [0.0265],\n",
      "        [0.0285],\n",
      "        [0.0260],\n",
      "        [0.0309],\n",
      "        [0.0296],\n",
      "        [0.0295],\n",
      "        [0.0273],\n",
      "        [0.0298],\n",
      "        [0.0308],\n",
      "        [0.0313],\n",
      "        [0.0317],\n",
      "        [0.0377],\n",
      "        [0.0349],\n",
      "        [0.0354],\n",
      "        [0.0377],\n",
      "        [0.0414],\n",
      "        [0.0451],\n",
      "        [0.0530],\n",
      "        [0.0540],\n",
      "        [0.0401],\n",
      "        [0.0530],\n",
      "        [0.0534],\n",
      "        [0.0587],\n",
      "        [0.0593],\n",
      "        [0.0652],\n",
      "        [0.0655],\n",
      "        [0.0696],\n",
      "        [0.0676],\n",
      "        [0.0638],\n",
      "        [0.0682],\n",
      "        [0.0692],\n",
      "        [0.0668],\n",
      "        [0.0739],\n",
      "        [0.0660],\n",
      "        [0.0791],\n",
      "        [0.0785],\n",
      "        [0.0751],\n",
      "        [0.0787]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.58753728866577\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 102\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0051067533822788e-08, 52)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [52, 21, 34, 12, 14, 62, 26, 74, 90, 56, 125, 3, 49, 57, 53, 75, 16, 15, 22, 104, 13, 76, 93, 89, 102, 0, 87, 51, 109, 91, 105, 63, 61, 18, 32, 23, 98, 68, 1, 9, 66, 8, 99, 33, 88, 28, 86, 100, 58, 27, 77, 10, 30, 94, 103, 20, 54, 92, 85, 55, 97, 65, 19, 96, 106, 107, 46, 36, 17, 95, 101, 2, 29, 50, 64, 25, 108, 31, 126, 11, 35, 48, 78, 47, 73, 7, 24, 110, 6, 59, 124, 112, 60, 4, 37, 72, 67, 133, 111, 127, 44, 113] 數值 torch.Size([102, 1])\n",
      "目前模型的Data狀態 torch.Size([102, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8836],\n",
      "        [0.8535],\n",
      "        [0.8141],\n",
      "        [0.9980],\n",
      "        [0.9819],\n",
      "        [0.8854],\n",
      "        [0.8343],\n",
      "        [0.7635],\n",
      "        [0.7069],\n",
      "        [0.9019],\n",
      "        [0.6183],\n",
      "        [0.8384],\n",
      "        [0.8592],\n",
      "        [0.9271],\n",
      "        [0.8603],\n",
      "        [0.7387],\n",
      "        [0.9351],\n",
      "        [0.9581],\n",
      "        [0.8455],\n",
      "        [0.6687],\n",
      "        [0.9933],\n",
      "        [0.7484],\n",
      "        [0.7054],\n",
      "        [0.7010],\n",
      "        [0.6303],\n",
      "        [0.8150],\n",
      "        [0.7243],\n",
      "        [0.8866],\n",
      "        [0.6837],\n",
      "        [0.6796],\n",
      "        [0.6914],\n",
      "        [0.8739],\n",
      "        [0.9078],\n",
      "        [0.9005],\n",
      "        [0.7955],\n",
      "        [0.8594],\n",
      "        [0.6621],\n",
      "        [0.8321],\n",
      "        [0.8296],\n",
      "        [0.9923],\n",
      "        [0.8524],\n",
      "        [0.9591],\n",
      "        [0.6420],\n",
      "        [0.8015],\n",
      "        [0.7154],\n",
      "        [0.8298],\n",
      "        [0.6972],\n",
      "        [0.6154],\n",
      "        [0.9149],\n",
      "        [0.8245],\n",
      "        [0.7428],\n",
      "        [1.0138],\n",
      "        [0.8717],\n",
      "        [0.6855],\n",
      "        [0.6367],\n",
      "        [0.8924],\n",
      "        [0.9161],\n",
      "        [0.6809],\n",
      "        [0.6859],\n",
      "        [0.8838],\n",
      "        [0.6576],\n",
      "        [0.8091],\n",
      "        [0.9094],\n",
      "        [0.6677],\n",
      "        [0.6691],\n",
      "        [0.6632],\n",
      "        [0.8718],\n",
      "        [0.8131],\n",
      "        [0.9185],\n",
      "        [0.6752],\n",
      "        [0.6167],\n",
      "        [0.8731],\n",
      "        [0.8609],\n",
      "        [0.9209],\n",
      "        [0.8395],\n",
      "        [0.8420],\n",
      "        [0.6515],\n",
      "        [0.8420],\n",
      "        [0.6572],\n",
      "        [1.0204],\n",
      "        [0.7950],\n",
      "        [0.8992],\n",
      "        [0.7161],\n",
      "        [0.8830],\n",
      "        [0.7995],\n",
      "        [0.9407],\n",
      "        [0.8521],\n",
      "        [0.7054],\n",
      "        [0.9096],\n",
      "        [0.8938],\n",
      "        [0.6511],\n",
      "        [0.7032],\n",
      "        [0.8876],\n",
      "        [0.8608],\n",
      "        [0.7943],\n",
      "        [0.8157],\n",
      "        [0.8675],\n",
      "        [0.7468],\n",
      "        [0.7239],\n",
      "        [0.6974],\n",
      "        [0.8227],\n",
      "        [0.7220]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0035],\n",
      "        [0.0040],\n",
      "        [0.0055],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0071],\n",
      "        [0.0076],\n",
      "        [0.0087],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0117],\n",
      "        [0.0119],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0140],\n",
      "        [0.0151],\n",
      "        [0.0155],\n",
      "        [0.0161],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0185],\n",
      "        [0.0189],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0200],\n",
      "        [0.0205],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0210],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0248],\n",
      "        [0.0248],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0265],\n",
      "        [0.0273],\n",
      "        [0.0273],\n",
      "        [0.0275],\n",
      "        [0.0275],\n",
      "        [0.0285],\n",
      "        [0.0295],\n",
      "        [0.0296],\n",
      "        [0.0298],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0313],\n",
      "        [0.0317],\n",
      "        [0.0349],\n",
      "        [0.0354],\n",
      "        [0.0377],\n",
      "        [0.0377],\n",
      "        [0.0401],\n",
      "        [0.0414],\n",
      "        [0.0451],\n",
      "        [0.0530],\n",
      "        [0.0530],\n",
      "        [0.0534],\n",
      "        [0.0540],\n",
      "        [0.0587],\n",
      "        [0.0593],\n",
      "        [0.0638],\n",
      "        [0.0652],\n",
      "        [0.0655],\n",
      "        [0.0660],\n",
      "        [0.0668],\n",
      "        [0.0676],\n",
      "        [0.0682],\n",
      "        [0.0692],\n",
      "        [0.0696],\n",
      "        [0.0739],\n",
      "        [0.0751],\n",
      "        [0.0785],\n",
      "        [0.0787],\n",
      "        [0.0791],\n",
      "        [0.0860]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0034],\n",
      "        [0.0043],\n",
      "        [0.0027],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0006],\n",
      "        [0.0017],\n",
      "        [0.0030],\n",
      "        [0.0002],\n",
      "        [0.0069],\n",
      "        [0.0185],\n",
      "        [0.0075],\n",
      "        [0.0030],\n",
      "        [0.0041],\n",
      "        [0.0092],\n",
      "        [0.0080],\n",
      "        [0.0124],\n",
      "        [0.0128],\n",
      "        [0.0138],\n",
      "        [0.0047],\n",
      "        [0.0062],\n",
      "        [0.0116],\n",
      "        [0.0137],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0097],\n",
      "        [0.0142],\n",
      "        [0.0153],\n",
      "        [0.0043],\n",
      "        [0.0149],\n",
      "        [0.0070],\n",
      "        [0.0101],\n",
      "        [0.0155],\n",
      "        [0.0171],\n",
      "        [0.0168],\n",
      "        [0.0097],\n",
      "        [0.0168],\n",
      "        [0.0126],\n",
      "        [0.0147],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0214],\n",
      "        [0.0199],\n",
      "        [0.0211],\n",
      "        [0.0221],\n",
      "        [0.0237],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0236],\n",
      "        [0.0163],\n",
      "        [0.0206],\n",
      "        [0.0184],\n",
      "        [0.0187],\n",
      "        [0.0253],\n",
      "        [0.0268],\n",
      "        [0.0282],\n",
      "        [0.0214],\n",
      "        [0.0276],\n",
      "        [0.0214],\n",
      "        [0.0281],\n",
      "        [0.0271],\n",
      "        [0.0296],\n",
      "        [0.0316],\n",
      "        [0.0279],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0260],\n",
      "        [0.0314],\n",
      "        [0.0259],\n",
      "        [0.0321],\n",
      "        [0.0333],\n",
      "        [0.0296],\n",
      "        [0.0271],\n",
      "        [0.0271],\n",
      "        [0.0370],\n",
      "        [0.0309],\n",
      "        [0.0439],\n",
      "        [0.0345],\n",
      "        [0.0263],\n",
      "        [0.0389],\n",
      "        [0.0476],\n",
      "        [0.0497],\n",
      "        [0.0530],\n",
      "        [0.0504],\n",
      "        [0.0539],\n",
      "        [0.0625],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0526],\n",
      "        [0.0576],\n",
      "        [0.0708],\n",
      "        [0.0699],\n",
      "        [0.0715],\n",
      "        [0.0695],\n",
      "        [0.0704],\n",
      "        [0.0586],\n",
      "        [0.0699],\n",
      "        [0.0643],\n",
      "        [0.0799],\n",
      "        [0.0767]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.865756034851074\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 103\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.5431215173484816e-08, 90)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [90, 62, 26, 12, 34, 14, 74, 49, 52, 57, 21, 109, 104, 13, 56, 105, 3, 75, 53, 0, 23, 63, 76, 16, 68, 15, 93, 9, 22, 66, 87, 89, 102, 1, 91, 51, 61, 27, 32, 98, 18, 10, 125, 30, 99, 77, 33, 8, 85, 54, 88, 86, 100, 58, 28, 94, 17, 46, 126, 103, 97, 50, 29, 92, 96, 55, 20, 65, 2, 25, 36, 19, 95, 106, 101, 107, 31, 64, 11, 108, 35, 48, 47, 124, 78, 73, 24, 110, 112, 133, 7, 127, 6, 59, 72, 111, 4, 67, 60, 37, 123, 130, 113] 數值 torch.Size([103, 1])\n",
      "目前模型的Data狀態 torch.Size([103, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7036],\n",
      "        [0.8827],\n",
      "        [0.8298],\n",
      "        [0.9945],\n",
      "        [0.8117],\n",
      "        [0.9772],\n",
      "        [0.7636],\n",
      "        [0.8554],\n",
      "        [0.8801],\n",
      "        [0.9241],\n",
      "        [0.8491],\n",
      "        [0.6764],\n",
      "        [0.6637],\n",
      "        [0.9895],\n",
      "        [0.8990],\n",
      "        [0.6865],\n",
      "        [0.8372],\n",
      "        [0.7383],\n",
      "        [0.8582],\n",
      "        [0.8137],\n",
      "        [0.8552],\n",
      "        [0.8718],\n",
      "        [0.7470],\n",
      "        [0.9314],\n",
      "        [0.8293],\n",
      "        [0.9545],\n",
      "        [0.7024],\n",
      "        [0.9893],\n",
      "        [0.8410],\n",
      "        [0.8494],\n",
      "        [0.7213],\n",
      "        [0.6975],\n",
      "        [0.6267],\n",
      "        [0.8283],\n",
      "        [0.6765],\n",
      "        [0.8825],\n",
      "        [0.9048],\n",
      "        [0.8200],\n",
      "        [0.7927],\n",
      "        [0.6603],\n",
      "        [0.8970],\n",
      "        [1.0111],\n",
      "        [0.6054],\n",
      "        [0.8683],\n",
      "        [0.6399],\n",
      "        [0.7423],\n",
      "        [0.7988],\n",
      "        [0.9553],\n",
      "        [0.6824],\n",
      "        [0.9140],\n",
      "        [0.7122],\n",
      "        [0.6944],\n",
      "        [0.6129],\n",
      "        [0.9118],\n",
      "        [0.8255],\n",
      "        [0.6825],\n",
      "        [0.9149],\n",
      "        [0.8693],\n",
      "        [0.6434],\n",
      "        [0.6324],\n",
      "        [0.6565],\n",
      "        [0.9163],\n",
      "        [0.8567],\n",
      "        [0.6781],\n",
      "        [0.6671],\n",
      "        [0.8815],\n",
      "        [0.8877],\n",
      "        [0.8060],\n",
      "        [0.8718],\n",
      "        [0.8375],\n",
      "        [0.8111],\n",
      "        [0.9051],\n",
      "        [0.6729],\n",
      "        [0.6638],\n",
      "        [0.6142],\n",
      "        [0.6574],\n",
      "        [0.8388],\n",
      "        [0.8373],\n",
      "        [1.0180],\n",
      "        [0.6454],\n",
      "        [0.7924],\n",
      "        [0.8960],\n",
      "        [0.8801],\n",
      "        [0.6377],\n",
      "        [0.7160],\n",
      "        [0.7995],\n",
      "        [0.8483],\n",
      "        [0.6972],\n",
      "        [0.6940],\n",
      "        [0.7303],\n",
      "        [0.9370],\n",
      "        [0.6830],\n",
      "        [0.9067],\n",
      "        [0.8906],\n",
      "        [0.8157],\n",
      "        [0.7153],\n",
      "        [0.8590],\n",
      "        [0.8640],\n",
      "        [0.8844],\n",
      "        [0.7920],\n",
      "        [0.6857],\n",
      "        [0.6566],\n",
      "        [0.7127]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0017],\n",
      "        [0.0027],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0034],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0101],\n",
      "        [0.0116],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0128],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0155],\n",
      "        [0.0163],\n",
      "        [0.0168],\n",
      "        [0.0168],\n",
      "        [0.0171],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0199],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0253],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0271],\n",
      "        [0.0271],\n",
      "        [0.0271],\n",
      "        [0.0276],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0282],\n",
      "        [0.0296],\n",
      "        [0.0296],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0333],\n",
      "        [0.0345],\n",
      "        [0.0370],\n",
      "        [0.0389],\n",
      "        [0.0439],\n",
      "        [0.0476],\n",
      "        [0.0497],\n",
      "        [0.0504],\n",
      "        [0.0526],\n",
      "        [0.0530],\n",
      "        [0.0539],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0576],\n",
      "        [0.0586],\n",
      "        [0.0625],\n",
      "        [0.0643],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0695],\n",
      "        [0.0699],\n",
      "        [0.0699],\n",
      "        [0.0704],\n",
      "        [0.0708],\n",
      "        [0.0715],\n",
      "        [0.0752],\n",
      "        [0.0760],\n",
      "        [0.0767]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0019],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0002],\n",
      "        [0.0014],\n",
      "        [0.0018],\n",
      "        [0.0081],\n",
      "        [0.0033],\n",
      "        [0.0029],\n",
      "        [0.0061],\n",
      "        [0.0039],\n",
      "        [0.0012],\n",
      "        [0.0043],\n",
      "        [0.0084],\n",
      "        [0.0054],\n",
      "        [0.0067],\n",
      "        [0.0040],\n",
      "        [0.0035],\n",
      "        [0.0074],\n",
      "        [0.0136],\n",
      "        [0.0102],\n",
      "        [0.0128],\n",
      "        [0.0078],\n",
      "        [0.0107],\n",
      "        [0.0151],\n",
      "        [0.0110],\n",
      "        [0.0116],\n",
      "        [0.0169],\n",
      "        [0.0135],\n",
      "        [0.0162],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0135],\n",
      "        [0.0184],\n",
      "        [0.0132],\n",
      "        [0.0154],\n",
      "        [0.0134],\n",
      "        [0.0163],\n",
      "        [0.0160],\n",
      "        [0.0137],\n",
      "        [0.0154],\n",
      "        [0.0219],\n",
      "        [0.0292],\n",
      "        [0.0196],\n",
      "        [0.0173],\n",
      "        [0.0252],\n",
      "        [0.0201],\n",
      "        [0.0194],\n",
      "        [0.0227],\n",
      "        [0.0237],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0217],\n",
      "        [0.0236],\n",
      "        [0.0235],\n",
      "        [0.0276],\n",
      "        [0.0279],\n",
      "        [0.0151],\n",
      "        [0.0266],\n",
      "        [0.0234],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0255],\n",
      "        [0.0237],\n",
      "        [0.0262],\n",
      "        [0.0277],\n",
      "        [0.0280],\n",
      "        [0.0336],\n",
      "        [0.0311],\n",
      "        [0.0296],\n",
      "        [0.0306],\n",
      "        [0.0297],\n",
      "        [0.0338],\n",
      "        [0.0313],\n",
      "        [0.0348],\n",
      "        [0.0351],\n",
      "        [0.0345],\n",
      "        [0.0430],\n",
      "        [0.0459],\n",
      "        [0.0464],\n",
      "        [0.0508],\n",
      "        [0.0518],\n",
      "        [0.0415],\n",
      "        [0.0577],\n",
      "        [0.0589],\n",
      "        [0.0562],\n",
      "        [0.0518],\n",
      "        [0.0528],\n",
      "        [0.0451],\n",
      "        [0.0608],\n",
      "        [0.0527],\n",
      "        [0.0655],\n",
      "        [0.0671],\n",
      "        [0.0745],\n",
      "        [0.0657],\n",
      "        [0.0665],\n",
      "        [0.0725],\n",
      "        [0.0691],\n",
      "        [0.0701],\n",
      "        [0.0633],\n",
      "        [0.0637],\n",
      "        [0.0720]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.143279552459717\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 104\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.0084845664978275e-08, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [12, 109, 34, 26, 62, 14, 90, 52, 49, 75, 21, 3, 104, 56, 57, 105, 53, 76, 74, 13, 23, 16, 15, 93, 87, 89, 63, 91, 61, 102, 22, 0, 98, 126, 68, 18, 51, 32, 66, 27, 9, 99, 1, 8, 30, 33, 88, 86, 100, 58, 10, 85, 97, 94, 28, 96, 54, 77, 92, 55, 103, 50, 29, 17, 20, 46, 65, 125, 36, 95, 19, 25, 101, 2, 106, 64, 107, 31, 124, 11, 133, 108, 35, 48, 47, 110, 127, 112, 24, 78, 73, 7, 123, 130, 6, 111, 132, 4, 59, 60, 37, 131, 113, 67] 數值 torch.Size([104, 1])\n",
      "目前模型的Data狀態 torch.Size([104, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9975],\n",
      "        [0.6732],\n",
      "        [0.8131],\n",
      "        [0.8300],\n",
      "        [0.8849],\n",
      "        [0.9782],\n",
      "        [0.7053],\n",
      "        [0.8806],\n",
      "        [0.8557],\n",
      "        [0.7428],\n",
      "        [0.8495],\n",
      "        [0.8407],\n",
      "        [0.6633],\n",
      "        [0.9005],\n",
      "        [0.9260],\n",
      "        [0.6861],\n",
      "        [0.8600],\n",
      "        [0.7508],\n",
      "        [0.7687],\n",
      "        [0.9918],\n",
      "        [0.8556],\n",
      "        [0.9331],\n",
      "        [0.9564],\n",
      "        [0.7045],\n",
      "        [0.7237],\n",
      "        [0.6991],\n",
      "        [0.8745],\n",
      "        [0.6782],\n",
      "        [0.9069],\n",
      "        [0.6277],\n",
      "        [0.8412],\n",
      "        [0.8176],\n",
      "        [0.6634],\n",
      "        [0.6321],\n",
      "        [0.8317],\n",
      "        [0.8987],\n",
      "        [0.8824],\n",
      "        [0.7934],\n",
      "        [0.8517],\n",
      "        [0.8199],\n",
      "        [0.9924],\n",
      "        [0.6424],\n",
      "        [0.8319],\n",
      "        [0.9573],\n",
      "        [0.8692],\n",
      "        [0.7999],\n",
      "        [0.7141],\n",
      "        [0.6966],\n",
      "        [0.6151],\n",
      "        [0.9137],\n",
      "        [1.0147],\n",
      "        [0.6838],\n",
      "        [0.6602],\n",
      "        [0.6843],\n",
      "        [0.8256],\n",
      "        [0.6714],\n",
      "        [0.9163],\n",
      "        [0.7469],\n",
      "        [0.6802],\n",
      "        [0.8834],\n",
      "        [0.6325],\n",
      "        [0.9162],\n",
      "        [0.8569],\n",
      "        [0.9166],\n",
      "        [0.8882],\n",
      "        [0.8711],\n",
      "        [0.8077],\n",
      "        [0.5947],\n",
      "        [0.8129],\n",
      "        [0.6753],\n",
      "        [0.9061],\n",
      "        [0.8377],\n",
      "        [0.6162],\n",
      "        [0.8758],\n",
      "        [0.6628],\n",
      "        [0.8399],\n",
      "        [0.6559],\n",
      "        [0.8394],\n",
      "        [0.6266],\n",
      "        [1.0221],\n",
      "        [0.7168],\n",
      "        [0.6434],\n",
      "        [0.7936],\n",
      "        [0.8970],\n",
      "        [0.8814],\n",
      "        [0.6934],\n",
      "        [0.6714],\n",
      "        [0.6892],\n",
      "        [0.8491],\n",
      "        [0.7208],\n",
      "        [0.8044],\n",
      "        [0.9386],\n",
      "        [0.6738],\n",
      "        [0.6444],\n",
      "        [0.9092],\n",
      "        [0.7111],\n",
      "        [0.7432],\n",
      "        [0.8624],\n",
      "        [0.8923],\n",
      "        [0.8862],\n",
      "        [0.7934],\n",
      "        [0.7015],\n",
      "        [0.7080],\n",
      "        [0.8661]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0029],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0054],\n",
      "        [0.0061],\n",
      "        [0.0067],\n",
      "        [0.0074],\n",
      "        [0.0078],\n",
      "        [0.0081],\n",
      "        [0.0084],\n",
      "        [0.0102],\n",
      "        [0.0107],\n",
      "        [0.0110],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0128],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.0173],\n",
      "        [0.0184],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0237],\n",
      "        [0.0252],\n",
      "        [0.0255],\n",
      "        [0.0262],\n",
      "        [0.0266],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0276],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0280],\n",
      "        [0.0292],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0311],\n",
      "        [0.0313],\n",
      "        [0.0336],\n",
      "        [0.0338],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0351],\n",
      "        [0.0415],\n",
      "        [0.0430],\n",
      "        [0.0451],\n",
      "        [0.0459],\n",
      "        [0.0464],\n",
      "        [0.0508],\n",
      "        [0.0518],\n",
      "        [0.0518],\n",
      "        [0.0527],\n",
      "        [0.0528],\n",
      "        [0.0562],\n",
      "        [0.0577],\n",
      "        [0.0589],\n",
      "        [0.0608],\n",
      "        [0.0633],\n",
      "        [0.0637],\n",
      "        [0.0655],\n",
      "        [0.0657],\n",
      "        [0.0665],\n",
      "        [0.0665],\n",
      "        [0.0671],\n",
      "        [0.0691],\n",
      "        [0.0701],\n",
      "        [0.0710],\n",
      "        [0.0720],\n",
      "        [0.0725]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0024],\n",
      "        [    0.0048],\n",
      "        [    0.0036],\n",
      "        [    0.0054],\n",
      "        [    0.0003],\n",
      "        [    0.0059],\n",
      "        [    0.0007],\n",
      "        [    0.0057],\n",
      "        [    0.0000],\n",
      "        [    0.0015],\n",
      "        [    0.0080],\n",
      "        [    0.0055],\n",
      "        [    0.0011],\n",
      "        [    0.0073],\n",
      "        [    0.0044],\n",
      "        [    0.0035],\n",
      "        [    0.0088],\n",
      "        [    0.0066],\n",
      "        [    0.0107],\n",
      "        [    0.0054],\n",
      "        [    0.0062],\n",
      "        [    0.0140],\n",
      "        [    0.0140],\n",
      "        [    0.0124],\n",
      "        [    0.0122],\n",
      "        [    0.0139],\n",
      "        [    0.0121],\n",
      "        [    0.0144],\n",
      "        [    0.0148],\n",
      "        [    0.0153],\n",
      "        [    0.0176],\n",
      "        [    0.0122],\n",
      "        [    0.0135],\n",
      "        [    0.0006],\n",
      "        [    0.0137],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0189],\n",
      "        [    0.0144],\n",
      "        [    0.0124],\n",
      "        [    0.0148],\n",
      "        [    0.0176],\n",
      "        [    0.0169],\n",
      "        [    0.0225],\n",
      "        [    0.0167],\n",
      "        [    0.0227],\n",
      "        [    0.0209],\n",
      "        [    0.0206],\n",
      "        [    0.0210],\n",
      "        [    0.0236],\n",
      "        [    0.0202],\n",
      "        [    0.0214],\n",
      "        [    0.0225],\n",
      "        [    0.0246],\n",
      "        [    0.0275],\n",
      "        [    0.0221],\n",
      "        [    0.0227],\n",
      "        [    0.0272],\n",
      "        [    0.0262],\n",
      "        [    0.0276],\n",
      "        [    0.0294],\n",
      "        [    0.0234],\n",
      "        [    0.0237],\n",
      "        [    0.0245],\n",
      "        [    0.0319],\n",
      "        [    0.0258],\n",
      "        [    0.0300],\n",
      "        [    0.0428],\n",
      "        [    0.0310],\n",
      "        [    0.0301],\n",
      "        [    0.0344],\n",
      "        [    0.0270],\n",
      "        [    0.0321],\n",
      "        [    0.0325],\n",
      "        [    0.0375],\n",
      "        [    0.0354],\n",
      "        [    0.0391],\n",
      "        [    0.0322],\n",
      "        [    0.0275],\n",
      "        [    0.0417],\n",
      "        [    0.0274],\n",
      "        [    0.0506],\n",
      "        [    0.0486],\n",
      "        [    0.0479],\n",
      "        [    0.0492],\n",
      "        [    0.0450],\n",
      "        [    0.0377],\n",
      "        [    0.0450],\n",
      "        [    0.0527],\n",
      "        [    0.0599],\n",
      "        [    0.0610],\n",
      "        [    0.0640],\n",
      "        [    0.0486],\n",
      "        [    0.0476],\n",
      "        [    0.0679],\n",
      "        [    0.0585],\n",
      "        [    0.0493],\n",
      "        [    0.0681],\n",
      "        [    0.0691],\n",
      "        [    0.0710],\n",
      "        [    0.0719],\n",
      "        [    0.0543],\n",
      "        [    0.0643],\n",
      "        [    0.0705]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.421664476394653\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 105\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.2960867934452835e-09, 49)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [49, 62, 126, 90, 104, 75, 12, 105, 34, 57, 109, 26, 13, 3, 52, 14, 23, 76, 56, 21, 53, 74, 63, 0, 87, 27, 93, 98, 68, 89, 16, 15, 66, 91, 9, 61, 102, 30, 1, 22, 99, 18, 51, 32, 10, 86, 88, 100, 85, 96, 8, 97, 54, 33, 50, 58, 29, 17, 94, 46, 92, 25, 77, 133, 124, 28, 55, 103, 65, 95, 36, 20, 101, 31, 2, 19, 64, 106, 127, 107, 11, 125, 110, 112, 130, 48, 35, 123, 47, 132, 108, 24, 131, 111, 78, 128, 73, 7, 113, 6, 4, 59, 67, 60, 37] 數值 torch.Size([105, 1])\n",
      "目前模型的Data狀態 torch.Size([105, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8524],\n",
      "        [0.8837],\n",
      "        [0.6177],\n",
      "        [0.7041],\n",
      "        [0.6601],\n",
      "        [0.7448],\n",
      "        [0.9948],\n",
      "        [0.6830],\n",
      "        [0.8108],\n",
      "        [0.9243],\n",
      "        [0.6672],\n",
      "        [0.8261],\n",
      "        [0.9888],\n",
      "        [0.8392],\n",
      "        [0.8778],\n",
      "        [0.9741],\n",
      "        [0.8517],\n",
      "        [0.7520],\n",
      "        [0.8986],\n",
      "        [0.8454],\n",
      "        [0.8586],\n",
      "        [0.7713],\n",
      "        [0.8737],\n",
      "        [0.8162],\n",
      "        [0.7232],\n",
      "        [0.8160],\n",
      "        [0.7037],\n",
      "        [0.6637],\n",
      "        [0.8303],\n",
      "        [0.6978],\n",
      "        [0.9298],\n",
      "        [0.9533],\n",
      "        [0.8499],\n",
      "        [0.6770],\n",
      "        [0.9903],\n",
      "        [0.9055],\n",
      "        [0.6259],\n",
      "        [0.8663],\n",
      "        [0.8305],\n",
      "        [0.8372],\n",
      "        [0.6421],\n",
      "        [0.8956],\n",
      "        [0.8790],\n",
      "        [0.7905],\n",
      "        [1.0130],\n",
      "        [0.6962],\n",
      "        [0.7133],\n",
      "        [0.6144],\n",
      "        [0.6825],\n",
      "        [0.6729],\n",
      "        [0.9542],\n",
      "        [0.6611],\n",
      "        [0.9153],\n",
      "        [0.7973],\n",
      "        [0.9126],\n",
      "        [0.9119],\n",
      "        [0.8533],\n",
      "        [0.9135],\n",
      "        [0.6832],\n",
      "        [0.8691],\n",
      "        [0.6794],\n",
      "        [0.8336],\n",
      "        [0.7490],\n",
      "        [0.6991],\n",
      "        [0.6126],\n",
      "        [0.8217],\n",
      "        [0.8820],\n",
      "        [0.6298],\n",
      "        [0.8056],\n",
      "        [0.6749],\n",
      "        [0.8115],\n",
      "        [0.8840],\n",
      "        [0.6154],\n",
      "        [0.8365],\n",
      "        [0.8746],\n",
      "        [0.9022],\n",
      "        [0.8389],\n",
      "        [0.6591],\n",
      "        [0.6565],\n",
      "        [0.6516],\n",
      "        [1.0207],\n",
      "        [0.5810],\n",
      "        [0.6866],\n",
      "        [0.6814],\n",
      "        [0.6283],\n",
      "        [0.8942],\n",
      "        [0.7914],\n",
      "        [0.6591],\n",
      "        [0.8789],\n",
      "        [0.7260],\n",
      "        [0.6386],\n",
      "        [0.8456],\n",
      "        [0.6849],\n",
      "        [0.7039],\n",
      "        [0.7229],\n",
      "        [0.6708],\n",
      "        [0.8065],\n",
      "        [0.9354],\n",
      "        [0.7003],\n",
      "        [0.9068],\n",
      "        [0.8609],\n",
      "        [0.8903],\n",
      "        [0.8642],\n",
      "        [0.8843],\n",
      "        [0.7916]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0011],\n",
      "        [    0.0015],\n",
      "        [    0.0024],\n",
      "        [    0.0035],\n",
      "        [    0.0036],\n",
      "        [    0.0044],\n",
      "        [    0.0048],\n",
      "        [    0.0054],\n",
      "        [    0.0054],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0062],\n",
      "        [    0.0066],\n",
      "        [    0.0073],\n",
      "        [    0.0080],\n",
      "        [    0.0088],\n",
      "        [    0.0107],\n",
      "        [    0.0121],\n",
      "        [    0.0122],\n",
      "        [    0.0122],\n",
      "        [    0.0124],\n",
      "        [    0.0124],\n",
      "        [    0.0135],\n",
      "        [    0.0137],\n",
      "        [    0.0139],\n",
      "        [    0.0140],\n",
      "        [    0.0140],\n",
      "        [    0.0144],\n",
      "        [    0.0144],\n",
      "        [    0.0148],\n",
      "        [    0.0148],\n",
      "        [    0.0153],\n",
      "        [    0.0167],\n",
      "        [    0.0169],\n",
      "        [    0.0176],\n",
      "        [    0.0176],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0189],\n",
      "        [    0.0202],\n",
      "        [    0.0206],\n",
      "        [    0.0209],\n",
      "        [    0.0210],\n",
      "        [    0.0214],\n",
      "        [    0.0221],\n",
      "        [    0.0225],\n",
      "        [    0.0225],\n",
      "        [    0.0227],\n",
      "        [    0.0227],\n",
      "        [    0.0234],\n",
      "        [    0.0236],\n",
      "        [    0.0237],\n",
      "        [    0.0245],\n",
      "        [    0.0246],\n",
      "        [    0.0258],\n",
      "        [    0.0262],\n",
      "        [    0.0270],\n",
      "        [    0.0272],\n",
      "        [    0.0274],\n",
      "        [    0.0275],\n",
      "        [    0.0275],\n",
      "        [    0.0276],\n",
      "        [    0.0294],\n",
      "        [    0.0300],\n",
      "        [    0.0301],\n",
      "        [    0.0310],\n",
      "        [    0.0319],\n",
      "        [    0.0321],\n",
      "        [    0.0322],\n",
      "        [    0.0325],\n",
      "        [    0.0344],\n",
      "        [    0.0354],\n",
      "        [    0.0375],\n",
      "        [    0.0377],\n",
      "        [    0.0391],\n",
      "        [    0.0417],\n",
      "        [    0.0428],\n",
      "        [    0.0450],\n",
      "        [    0.0450],\n",
      "        [    0.0476],\n",
      "        [    0.0479],\n",
      "        [    0.0486],\n",
      "        [    0.0486],\n",
      "        [    0.0492],\n",
      "        [    0.0493],\n",
      "        [    0.0506],\n",
      "        [    0.0527],\n",
      "        [    0.0543],\n",
      "        [    0.0585],\n",
      "        [    0.0599],\n",
      "        [    0.0600],\n",
      "        [    0.0610],\n",
      "        [    0.0640],\n",
      "        [    0.0643],\n",
      "        [    0.0679],\n",
      "        [    0.0681],\n",
      "        [    0.0691],\n",
      "        [    0.0705],\n",
      "        [    0.0710],\n",
      "        [    0.0719]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0012],\n",
      "        [0.0088],\n",
      "        [0.0017],\n",
      "        [0.0007],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0033],\n",
      "        [0.0032],\n",
      "        [0.0050],\n",
      "        [0.0073],\n",
      "        [0.0063],\n",
      "        [0.0051],\n",
      "        [0.0044],\n",
      "        [0.0060],\n",
      "        [0.0071],\n",
      "        [0.0053],\n",
      "        [0.0039],\n",
      "        [0.0069],\n",
      "        [0.0090],\n",
      "        [0.0079],\n",
      "        [0.0144],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0106],\n",
      "        [0.0114],\n",
      "        [0.0110],\n",
      "        [0.0111],\n",
      "        [0.0146],\n",
      "        [0.0129],\n",
      "        [0.0146],\n",
      "        [0.0144],\n",
      "        [0.0149],\n",
      "        [0.0133],\n",
      "        [0.0156],\n",
      "        [0.0141],\n",
      "        [0.0146],\n",
      "        [0.0167],\n",
      "        [0.0180],\n",
      "        [0.0186],\n",
      "        [0.0157],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0191],\n",
      "        [0.0212],\n",
      "        [0.0190],\n",
      "        [0.0196],\n",
      "        [0.0193],\n",
      "        [0.0223],\n",
      "        [0.0186],\n",
      "        [0.0225],\n",
      "        [0.0195],\n",
      "        [0.0239],\n",
      "        [0.0227],\n",
      "        [0.0225],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0240],\n",
      "        [0.0234],\n",
      "        [0.0260],\n",
      "        [0.0247],\n",
      "        [0.0259],\n",
      "        [0.0306],\n",
      "        [0.0155],\n",
      "        [0.0184],\n",
      "        [0.0283],\n",
      "        [0.0268],\n",
      "        [0.0294],\n",
      "        [0.0299],\n",
      "        [0.0283],\n",
      "        [0.0301],\n",
      "        [0.0331],\n",
      "        [0.0305],\n",
      "        [0.0320],\n",
      "        [0.0338],\n",
      "        [0.0354],\n",
      "        [0.0344],\n",
      "        [0.0381],\n",
      "        [0.0278],\n",
      "        [0.0402],\n",
      "        [0.0430],\n",
      "        [0.0516],\n",
      "        [0.0419],\n",
      "        [0.0411],\n",
      "        [0.0368],\n",
      "        [0.0475],\n",
      "        [0.0483],\n",
      "        [0.0390],\n",
      "        [0.0490],\n",
      "        [0.0378],\n",
      "        [0.0521],\n",
      "        [0.0521],\n",
      "        [0.0433],\n",
      "        [0.0551],\n",
      "        [0.0633],\n",
      "        [0.0492],\n",
      "        [0.0644],\n",
      "        [0.0643],\n",
      "        [0.0605],\n",
      "        [0.0675],\n",
      "        [0.0670],\n",
      "        [0.0688],\n",
      "        [0.0709],\n",
      "        [0.0705],\n",
      "        [0.0713]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.699779748916626\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 106\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.768176040670369e-07, 104)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [104, 49, 62, 90, 75, 12, 34, 105, 76, 3, 57, 13, 23, 52, 26, 56, 14, 109, 53, 126, 21, 87, 93, 98, 27, 89, 63, 0, 91, 61, 15, 74, 68, 16, 102, 66, 133, 9, 99, 30, 1, 124, 96, 22, 18, 86, 32, 100, 97, 88, 51, 10, 85, 50, 8, 33, 58, 29, 94, 54, 17, 92, 25, 46, 55, 127, 95, 28, 103, 65, 36, 101, 77, 31, 20, 2, 64, 19, 130, 132, 106, 123, 107, 112, 110, 11, 131, 48, 35, 47, 128, 125, 108, 24, 111, 113, 78, 7, 73, 4, 6, 59, 60, 67, 37, 137] 數值 torch.Size([106, 1])\n",
      "目前模型的Data狀態 torch.Size([106, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6597],\n",
      "        [0.8515],\n",
      "        [0.8845],\n",
      "        [0.7051],\n",
      "        [0.7481],\n",
      "        [0.9950],\n",
      "        [0.8112],\n",
      "        [0.6827],\n",
      "        [0.7547],\n",
      "        [0.8403],\n",
      "        [0.9250],\n",
      "        [0.9885],\n",
      "        [0.8507],\n",
      "        [0.8775],\n",
      "        [0.8252],\n",
      "        [0.8990],\n",
      "        [0.9729],\n",
      "        [0.6647],\n",
      "        [0.8595],\n",
      "        [0.6082],\n",
      "        [0.8443],\n",
      "        [0.7249],\n",
      "        [0.7050],\n",
      "        [0.6661],\n",
      "        [0.8151],\n",
      "        [0.6988],\n",
      "        [0.8750],\n",
      "        [0.8173],\n",
      "        [0.6781],\n",
      "        [0.9062],\n",
      "        [0.9530],\n",
      "        [0.7750],\n",
      "        [0.8312],\n",
      "        [0.9292],\n",
      "        [0.6266],\n",
      "        [0.8504],\n",
      "        [0.6872],\n",
      "        [0.9911],\n",
      "        [0.6441],\n",
      "        [0.8663],\n",
      "        [0.8315],\n",
      "        [0.6036],\n",
      "        [0.6764],\n",
      "        [0.8361],\n",
      "        [0.8952],\n",
      "        [0.6978],\n",
      "        [0.7903],\n",
      "        [0.6161],\n",
      "        [0.6640],\n",
      "        [0.7146],\n",
      "        [0.8782],\n",
      "        [1.0140],\n",
      "        [0.6834],\n",
      "        [0.9117],\n",
      "        [0.9542],\n",
      "        [0.7973],\n",
      "        [0.9124],\n",
      "        [0.8526],\n",
      "        [0.6844],\n",
      "        [0.9164],\n",
      "        [0.9130],\n",
      "        [0.6810],\n",
      "        [0.8325],\n",
      "        [0.8693],\n",
      "        [0.8828],\n",
      "        [0.6465],\n",
      "        [0.6767],\n",
      "        [0.8208],\n",
      "        [0.6298],\n",
      "        [0.8058],\n",
      "        [0.8124],\n",
      "        [0.6169],\n",
      "        [0.7524],\n",
      "        [0.8363],\n",
      "        [0.8828],\n",
      "        [0.8760],\n",
      "        [0.8400],\n",
      "        [0.9013],\n",
      "        [0.6175],\n",
      "        [0.7145],\n",
      "        [0.6584],\n",
      "        [0.6495],\n",
      "        [0.6505],\n",
      "        [0.6774],\n",
      "        [0.6834],\n",
      "        [1.0221],\n",
      "        [0.6738],\n",
      "        [0.8937],\n",
      "        [0.7917],\n",
      "        [0.8787],\n",
      "        [0.6600],\n",
      "        [0.5722],\n",
      "        [0.6372],\n",
      "        [0.8450],\n",
      "        [0.7005],\n",
      "        [0.6965],\n",
      "        [0.7264],\n",
      "        [0.9351],\n",
      "        [0.8099],\n",
      "        [0.8620],\n",
      "        [0.9072],\n",
      "        [0.8906],\n",
      "        [0.8847],\n",
      "        [0.8645],\n",
      "        [0.7922],\n",
      "        [0.7029]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0039],\n",
      "        [0.0044],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0060],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0106],\n",
      "        [0.0110],\n",
      "        [0.0111],\n",
      "        [0.0114],\n",
      "        [0.0129],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0146],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.0156],\n",
      "        [0.0157],\n",
      "        [0.0167],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0212],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0234],\n",
      "        [0.0239],\n",
      "        [0.0240],\n",
      "        [0.0247],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0268],\n",
      "        [0.0278],\n",
      "        [0.0283],\n",
      "        [0.0283],\n",
      "        [0.0294],\n",
      "        [0.0299],\n",
      "        [0.0301],\n",
      "        [0.0305],\n",
      "        [0.0306],\n",
      "        [0.0320],\n",
      "        [0.0331],\n",
      "        [0.0338],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0368],\n",
      "        [0.0378],\n",
      "        [0.0381],\n",
      "        [0.0390],\n",
      "        [0.0402],\n",
      "        [0.0411],\n",
      "        [0.0419],\n",
      "        [0.0430],\n",
      "        [0.0433],\n",
      "        [0.0475],\n",
      "        [0.0483],\n",
      "        [0.0490],\n",
      "        [0.0492],\n",
      "        [0.0516],\n",
      "        [0.0521],\n",
      "        [0.0521],\n",
      "        [0.0551],\n",
      "        [0.0605],\n",
      "        [0.0633],\n",
      "        [0.0643],\n",
      "        [0.0644],\n",
      "        [0.0670],\n",
      "        [0.0675],\n",
      "        [0.0688],\n",
      "        [0.0705],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0742]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0025],\n",
      "        [0.0005],\n",
      "        [0.0040],\n",
      "        [0.0047],\n",
      "        [0.0067],\n",
      "        [0.0002],\n",
      "        [0.0007],\n",
      "        [0.0052],\n",
      "        [0.0004],\n",
      "        [0.0011],\n",
      "        [0.0078],\n",
      "        [0.0070],\n",
      "        [0.0066],\n",
      "        [0.0040],\n",
      "        [0.0048],\n",
      "        [0.0044],\n",
      "        [0.0061],\n",
      "        [0.0073],\n",
      "        [0.0049],\n",
      "        [0.0150],\n",
      "        [0.0077],\n",
      "        [0.0072],\n",
      "        [0.0078],\n",
      "        [0.0068],\n",
      "        [0.0129],\n",
      "        [0.0101],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0103],\n",
      "        [0.0113],\n",
      "        [0.0125],\n",
      "        [0.0197],\n",
      "        [0.0174],\n",
      "        [0.0129],\n",
      "        [0.0118],\n",
      "        [0.0175],\n",
      "        [0.0071],\n",
      "        [0.0185],\n",
      "        [0.0118],\n",
      "        [0.0190],\n",
      "        [0.0213],\n",
      "        [0.0128],\n",
      "        [0.0135],\n",
      "        [0.0173],\n",
      "        [0.0169],\n",
      "        [0.0156],\n",
      "        [0.0172],\n",
      "        [0.0156],\n",
      "        [0.0148],\n",
      "        [0.0165],\n",
      "        [0.0181],\n",
      "        [0.0243],\n",
      "        [0.0252],\n",
      "        [0.0240],\n",
      "        [0.0202],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0248],\n",
      "        [0.0203],\n",
      "        [0.0271],\n",
      "        [0.0258],\n",
      "        [0.0214],\n",
      "        [0.0273],\n",
      "        [0.0284],\n",
      "        [0.0239],\n",
      "        [0.0211],\n",
      "        [0.0247],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.0277],\n",
      "        [0.0272],\n",
      "        [0.0270],\n",
      "        [0.0354],\n",
      "        [0.0340],\n",
      "        [0.0319],\n",
      "        [0.0374],\n",
      "        [0.0313],\n",
      "        [0.0340],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0365],\n",
      "        [0.0329],\n",
      "        [0.0390],\n",
      "        [0.0398],\n",
      "        [0.0413],\n",
      "        [0.0464],\n",
      "        [0.0357],\n",
      "        [0.0492],\n",
      "        [0.0460],\n",
      "        [0.0510],\n",
      "        [0.0418],\n",
      "        [0.0571],\n",
      "        [0.0511],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0593],\n",
      "        [0.0683],\n",
      "        [0.0623],\n",
      "        [0.0692],\n",
      "        [0.0636],\n",
      "        [0.0648],\n",
      "        [0.0664],\n",
      "        [0.0680],\n",
      "        [0.0733],\n",
      "        [0.0687],\n",
      "        [0.0664]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.977336406707764\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 107\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.675028554222081e-08, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [12, 76, 49, 34, 3, 104, 52, 62, 56, 90, 26, 53, 105, 14, 23, 75, 98, 13, 133, 87, 109, 21, 57, 93, 89, 91, 61, 99, 102, 15, 124, 27, 16, 96, 97, 126, 100, 86, 88, 63, 0, 18, 32, 22, 68, 66, 51, 9, 30, 74, 8, 94, 58, 33, 127, 1, 92, 55, 50, 10, 95, 29, 85, 17, 28, 101, 54, 103, 36, 25, 65, 46, 130, 132, 64, 20, 123, 19, 31, 77, 131, 106, 2, 107, 112, 110, 128, 35, 11, 48, 47, 108, 24, 111, 125, 113, 7, 4, 6, 59, 137, 60, 78, 37, 73, 134, 44] 數值 torch.Size([107, 1])\n",
      "目前模型的Data狀態 torch.Size([107, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9974],\n",
      "        [0.7590],\n",
      "        [0.8530],\n",
      "        [0.8137],\n",
      "        [0.8436],\n",
      "        [0.6615],\n",
      "        [0.8795],\n",
      "        [0.8874],\n",
      "        [0.9015],\n",
      "        [0.7080],\n",
      "        [0.8267],\n",
      "        [0.8625],\n",
      "        [0.6847],\n",
      "        [0.9739],\n",
      "        [0.8521],\n",
      "        [0.7530],\n",
      "        [0.6703],\n",
      "        [0.9904],\n",
      "        [0.6788],\n",
      "        [0.7283],\n",
      "        [0.6647],\n",
      "        [0.8457],\n",
      "        [0.9277],\n",
      "        [0.7083],\n",
      "        [0.7017],\n",
      "        [0.6811],\n",
      "        [0.9090],\n",
      "        [0.6480],\n",
      "        [0.6294],\n",
      "        [0.9548],\n",
      "        [0.5979],\n",
      "        [0.8165],\n",
      "        [0.9309],\n",
      "        [0.6815],\n",
      "        [0.6687],\n",
      "        [0.6021],\n",
      "        [0.6197],\n",
      "        [0.7011],\n",
      "        [0.7178],\n",
      "        [0.8782],\n",
      "        [0.8206],\n",
      "        [0.8972],\n",
      "        [0.7923],\n",
      "        [0.8374],\n",
      "        [0.8340],\n",
      "        [0.8530],\n",
      "        [0.8797],\n",
      "        [0.9940],\n",
      "        [0.8686],\n",
      "        [0.7803],\n",
      "        [0.9565],\n",
      "        [0.6875],\n",
      "        [0.9150],\n",
      "        [0.7995],\n",
      "        [0.6398],\n",
      "        [0.8348],\n",
      "        [0.6843],\n",
      "        [0.8857],\n",
      "        [0.9132],\n",
      "        [1.0171],\n",
      "        [0.6803],\n",
      "        [0.8543],\n",
      "        [0.6862],\n",
      "        [0.9148],\n",
      "        [0.8223],\n",
      "        [0.6205],\n",
      "        [0.9197],\n",
      "        [0.6320],\n",
      "        [0.8153],\n",
      "        [0.8338],\n",
      "        [0.8080],\n",
      "        [0.8716],\n",
      "        [0.6102],\n",
      "        [0.7065],\n",
      "        [0.8430],\n",
      "        [0.8840],\n",
      "        [0.6434],\n",
      "        [0.9027],\n",
      "        [0.8383],\n",
      "        [0.7572],\n",
      "        [0.6662],\n",
      "        [0.6601],\n",
      "        [0.8795],\n",
      "        [0.6518],\n",
      "        [0.6762],\n",
      "        [0.6829],\n",
      "        [0.6525],\n",
      "        [0.7940],\n",
      "        [1.0254],\n",
      "        [0.8955],\n",
      "        [0.8807],\n",
      "        [0.6381],\n",
      "        [0.8467],\n",
      "        [0.6996],\n",
      "        [0.5667],\n",
      "        [0.6953],\n",
      "        [0.9371],\n",
      "        [0.8654],\n",
      "        [0.9099],\n",
      "        [0.8930],\n",
      "        [0.6950],\n",
      "        [0.8872],\n",
      "        [0.7313],\n",
      "        [0.7948],\n",
      "        [0.8147],\n",
      "        [0.7153],\n",
      "        [0.8286]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0025],\n",
      "        [0.0040],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0047],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0067],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0078],\n",
      "        [0.0101],\n",
      "        [0.0103],\n",
      "        [0.0113],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0148],\n",
      "        [0.0150],\n",
      "        [0.0156],\n",
      "        [0.0156],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0181],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0239],\n",
      "        [0.0240],\n",
      "        [0.0243],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0258],\n",
      "        [0.0269],\n",
      "        [0.0270],\n",
      "        [0.0271],\n",
      "        [0.0272],\n",
      "        [0.0272],\n",
      "        [0.0273],\n",
      "        [0.0277],\n",
      "        [0.0284],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0313],\n",
      "        [0.0319],\n",
      "        [0.0329],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0365],\n",
      "        [0.0374],\n",
      "        [0.0390],\n",
      "        [0.0398],\n",
      "        [0.0413],\n",
      "        [0.0418],\n",
      "        [0.0460],\n",
      "        [0.0464],\n",
      "        [0.0492],\n",
      "        [0.0510],\n",
      "        [0.0511],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0571],\n",
      "        [0.0593],\n",
      "        [0.0623],\n",
      "        [0.0636],\n",
      "        [0.0648],\n",
      "        [0.0664],\n",
      "        [0.0664],\n",
      "        [0.0680],\n",
      "        [0.0683],\n",
      "        [0.0687],\n",
      "        [0.0692],\n",
      "        [0.0723],\n",
      "        [0.0732]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0012],\n",
      "        [    0.0007],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0014],\n",
      "        [    0.0036],\n",
      "        [    0.0050],\n",
      "        [    0.0034],\n",
      "        [    0.0039],\n",
      "        [    0.0050],\n",
      "        [    0.0035],\n",
      "        [    0.0041],\n",
      "        [    0.0073],\n",
      "        [    0.0062],\n",
      "        [    0.0085],\n",
      "        [    0.0059],\n",
      "        [    0.0065],\n",
      "        [    0.0033],\n",
      "        [    0.0078],\n",
      "        [    0.0102],\n",
      "        [    0.0081],\n",
      "        [    0.0087],\n",
      "        [    0.0079],\n",
      "        [    0.0110],\n",
      "        [    0.0108],\n",
      "        [    0.0105],\n",
      "        [    0.0111],\n",
      "        [    0.0120],\n",
      "        [    0.0128],\n",
      "        [    0.0049],\n",
      "        [    0.0126],\n",
      "        [    0.0132],\n",
      "        [    0.0117],\n",
      "        [    0.0134],\n",
      "        [    0.0232],\n",
      "        [    0.0150],\n",
      "        [    0.0163],\n",
      "        [    0.0172],\n",
      "        [    0.0179],\n",
      "        [    0.0174],\n",
      "        [    0.0169],\n",
      "        [    0.0169],\n",
      "        [    0.0178],\n",
      "        [    0.0178],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0192],\n",
      "        [    0.0219],\n",
      "        [    0.0211],\n",
      "        [    0.0203],\n",
      "        [    0.0197],\n",
      "        [    0.0201],\n",
      "        [    0.0125],\n",
      "        [    0.0220],\n",
      "        [    0.0215],\n",
      "        [    0.0226],\n",
      "        [    0.0238],\n",
      "        [    0.0243],\n",
      "        [    0.0242],\n",
      "        [    0.0244],\n",
      "        [    0.0242],\n",
      "        [    0.0257],\n",
      "        [    0.0273],\n",
      "        [    0.0264],\n",
      "        [    0.0286],\n",
      "        [    0.0279],\n",
      "        [    0.0261],\n",
      "        [    0.0268],\n",
      "        [    0.0271],\n",
      "        [    0.0296],\n",
      "        [    0.0203],\n",
      "        [    0.0195],\n",
      "        [    0.0299],\n",
      "        [    0.0326],\n",
      "        [    0.0246],\n",
      "        [    0.0345],\n",
      "        [    0.0342],\n",
      "        [    0.0369],\n",
      "        [    0.0260],\n",
      "        [    0.0379],\n",
      "        [    0.0383],\n",
      "        [    0.0407],\n",
      "        [    0.0359],\n",
      "        [    0.0380],\n",
      "        [    0.0324],\n",
      "        [    0.0453],\n",
      "        [    0.0467],\n",
      "        [    0.0497],\n",
      "        [    0.0518],\n",
      "        [    0.0531],\n",
      "        [    0.0537],\n",
      "        [    0.0507],\n",
      "        [    0.0647],\n",
      "        [    0.0555],\n",
      "        [    0.0633],\n",
      "        [    0.0629],\n",
      "        [    0.0652],\n",
      "        [    0.0659],\n",
      "        [    0.0567],\n",
      "        [    0.0675],\n",
      "        [    0.0701],\n",
      "        [    0.0678],\n",
      "        [    0.0712],\n",
      "        [    0.0618],\n",
      "        [    0.0710]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.2565598487854\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 108\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.755836613412612e-09, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 12, 3, 49, 76, 104, 133, 56, 53, 52, 90, 105, 124, 26, 62, 98, 23, 13, 14, 87, 93, 21, 75, 57, 109, 61, 91, 89, 99, 96, 102, 127, 27, 15, 16, 97, 100, 86, 18, 32, 88, 0, 22, 68, 63, 66, 51, 9, 30, 132, 58, 33, 130, 94, 8, 92, 74, 1, 55, 126, 50, 95, 85, 10, 29, 123, 17, 131, 36, 101, 25, 65, 28, 103, 54, 46, 64, 128, 20, 31, 19, 112, 77, 106, 110, 2, 107, 35, 11, 48, 111, 47, 108, 24, 113, 137, 134, 4, 7, 125, 6, 59, 135, 60, 37, 78, 44, 73] 數值 torch.Size([108, 1])\n",
      "目前模型的Data狀態 torch.Size([108, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8144],\n",
      "        [0.9971],\n",
      "        [0.8443],\n",
      "        [0.8531],\n",
      "        [0.7598],\n",
      "        [0.6604],\n",
      "        [0.6684],\n",
      "        [0.9025],\n",
      "        [0.8639],\n",
      "        [0.8799],\n",
      "        [0.7073],\n",
      "        [0.6836],\n",
      "        [0.5901],\n",
      "        [0.8265],\n",
      "        [0.8883],\n",
      "        [0.6713],\n",
      "        [0.8516],\n",
      "        [0.9898],\n",
      "        [0.9727],\n",
      "        [0.7277],\n",
      "        [0.7082],\n",
      "        [0.8453],\n",
      "        [0.7548],\n",
      "        [0.9287],\n",
      "        [0.6619],\n",
      "        [0.9098],\n",
      "        [0.6806],\n",
      "        [0.7007],\n",
      "        [0.6487],\n",
      "        [0.6833],\n",
      "        [0.6292],\n",
      "        [0.6312],\n",
      "        [0.8162],\n",
      "        [0.9545],\n",
      "        [0.9306],\n",
      "        [0.6701],\n",
      "        [0.6203],\n",
      "        [0.7005],\n",
      "        [0.8972],\n",
      "        [0.7925],\n",
      "        [0.7170],\n",
      "        [0.8213],\n",
      "        [0.8370],\n",
      "        [0.8345],\n",
      "        [0.8796],\n",
      "        [0.8534],\n",
      "        [0.8797],\n",
      "        [0.9938],\n",
      "        [0.8687],\n",
      "        [0.6962],\n",
      "        [0.9158],\n",
      "        [0.7999],\n",
      "        [0.6010],\n",
      "        [0.6875],\n",
      "        [0.9556],\n",
      "        [0.6842],\n",
      "        [0.7825],\n",
      "        [0.8356],\n",
      "        [0.8870],\n",
      "        [0.5939],\n",
      "        [0.9130],\n",
      "        [0.6808],\n",
      "        [0.6853],\n",
      "        [1.0170],\n",
      "        [0.8540],\n",
      "        [0.6351],\n",
      "        [0.9147],\n",
      "        [0.6565],\n",
      "        [0.8165],\n",
      "        [0.6210],\n",
      "        [0.8333],\n",
      "        [0.8085],\n",
      "        [0.8219],\n",
      "        [0.6313],\n",
      "        [0.9212],\n",
      "        [0.8729],\n",
      "        [0.8444],\n",
      "        [0.6431],\n",
      "        [0.8833],\n",
      "        [0.8385],\n",
      "        [0.9021],\n",
      "        [0.6723],\n",
      "        [0.7587],\n",
      "        [0.6587],\n",
      "        [0.6796],\n",
      "        [0.8804],\n",
      "        [0.6500],\n",
      "        [0.7947],\n",
      "        [1.0258],\n",
      "        [0.8960],\n",
      "        [0.6960],\n",
      "        [0.8815],\n",
      "        [0.6362],\n",
      "        [0.8466],\n",
      "        [0.6915],\n",
      "        [0.6854],\n",
      "        [0.7048],\n",
      "        [0.8661],\n",
      "        [0.9361],\n",
      "        [0.5592],\n",
      "        [0.9095],\n",
      "        [0.8935],\n",
      "        [0.6885],\n",
      "        [0.8877],\n",
      "        [0.7957],\n",
      "        [0.7331],\n",
      "        [0.8309],\n",
      "        [0.8167]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0012],\n",
      "        [    0.0014],\n",
      "        [    0.0033],\n",
      "        [    0.0034],\n",
      "        [    0.0035],\n",
      "        [    0.0036],\n",
      "        [    0.0039],\n",
      "        [    0.0041],\n",
      "        [    0.0049],\n",
      "        [    0.0050],\n",
      "        [    0.0050],\n",
      "        [    0.0059],\n",
      "        [    0.0062],\n",
      "        [    0.0065],\n",
      "        [    0.0073],\n",
      "        [    0.0078],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0102],\n",
      "        [    0.0105],\n",
      "        [    0.0108],\n",
      "        [    0.0110],\n",
      "        [    0.0111],\n",
      "        [    0.0117],\n",
      "        [    0.0120],\n",
      "        [    0.0125],\n",
      "        [    0.0126],\n",
      "        [    0.0128],\n",
      "        [    0.0132],\n",
      "        [    0.0134],\n",
      "        [    0.0150],\n",
      "        [    0.0163],\n",
      "        [    0.0169],\n",
      "        [    0.0169],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0178],\n",
      "        [    0.0178],\n",
      "        [    0.0179],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0192],\n",
      "        [    0.0195],\n",
      "        [    0.0197],\n",
      "        [    0.0201],\n",
      "        [    0.0203],\n",
      "        [    0.0203],\n",
      "        [    0.0211],\n",
      "        [    0.0215],\n",
      "        [    0.0219],\n",
      "        [    0.0220],\n",
      "        [    0.0226],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0242],\n",
      "        [    0.0242],\n",
      "        [    0.0243],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0257],\n",
      "        [    0.0260],\n",
      "        [    0.0261],\n",
      "        [    0.0264],\n",
      "        [    0.0268],\n",
      "        [    0.0271],\n",
      "        [    0.0273],\n",
      "        [    0.0279],\n",
      "        [    0.0286],\n",
      "        [    0.0296],\n",
      "        [    0.0299],\n",
      "        [    0.0324],\n",
      "        [    0.0326],\n",
      "        [    0.0342],\n",
      "        [    0.0345],\n",
      "        [    0.0359],\n",
      "        [    0.0369],\n",
      "        [    0.0379],\n",
      "        [    0.0380],\n",
      "        [    0.0383],\n",
      "        [    0.0407],\n",
      "        [    0.0453],\n",
      "        [    0.0467],\n",
      "        [    0.0497],\n",
      "        [    0.0507],\n",
      "        [    0.0518],\n",
      "        [    0.0531],\n",
      "        [    0.0537],\n",
      "        [    0.0555],\n",
      "        [    0.0567],\n",
      "        [    0.0618],\n",
      "        [    0.0629],\n",
      "        [    0.0633],\n",
      "        [    0.0647],\n",
      "        [    0.0652],\n",
      "        [    0.0659],\n",
      "        [    0.0669],\n",
      "        [    0.0675],\n",
      "        [    0.0678],\n",
      "        [    0.0701],\n",
      "        [    0.0710],\n",
      "        [    0.0712]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0011],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0023],\n",
      "        [0.0014],\n",
      "        [0.0113],\n",
      "        [0.0029],\n",
      "        [0.0025],\n",
      "        [0.0033],\n",
      "        [0.0042],\n",
      "        [0.0041],\n",
      "        [0.0003],\n",
      "        [0.0052],\n",
      "        [0.0055],\n",
      "        [0.0041],\n",
      "        [0.0057],\n",
      "        [0.0053],\n",
      "        [0.0091],\n",
      "        [0.0074],\n",
      "        [0.0072],\n",
      "        [0.0085],\n",
      "        [0.0103],\n",
      "        [0.0091],\n",
      "        [0.0114],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0109],\n",
      "        [0.0094],\n",
      "        [0.0092],\n",
      "        [0.0111],\n",
      "        [0.0062],\n",
      "        [0.0123],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0113],\n",
      "        [0.0134],\n",
      "        [0.0158],\n",
      "        [0.0172],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0181],\n",
      "        [0.0188],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0178],\n",
      "        [0.0192],\n",
      "        [0.0116],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0135],\n",
      "        [0.0195],\n",
      "        [0.0219],\n",
      "        [0.0206],\n",
      "        [0.0240],\n",
      "        [0.0228],\n",
      "        [0.0216],\n",
      "        [0.0289],\n",
      "        [0.0233],\n",
      "        [0.0229],\n",
      "        [0.0244],\n",
      "        [0.0238],\n",
      "        [0.0241],\n",
      "        [0.0188],\n",
      "        [0.0252],\n",
      "        [0.0188],\n",
      "        [0.0252],\n",
      "        [0.0249],\n",
      "        [0.0264],\n",
      "        [0.0268],\n",
      "        [0.0276],\n",
      "        [0.0274],\n",
      "        [0.0296],\n",
      "        [0.0301],\n",
      "        [0.0290],\n",
      "        [0.0254],\n",
      "        [0.0335],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0336],\n",
      "        [0.0386],\n",
      "        [0.0380],\n",
      "        [0.0363],\n",
      "        [0.0390],\n",
      "        [0.0410],\n",
      "        [0.0447],\n",
      "        [0.0465],\n",
      "        [0.0496],\n",
      "        [0.0486],\n",
      "        [0.0520],\n",
      "        [0.0535],\n",
      "        [0.0536],\n",
      "        [0.0532],\n",
      "        [0.0494],\n",
      "        [0.0537],\n",
      "        [0.0622],\n",
      "        [0.0643],\n",
      "        [0.0696],\n",
      "        [0.0656],\n",
      "        [0.0658],\n",
      "        [0.0589],\n",
      "        [0.0673],\n",
      "        [0.0670],\n",
      "        [0.0721],\n",
      "        [0.0693],\n",
      "        [0.0729]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.535849809646606\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 109\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.786119565229455e-08, 124)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [124, 3, 49, 34, 12, 104, 76, 53, 56, 52, 98, 105, 90, 26, 13, 62, 23, 127, 93, 87, 21, 14, 57, 96, 99, 91, 61, 75, 89, 102, 133, 97, 109, 132, 27, 100, 130, 15, 16, 86, 32, 88, 18, 9, 0, 66, 68, 22, 51, 131, 63, 123, 30, 58, 94, 33, 92, 55, 8, 1, 95, 50, 10, 74, 29, 85, 101, 17, 36, 128, 25, 65, 103, 28, 126, 64, 54, 46, 20, 112, 31, 19, 110, 106, 77, 2, 107, 35, 11, 111, 137, 48, 47, 113, 108, 24, 134, 135, 4, 7, 136, 6, 59, 37, 60, 138, 44, 125, 78] 數值 torch.Size([109, 1])\n",
      "目前模型的Data狀態 torch.Size([109, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5848],\n",
      "        [0.8450],\n",
      "        [0.8529],\n",
      "        [0.8150],\n",
      "        [0.9961],\n",
      "        [0.6604],\n",
      "        [0.7609],\n",
      "        [0.8649],\n",
      "        [0.9030],\n",
      "        [0.8801],\n",
      "        [0.6731],\n",
      "        [0.6836],\n",
      "        [0.7075],\n",
      "        [0.8263],\n",
      "        [0.9887],\n",
      "        [0.8888],\n",
      "        [0.8511],\n",
      "        [0.6249],\n",
      "        [0.7089],\n",
      "        [0.7281],\n",
      "        [0.8449],\n",
      "        [0.9710],\n",
      "        [0.9291],\n",
      "        [0.6858],\n",
      "        [0.6503],\n",
      "        [0.6812],\n",
      "        [0.9101],\n",
      "        [0.7566],\n",
      "        [0.7009],\n",
      "        [0.6301],\n",
      "        [0.6604],\n",
      "        [0.6723],\n",
      "        [0.6606],\n",
      "        [0.6884],\n",
      "        [0.8159],\n",
      "        [0.6219],\n",
      "        [0.5942],\n",
      "        [0.9537],\n",
      "        [0.9299],\n",
      "        [0.7010],\n",
      "        [0.7928],\n",
      "        [0.7173],\n",
      "        [0.8969],\n",
      "        [0.9933],\n",
      "        [0.8220],\n",
      "        [0.8536],\n",
      "        [0.8348],\n",
      "        [0.8365],\n",
      "        [0.8795],\n",
      "        [0.6493],\n",
      "        [0.8804],\n",
      "        [0.6293],\n",
      "        [0.8688],\n",
      "        [0.9160],\n",
      "        [0.6883],\n",
      "        [0.8003],\n",
      "        [0.6851],\n",
      "        [0.8879],\n",
      "        [0.9548],\n",
      "        [0.8363],\n",
      "        [0.6822],\n",
      "        [0.9125],\n",
      "        [1.0166],\n",
      "        [0.7846],\n",
      "        [0.8536],\n",
      "        [0.6854],\n",
      "        [0.6225],\n",
      "        [0.9141],\n",
      "        [0.8174],\n",
      "        [0.6361],\n",
      "        [0.8329],\n",
      "        [0.8088],\n",
      "        [0.6317],\n",
      "        [0.8216],\n",
      "        [0.5882],\n",
      "        [0.8453],\n",
      "        [0.9222],\n",
      "        [0.8734],\n",
      "        [0.8824],\n",
      "        [0.6700],\n",
      "        [0.8387],\n",
      "        [0.9013],\n",
      "        [0.6779],\n",
      "        [0.6586],\n",
      "        [0.7604],\n",
      "        [0.8812],\n",
      "        [0.6497],\n",
      "        [0.7953],\n",
      "        [1.0255],\n",
      "        [0.6940],\n",
      "        [0.6780],\n",
      "        [0.8958],\n",
      "        [0.8817],\n",
      "        [0.6892],\n",
      "        [0.6358],\n",
      "        [0.8465],\n",
      "        [0.6967],\n",
      "        [0.6804],\n",
      "        [0.8668],\n",
      "        [0.9351],\n",
      "        [0.6984],\n",
      "        [0.9092],\n",
      "        [0.8936],\n",
      "        [0.7965],\n",
      "        [0.8879],\n",
      "        [0.6760],\n",
      "        [0.8326],\n",
      "        [0.5542],\n",
      "        [0.7351]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0011],\n",
      "        [0.0014],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0033],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0072],\n",
      "        [0.0074],\n",
      "        [0.0085],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0113],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0116],\n",
      "        [0.0123],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0158],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0178],\n",
      "        [0.0181],\n",
      "        [0.0181],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0183],\n",
      "        [0.0188],\n",
      "        [0.0188],\n",
      "        [0.0188],\n",
      "        [0.0192],\n",
      "        [0.0195],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0206],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0233],\n",
      "        [0.0238],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0244],\n",
      "        [0.0249],\n",
      "        [0.0252],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0264],\n",
      "        [0.0268],\n",
      "        [0.0274],\n",
      "        [0.0276],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0296],\n",
      "        [0.0301],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0363],\n",
      "        [0.0380],\n",
      "        [0.0386],\n",
      "        [0.0390],\n",
      "        [0.0410],\n",
      "        [0.0447],\n",
      "        [0.0465],\n",
      "        [0.0486],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0520],\n",
      "        [0.0532],\n",
      "        [0.0535],\n",
      "        [0.0536],\n",
      "        [0.0537],\n",
      "        [0.0589],\n",
      "        [0.0622],\n",
      "        [0.0643],\n",
      "        [0.0651],\n",
      "        [0.0656],\n",
      "        [0.0658],\n",
      "        [0.0670],\n",
      "        [0.0673],\n",
      "        [0.0690],\n",
      "        [0.0693],\n",
      "        [0.0696],\n",
      "        [0.0721]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0050],\n",
      "        [    0.0033],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0005],\n",
      "        [    0.0034],\n",
      "        [    0.0065],\n",
      "        [    0.0010],\n",
      "        [    0.0001],\n",
      "        [    0.0008],\n",
      "        [    0.0001],\n",
      "        [    0.0061],\n",
      "        [    0.0069],\n",
      "        [    0.0035],\n",
      "        [    0.0068],\n",
      "        [    0.0086],\n",
      "        [    0.0072],\n",
      "        [    0.0004],\n",
      "        [    0.0040],\n",
      "        [    0.0044],\n",
      "        [    0.0069],\n",
      "        [    0.0082],\n",
      "        [    0.0120],\n",
      "        [    0.0041],\n",
      "        [    0.0056],\n",
      "        [    0.0073],\n",
      "        [    0.0073],\n",
      "        [    0.0156],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0190],\n",
      "        [    0.0067],\n",
      "        [    0.0114],\n",
      "        [    0.0042],\n",
      "        [    0.0139],\n",
      "        [    0.0098],\n",
      "        [    0.0070],\n",
      "        [    0.0119],\n",
      "        [    0.0122],\n",
      "        [    0.0127],\n",
      "        [    0.0146],\n",
      "        [    0.0141],\n",
      "        [    0.0153],\n",
      "        [    0.0197],\n",
      "        [    0.0212],\n",
      "        [    0.0210],\n",
      "        [    0.0210],\n",
      "        [    0.0167],\n",
      "        [    0.0162],\n",
      "        [    0.0118],\n",
      "        [    0.0224],\n",
      "        [    0.0138],\n",
      "        [    0.0211],\n",
      "        [    0.0168],\n",
      "        [    0.0162],\n",
      "        [    0.0174],\n",
      "        [    0.0174],\n",
      "        [    0.0182],\n",
      "        [    0.0205],\n",
      "        [    0.0260],\n",
      "        [    0.0190],\n",
      "        [    0.0253],\n",
      "        [    0.0258],\n",
      "        [    0.0296],\n",
      "        [    0.0256],\n",
      "        [    0.0272],\n",
      "        [    0.0214],\n",
      "        [    0.0270],\n",
      "        [    0.0223],\n",
      "        [    0.0188],\n",
      "        [    0.0279],\n",
      "        [    0.0239],\n",
      "        [    0.0250],\n",
      "        [    0.0263],\n",
      "        [    0.0342],\n",
      "        [    0.0254],\n",
      "        [    0.0331],\n",
      "        [    0.0329],\n",
      "        [    0.0323],\n",
      "        [    0.0325],\n",
      "        [    0.0364],\n",
      "        [    0.0341],\n",
      "        [    0.0359],\n",
      "        [    0.0364],\n",
      "        [    0.0434],\n",
      "        [    0.0422],\n",
      "        [    0.0398],\n",
      "        [    0.0422],\n",
      "        [    0.0487],\n",
      "        [    0.0479],\n",
      "        [    0.0421],\n",
      "        [    0.0519],\n",
      "        [    0.0545],\n",
      "        [    0.0521],\n",
      "        [    0.0524],\n",
      "        [    0.0555],\n",
      "        [    0.0459],\n",
      "        [    0.0509],\n",
      "        [    0.0591],\n",
      "        [    0.0629],\n",
      "        [    0.0577],\n",
      "        [    0.0636],\n",
      "        [    0.0632],\n",
      "        [    0.0643],\n",
      "        [    0.0646],\n",
      "        [    0.0621],\n",
      "        [    0.0655],\n",
      "        [    0.0742],\n",
      "        [    0.0772]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.81518840789795\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 110\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.21899348579791e-09, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 98, 127, 12, 52, 53, 49, 34, 3, 104, 26, 93, 96, 132, 87, 124, 99, 105, 76, 97, 13, 21, 90, 130, 23, 91, 61, 89, 14, 102, 62, 100, 109, 131, 15, 57, 16, 86, 123, 27, 88, 32, 18, 75, 51, 94, 22, 58, 92, 33, 55, 128, 95, 133, 9, 8, 66, 68, 30, 0, 101, 36, 63, 65, 103, 50, 64, 29, 10, 1, 28, 17, 85, 25, 74, 20, 112, 46, 54, 19, 126, 110, 106, 31, 107, 137, 35, 2, 77, 134, 111, 11, 135, 48, 113, 108, 47, 24, 136, 4, 138, 7, 59, 6, 37, 60, 44, 129, 125, 67] 數值 torch.Size([110, 1])\n",
      "目前模型的Data狀態 torch.Size([110, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9060],\n",
      "        [0.6773],\n",
      "        [0.6191],\n",
      "        [0.9978],\n",
      "        [0.8827],\n",
      "        [0.8684],\n",
      "        [0.8552],\n",
      "        [0.8175],\n",
      "        [0.8480],\n",
      "        [0.6624],\n",
      "        [0.8280],\n",
      "        [0.7120],\n",
      "        [0.6909],\n",
      "        [0.6809],\n",
      "        [0.7311],\n",
      "        [0.5802],\n",
      "        [0.6541],\n",
      "        [0.6855],\n",
      "        [0.7651],\n",
      "        [0.6769],\n",
      "        [0.9902],\n",
      "        [0.8464],\n",
      "        [0.7103],\n",
      "        [0.5877],\n",
      "        [0.8526],\n",
      "        [0.6842],\n",
      "        [0.9130],\n",
      "        [0.7036],\n",
      "        [0.9718],\n",
      "        [0.6329],\n",
      "        [0.8919],\n",
      "        [0.6255],\n",
      "        [0.6607],\n",
      "        [0.6424],\n",
      "        [0.9554],\n",
      "        [0.9320],\n",
      "        [0.9316],\n",
      "        [0.7040],\n",
      "        [0.6243],\n",
      "        [0.8175],\n",
      "        [0.7202],\n",
      "        [0.7949],\n",
      "        [0.8988],\n",
      "        [0.7619],\n",
      "        [0.8817],\n",
      "        [0.6915],\n",
      "        [0.8381],\n",
      "        [0.9187],\n",
      "        [0.6883],\n",
      "        [0.8026],\n",
      "        [0.8914],\n",
      "        [0.6296],\n",
      "        [0.6860],\n",
      "        [0.6527],\n",
      "        [0.9952],\n",
      "        [0.9562],\n",
      "        [0.8565],\n",
      "        [0.8377],\n",
      "        [0.8707],\n",
      "        [0.8252],\n",
      "        [0.6260],\n",
      "        [0.8203],\n",
      "        [0.8840],\n",
      "        [0.8117],\n",
      "        [0.6341],\n",
      "        [0.9145],\n",
      "        [0.8489],\n",
      "        [0.8551],\n",
      "        [1.0186],\n",
      "        [0.8395],\n",
      "        [0.8229],\n",
      "        [0.9159],\n",
      "        [0.6883],\n",
      "        [0.8344],\n",
      "        [0.7902],\n",
      "        [0.8836],\n",
      "        [0.6689],\n",
      "        [0.8762],\n",
      "        [0.9257],\n",
      "        [0.9025],\n",
      "        [0.5829],\n",
      "        [0.6775],\n",
      "        [0.6602],\n",
      "        [0.8407],\n",
      "        [0.6510],\n",
      "        [0.6707],\n",
      "        [0.7979],\n",
      "        [0.8843],\n",
      "        [0.7652],\n",
      "        [0.6888],\n",
      "        [0.6933],\n",
      "        [1.0278],\n",
      "        [0.6725],\n",
      "        [0.8981],\n",
      "        [0.6881],\n",
      "        [0.6368],\n",
      "        [0.8842],\n",
      "        [0.8483],\n",
      "        [0.6910],\n",
      "        [0.8698],\n",
      "        [0.6691],\n",
      "        [0.9365],\n",
      "        [0.8962],\n",
      "        [0.9112],\n",
      "        [0.7992],\n",
      "        [0.8906],\n",
      "        [0.8363],\n",
      "        [0.6303],\n",
      "        [0.5497],\n",
      "        [0.8694]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0008],\n",
      "        [    0.0010],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0033],\n",
      "        [    0.0034],\n",
      "        [    0.0035],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0042],\n",
      "        [    0.0044],\n",
      "        [    0.0050],\n",
      "        [    0.0056],\n",
      "        [    0.0061],\n",
      "        [    0.0065],\n",
      "        [    0.0067],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0069],\n",
      "        [    0.0070],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0073],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0086],\n",
      "        [    0.0098],\n",
      "        [    0.0114],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0120],\n",
      "        [    0.0122],\n",
      "        [    0.0127],\n",
      "        [    0.0138],\n",
      "        [    0.0139],\n",
      "        [    0.0141],\n",
      "        [    0.0146],\n",
      "        [    0.0153],\n",
      "        [    0.0156],\n",
      "        [    0.0162],\n",
      "        [    0.0162],\n",
      "        [    0.0167],\n",
      "        [    0.0168],\n",
      "        [    0.0174],\n",
      "        [    0.0174],\n",
      "        [    0.0182],\n",
      "        [    0.0188],\n",
      "        [    0.0190],\n",
      "        [    0.0190],\n",
      "        [    0.0197],\n",
      "        [    0.0205],\n",
      "        [    0.0210],\n",
      "        [    0.0210],\n",
      "        [    0.0211],\n",
      "        [    0.0212],\n",
      "        [    0.0214],\n",
      "        [    0.0223],\n",
      "        [    0.0224],\n",
      "        [    0.0239],\n",
      "        [    0.0250],\n",
      "        [    0.0253],\n",
      "        [    0.0254],\n",
      "        [    0.0256],\n",
      "        [    0.0258],\n",
      "        [    0.0260],\n",
      "        [    0.0263],\n",
      "        [    0.0270],\n",
      "        [    0.0272],\n",
      "        [    0.0279],\n",
      "        [    0.0296],\n",
      "        [    0.0323],\n",
      "        [    0.0325],\n",
      "        [    0.0329],\n",
      "        [    0.0331],\n",
      "        [    0.0341],\n",
      "        [    0.0342],\n",
      "        [    0.0359],\n",
      "        [    0.0364],\n",
      "        [    0.0364],\n",
      "        [    0.0398],\n",
      "        [    0.0421],\n",
      "        [    0.0422],\n",
      "        [    0.0422],\n",
      "        [    0.0434],\n",
      "        [    0.0459],\n",
      "        [    0.0479],\n",
      "        [    0.0487],\n",
      "        [    0.0509],\n",
      "        [    0.0519],\n",
      "        [    0.0521],\n",
      "        [    0.0524],\n",
      "        [    0.0545],\n",
      "        [    0.0555],\n",
      "        [    0.0577],\n",
      "        [    0.0591],\n",
      "        [    0.0621],\n",
      "        [    0.0629],\n",
      "        [    0.0632],\n",
      "        [    0.0636],\n",
      "        [    0.0643],\n",
      "        [    0.0646],\n",
      "        [    0.0655],\n",
      "        [    0.0665],\n",
      "        [    0.0742],\n",
      "        [    0.0758]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0033],\n",
      "        [    0.0052],\n",
      "        [    0.0017],\n",
      "        [    0.0006],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0044],\n",
      "        [    0.0048],\n",
      "        [    0.0021],\n",
      "        [    0.0002],\n",
      "        [    0.0033],\n",
      "        [    0.0021],\n",
      "        [    0.0090],\n",
      "        [    0.0028],\n",
      "        [    0.0072],\n",
      "        [    0.0092],\n",
      "        [    0.0031],\n",
      "        [    0.0045],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0008],\n",
      "        [    0.0057],\n",
      "        [    0.0049],\n",
      "        [    0.0076],\n",
      "        [    0.0061],\n",
      "        [    0.0112],\n",
      "        [    0.0064],\n",
      "        [    0.0085],\n",
      "        [    0.0071],\n",
      "        [    0.0116],\n",
      "        [    0.0051],\n",
      "        [    0.0138],\n",
      "        [    0.0115],\n",
      "        [    0.0143],\n",
      "        [    0.0099],\n",
      "        [    0.0093],\n",
      "        [    0.0127],\n",
      "        [    0.0118],\n",
      "        [    0.0146],\n",
      "        [    0.0170],\n",
      "        [    0.0188],\n",
      "        [    0.0167],\n",
      "        [    0.0143],\n",
      "        [    0.0183],\n",
      "        [    0.0174],\n",
      "        [    0.0149],\n",
      "        [    0.0171],\n",
      "        [    0.0177],\n",
      "        [    0.0125],\n",
      "        [    0.0164],\n",
      "        [    0.0266],\n",
      "        [    0.0189],\n",
      "        [    0.0215],\n",
      "        [    0.0205],\n",
      "        [    0.0210],\n",
      "        [    0.0208],\n",
      "        [    0.0212],\n",
      "        [    0.0189],\n",
      "        [    0.0215],\n",
      "        [    0.0228],\n",
      "        [    0.0243],\n",
      "        [    0.0236],\n",
      "        [    0.0242],\n",
      "        [    0.0251],\n",
      "        [    0.0247],\n",
      "        [    0.0248],\n",
      "        [    0.0261],\n",
      "        [    0.0273],\n",
      "        [    0.0248],\n",
      "        [    0.0294],\n",
      "        [    0.0265],\n",
      "        [    0.0331],\n",
      "        [    0.0347],\n",
      "        [    0.0309],\n",
      "        [    0.0321],\n",
      "        [    0.0338],\n",
      "        [    0.0365],\n",
      "        [    0.0391],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0363],\n",
      "        [    0.0389],\n",
      "        [    0.0347],\n",
      "        [    0.0418],\n",
      "        [    0.0423],\n",
      "        [    0.0467],\n",
      "        [    0.0380],\n",
      "        [    0.0466],\n",
      "        [    0.0475],\n",
      "        [    0.0431],\n",
      "        [    0.0506],\n",
      "        [    0.0504],\n",
      "        [    0.0518],\n",
      "        [    0.0536],\n",
      "        [    0.0544],\n",
      "        [    0.0503],\n",
      "        [    0.0590],\n",
      "        [    0.0549],\n",
      "        [    0.0639],\n",
      "        [    0.0637],\n",
      "        [    0.0640],\n",
      "        [    0.0637],\n",
      "        [    0.0649],\n",
      "        [    0.0646],\n",
      "        [    0.0599],\n",
      "        [    0.0781],\n",
      "        [    0.0750]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.09336256980896\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 111\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6524523971384042e-09, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 96, 52, 130, 12, 53, 49, 87, 93, 99, 97, 132, 98, 34, 3, 104, 13, 26, 91, 131, 127, 23, 89, 102, 100, 105, 61, 62, 21, 90, 124, 76, 123, 86, 14, 57, 109, 88, 128, 27, 15, 94, 16, 32, 92, 95, 51, 18, 33, 58, 55, 22, 75, 9, 101, 66, 30, 68, 0, 36, 8, 63, 103, 50, 65, 29, 17, 10, 64, 1, 25, 133, 28, 85, 112, 46, 74, 54, 20, 137, 110, 106, 31, 19, 134, 107, 126, 35, 2, 135, 111, 77, 11, 136, 113, 48, 108, 47, 24, 138, 4, 129, 37, 59, 7, 6, 44, 60, 139, 67, 125] 數值 torch.Size([111, 1])\n",
      "目前模型的Data狀態 torch.Size([111, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9059],\n",
      "        [0.6948],\n",
      "        [0.8828],\n",
      "        [0.5815],\n",
      "        [0.9956],\n",
      "        [0.8692],\n",
      "        [0.8543],\n",
      "        [0.7334],\n",
      "        [0.7140],\n",
      "        [0.6570],\n",
      "        [0.6805],\n",
      "        [0.6735],\n",
      "        [0.6805],\n",
      "        [0.8181],\n",
      "        [0.8484],\n",
      "        [0.6634],\n",
      "        [0.9879],\n",
      "        [0.8268],\n",
      "        [0.6865],\n",
      "        [0.6357],\n",
      "        [0.6135],\n",
      "        [0.8512],\n",
      "        [0.7056],\n",
      "        [0.6348],\n",
      "        [0.6282],\n",
      "        [0.6867],\n",
      "        [0.9127],\n",
      "        [0.8919],\n",
      "        [0.8447],\n",
      "        [0.7123],\n",
      "        [0.5761],\n",
      "        [0.7678],\n",
      "        [0.6198],\n",
      "        [0.7069],\n",
      "        [0.9688],\n",
      "        [0.9315],\n",
      "        [0.6605],\n",
      "        [0.7224],\n",
      "        [0.6232],\n",
      "        [0.8164],\n",
      "        [0.9535],\n",
      "        [0.6935],\n",
      "        [0.9295],\n",
      "        [0.7948],\n",
      "        [0.6907],\n",
      "        [0.6887],\n",
      "        [0.8812],\n",
      "        [0.8971],\n",
      "        [0.8029],\n",
      "        [0.9180],\n",
      "        [0.8919],\n",
      "        [0.8364],\n",
      "        [0.7651],\n",
      "        [0.9944],\n",
      "        [0.6285],\n",
      "        [0.8560],\n",
      "        [0.8704],\n",
      "        [0.8376],\n",
      "        [0.8251],\n",
      "        [0.8211],\n",
      "        [0.9552],\n",
      "        [0.8845],\n",
      "        [0.6356],\n",
      "        [0.9134],\n",
      "        [0.8114],\n",
      "        [0.8542],\n",
      "        [0.9137],\n",
      "        [1.0175],\n",
      "        [0.8493],\n",
      "        [0.8396],\n",
      "        [0.8330],\n",
      "        [0.6451],\n",
      "        [0.8219],\n",
      "        [0.6905],\n",
      "        [0.6672],\n",
      "        [0.8754],\n",
      "        [0.7937],\n",
      "        [0.9264],\n",
      "        [0.8812],\n",
      "        [0.6634],\n",
      "        [0.6766],\n",
      "        [0.6613],\n",
      "        [0.8406],\n",
      "        [0.9001],\n",
      "        [0.6810],\n",
      "        [0.6518],\n",
      "        [0.5780],\n",
      "        [0.7982],\n",
      "        [0.8845],\n",
      "        [0.6647],\n",
      "        [0.6920],\n",
      "        [0.7685],\n",
      "        [1.0265],\n",
      "        [0.6837],\n",
      "        [0.6864],\n",
      "        [0.8969],\n",
      "        [0.6375],\n",
      "        [0.8832],\n",
      "        [0.8473],\n",
      "        [0.6619],\n",
      "        [0.8699],\n",
      "        [0.6236],\n",
      "        [0.7998],\n",
      "        [0.8957],\n",
      "        [0.9355],\n",
      "        [0.9107],\n",
      "        [0.8372],\n",
      "        [0.8903],\n",
      "        [0.6887],\n",
      "        [0.8687],\n",
      "        [0.5457]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0008],\n",
      "        [    0.0017],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0028],\n",
      "        [    0.0031],\n",
      "        [    0.0033],\n",
      "        [    0.0033],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0044],\n",
      "        [    0.0045],\n",
      "        [    0.0048],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0052],\n",
      "        [    0.0057],\n",
      "        [    0.0061],\n",
      "        [    0.0064],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0076],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0093],\n",
      "        [    0.0099],\n",
      "        [    0.0112],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0118],\n",
      "        [    0.0125],\n",
      "        [    0.0127],\n",
      "        [    0.0138],\n",
      "        [    0.0143],\n",
      "        [    0.0143],\n",
      "        [    0.0146],\n",
      "        [    0.0149],\n",
      "        [    0.0164],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0174],\n",
      "        [    0.0177],\n",
      "        [    0.0183],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0189],\n",
      "        [    0.0205],\n",
      "        [    0.0208],\n",
      "        [    0.0210],\n",
      "        [    0.0212],\n",
      "        [    0.0215],\n",
      "        [    0.0215],\n",
      "        [    0.0228],\n",
      "        [    0.0236],\n",
      "        [    0.0242],\n",
      "        [    0.0243],\n",
      "        [    0.0247],\n",
      "        [    0.0248],\n",
      "        [    0.0248],\n",
      "        [    0.0251],\n",
      "        [    0.0261],\n",
      "        [    0.0265],\n",
      "        [    0.0266],\n",
      "        [    0.0273],\n",
      "        [    0.0294],\n",
      "        [    0.0309],\n",
      "        [    0.0321],\n",
      "        [    0.0331],\n",
      "        [    0.0338],\n",
      "        [    0.0347],\n",
      "        [    0.0347],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0363],\n",
      "        [    0.0365],\n",
      "        [    0.0380],\n",
      "        [    0.0389],\n",
      "        [    0.0391],\n",
      "        [    0.0418],\n",
      "        [    0.0423],\n",
      "        [    0.0431],\n",
      "        [    0.0466],\n",
      "        [    0.0467],\n",
      "        [    0.0475],\n",
      "        [    0.0503],\n",
      "        [    0.0504],\n",
      "        [    0.0506],\n",
      "        [    0.0518],\n",
      "        [    0.0536],\n",
      "        [    0.0544],\n",
      "        [    0.0549],\n",
      "        [    0.0590],\n",
      "        [    0.0599],\n",
      "        [    0.0637],\n",
      "        [    0.0637],\n",
      "        [    0.0639],\n",
      "        [    0.0640],\n",
      "        [    0.0646],\n",
      "        [    0.0649],\n",
      "        [    0.0721],\n",
      "        [    0.0750],\n",
      "        [    0.0781]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0014],\n",
      "        [0.0011],\n",
      "        [0.0015],\n",
      "        [0.0058],\n",
      "        [0.0051],\n",
      "        [0.0013],\n",
      "        [0.0001],\n",
      "        [0.0022],\n",
      "        [0.0026],\n",
      "        [0.0023],\n",
      "        [0.0020],\n",
      "        [0.0109],\n",
      "        [0.0040],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0033],\n",
      "        [0.0010],\n",
      "        [0.0072],\n",
      "        [0.0049],\n",
      "        [0.0019],\n",
      "        [0.0113],\n",
      "        [0.0031],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0068],\n",
      "        [0.0063],\n",
      "        [0.0094],\n",
      "        [0.0069],\n",
      "        [0.0115],\n",
      "        [0.0086],\n",
      "        [0.0136],\n",
      "        [0.0093],\n",
      "        [0.0042],\n",
      "        [0.0096],\n",
      "        [0.0153],\n",
      "        [0.0098],\n",
      "        [0.0135],\n",
      "        [0.0120],\n",
      "        [0.0057],\n",
      "        [0.0104],\n",
      "        [0.0168],\n",
      "        [0.0147],\n",
      "        [0.0175],\n",
      "        [0.0158],\n",
      "        [0.0150],\n",
      "        [0.0161],\n",
      "        [0.0182],\n",
      "        [0.0198],\n",
      "        [0.0180],\n",
      "        [0.0194],\n",
      "        [0.0184],\n",
      "        [0.0211],\n",
      "        [0.0195],\n",
      "        [0.0169],\n",
      "        [0.0188],\n",
      "        [0.0184],\n",
      "        [0.0194],\n",
      "        [0.0191],\n",
      "        [0.0198],\n",
      "        [0.0220],\n",
      "        [0.0236],\n",
      "        [0.0216],\n",
      "        [0.0243],\n",
      "        [0.0221],\n",
      "        [0.0263],\n",
      "        [0.0227],\n",
      "        [0.0215],\n",
      "        [0.0225],\n",
      "        [0.0264],\n",
      "        [0.0249],\n",
      "        [0.0239],\n",
      "        [0.0343],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0276],\n",
      "        [0.0301],\n",
      "        [0.0340],\n",
      "        [0.0332],\n",
      "        [0.0380],\n",
      "        [0.0273],\n",
      "        [0.0324],\n",
      "        [0.0362],\n",
      "        [0.0351],\n",
      "        [0.0399],\n",
      "        [0.0300],\n",
      "        [0.0400],\n",
      "        [0.0445],\n",
      "        [0.0427],\n",
      "        [0.0412],\n",
      "        [0.0351],\n",
      "        [0.0437],\n",
      "        [0.0475],\n",
      "        [0.0449],\n",
      "        [0.0429],\n",
      "        [0.0472],\n",
      "        [0.0483],\n",
      "        [0.0530],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0475],\n",
      "        [0.0602],\n",
      "        [0.0528],\n",
      "        [0.0644],\n",
      "        [0.0657],\n",
      "        [0.0659],\n",
      "        [0.0656],\n",
      "        [0.0649],\n",
      "        [0.0668],\n",
      "        [0.0640],\n",
      "        [0.0726],\n",
      "        [0.0827]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.371899127960205\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 112\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.717950581792138e-08, 49)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [49, 13, 96, 53, 56, 52, 131, 97, 87, 99, 93, 3, 34, 23, 104, 98, 123, 91, 12, 128, 130, 105, 89, 102, 100, 62, 26, 90, 76, 61, 86, 57, 27, 132, 127, 21, 88, 109, 124, 94, 92, 14, 32, 95, 15, 9, 16, 33, 51, 66, 55, 101, 68, 30, 58, 75, 18, 0, 22, 17, 63, 36, 50, 10, 29, 8, 25, 103, 1, 65, 64, 137, 112, 85, 28, 134, 46, 110, 54, 74, 133, 31, 135, 106, 20, 19, 107, 2, 35, 136, 111, 126, 11, 113, 77, 138, 48, 47, 24, 129, 108, 4, 139, 37, 44, 6, 59, 7, 60, 67, 122, 73] 數值 torch.Size([112, 1])\n",
      "目前模型的Data狀態 torch.Size([112, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8523],\n",
      "        [0.9844],\n",
      "        [0.6962],\n",
      "        [0.8687],\n",
      "        [0.9045],\n",
      "        [0.8819],\n",
      "        [0.6287],\n",
      "        [0.6815],\n",
      "        [0.7333],\n",
      "        [0.6574],\n",
      "        [0.7135],\n",
      "        [0.8477],\n",
      "        [0.8174],\n",
      "        [0.8486],\n",
      "        [0.6623],\n",
      "        [0.6812],\n",
      "        [0.6147],\n",
      "        [0.6865],\n",
      "        [0.9922],\n",
      "        [0.6165],\n",
      "        [0.5749],\n",
      "        [0.6858],\n",
      "        [0.7052],\n",
      "        [0.6344],\n",
      "        [0.6285],\n",
      "        [0.8902],\n",
      "        [0.8243],\n",
      "        [0.7120],\n",
      "        [0.7679],\n",
      "        [0.9108],\n",
      "        [0.7072],\n",
      "        [0.9298],\n",
      "        [0.8141],\n",
      "        [0.6658],\n",
      "        [0.6074],\n",
      "        [0.8419],\n",
      "        [0.7222],\n",
      "        [0.6585],\n",
      "        [0.5715],\n",
      "        [0.6931],\n",
      "        [0.6907],\n",
      "        [0.9648],\n",
      "        [0.7937],\n",
      "        [0.6889],\n",
      "        [0.9506],\n",
      "        [0.9924],\n",
      "        [0.9263],\n",
      "        [0.8020],\n",
      "        [0.8797],\n",
      "        [0.8539],\n",
      "        [0.8911],\n",
      "        [0.6287],\n",
      "        [0.8358],\n",
      "        [0.8689],\n",
      "        [0.9160],\n",
      "        [0.7658],\n",
      "        [0.8943],\n",
      "        [0.8238],\n",
      "        [0.8337],\n",
      "        [0.9105],\n",
      "        [0.8833],\n",
      "        [0.8205],\n",
      "        [0.9113],\n",
      "        [1.0153],\n",
      "        [0.8523],\n",
      "        [0.9531],\n",
      "        [0.8305],\n",
      "        [0.6348],\n",
      "        [0.8385],\n",
      "        [0.8094],\n",
      "        [0.8480],\n",
      "        [0.6559],\n",
      "        [0.6640],\n",
      "        [0.6903],\n",
      "        [0.8199],\n",
      "        [0.6730],\n",
      "        [0.8733],\n",
      "        [0.6740],\n",
      "        [0.9258],\n",
      "        [0.7946],\n",
      "        [0.6374],\n",
      "        [0.8394],\n",
      "        [0.6567],\n",
      "        [0.6604],\n",
      "        [0.8779],\n",
      "        [0.8967],\n",
      "        [0.6507],\n",
      "        [0.8834],\n",
      "        [0.7973],\n",
      "        [0.6762],\n",
      "        [0.6891],\n",
      "        [0.5725],\n",
      "        [1.0240],\n",
      "        [0.6832],\n",
      "        [0.7692],\n",
      "        [0.6545],\n",
      "        [0.8946],\n",
      "        [0.8811],\n",
      "        [0.8451],\n",
      "        [0.6166],\n",
      "        [0.6363],\n",
      "        [0.8688],\n",
      "        [0.6806],\n",
      "        [0.7991],\n",
      "        [0.8370],\n",
      "        [0.9091],\n",
      "        [0.8937],\n",
      "        [0.9335],\n",
      "        [0.8885],\n",
      "        [0.8662],\n",
      "        [0.6245],\n",
      "        [0.8270]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0086],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0109],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0120],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0158],\n",
      "        [0.0161],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0175],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0184],\n",
      "        [0.0184],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0211],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0236],\n",
      "        [0.0239],\n",
      "        [0.0243],\n",
      "        [0.0249],\n",
      "        [0.0263],\n",
      "        [0.0264],\n",
      "        [0.0273],\n",
      "        [0.0276],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0300],\n",
      "        [0.0301],\n",
      "        [0.0324],\n",
      "        [0.0332],\n",
      "        [0.0340],\n",
      "        [0.0343],\n",
      "        [0.0351],\n",
      "        [0.0351],\n",
      "        [0.0362],\n",
      "        [0.0380],\n",
      "        [0.0399],\n",
      "        [0.0400],\n",
      "        [0.0412],\n",
      "        [0.0427],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0445],\n",
      "        [0.0449],\n",
      "        [0.0472],\n",
      "        [0.0475],\n",
      "        [0.0475],\n",
      "        [0.0483],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0528],\n",
      "        [0.0530],\n",
      "        [0.0602],\n",
      "        [0.0640],\n",
      "        [0.0644],\n",
      "        [0.0649],\n",
      "        [0.0656],\n",
      "        [0.0657],\n",
      "        [0.0659],\n",
      "        [0.0668],\n",
      "        [0.0726],\n",
      "        [0.0731],\n",
      "        [0.0815]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0019],\n",
      "        [0.0009],\n",
      "        [0.0029],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0076],\n",
      "        [0.0024],\n",
      "        [0.0035],\n",
      "        [0.0030],\n",
      "        [0.0038],\n",
      "        [0.0048],\n",
      "        [0.0044],\n",
      "        [0.0034],\n",
      "        [0.0013],\n",
      "        [0.0035],\n",
      "        [0.0017],\n",
      "        [0.0061],\n",
      "        [0.0039],\n",
      "        [0.0003],\n",
      "        [0.0116],\n",
      "        [0.0047],\n",
      "        [0.0082],\n",
      "        [0.0082],\n",
      "        [0.0077],\n",
      "        [0.0076],\n",
      "        [0.0072],\n",
      "        [0.0072],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0105],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0111],\n",
      "        [0.0135],\n",
      "        [0.0157],\n",
      "        [0.0190],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0150],\n",
      "        [0.0149],\n",
      "        [0.0172],\n",
      "        [0.0157],\n",
      "        [0.0187],\n",
      "        [0.0167],\n",
      "        [0.0170],\n",
      "        [0.0174],\n",
      "        [0.0184],\n",
      "        [0.0170],\n",
      "        [0.0197],\n",
      "        [0.0191],\n",
      "        [0.0203],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0211],\n",
      "        [0.0210],\n",
      "        [0.0222],\n",
      "        [0.0226],\n",
      "        [0.0208],\n",
      "        [0.0225],\n",
      "        [0.0244],\n",
      "        [0.0232],\n",
      "        [0.0223],\n",
      "        [0.0242],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0264],\n",
      "        [0.0256],\n",
      "        [0.0218],\n",
      "        [0.0246],\n",
      "        [0.0273],\n",
      "        [0.0290],\n",
      "        [0.0239],\n",
      "        [0.0313],\n",
      "        [0.0299],\n",
      "        [0.0349],\n",
      "        [0.0339],\n",
      "        [0.0405],\n",
      "        [0.0360],\n",
      "        [0.0290],\n",
      "        [0.0380],\n",
      "        [0.0376],\n",
      "        [0.0395],\n",
      "        [0.0419],\n",
      "        [0.0430],\n",
      "        [0.0419],\n",
      "        [0.0374],\n",
      "        [0.0410],\n",
      "        [0.0501],\n",
      "        [0.0469],\n",
      "        [0.0443],\n",
      "        [0.0472],\n",
      "        [0.0421],\n",
      "        [0.0493],\n",
      "        [0.0526],\n",
      "        [0.0529],\n",
      "        [0.0467],\n",
      "        [0.0550],\n",
      "        [0.0588],\n",
      "        [0.0581],\n",
      "        [0.0635],\n",
      "        [0.0625],\n",
      "        [0.0645],\n",
      "        [0.0651],\n",
      "        [0.0650],\n",
      "        [0.0662],\n",
      "        [0.0723],\n",
      "        [0.0671],\n",
      "        [0.0817]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.6506564617157\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 113\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.502235004925751e-08, 56)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [56, 49, 128, 52, 96, 104, 123, 13, 97, 53, 99, 23, 87, 98, 93, 12, 34, 105, 3, 91, 90, 26, 131, 62, 100, 89, 102, 76, 61, 27, 86, 57, 21, 130, 88, 32, 14, 15, 109, 94, 92, 16, 132, 127, 55, 33, 95, 51, 66, 58, 9, 75, 18, 124, 68, 101, 30, 36, 22, 0, 137, 17, 8, 50, 63, 29, 134, 25, 10, 112, 64, 103, 1, 65, 85, 28, 135, 110, 46, 74, 54, 31, 136, 20, 106, 19, 133, 111, 35, 107, 138, 2, 113, 129, 11, 77, 48, 126, 47, 24, 108, 139, 4, 44, 37, 6, 7, 59, 60, 122, 67, 140, 73] 數值 torch.Size([113, 1])\n",
      "目前模型的Data狀態 torch.Size([113, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9056],\n",
      "        [0.8527],\n",
      "        [0.6104],\n",
      "        [0.8831],\n",
      "        [0.6960],\n",
      "        [0.6603],\n",
      "        [0.6088],\n",
      "        [0.9853],\n",
      "        [0.6812],\n",
      "        [0.8703],\n",
      "        [0.6567],\n",
      "        [0.8488],\n",
      "        [0.7320],\n",
      "        [0.6807],\n",
      "        [0.7123],\n",
      "        [0.9933],\n",
      "        [0.8188],\n",
      "        [0.6842],\n",
      "        [0.8495],\n",
      "        [0.6854],\n",
      "        [0.7106],\n",
      "        [0.8243],\n",
      "        [0.6230],\n",
      "        [0.8909],\n",
      "        [0.6276],\n",
      "        [0.7036],\n",
      "        [0.6329],\n",
      "        [0.7671],\n",
      "        [0.9115],\n",
      "        [0.8142],\n",
      "        [0.7059],\n",
      "        [0.9309],\n",
      "        [0.8422],\n",
      "        [0.5690],\n",
      "        [0.7208],\n",
      "        [0.7945],\n",
      "        [0.9651],\n",
      "        [0.9516],\n",
      "        [0.6563],\n",
      "        [0.6918],\n",
      "        [0.6896],\n",
      "        [0.9271],\n",
      "        [0.6599],\n",
      "        [0.6018],\n",
      "        [0.8926],\n",
      "        [0.8030],\n",
      "        [0.6878],\n",
      "        [0.8804],\n",
      "        [0.8539],\n",
      "        [0.9168],\n",
      "        [0.9942],\n",
      "        [0.7652],\n",
      "        [0.8952],\n",
      "        [0.5661],\n",
      "        [0.8357],\n",
      "        [0.6277],\n",
      "        [0.8699],\n",
      "        [0.8218],\n",
      "        [0.8338],\n",
      "        [0.8250],\n",
      "        [0.6504],\n",
      "        [0.9111],\n",
      "        [0.9544],\n",
      "        [0.9118],\n",
      "        [0.8843],\n",
      "        [0.8528],\n",
      "        [0.6668],\n",
      "        [0.8307],\n",
      "        [1.0172],\n",
      "        [0.6610],\n",
      "        [0.8487],\n",
      "        [0.6330],\n",
      "        [0.8398],\n",
      "        [0.8093],\n",
      "        [0.6884],\n",
      "        [0.8202],\n",
      "        [0.6506],\n",
      "        [0.6714],\n",
      "        [0.8746],\n",
      "        [0.7945],\n",
      "        [0.9275],\n",
      "        [0.8403],\n",
      "        [0.6707],\n",
      "        [0.8783],\n",
      "        [0.6586],\n",
      "        [0.8972],\n",
      "        [0.6312],\n",
      "        [0.6864],\n",
      "        [0.7981],\n",
      "        [0.6488],\n",
      "        [0.6490],\n",
      "        [0.8852],\n",
      "        [0.6803],\n",
      "        [0.6105],\n",
      "        [1.0260],\n",
      "        [0.7689],\n",
      "        [0.8955],\n",
      "        [0.5670],\n",
      "        [0.8822],\n",
      "        [0.8457],\n",
      "        [0.6343],\n",
      "        [0.6746],\n",
      "        [0.8702],\n",
      "        [0.8394],\n",
      "        [0.8000],\n",
      "        [0.9103],\n",
      "        [0.9344],\n",
      "        [0.8942],\n",
      "        [0.8891],\n",
      "        [0.6185],\n",
      "        [0.8659],\n",
      "        [0.7222],\n",
      "        [0.8272]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0009],\n",
      "        [0.0013],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0044],\n",
      "        [0.0047],\n",
      "        [0.0048],\n",
      "        [0.0061],\n",
      "        [0.0072],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0116],\n",
      "        [0.0135],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0157],\n",
      "        [0.0157],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0197],\n",
      "        [0.0203],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0226],\n",
      "        [0.0232],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0246],\n",
      "        [0.0256],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0264],\n",
      "        [0.0273],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0299],\n",
      "        [0.0313],\n",
      "        [0.0339],\n",
      "        [0.0349],\n",
      "        [0.0360],\n",
      "        [0.0374],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0395],\n",
      "        [0.0405],\n",
      "        [0.0410],\n",
      "        [0.0419],\n",
      "        [0.0419],\n",
      "        [0.0421],\n",
      "        [0.0430],\n",
      "        [0.0443],\n",
      "        [0.0467],\n",
      "        [0.0469],\n",
      "        [0.0472],\n",
      "        [0.0493],\n",
      "        [0.0501],\n",
      "        [0.0526],\n",
      "        [0.0529],\n",
      "        [0.0550],\n",
      "        [0.0581],\n",
      "        [0.0588],\n",
      "        [0.0625],\n",
      "        [0.0635],\n",
      "        [0.0645],\n",
      "        [0.0650],\n",
      "        [0.0651],\n",
      "        [0.0662],\n",
      "        [0.0671],\n",
      "        [0.0723],\n",
      "        [0.0783],\n",
      "        [0.0817]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0019],\n",
      "        [0.0018],\n",
      "        [0.0057],\n",
      "        [0.0014],\n",
      "        [0.0010],\n",
      "        [0.0002],\n",
      "        [0.0066],\n",
      "        [0.0005],\n",
      "        [0.0023],\n",
      "        [0.0021],\n",
      "        [0.0032],\n",
      "        [0.0015],\n",
      "        [0.0043],\n",
      "        [0.0034],\n",
      "        [0.0049],\n",
      "        [0.0062],\n",
      "        [0.0040],\n",
      "        [0.0034],\n",
      "        [0.0048],\n",
      "        [0.0065],\n",
      "        [0.0062],\n",
      "        [0.0093],\n",
      "        [0.0128],\n",
      "        [0.0060],\n",
      "        [0.0080],\n",
      "        [0.0092],\n",
      "        [0.0091],\n",
      "        [0.0074],\n",
      "        [0.0106],\n",
      "        [0.0087],\n",
      "        [0.0113],\n",
      "        [0.0091],\n",
      "        [0.0130],\n",
      "        [0.0167],\n",
      "        [0.0144],\n",
      "        [0.0158],\n",
      "        [0.0179],\n",
      "        [0.0178],\n",
      "        [0.0174],\n",
      "        [0.0171],\n",
      "        [0.0165],\n",
      "        [0.0189],\n",
      "        [0.0227],\n",
      "        [0.0219],\n",
      "        [0.0180],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0190],\n",
      "        [0.0163],\n",
      "        [0.0206],\n",
      "        [0.0177],\n",
      "        [0.0176],\n",
      "        [0.0208],\n",
      "        [0.0233],\n",
      "        [0.0173],\n",
      "        [0.0201],\n",
      "        [0.0191],\n",
      "        [0.0215],\n",
      "        [0.0231],\n",
      "        [0.0203],\n",
      "        [0.0163],\n",
      "        [0.0198],\n",
      "        [0.0234],\n",
      "        [0.0203],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0177],\n",
      "        [0.0223],\n",
      "        [0.0233],\n",
      "        [0.0220],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0256],\n",
      "        [0.0283],\n",
      "        [0.0260],\n",
      "        [0.0305],\n",
      "        [0.0231],\n",
      "        [0.0278],\n",
      "        [0.0296],\n",
      "        [0.0330],\n",
      "        [0.0340],\n",
      "        [0.0350],\n",
      "        [0.0319],\n",
      "        [0.0397],\n",
      "        [0.0392],\n",
      "        [0.0417],\n",
      "        [0.0464],\n",
      "        [0.0387],\n",
      "        [0.0429],\n",
      "        [0.0432],\n",
      "        [0.0366],\n",
      "        [0.0425],\n",
      "        [0.0417],\n",
      "        [0.0412],\n",
      "        [0.0456],\n",
      "        [0.0465],\n",
      "        [0.0472],\n",
      "        [0.0546],\n",
      "        [0.0508],\n",
      "        [0.0514],\n",
      "        [0.0562],\n",
      "        [0.0519],\n",
      "        [0.0595],\n",
      "        [0.0624],\n",
      "        [0.0644],\n",
      "        [0.0655],\n",
      "        [0.0663],\n",
      "        [0.0670],\n",
      "        [0.0679],\n",
      "        [0.0619],\n",
      "        [0.0699],\n",
      "        [0.0718],\n",
      "        [0.0805]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.9299156665802\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 114\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.5599741699552396e-08, 104)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [104, 13, 96, 52, 23, 49, 56, 53, 97, 99, 98, 105, 34, 87, 3, 93, 128, 62, 12, 90, 91, 123, 76, 100, 27, 102, 57, 89, 26, 61, 86, 131, 21, 88, 32, 66, 137, 92, 130, 94, 68, 109, 75, 9, 33, 134, 15, 14, 95, 55, 16, 51, 30, 17, 101, 0, 50, 58, 18, 63, 36, 29, 127, 112, 25, 132, 22, 135, 10, 124, 8, 1, 85, 64, 103, 110, 65, 46, 28, 136, 74, 54, 31, 138, 111, 106, 20, 129, 113, 19, 2, 35, 107, 11, 133, 77, 48, 47, 24, 139, 126, 108, 4, 122, 44, 37, 6, 7, 59, 60, 67, 140, 121, 73] 數值 torch.Size([114, 1])\n",
      "目前模型的Data狀態 torch.Size([114, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6588],\n",
      "        [0.9828],\n",
      "        [0.6960],\n",
      "        [0.8820],\n",
      "        [0.8469],\n",
      "        [0.8506],\n",
      "        [0.9041],\n",
      "        [0.8695],\n",
      "        [0.6813],\n",
      "        [0.6565],\n",
      "        [0.6806],\n",
      "        [0.6829],\n",
      "        [0.8184],\n",
      "        [0.7312],\n",
      "        [0.8495],\n",
      "        [0.7112],\n",
      "        [0.6050],\n",
      "        [0.8893],\n",
      "        [0.9911],\n",
      "        [0.7096],\n",
      "        [0.6849],\n",
      "        [0.6039],\n",
      "        [0.7660],\n",
      "        [0.6274],\n",
      "        [0.8124],\n",
      "        [0.6321],\n",
      "        [0.9291],\n",
      "        [0.7026],\n",
      "        [0.8223],\n",
      "        [0.9096],\n",
      "        [0.7054],\n",
      "        [0.6177],\n",
      "        [0.8403],\n",
      "        [0.7198],\n",
      "        [0.7936],\n",
      "        [0.8518],\n",
      "        [0.6450],\n",
      "        [0.6892],\n",
      "        [0.5640],\n",
      "        [0.6907],\n",
      "        [0.8339],\n",
      "        [0.6547],\n",
      "        [0.7639],\n",
      "        [0.9932],\n",
      "        [0.8023],\n",
      "        [0.6607],\n",
      "        [0.9496],\n",
      "        [0.9621],\n",
      "        [0.6871],\n",
      "        [0.8916],\n",
      "        [0.9249],\n",
      "        [0.8788],\n",
      "        [0.8687],\n",
      "        [0.9088],\n",
      "        [0.6273],\n",
      "        [0.8243],\n",
      "        [0.9096],\n",
      "        [0.9148],\n",
      "        [0.8934],\n",
      "        [0.8830],\n",
      "        [0.8211],\n",
      "        [0.8512],\n",
      "        [0.5969],\n",
      "        [0.6584],\n",
      "        [0.8288],\n",
      "        [0.6540],\n",
      "        [0.8317],\n",
      "        [0.6447],\n",
      "        [1.0160],\n",
      "        [0.5618],\n",
      "        [0.9533],\n",
      "        [0.8392],\n",
      "        [0.6871],\n",
      "        [0.8473],\n",
      "        [0.6318],\n",
      "        [0.6693],\n",
      "        [0.8073],\n",
      "        [0.8729],\n",
      "        [0.8187],\n",
      "        [0.6652],\n",
      "        [0.7936],\n",
      "        [0.9265],\n",
      "        [0.8392],\n",
      "        [0.6436],\n",
      "        [0.6841],\n",
      "        [0.6574],\n",
      "        [0.8762],\n",
      "        [0.6050],\n",
      "        [0.6777],\n",
      "        [0.8949],\n",
      "        [0.8847],\n",
      "        [0.7972],\n",
      "        [0.6475],\n",
      "        [1.0246],\n",
      "        [0.6253],\n",
      "        [0.7683],\n",
      "        [0.8935],\n",
      "        [0.8804],\n",
      "        [0.8442],\n",
      "        [0.6684],\n",
      "        [0.5625],\n",
      "        [0.6331],\n",
      "        [0.8694],\n",
      "        [0.6133],\n",
      "        [0.8394],\n",
      "        [0.7991],\n",
      "        [0.9092],\n",
      "        [0.9331],\n",
      "        [0.8924],\n",
      "        [0.8874],\n",
      "        [0.8635],\n",
      "        [0.7157],\n",
      "        [0.6045],\n",
      "        [0.8261]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0005],\n",
      "        [0.0010],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0034],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0074],\n",
      "        [0.0080],\n",
      "        [0.0087],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0106],\n",
      "        [0.0113],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0144],\n",
      "        [0.0158],\n",
      "        [0.0163],\n",
      "        [0.0163],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0171],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0177],\n",
      "        [0.0177],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0198],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0213],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0223],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0256],\n",
      "        [0.0260],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0278],\n",
      "        [0.0283],\n",
      "        [0.0296],\n",
      "        [0.0305],\n",
      "        [0.0319],\n",
      "        [0.0330],\n",
      "        [0.0340],\n",
      "        [0.0350],\n",
      "        [0.0366],\n",
      "        [0.0387],\n",
      "        [0.0392],\n",
      "        [0.0397],\n",
      "        [0.0412],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0429],\n",
      "        [0.0432],\n",
      "        [0.0456],\n",
      "        [0.0464],\n",
      "        [0.0465],\n",
      "        [0.0472],\n",
      "        [0.0508],\n",
      "        [0.0514],\n",
      "        [0.0519],\n",
      "        [0.0546],\n",
      "        [0.0562],\n",
      "        [0.0595],\n",
      "        [0.0619],\n",
      "        [0.0624],\n",
      "        [0.0644],\n",
      "        [0.0655],\n",
      "        [0.0663],\n",
      "        [0.0670],\n",
      "        [0.0679],\n",
      "        [0.0699],\n",
      "        [0.0718],\n",
      "        [0.0780],\n",
      "        [0.0805]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0005],\n",
      "        [0.0014],\n",
      "        [0.0011],\n",
      "        [0.0015],\n",
      "        [0.0024],\n",
      "        [0.0019],\n",
      "        [0.0027],\n",
      "        [0.0018],\n",
      "        [0.0029],\n",
      "        [0.0038],\n",
      "        [0.0027],\n",
      "        [0.0051],\n",
      "        [0.0044],\n",
      "        [0.0066],\n",
      "        [0.0052],\n",
      "        [0.0093],\n",
      "        [0.0059],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0104],\n",
      "        [0.0070],\n",
      "        [0.0078],\n",
      "        [0.0085],\n",
      "        [0.0094],\n",
      "        [0.0089],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0160],\n",
      "        [0.0129],\n",
      "        [0.0146],\n",
      "        [0.0153],\n",
      "        [0.0159],\n",
      "        [0.0132],\n",
      "        [0.0162],\n",
      "        [0.0197],\n",
      "        [0.0175],\n",
      "        [0.0170],\n",
      "        [0.0181],\n",
      "        [0.0168],\n",
      "        [0.0191],\n",
      "        [0.0169],\n",
      "        [0.0138],\n",
      "        [0.0174],\n",
      "        [0.0183],\n",
      "        [0.0182],\n",
      "        [0.0176],\n",
      "        [0.0187],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0215],\n",
      "        [0.0196],\n",
      "        [0.0210],\n",
      "        [0.0204],\n",
      "        [0.0215],\n",
      "        [0.0208],\n",
      "        [0.0217],\n",
      "        [0.0251],\n",
      "        [0.0205],\n",
      "        [0.0223],\n",
      "        [0.0263],\n",
      "        [0.0232],\n",
      "        [0.0194],\n",
      "        [0.0246],\n",
      "        [0.0265],\n",
      "        [0.0223],\n",
      "        [0.0268],\n",
      "        [0.0253],\n",
      "        [0.0269],\n",
      "        [0.0280],\n",
      "        [0.0266],\n",
      "        [0.0287],\n",
      "        [0.0298],\n",
      "        [0.0304],\n",
      "        [0.0287],\n",
      "        [0.0326],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0333],\n",
      "        [0.0374],\n",
      "        [0.0398],\n",
      "        [0.0397],\n",
      "        [0.0377],\n",
      "        [0.0401],\n",
      "        [0.0417],\n",
      "        [0.0440],\n",
      "        [0.0424],\n",
      "        [0.0438],\n",
      "        [0.0467],\n",
      "        [0.0501],\n",
      "        [0.0466],\n",
      "        [0.0470],\n",
      "        [0.0508],\n",
      "        [0.0517],\n",
      "        [0.0479],\n",
      "        [0.0577],\n",
      "        [0.0568],\n",
      "        [0.0586],\n",
      "        [0.0576],\n",
      "        [0.0607],\n",
      "        [0.0639],\n",
      "        [0.0646],\n",
      "        [0.0656],\n",
      "        [0.0673],\n",
      "        [0.0680],\n",
      "        [0.0691],\n",
      "        [0.0675],\n",
      "        [0.0742],\n",
      "        [0.0803]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.209166049957275\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 115\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.2891103412803204e-07, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [13, 52, 104, 96, 23, 97, 56, 49, 53, 105, 99, 98, 87, 34, 93, 12, 62, 90, 91, 3, 76, 100, 27, 57, 128, 102, 89, 26, 123, 61, 86, 21, 137, 134, 88, 32, 66, 131, 92, 75, 33, 68, 15, 94, 55, 109, 95, 14, 16, 9, 51, 135, 30, 50, 130, 17, 101, 18, 112, 36, 58, 0, 63, 29, 25, 8, 22, 10, 127, 85, 132, 124, 110, 1, 64, 103, 65, 136, 46, 28, 74, 138, 54, 31, 111, 129, 20, 106, 113, 19, 35, 107, 2, 77, 11, 48, 139, 133, 47, 24, 108, 122, 126, 4, 44, 37, 6, 7, 59, 140, 60, 67, 121, 73, 78] 數值 torch.Size([115, 1])\n",
      "目前模型的Data狀態 torch.Size([115, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9829],\n",
      "        [0.8824],\n",
      "        [0.6578],\n",
      "        [0.6964],\n",
      "        [0.8469],\n",
      "        [0.6817],\n",
      "        [0.9040],\n",
      "        [0.8500],\n",
      "        [0.8701],\n",
      "        [0.6822],\n",
      "        [0.6568],\n",
      "        [0.6809],\n",
      "        [0.7311],\n",
      "        [0.8195],\n",
      "        [0.7109],\n",
      "        [0.9914],\n",
      "        [0.8892],\n",
      "        [0.7095],\n",
      "        [0.6852],\n",
      "        [0.8513],\n",
      "        [0.7656],\n",
      "        [0.6276],\n",
      "        [0.8121],\n",
      "        [0.9289],\n",
      "        [0.6015],\n",
      "        [0.6317],\n",
      "        [0.7023],\n",
      "        [0.8218],\n",
      "        [0.6001],\n",
      "        [0.9094],\n",
      "        [0.7055],\n",
      "        [0.8404],\n",
      "        [0.6418],\n",
      "        [0.6568],\n",
      "        [0.7196],\n",
      "        [0.7942],\n",
      "        [0.8513],\n",
      "        [0.6146],\n",
      "        [0.6894],\n",
      "        [0.7631],\n",
      "        [0.8031],\n",
      "        [0.8337],\n",
      "        [0.9499],\n",
      "        [0.6903],\n",
      "        [0.8920],\n",
      "        [0.6539],\n",
      "        [0.6868],\n",
      "        [0.9617],\n",
      "        [0.9251],\n",
      "        [0.9946],\n",
      "        [0.8787],\n",
      "        [0.6410],\n",
      "        [0.8691],\n",
      "        [0.9089],\n",
      "        [0.5610],\n",
      "        [0.9087],\n",
      "        [0.6274],\n",
      "        [0.8937],\n",
      "        [0.6569],\n",
      "        [0.8217],\n",
      "        [0.9145],\n",
      "        [0.8254],\n",
      "        [0.8832],\n",
      "        [0.8512],\n",
      "        [0.8288],\n",
      "        [0.9544],\n",
      "        [0.8315],\n",
      "        [1.0173],\n",
      "        [0.5936],\n",
      "        [0.6864],\n",
      "        [0.6505],\n",
      "        [0.5586],\n",
      "        [0.6682],\n",
      "        [0.8403],\n",
      "        [0.8474],\n",
      "        [0.6311],\n",
      "        [0.8070],\n",
      "        [0.6620],\n",
      "        [0.8731],\n",
      "        [0.8188],\n",
      "        [0.7932],\n",
      "        [0.6403],\n",
      "        [0.9270],\n",
      "        [0.8397],\n",
      "        [0.6828],\n",
      "        [0.6015],\n",
      "        [0.8762],\n",
      "        [0.6568],\n",
      "        [0.6761],\n",
      "        [0.8950],\n",
      "        [0.7976],\n",
      "        [0.6469],\n",
      "        [0.8862],\n",
      "        [0.7684],\n",
      "        [1.0258],\n",
      "        [0.8933],\n",
      "        [0.6645],\n",
      "        [0.6216],\n",
      "        [0.8805],\n",
      "        [0.8446],\n",
      "        [0.6325],\n",
      "        [0.6090],\n",
      "        [0.5594],\n",
      "        [0.8704],\n",
      "        [0.8411],\n",
      "        [0.7996],\n",
      "        [0.9101],\n",
      "        [0.9339],\n",
      "        [0.8921],\n",
      "        [0.7115],\n",
      "        [0.8873],\n",
      "        [0.8627],\n",
      "        [0.6007],\n",
      "        [0.8258],\n",
      "        [0.7451]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0038],\n",
      "        [0.0044],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0059],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0070],\n",
      "        [0.0078],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0104],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0129],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0146],\n",
      "        [0.0153],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0176],\n",
      "        [0.0181],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0215],\n",
      "        [0.0215],\n",
      "        [0.0217],\n",
      "        [0.0223],\n",
      "        [0.0223],\n",
      "        [0.0232],\n",
      "        [0.0246],\n",
      "        [0.0251],\n",
      "        [0.0253],\n",
      "        [0.0263],\n",
      "        [0.0265],\n",
      "        [0.0266],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0280],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0298],\n",
      "        [0.0304],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0374],\n",
      "        [0.0377],\n",
      "        [0.0397],\n",
      "        [0.0398],\n",
      "        [0.0401],\n",
      "        [0.0417],\n",
      "        [0.0424],\n",
      "        [0.0438],\n",
      "        [0.0440],\n",
      "        [0.0466],\n",
      "        [0.0467],\n",
      "        [0.0470],\n",
      "        [0.0479],\n",
      "        [0.0501],\n",
      "        [0.0508],\n",
      "        [0.0517],\n",
      "        [0.0568],\n",
      "        [0.0576],\n",
      "        [0.0577],\n",
      "        [0.0586],\n",
      "        [0.0607],\n",
      "        [0.0639],\n",
      "        [0.0646],\n",
      "        [0.0656],\n",
      "        [0.0673],\n",
      "        [0.0675],\n",
      "        [0.0680],\n",
      "        [0.0691],\n",
      "        [0.0742],\n",
      "        [0.0803],\n",
      "        [0.0821]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0013],\n",
      "        [0.0026],\n",
      "        [0.0004],\n",
      "        [0.0016],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0031],\n",
      "        [0.0026],\n",
      "        [0.0016],\n",
      "        [0.0035],\n",
      "        [0.0030],\n",
      "        [0.0060],\n",
      "        [0.0058],\n",
      "        [0.0066],\n",
      "        [0.0061],\n",
      "        [0.0049],\n",
      "        [0.0048],\n",
      "        [0.0071],\n",
      "        [0.0079],\n",
      "        [0.0047],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0080],\n",
      "        [0.0109],\n",
      "        [0.0103],\n",
      "        [0.0110],\n",
      "        [0.0100],\n",
      "        [0.0125],\n",
      "        [0.0121],\n",
      "        [0.0127],\n",
      "        [0.0126],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0162],\n",
      "        [0.0148],\n",
      "        [0.0147],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0142],\n",
      "        [0.0163],\n",
      "        [0.0160],\n",
      "        [0.0176],\n",
      "        [0.0188],\n",
      "        [0.0179],\n",
      "        [0.0187],\n",
      "        [0.0195],\n",
      "        [0.0191],\n",
      "        [0.0189],\n",
      "        [0.0196],\n",
      "        [0.0195],\n",
      "        [0.0181],\n",
      "        [0.0198],\n",
      "        [0.0186],\n",
      "        [0.0206],\n",
      "        [0.0195],\n",
      "        [0.0206],\n",
      "        [0.0201],\n",
      "        [0.0195],\n",
      "        [0.0206],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0207],\n",
      "        [0.0217],\n",
      "        [0.0224],\n",
      "        [0.0219],\n",
      "        [0.0232],\n",
      "        [0.0249],\n",
      "        [0.0265],\n",
      "        [0.0234],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0257],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0323],\n",
      "        [0.0339],\n",
      "        [0.0356],\n",
      "        [0.0365],\n",
      "        [0.0362],\n",
      "        [0.0394],\n",
      "        [0.0407],\n",
      "        [0.0391],\n",
      "        [0.0416],\n",
      "        [0.0423],\n",
      "        [0.0445],\n",
      "        [0.0448],\n",
      "        [0.0446],\n",
      "        [0.0469],\n",
      "        [0.0465],\n",
      "        [0.0462],\n",
      "        [0.0514],\n",
      "        [0.0505],\n",
      "        [0.0521],\n",
      "        [0.0573],\n",
      "        [0.0550],\n",
      "        [0.0589],\n",
      "        [0.0581],\n",
      "        [0.0595],\n",
      "        [0.0639],\n",
      "        [0.0643],\n",
      "        [0.0654],\n",
      "        [0.0683],\n",
      "        [0.0654],\n",
      "        [0.0689],\n",
      "        [0.0676],\n",
      "        [0.0718],\n",
      "        [0.0781],\n",
      "        [0.0805]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.488145351409912\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 116\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.837636318668956e-07, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 13, 52, 23, 105, 97, 104, 53, 56, 98, 49, 99, 76, 90, 62, 34, 87, 12, 93, 91, 3, 57, 100, 27, 26, 102, 128, 89, 61, 134, 137, 123, 21, 86, 75, 66, 32, 68, 88, 33, 131, 92, 15, 55, 135, 50, 109, 94, 16, 14, 51, 95, 17, 112, 9, 30, 18, 101, 130, 36, 63, 29, 8, 58, 0, 25, 22, 85, 10, 110, 127, 1, 136, 132, 64, 124, 103, 46, 65, 28, 74, 138, 54, 31, 129, 111, 113, 20, 106, 19, 35, 107, 77, 2, 139, 48, 11, 47, 133, 24, 122, 108, 4, 126, 44, 37, 6, 7, 140, 67, 59, 60, 121, 73, 78, 72] 數值 torch.Size([116, 1])\n",
      "目前模型的Data狀態 torch.Size([116, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6955],\n",
      "        [0.9823],\n",
      "        [0.8822],\n",
      "        [0.8470],\n",
      "        [0.6811],\n",
      "        [0.6810],\n",
      "        [0.6564],\n",
      "        [0.8700],\n",
      "        [0.9033],\n",
      "        [0.6802],\n",
      "        [0.8493],\n",
      "        [0.6562],\n",
      "        [0.7633],\n",
      "        [0.7081],\n",
      "        [0.8882],\n",
      "        [0.8203],\n",
      "        [0.7295],\n",
      "        [0.9912],\n",
      "        [0.7094],\n",
      "        [0.6844],\n",
      "        [0.8526],\n",
      "        [0.9280],\n",
      "        [0.6270],\n",
      "        [0.8121],\n",
      "        [0.8215],\n",
      "        [0.6309],\n",
      "        [0.5999],\n",
      "        [0.7007],\n",
      "        [0.9082],\n",
      "        [0.6552],\n",
      "        [0.6409],\n",
      "        [0.5980],\n",
      "        [0.8408],\n",
      "        [0.7040],\n",
      "        [0.7605],\n",
      "        [0.8502],\n",
      "        [0.7947],\n",
      "        [0.8326],\n",
      "        [0.7180],\n",
      "        [0.8036],\n",
      "        [0.6136],\n",
      "        [0.6885],\n",
      "        [0.9497],\n",
      "        [0.8917],\n",
      "        [0.6397],\n",
      "        [0.9078],\n",
      "        [0.6534],\n",
      "        [0.6889],\n",
      "        [0.9249],\n",
      "        [0.9609],\n",
      "        [0.8784],\n",
      "        [0.6855],\n",
      "        [0.9085],\n",
      "        [0.6559],\n",
      "        [0.9951],\n",
      "        [0.8693],\n",
      "        [0.8940],\n",
      "        [0.6269],\n",
      "        [0.5601],\n",
      "        [0.8219],\n",
      "        [0.8823],\n",
      "        [0.8512],\n",
      "        [0.9548],\n",
      "        [0.9134],\n",
      "        [0.8262],\n",
      "        [0.8290],\n",
      "        [0.8315],\n",
      "        [0.6845],\n",
      "        [1.0177],\n",
      "        [0.6673],\n",
      "        [0.5922],\n",
      "        [0.8410],\n",
      "        [0.6610],\n",
      "        [0.6491],\n",
      "        [0.8467],\n",
      "        [0.5573],\n",
      "        [0.6301],\n",
      "        [0.8729],\n",
      "        [0.8059],\n",
      "        [0.8190],\n",
      "        [0.7909],\n",
      "        [0.6393],\n",
      "        [0.9265],\n",
      "        [0.8399],\n",
      "        [0.5999],\n",
      "        [0.6819],\n",
      "        [0.6751],\n",
      "        [0.8765],\n",
      "        [0.6559],\n",
      "        [0.8950],\n",
      "        [0.7977],\n",
      "        [0.6462],\n",
      "        [0.7664],\n",
      "        [0.8870],\n",
      "        [0.6628],\n",
      "        [0.8928],\n",
      "        [1.0260],\n",
      "        [0.8802],\n",
      "        [0.6203],\n",
      "        [0.8450],\n",
      "        [0.6065],\n",
      "        [0.6320],\n",
      "        [0.8709],\n",
      "        [0.5582],\n",
      "        [0.8423],\n",
      "        [0.7997],\n",
      "        [0.9104],\n",
      "        [0.9341],\n",
      "        [0.7094],\n",
      "        [0.8612],\n",
      "        [0.8911],\n",
      "        [0.8864],\n",
      "        [0.5983],\n",
      "        [0.8236],\n",
      "        [0.7436],\n",
      "        [0.8406]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0010],\n",
      "        [0.0013],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0047],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0071],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0127],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0186],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0195],\n",
      "        [0.0195],\n",
      "        [0.0195],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.0206],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0224],\n",
      "        [0.0232],\n",
      "        [0.0234],\n",
      "        [0.0249],\n",
      "        [0.0257],\n",
      "        [0.0265],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0323],\n",
      "        [0.0339],\n",
      "        [0.0356],\n",
      "        [0.0362],\n",
      "        [0.0365],\n",
      "        [0.0391],\n",
      "        [0.0394],\n",
      "        [0.0407],\n",
      "        [0.0416],\n",
      "        [0.0423],\n",
      "        [0.0445],\n",
      "        [0.0446],\n",
      "        [0.0448],\n",
      "        [0.0462],\n",
      "        [0.0465],\n",
      "        [0.0469],\n",
      "        [0.0505],\n",
      "        [0.0514],\n",
      "        [0.0521],\n",
      "        [0.0550],\n",
      "        [0.0573],\n",
      "        [0.0581],\n",
      "        [0.0589],\n",
      "        [0.0595],\n",
      "        [0.0639],\n",
      "        [0.0643],\n",
      "        [0.0654],\n",
      "        [0.0654],\n",
      "        [0.0676],\n",
      "        [0.0683],\n",
      "        [0.0689],\n",
      "        [0.0718],\n",
      "        [0.0781],\n",
      "        [0.0805],\n",
      "        [0.0944]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0011],\n",
      "        [    0.0011],\n",
      "        [    0.0024],\n",
      "        [    0.0018],\n",
      "        [    0.0026],\n",
      "        [    0.0026],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0034],\n",
      "        [    0.0031],\n",
      "        [    0.0027],\n",
      "        [    0.0042],\n",
      "        [    0.0042],\n",
      "        [    0.0072],\n",
      "        [    0.0068],\n",
      "        [    0.0058],\n",
      "        [    0.0073],\n",
      "        [    0.0071],\n",
      "        [    0.0100],\n",
      "        [    0.0075],\n",
      "        [    0.0077],\n",
      "        [    0.0094],\n",
      "        [    0.0094],\n",
      "        [    0.0099],\n",
      "        [    0.0101],\n",
      "        [    0.0118],\n",
      "        [    0.0131],\n",
      "        [    0.0131],\n",
      "        [    0.0138],\n",
      "        [    0.0122],\n",
      "        [    0.0113],\n",
      "        [    0.0135],\n",
      "        [    0.0117],\n",
      "        [    0.0140],\n",
      "        [    0.0136],\n",
      "        [    0.0155],\n",
      "        [    0.0171],\n",
      "        [    0.0151],\n",
      "        [    0.0155],\n",
      "        [    0.0173],\n",
      "        [    0.0173],\n",
      "        [    0.0180],\n",
      "        [    0.0193],\n",
      "        [    0.0180],\n",
      "        [    0.0176],\n",
      "        [    0.0193],\n",
      "        [    0.0185],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0201],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0206],\n",
      "        [    0.0192],\n",
      "        [    0.0200],\n",
      "        [    0.0190],\n",
      "        [    0.0201],\n",
      "        [    0.0200],\n",
      "        [    0.0225],\n",
      "        [    0.0208],\n",
      "        [    0.0226],\n",
      "        [    0.0239],\n",
      "        [    0.0235],\n",
      "        [    0.0224],\n",
      "        [    0.0223],\n",
      "        [    0.0257],\n",
      "        [    0.0266],\n",
      "        [    0.0257],\n",
      "        [    0.0289],\n",
      "        [    0.0290],\n",
      "        [    0.0267],\n",
      "        [    0.0282],\n",
      "        [    0.0269],\n",
      "        [    0.0289],\n",
      "        [    0.0299],\n",
      "        [    0.0303],\n",
      "        [    0.0290],\n",
      "        [    0.0278],\n",
      "        [    0.0337],\n",
      "        [    0.0337],\n",
      "        [    0.0364],\n",
      "        [    0.0370],\n",
      "        [    0.0372],\n",
      "        [    0.0398],\n",
      "        [    0.0383],\n",
      "        [    0.0402],\n",
      "        [    0.0408],\n",
      "        [    0.0417],\n",
      "        [    0.0438],\n",
      "        [    0.0428],\n",
      "        [    0.0464],\n",
      "        [    0.0470],\n",
      "        [    0.0463],\n",
      "        [    0.0476],\n",
      "        [    0.0506],\n",
      "        [    0.0502],\n",
      "        [    0.0532],\n",
      "        [    0.0547],\n",
      "        [    0.0563],\n",
      "        [    0.0569],\n",
      "        [    0.0577],\n",
      "        [    0.0581],\n",
      "        [    0.0634],\n",
      "        [    0.0633],\n",
      "        [    0.0645],\n",
      "        [    0.0656],\n",
      "        [    0.0665],\n",
      "        [    0.0688],\n",
      "        [    0.0694],\n",
      "        [    0.0714],\n",
      "        [    0.0757],\n",
      "        [    0.0791],\n",
      "        [    0.0922]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.76664161682129\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 117\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.6705357925038697e-09, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 52, 13, 105, 23, 97, 104, 76, 53, 56, 99, 98, 49, 90, 62, 12, 87, 91, 34, 93, 57, 100, 27, 26, 102, 3, 128, 21, 75, 89, 123, 61, 134, 86, 32, 137, 66, 33, 68, 131, 88, 92, 15, 109, 50, 55, 16, 130, 18, 135, 51, 14, 94, 17, 101, 63, 95, 36, 112, 9, 30, 8, 85, 22, 29, 58, 25, 0, 127, 10, 110, 132, 124, 74, 64, 103, 1, 136, 28, 46, 65, 54, 138, 31, 129, 111, 20, 113, 106, 19, 35, 77, 107, 48, 2, 139, 11, 133, 47, 24, 122, 108, 4, 126, 44, 6, 37, 7, 140, 67, 59, 60, 121, 73, 78, 72, 125] 數值 torch.Size([117, 1])\n",
      "目前模型的Data狀態 torch.Size([117, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6951],\n",
      "        [0.8824],\n",
      "        [0.9823],\n",
      "        [0.6812],\n",
      "        [0.8478],\n",
      "        [0.6810],\n",
      "        [0.6564],\n",
      "        [0.7613],\n",
      "        [0.8702],\n",
      "        [0.9029],\n",
      "        [0.6566],\n",
      "        [0.6803],\n",
      "        [0.8490],\n",
      "        [0.7076],\n",
      "        [0.8875],\n",
      "        [0.9915],\n",
      "        [0.7287],\n",
      "        [0.6844],\n",
      "        [0.8216],\n",
      "        [0.7088],\n",
      "        [0.9274],\n",
      "        [0.6277],\n",
      "        [0.8130],\n",
      "        [0.8221],\n",
      "        [0.6313],\n",
      "        [0.8547],\n",
      "        [0.6006],\n",
      "        [0.8420],\n",
      "        [0.7580],\n",
      "        [0.6999],\n",
      "        [0.5983],\n",
      "        [0.9072],\n",
      "        [0.6561],\n",
      "        [0.7033],\n",
      "        [0.7958],\n",
      "        [0.6425],\n",
      "        [0.8495],\n",
      "        [0.8048],\n",
      "        [0.8321],\n",
      "        [0.6151],\n",
      "        [0.7172],\n",
      "        [0.6884],\n",
      "        [0.9501],\n",
      "        [0.6544],\n",
      "        [0.9072],\n",
      "        [0.8916],\n",
      "        [0.9253],\n",
      "        [0.5617],\n",
      "        [0.8949],\n",
      "        [0.6408],\n",
      "        [0.8785],\n",
      "        [0.9607],\n",
      "        [0.6884],\n",
      "        [0.9089],\n",
      "        [0.6274],\n",
      "        [0.8817],\n",
      "        [0.6849],\n",
      "        [0.8224],\n",
      "        [0.6567],\n",
      "        [0.9961],\n",
      "        [0.8702],\n",
      "        [0.9559],\n",
      "        [0.6834],\n",
      "        [0.8324],\n",
      "        [0.8520],\n",
      "        [0.9128],\n",
      "        [0.8300],\n",
      "        [0.8278],\n",
      "        [0.5931],\n",
      "        [1.0185],\n",
      "        [0.6681],\n",
      "        [0.6501],\n",
      "        [0.5582],\n",
      "        [0.7884],\n",
      "        [0.8462],\n",
      "        [0.6303],\n",
      "        [0.8424],\n",
      "        [0.6623],\n",
      "        [0.8201],\n",
      "        [0.8732],\n",
      "        [0.8054],\n",
      "        [0.9262],\n",
      "        [0.6406],\n",
      "        [0.8407],\n",
      "        [0.6007],\n",
      "        [0.6826],\n",
      "        [0.8776],\n",
      "        [0.6758],\n",
      "        [0.6564],\n",
      "        [0.8958],\n",
      "        [0.7983],\n",
      "        [0.7645],\n",
      "        [0.6469],\n",
      "        [0.8926],\n",
      "        [0.8885],\n",
      "        [0.6636],\n",
      "        [1.0266],\n",
      "        [0.6215],\n",
      "        [0.8802],\n",
      "        [0.8461],\n",
      "        [0.6062],\n",
      "        [0.6330],\n",
      "        [0.8721],\n",
      "        [0.5594],\n",
      "        [0.8438],\n",
      "        [0.9114],\n",
      "        [0.8002],\n",
      "        [0.9350],\n",
      "        [0.7096],\n",
      "        [0.8602],\n",
      "        [0.8906],\n",
      "        [0.8859],\n",
      "        [0.5979],\n",
      "        [0.8212],\n",
      "        [0.7422],\n",
      "        [0.8384],\n",
      "        [0.5291]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0011],\n",
      "        [    0.0011],\n",
      "        [    0.0018],\n",
      "        [    0.0024],\n",
      "        [    0.0026],\n",
      "        [    0.0026],\n",
      "        [    0.0027],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0031],\n",
      "        [    0.0034],\n",
      "        [    0.0042],\n",
      "        [    0.0042],\n",
      "        [    0.0058],\n",
      "        [    0.0068],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0075],\n",
      "        [    0.0077],\n",
      "        [    0.0094],\n",
      "        [    0.0094],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0113],\n",
      "        [    0.0117],\n",
      "        [    0.0118],\n",
      "        [    0.0122],\n",
      "        [    0.0131],\n",
      "        [    0.0131],\n",
      "        [    0.0135],\n",
      "        [    0.0136],\n",
      "        [    0.0138],\n",
      "        [    0.0140],\n",
      "        [    0.0151],\n",
      "        [    0.0155],\n",
      "        [    0.0155],\n",
      "        [    0.0171],\n",
      "        [    0.0173],\n",
      "        [    0.0173],\n",
      "        [    0.0176],\n",
      "        [    0.0180],\n",
      "        [    0.0180],\n",
      "        [    0.0185],\n",
      "        [    0.0190],\n",
      "        [    0.0192],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0199],\n",
      "        [    0.0200],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0206],\n",
      "        [    0.0208],\n",
      "        [    0.0223],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0226],\n",
      "        [    0.0235],\n",
      "        [    0.0239],\n",
      "        [    0.0257],\n",
      "        [    0.0257],\n",
      "        [    0.0266],\n",
      "        [    0.0267],\n",
      "        [    0.0269],\n",
      "        [    0.0278],\n",
      "        [    0.0282],\n",
      "        [    0.0289],\n",
      "        [    0.0289],\n",
      "        [    0.0290],\n",
      "        [    0.0290],\n",
      "        [    0.0299],\n",
      "        [    0.0303],\n",
      "        [    0.0337],\n",
      "        [    0.0337],\n",
      "        [    0.0364],\n",
      "        [    0.0370],\n",
      "        [    0.0372],\n",
      "        [    0.0383],\n",
      "        [    0.0398],\n",
      "        [    0.0402],\n",
      "        [    0.0408],\n",
      "        [    0.0417],\n",
      "        [    0.0428],\n",
      "        [    0.0438],\n",
      "        [    0.0463],\n",
      "        [    0.0464],\n",
      "        [    0.0470],\n",
      "        [    0.0476],\n",
      "        [    0.0502],\n",
      "        [    0.0506],\n",
      "        [    0.0532],\n",
      "        [    0.0547],\n",
      "        [    0.0563],\n",
      "        [    0.0569],\n",
      "        [    0.0577],\n",
      "        [    0.0581],\n",
      "        [    0.0633],\n",
      "        [    0.0634],\n",
      "        [    0.0645],\n",
      "        [    0.0656],\n",
      "        [    0.0665],\n",
      "        [    0.0688],\n",
      "        [    0.0694],\n",
      "        [    0.0714],\n",
      "        [    0.0757],\n",
      "        [    0.0791],\n",
      "        [    0.0922],\n",
      "        [    0.0947]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0024],\n",
      "        [0.0039],\n",
      "        [0.0022],\n",
      "        [0.0016],\n",
      "        [0.0024],\n",
      "        [0.0021],\n",
      "        [0.0003],\n",
      "        [0.0014],\n",
      "        [0.0053],\n",
      "        [0.0024],\n",
      "        [0.0034],\n",
      "        [0.0053],\n",
      "        [0.0037],\n",
      "        [0.0016],\n",
      "        [0.0083],\n",
      "        [0.0077],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0080],\n",
      "        [0.0048],\n",
      "        [0.0065],\n",
      "        [0.0090],\n",
      "        [0.0102],\n",
      "        [0.0090],\n",
      "        [0.0105],\n",
      "        [0.0081],\n",
      "        [0.0117],\n",
      "        [0.0087],\n",
      "        [0.0124],\n",
      "        [0.0102],\n",
      "        [0.0160],\n",
      "        [0.0148],\n",
      "        [0.0139],\n",
      "        [0.0137],\n",
      "        [0.0162],\n",
      "        [0.0118],\n",
      "        [0.0152],\n",
      "        [0.0136],\n",
      "        [0.0129],\n",
      "        [0.0178],\n",
      "        [0.0172],\n",
      "        [0.0195],\n",
      "        [0.0160],\n",
      "        [0.0154],\n",
      "        [0.0199],\n",
      "        [0.0205],\n",
      "        [0.0160],\n",
      "        [0.0204],\n",
      "        [0.0213],\n",
      "        [0.0209],\n",
      "        [0.0222],\n",
      "        [0.0197],\n",
      "        [0.0180],\n",
      "        [0.0189],\n",
      "        [0.0176],\n",
      "        [0.0205],\n",
      "        [0.0210],\n",
      "        [0.0217],\n",
      "        [0.0191],\n",
      "        [0.0199],\n",
      "        [0.0219],\n",
      "        [0.0217],\n",
      "        [0.0231],\n",
      "        [0.0218],\n",
      "        [0.0253],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0279],\n",
      "        [0.0249],\n",
      "        [0.0241],\n",
      "        [0.0247],\n",
      "        [0.0302],\n",
      "        [0.0280],\n",
      "        [0.0287],\n",
      "        [0.0311],\n",
      "        [0.0292],\n",
      "        [0.0281],\n",
      "        [0.0321],\n",
      "        [0.0314],\n",
      "        [0.0358],\n",
      "        [0.0358],\n",
      "        [0.0390],\n",
      "        [0.0383],\n",
      "        [0.0393],\n",
      "        [0.0409],\n",
      "        [0.0391],\n",
      "        [0.0422],\n",
      "        [0.0422],\n",
      "        [0.0406],\n",
      "        [0.0424],\n",
      "        [0.0440],\n",
      "        [0.0460],\n",
      "        [0.0483],\n",
      "        [0.0451],\n",
      "        [0.0480],\n",
      "        [0.0486],\n",
      "        [0.0528],\n",
      "        [0.0559],\n",
      "        [0.0545],\n",
      "        [0.0574],\n",
      "        [0.0550],\n",
      "        [0.0584],\n",
      "        [0.0642],\n",
      "        [0.0640],\n",
      "        [0.0656],\n",
      "        [0.0662],\n",
      "        [0.0639],\n",
      "        [0.0712],\n",
      "        [0.0716],\n",
      "        [0.0724],\n",
      "        [0.0722],\n",
      "        [0.0775],\n",
      "        [0.0888],\n",
      "        [0.0915]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.046334743499756\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 118\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.6556181132473284e-08, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 76, 53, 23, 62, 104, 105, 99, 97, 52, 98, 90, 13, 57, 56, 49, 100, 91, 34, 87, 93, 128, 12, 75, 102, 27, 26, 123, 3, 21, 66, 89, 131, 68, 32, 86, 134, 33, 50, 130, 61, 109, 137, 92, 63, 88, 17, 101, 9, 15, 94, 55, 30, 18, 16, 95, 51, 36, 135, 112, 85, 29, 8, 14, 25, 22, 127, 10, 0, 124, 74, 132, 58, 110, 103, 46, 1, 28, 64, 136, 54, 65, 138, 31, 111, 129, 106, 20, 77, 113, 19, 35, 107, 48, 11, 2, 133, 139, 47, 24, 108, 126, 122, 4, 44, 67, 37, 6, 7, 140, 59, 60, 73, 121, 78, 72, 125, 120] 數值 torch.Size([118, 1])\n",
      "目前模型的Data狀態 torch.Size([118, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6948],\n",
      "        [0.7589],\n",
      "        [0.8689],\n",
      "        [0.8471],\n",
      "        [0.8850],\n",
      "        [0.6568],\n",
      "        [0.6817],\n",
      "        [0.6574],\n",
      "        [0.6811],\n",
      "        [0.8810],\n",
      "        [0.6806],\n",
      "        [0.7071],\n",
      "        [0.9794],\n",
      "        [0.9248],\n",
      "        [0.9006],\n",
      "        [0.8471],\n",
      "        [0.6288],\n",
      "        [0.6846],\n",
      "        [0.8216],\n",
      "        [0.7277],\n",
      "        [0.7081],\n",
      "        [0.6026],\n",
      "        [0.9890],\n",
      "        [0.7550],\n",
      "        [0.6322],\n",
      "        [0.8126],\n",
      "        [0.8213],\n",
      "        [0.6003],\n",
      "        [0.8552],\n",
      "        [0.8416],\n",
      "        [0.8473],\n",
      "        [0.6993],\n",
      "        [0.6176],\n",
      "        [0.8302],\n",
      "        [0.7958],\n",
      "        [0.7028],\n",
      "        [0.6578],\n",
      "        [0.8048],\n",
      "        [0.9046],\n",
      "        [0.5647],\n",
      "        [0.9043],\n",
      "        [0.6560],\n",
      "        [0.6448],\n",
      "        [0.6884],\n",
      "        [0.8793],\n",
      "        [0.7164],\n",
      "        [0.9070],\n",
      "        [0.6285],\n",
      "        [0.9946],\n",
      "        [0.9478],\n",
      "        [0.6880],\n",
      "        [0.8897],\n",
      "        [0.8695],\n",
      "        [0.8937],\n",
      "        [0.9233],\n",
      "        [0.6845],\n",
      "        [0.8770],\n",
      "        [0.8216],\n",
      "        [0.6429],\n",
      "        [0.6580],\n",
      "        [0.6828],\n",
      "        [0.8513],\n",
      "        [0.9548],\n",
      "        [0.9578],\n",
      "        [0.8296],\n",
      "        [0.8317],\n",
      "        [0.5953],\n",
      "        [1.0165],\n",
      "        [0.8278],\n",
      "        [0.5610],\n",
      "        [0.7853],\n",
      "        [0.6519],\n",
      "        [0.9102],\n",
      "        [0.6694],\n",
      "        [0.6312],\n",
      "        [0.8714],\n",
      "        [0.8422],\n",
      "        [0.8200],\n",
      "        [0.8441],\n",
      "        [0.6644],\n",
      "        [0.9239],\n",
      "        [0.8036],\n",
      "        [0.6427],\n",
      "        [0.8401],\n",
      "        [0.6837],\n",
      "        [0.6028],\n",
      "        [0.6575],\n",
      "        [0.8766],\n",
      "        [0.7623],\n",
      "        [0.6769],\n",
      "        [0.8944],\n",
      "        [0.7978],\n",
      "        [0.6483],\n",
      "        [0.8903],\n",
      "        [1.0242],\n",
      "        [0.8881],\n",
      "        [0.6237],\n",
      "        [0.6649],\n",
      "        [0.8783],\n",
      "        [0.8456],\n",
      "        [0.6347],\n",
      "        [0.5621],\n",
      "        [0.6074],\n",
      "        [0.8716],\n",
      "        [0.8435],\n",
      "        [0.8575],\n",
      "        [0.7995],\n",
      "        [0.9105],\n",
      "        [0.9338],\n",
      "        [0.7101],\n",
      "        [0.8882],\n",
      "        [0.8837],\n",
      "        [0.8177],\n",
      "        [0.5990],\n",
      "        [0.7405],\n",
      "        [0.8350],\n",
      "        [0.5324],\n",
      "        [0.6075]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0048],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0105],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0129],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0139],\n",
      "        [0.0148],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0178],\n",
      "        [0.0180],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0213],\n",
      "        [0.0217],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0238],\n",
      "        [0.0241],\n",
      "        [0.0247],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0279],\n",
      "        [0.0280],\n",
      "        [0.0281],\n",
      "        [0.0287],\n",
      "        [0.0292],\n",
      "        [0.0302],\n",
      "        [0.0311],\n",
      "        [0.0314],\n",
      "        [0.0321],\n",
      "        [0.0358],\n",
      "        [0.0358],\n",
      "        [0.0383],\n",
      "        [0.0390],\n",
      "        [0.0391],\n",
      "        [0.0393],\n",
      "        [0.0406],\n",
      "        [0.0409],\n",
      "        [0.0422],\n",
      "        [0.0422],\n",
      "        [0.0424],\n",
      "        [0.0440],\n",
      "        [0.0451],\n",
      "        [0.0460],\n",
      "        [0.0480],\n",
      "        [0.0483],\n",
      "        [0.0486],\n",
      "        [0.0528],\n",
      "        [0.0545],\n",
      "        [0.0550],\n",
      "        [0.0559],\n",
      "        [0.0574],\n",
      "        [0.0584],\n",
      "        [0.0639],\n",
      "        [0.0640],\n",
      "        [0.0642],\n",
      "        [0.0656],\n",
      "        [0.0662],\n",
      "        [0.0712],\n",
      "        [0.0716],\n",
      "        [0.0722],\n",
      "        [0.0724],\n",
      "        [0.0775],\n",
      "        [0.0888],\n",
      "        [0.0915],\n",
      "        [0.0961]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0020],\n",
      "        [0.0013],\n",
      "        [0.0018],\n",
      "        [0.0007],\n",
      "        [0.0039],\n",
      "        [0.0007],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0059],\n",
      "        [0.0063],\n",
      "        [0.0075],\n",
      "        [0.0079],\n",
      "        [0.0078],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0097],\n",
      "        [0.0080],\n",
      "        [0.0059],\n",
      "        [0.0103],\n",
      "        [0.0089],\n",
      "        [0.0105],\n",
      "        [0.0124],\n",
      "        [0.0122],\n",
      "        [0.0113],\n",
      "        [0.0107],\n",
      "        [0.0143],\n",
      "        [0.0140],\n",
      "        [0.0126],\n",
      "        [0.0132],\n",
      "        [0.0156],\n",
      "        [0.0132],\n",
      "        [0.0146],\n",
      "        [0.0143],\n",
      "        [0.0170],\n",
      "        [0.0171],\n",
      "        [0.0171],\n",
      "        [0.0153],\n",
      "        [0.0185],\n",
      "        [0.0168],\n",
      "        [0.0198],\n",
      "        [0.0182],\n",
      "        [0.0199],\n",
      "        [0.0202],\n",
      "        [0.0193],\n",
      "        [0.0215],\n",
      "        [0.0202],\n",
      "        [0.0201],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0223],\n",
      "        [0.0212],\n",
      "        [0.0210],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0196],\n",
      "        [0.0218],\n",
      "        [0.0209],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0230],\n",
      "        [0.0249],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0256],\n",
      "        [0.0220],\n",
      "        [0.0263],\n",
      "        [0.0261],\n",
      "        [0.0266],\n",
      "        [0.0294],\n",
      "        [0.0280],\n",
      "        [0.0299],\n",
      "        [0.0290],\n",
      "        [0.0311],\n",
      "        [0.0301],\n",
      "        [0.0310],\n",
      "        [0.0332],\n",
      "        [0.0347],\n",
      "        [0.0360],\n",
      "        [0.0369],\n",
      "        [0.0374],\n",
      "        [0.0404],\n",
      "        [0.0388],\n",
      "        [0.0385],\n",
      "        [0.0393],\n",
      "        [0.0419],\n",
      "        [0.0423],\n",
      "        [0.0436],\n",
      "        [0.0435],\n",
      "        [0.0459],\n",
      "        [0.0474],\n",
      "        [0.0494],\n",
      "        [0.0466],\n",
      "        [0.0484],\n",
      "        [0.0532],\n",
      "        [0.0556],\n",
      "        [0.0563],\n",
      "        [0.0530],\n",
      "        [0.0566],\n",
      "        [0.0572],\n",
      "        [0.0625],\n",
      "        [0.0641],\n",
      "        [0.0635],\n",
      "        [0.0649],\n",
      "        [0.0641],\n",
      "        [0.0720],\n",
      "        [0.0724],\n",
      "        [0.0697],\n",
      "        [0.0694],\n",
      "        [0.0758],\n",
      "        [0.0867],\n",
      "        [0.0927],\n",
      "        [0.0933]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.32642388343811\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 119\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.5156082251196494e-07, 62)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [62, 105, 53, 96, 23, 76, 90, 98, 52, 99, 97, 104, 13, 57, 75, 56, 49, 100, 34, 91, 12, 27, 87, 128, 93, 102, 26, 66, 21, 3, 123, 68, 32, 134, 131, 89, 50, 33, 137, 86, 63, 130, 109, 61, 17, 92, 15, 85, 88, 18, 101, 135, 30, 55, 112, 9, 16, 8, 36, 51, 94, 29, 74, 95, 14, 22, 25, 10, 127, 0, 124, 58, 132, 110, 46, 28, 103, 1, 136, 54, 64, 65, 138, 31, 111, 129, 77, 20, 113, 106, 19, 35, 48, 107, 11, 139, 2, 47, 133, 122, 24, 108, 126, 4, 44, 67, 6, 140, 37, 7, 121, 73, 59, 60, 78, 72, 125, 120, 141] 數值 torch.Size([119, 1])\n",
      "目前模型的Data狀態 torch.Size([119, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8840],\n",
      "        [0.6801],\n",
      "        [0.8688],\n",
      "        [0.6934],\n",
      "        [0.8473],\n",
      "        [0.7566],\n",
      "        [0.7055],\n",
      "        [0.6794],\n",
      "        [0.8809],\n",
      "        [0.6563],\n",
      "        [0.6800],\n",
      "        [0.6551],\n",
      "        [0.9794],\n",
      "        [0.9240],\n",
      "        [0.7522],\n",
      "        [0.9000],\n",
      "        [0.8462],\n",
      "        [0.6278],\n",
      "        [0.8223],\n",
      "        [0.6835],\n",
      "        [0.9893],\n",
      "        [0.8125],\n",
      "        [0.7259],\n",
      "        [0.6010],\n",
      "        [0.7062],\n",
      "        [0.6309],\n",
      "        [0.8210],\n",
      "        [0.8462],\n",
      "        [0.8420],\n",
      "        [0.8569],\n",
      "        [0.5981],\n",
      "        [0.8292],\n",
      "        [0.7963],\n",
      "        [0.6562],\n",
      "        [0.6166],\n",
      "        [0.6975],\n",
      "        [0.9036],\n",
      "        [0.8053],\n",
      "        [0.6439],\n",
      "        [0.7012],\n",
      "        [0.8785],\n",
      "        [0.5637],\n",
      "        [0.6550],\n",
      "        [0.9032],\n",
      "        [0.9071],\n",
      "        [0.6871],\n",
      "        [0.9481],\n",
      "        [0.6806],\n",
      "        [0.7145],\n",
      "        [0.8943],\n",
      "        [0.6275],\n",
      "        [0.6416],\n",
      "        [0.8697],\n",
      "        [0.8894],\n",
      "        [0.6565],\n",
      "        [0.9957],\n",
      "        [0.9235],\n",
      "        [0.9557],\n",
      "        [0.8215],\n",
      "        [0.8766],\n",
      "        [0.6862],\n",
      "        [0.8514],\n",
      "        [0.7826],\n",
      "        [0.6827],\n",
      "        [0.9575],\n",
      "        [0.8317],\n",
      "        [0.8297],\n",
      "        [1.0174],\n",
      "        [0.5938],\n",
      "        [0.8290],\n",
      "        [0.5595],\n",
      "        [0.9093],\n",
      "        [0.6505],\n",
      "        [0.6681],\n",
      "        [0.8713],\n",
      "        [0.8202],\n",
      "        [0.6297],\n",
      "        [0.8434],\n",
      "        [0.6634],\n",
      "        [0.9235],\n",
      "        [0.8433],\n",
      "        [0.8025],\n",
      "        [0.6417],\n",
      "        [0.8403],\n",
      "        [0.6823],\n",
      "        [0.6012],\n",
      "        [0.7603],\n",
      "        [0.8771],\n",
      "        [0.6753],\n",
      "        [0.6562],\n",
      "        [0.8948],\n",
      "        [0.7977],\n",
      "        [0.8898],\n",
      "        [0.6471],\n",
      "        [1.0250],\n",
      "        [0.6632],\n",
      "        [0.8895],\n",
      "        [0.8780],\n",
      "        [0.6223],\n",
      "        [0.6044],\n",
      "        [0.8461],\n",
      "        [0.6337],\n",
      "        [0.5608],\n",
      "        [0.8723],\n",
      "        [0.8447],\n",
      "        [0.8561],\n",
      "        [0.9113],\n",
      "        [0.7080],\n",
      "        [0.7994],\n",
      "        [0.9345],\n",
      "        [0.5959],\n",
      "        [0.8153],\n",
      "        [0.8874],\n",
      "        [0.8829],\n",
      "        [0.7388],\n",
      "        [0.8328],\n",
      "        [0.5311],\n",
      "        [0.6046],\n",
      "        [0.7270]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0007],\n",
      "        [0.0013],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0059],\n",
      "        [0.0059],\n",
      "        [0.0063],\n",
      "        [0.0075],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0089],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0103],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0122],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0140],\n",
      "        [0.0143],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0153],\n",
      "        [0.0156],\n",
      "        [0.0168],\n",
      "        [0.0170],\n",
      "        [0.0171],\n",
      "        [0.0171],\n",
      "        [0.0182],\n",
      "        [0.0185],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0218],\n",
      "        [0.0220],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0232],\n",
      "        [0.0247],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0256],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0280],\n",
      "        [0.0290],\n",
      "        [0.0294],\n",
      "        [0.0299],\n",
      "        [0.0301],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0332],\n",
      "        [0.0347],\n",
      "        [0.0360],\n",
      "        [0.0369],\n",
      "        [0.0374],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.0393],\n",
      "        [0.0404],\n",
      "        [0.0419],\n",
      "        [0.0423],\n",
      "        [0.0435],\n",
      "        [0.0436],\n",
      "        [0.0459],\n",
      "        [0.0466],\n",
      "        [0.0474],\n",
      "        [0.0484],\n",
      "        [0.0494],\n",
      "        [0.0530],\n",
      "        [0.0532],\n",
      "        [0.0556],\n",
      "        [0.0563],\n",
      "        [0.0566],\n",
      "        [0.0572],\n",
      "        [0.0625],\n",
      "        [0.0635],\n",
      "        [0.0641],\n",
      "        [0.0641],\n",
      "        [0.0649],\n",
      "        [0.0694],\n",
      "        [0.0697],\n",
      "        [0.0720],\n",
      "        [0.0724],\n",
      "        [0.0758],\n",
      "        [0.0867],\n",
      "        [0.0927],\n",
      "        [0.0933],\n",
      "        [0.0992]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0013],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0029],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0027],\n",
      "        [    0.0030],\n",
      "        [    0.0032],\n",
      "        [    0.0047],\n",
      "        [    0.0048],\n",
      "        [    0.0030],\n",
      "        [    0.0046],\n",
      "        [    0.0067],\n",
      "        [    0.0074],\n",
      "        [    0.0071],\n",
      "        [    0.0083],\n",
      "        [    0.0077],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0101],\n",
      "        [    0.0124],\n",
      "        [    0.0104],\n",
      "        [    0.0104],\n",
      "        [    0.0111],\n",
      "        [    0.0098],\n",
      "        [    0.0113],\n",
      "        [    0.0137],\n",
      "        [    0.0151],\n",
      "        [    0.0119],\n",
      "        [    0.0129],\n",
      "        [    0.0100],\n",
      "        [    0.0162],\n",
      "        [    0.0147],\n",
      "        [    0.0130],\n",
      "        [    0.0143],\n",
      "        [    0.0129],\n",
      "        [    0.0158],\n",
      "        [    0.0163],\n",
      "        [    0.0190],\n",
      "        [    0.0176],\n",
      "        [    0.0182],\n",
      "        [    0.0177],\n",
      "        [    0.0185],\n",
      "        [    0.0197],\n",
      "        [    0.0189],\n",
      "        [    0.0203],\n",
      "        [    0.0199],\n",
      "        [    0.0196],\n",
      "        [    0.0171],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0189],\n",
      "        [    0.0206],\n",
      "        [    0.0207],\n",
      "        [    0.0206],\n",
      "        [    0.0211],\n",
      "        [    0.0217],\n",
      "        [    0.0220],\n",
      "        [    0.0213],\n",
      "        [    0.0209],\n",
      "        [    0.0227],\n",
      "        [    0.0237],\n",
      "        [    0.0233],\n",
      "        [    0.0230],\n",
      "        [    0.0249],\n",
      "        [    0.0272],\n",
      "        [    0.0261],\n",
      "        [    0.0275],\n",
      "        [    0.0272],\n",
      "        [    0.0291],\n",
      "        [    0.0257],\n",
      "        [    0.0276],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0310],\n",
      "        [    0.0276],\n",
      "        [    0.0305],\n",
      "        [    0.0315],\n",
      "        [    0.0339],\n",
      "        [    0.0321],\n",
      "        [    0.0359],\n",
      "        [    0.0358],\n",
      "        [    0.0348],\n",
      "        [    0.0380],\n",
      "        [    0.0390],\n",
      "        [    0.0380],\n",
      "        [    0.0409],\n",
      "        [    0.0423],\n",
      "        [    0.0424],\n",
      "        [    0.0427],\n",
      "        [    0.0440],\n",
      "        [    0.0459],\n",
      "        [    0.0433],\n",
      "        [    0.0485],\n",
      "        [    0.0478],\n",
      "        [    0.0523],\n",
      "        [    0.0497],\n",
      "        [    0.0534],\n",
      "        [    0.0558],\n",
      "        [    0.0583],\n",
      "        [    0.0560],\n",
      "        [    0.0560],\n",
      "        [    0.0612],\n",
      "        [    0.0631],\n",
      "        [    0.0602],\n",
      "        [    0.0642],\n",
      "        [    0.0648],\n",
      "        [    0.0663],\n",
      "        [    0.0687],\n",
      "        [    0.0729],\n",
      "        [    0.0731],\n",
      "        [    0.0757],\n",
      "        [    0.0859],\n",
      "        [    0.0943],\n",
      "        [    0.0905],\n",
      "        [    0.0953]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.60657501220703\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 120\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.356870750958478e-10, 105)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [105, 62, 53, 96, 23, 90, 98, 52, 76, 57, 99, 97, 75, 104, 13, 56, 100, 49, 91, 34, 27, 12, 66, 134, 87, 102, 93, 26, 21, 68, 128, 137, 32, 50, 3, 33, 89, 123, 86, 131, 63, 135, 109, 17, 61, 92, 85, 112, 130, 101, 15, 18, 30, 88, 55, 8, 9, 16, 74, 36, 29, 51, 94, 95, 25, 22, 14, 10, 110, 0, 127, 58, 124, 136, 46, 132, 28, 103, 54, 1, 64, 138, 65, 129, 111, 31, 113, 77, 20, 106, 19, 35, 48, 139, 107, 11, 47, 2, 122, 133, 24, 108, 4, 44, 126, 140, 67, 6, 37, 7, 121, 73, 59, 60, 78, 72, 120, 125, 141, 83] 數值 torch.Size([120, 1])\n",
      "目前模型的Data狀態 torch.Size([120, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6795],\n",
      "        [0.8832],\n",
      "        [0.8687],\n",
      "        [0.6936],\n",
      "        [0.8471],\n",
      "        [0.7053],\n",
      "        [0.6797],\n",
      "        [0.8808],\n",
      "        [0.7557],\n",
      "        [0.9230],\n",
      "        [0.6567],\n",
      "        [0.6803],\n",
      "        [0.7509],\n",
      "        [0.6543],\n",
      "        [0.9786],\n",
      "        [0.8992],\n",
      "        [0.6282],\n",
      "        [0.8450],\n",
      "        [0.6837],\n",
      "        [0.8228],\n",
      "        [0.8121],\n",
      "        [0.9887],\n",
      "        [0.8453],\n",
      "        [0.6530],\n",
      "        [0.7254],\n",
      "        [0.6308],\n",
      "        [0.7056],\n",
      "        [0.8204],\n",
      "        [0.8420],\n",
      "        [0.8285],\n",
      "        [0.5984],\n",
      "        [0.6415],\n",
      "        [0.7965],\n",
      "        [0.9023],\n",
      "        [0.8584],\n",
      "        [0.8057],\n",
      "        [0.6970],\n",
      "        [0.5954],\n",
      "        [0.7010],\n",
      "        [0.6143],\n",
      "        [0.8780],\n",
      "        [0.6387],\n",
      "        [0.6545],\n",
      "        [0.9066],\n",
      "        [0.9021],\n",
      "        [0.6872],\n",
      "        [0.6799],\n",
      "        [0.6553],\n",
      "        [0.5617],\n",
      "        [0.6278],\n",
      "        [0.9477],\n",
      "        [0.8942],\n",
      "        [0.8695],\n",
      "        [0.7139],\n",
      "        [0.8892],\n",
      "        [0.9561],\n",
      "        [0.9961],\n",
      "        [0.9231],\n",
      "        [0.7815],\n",
      "        [0.8215],\n",
      "        [0.8509],\n",
      "        [0.8761],\n",
      "        [0.6857],\n",
      "        [0.6824],\n",
      "        [0.8295],\n",
      "        [0.8314],\n",
      "        [0.9563],\n",
      "        [1.0176],\n",
      "        [0.6672],\n",
      "        [0.8301],\n",
      "        [0.5916],\n",
      "        [0.9082],\n",
      "        [0.5576],\n",
      "        [0.6609],\n",
      "        [0.8709],\n",
      "        [0.6476],\n",
      "        [0.8199],\n",
      "        [0.6293],\n",
      "        [0.9231],\n",
      "        [0.8446],\n",
      "        [0.8428],\n",
      "        [0.6391],\n",
      "        [0.8018],\n",
      "        [0.5986],\n",
      "        [0.6811],\n",
      "        [0.8401],\n",
      "        [0.6740],\n",
      "        [0.7598],\n",
      "        [0.8769],\n",
      "        [0.6557],\n",
      "        [0.8944],\n",
      "        [0.7976],\n",
      "        [0.8889],\n",
      "        [0.6598],\n",
      "        [0.6467],\n",
      "        [1.0249],\n",
      "        [0.8775],\n",
      "        [0.8907],\n",
      "        [0.6011],\n",
      "        [0.6194],\n",
      "        [0.8462],\n",
      "        [0.6334],\n",
      "        [0.8729],\n",
      "        [0.8458],\n",
      "        [0.5588],\n",
      "        [0.7042],\n",
      "        [0.8548],\n",
      "        [0.9116],\n",
      "        [0.7993],\n",
      "        [0.9346],\n",
      "        [0.5929],\n",
      "        [0.8142],\n",
      "        [0.8865],\n",
      "        [0.8821],\n",
      "        [0.7387],\n",
      "        [0.8320],\n",
      "        [0.6019],\n",
      "        [0.5295],\n",
      "        [0.7231],\n",
      "        [0.7528]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0013],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0027],\n",
      "        [    0.0029],\n",
      "        [    0.0030],\n",
      "        [    0.0030],\n",
      "        [    0.0032],\n",
      "        [    0.0046],\n",
      "        [    0.0047],\n",
      "        [    0.0048],\n",
      "        [    0.0067],\n",
      "        [    0.0071],\n",
      "        [    0.0074],\n",
      "        [    0.0077],\n",
      "        [    0.0083],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0104],\n",
      "        [    0.0104],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0119],\n",
      "        [    0.0124],\n",
      "        [    0.0129],\n",
      "        [    0.0129],\n",
      "        [    0.0130],\n",
      "        [    0.0137],\n",
      "        [    0.0143],\n",
      "        [    0.0147],\n",
      "        [    0.0151],\n",
      "        [    0.0158],\n",
      "        [    0.0162],\n",
      "        [    0.0163],\n",
      "        [    0.0171],\n",
      "        [    0.0176],\n",
      "        [    0.0177],\n",
      "        [    0.0182],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0189],\n",
      "        [    0.0190],\n",
      "        [    0.0196],\n",
      "        [    0.0197],\n",
      "        [    0.0199],\n",
      "        [    0.0199],\n",
      "        [    0.0203],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0206],\n",
      "        [    0.0207],\n",
      "        [    0.0209],\n",
      "        [    0.0211],\n",
      "        [    0.0213],\n",
      "        [    0.0217],\n",
      "        [    0.0220],\n",
      "        [    0.0227],\n",
      "        [    0.0230],\n",
      "        [    0.0233],\n",
      "        [    0.0237],\n",
      "        [    0.0249],\n",
      "        [    0.0257],\n",
      "        [    0.0261],\n",
      "        [    0.0272],\n",
      "        [    0.0272],\n",
      "        [    0.0275],\n",
      "        [    0.0276],\n",
      "        [    0.0276],\n",
      "        [    0.0291],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0305],\n",
      "        [    0.0310],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0339],\n",
      "        [    0.0348],\n",
      "        [    0.0358],\n",
      "        [    0.0359],\n",
      "        [    0.0380],\n",
      "        [    0.0380],\n",
      "        [    0.0390],\n",
      "        [    0.0409],\n",
      "        [    0.0423],\n",
      "        [    0.0424],\n",
      "        [    0.0427],\n",
      "        [    0.0433],\n",
      "        [    0.0440],\n",
      "        [    0.0459],\n",
      "        [    0.0478],\n",
      "        [    0.0485],\n",
      "        [    0.0497],\n",
      "        [    0.0523],\n",
      "        [    0.0534],\n",
      "        [    0.0558],\n",
      "        [    0.0560],\n",
      "        [    0.0560],\n",
      "        [    0.0583],\n",
      "        [    0.0602],\n",
      "        [    0.0612],\n",
      "        [    0.0631],\n",
      "        [    0.0642],\n",
      "        [    0.0648],\n",
      "        [    0.0663],\n",
      "        [    0.0687],\n",
      "        [    0.0729],\n",
      "        [    0.0731],\n",
      "        [    0.0757],\n",
      "        [    0.0859],\n",
      "        [    0.0905],\n",
      "        [    0.0943],\n",
      "        [    0.0953],\n",
      "        [    0.0975]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0028],\n",
      "        [0.0015],\n",
      "        [0.0033],\n",
      "        [0.0012],\n",
      "        [0.0028],\n",
      "        [0.0013],\n",
      "        [0.0042],\n",
      "        [0.0035],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0050],\n",
      "        [0.0043],\n",
      "        [0.0058],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0104],\n",
      "        [0.0099],\n",
      "        [0.0079],\n",
      "        [0.0101],\n",
      "        [0.0094],\n",
      "        [0.0114],\n",
      "        [0.0101],\n",
      "        [0.0112],\n",
      "        [0.0099],\n",
      "        [0.0096],\n",
      "        [0.0121],\n",
      "        [0.0127],\n",
      "        [0.0133],\n",
      "        [0.0110],\n",
      "        [0.0134],\n",
      "        [0.0161],\n",
      "        [0.0124],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0168],\n",
      "        [0.0159],\n",
      "        [0.0172],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0189],\n",
      "        [0.0180],\n",
      "        [0.0186],\n",
      "        [0.0176],\n",
      "        [0.0189],\n",
      "        [0.0184],\n",
      "        [0.0192],\n",
      "        [0.0186],\n",
      "        [0.0182],\n",
      "        [0.0211],\n",
      "        [0.0217],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0220],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0224],\n",
      "        [0.0206],\n",
      "        [0.0226],\n",
      "        [0.0231],\n",
      "        [0.0245],\n",
      "        [0.0218],\n",
      "        [0.0233],\n",
      "        [0.0260],\n",
      "        [0.0258],\n",
      "        [0.0281],\n",
      "        [0.0272],\n",
      "        [0.0268],\n",
      "        [0.0274],\n",
      "        [0.0277],\n",
      "        [0.0292],\n",
      "        [0.0294],\n",
      "        [0.0279],\n",
      "        [0.0297],\n",
      "        [0.0316],\n",
      "        [0.0331],\n",
      "        [0.0305],\n",
      "        [0.0323],\n",
      "        [0.0331],\n",
      "        [0.0346],\n",
      "        [0.0357],\n",
      "        [0.0372],\n",
      "        [0.0377],\n",
      "        [0.0371],\n",
      "        [0.0375],\n",
      "        [0.0409],\n",
      "        [0.0409],\n",
      "        [0.0410],\n",
      "        [0.0437],\n",
      "        [0.0427],\n",
      "        [0.0437],\n",
      "        [0.0468],\n",
      "        [0.0493],\n",
      "        [0.0506],\n",
      "        [0.0482],\n",
      "        [0.0525],\n",
      "        [0.0553],\n",
      "        [0.0553],\n",
      "        [0.0547],\n",
      "        [0.0530],\n",
      "        [0.0582],\n",
      "        [0.0591],\n",
      "        [0.0609],\n",
      "        [0.0621],\n",
      "        [0.0628],\n",
      "        [0.0638],\n",
      "        [0.0648],\n",
      "        [0.0674],\n",
      "        [0.0724],\n",
      "        [0.0725],\n",
      "        [0.0753],\n",
      "        [0.0850],\n",
      "        [0.0892],\n",
      "        [0.0939],\n",
      "        [0.0940],\n",
      "        [0.0946]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.886427879333496\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 121\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.596576002631082e-08, 105)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [105, 62, 90, 52, 96, 99, 98, 53, 97, 75, 23, 57, 76, 13, 104, 56, 100, 49, 91, 12, 134, 21, 27, 26, 66, 102, 34, 32, 93, 87, 68, 33, 128, 137, 50, 123, 89, 131, 3, 86, 135, 63, 109, 85, 61, 18, 130, 15, 92, 17, 112, 55, 101, 8, 16, 74, 36, 51, 30, 88, 22, 9, 29, 94, 95, 14, 25, 110, 10, 58, 127, 124, 136, 28, 0, 46, 132, 103, 64, 54, 138, 65, 1, 129, 111, 77, 31, 20, 113, 106, 19, 35, 139, 48, 107, 11, 122, 47, 2, 133, 44, 4, 24, 108, 126, 140, 67, 6, 37, 7, 121, 73, 59, 60, 78, 72, 120, 125, 141, 83, 114] 數值 torch.Size([121, 1])\n",
      "目前模型的Data狀態 torch.Size([121, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6792],\n",
      "        [0.8837],\n",
      "        [0.7046],\n",
      "        [0.8822],\n",
      "        [0.6935],\n",
      "        [0.6571],\n",
      "        [0.6799],\n",
      "        [0.8702],\n",
      "        [0.6805],\n",
      "        [0.7494],\n",
      "        [0.8487],\n",
      "        [0.9235],\n",
      "        [0.7544],\n",
      "        [0.9791],\n",
      "        [0.6540],\n",
      "        [0.9002],\n",
      "        [0.6288],\n",
      "        [0.8455],\n",
      "        [0.6837],\n",
      "        [0.9894],\n",
      "        [0.6524],\n",
      "        [0.8437],\n",
      "        [0.8135],\n",
      "        [0.8216],\n",
      "        [0.8456],\n",
      "        [0.6310],\n",
      "        [0.8249],\n",
      "        [0.7984],\n",
      "        [0.7049],\n",
      "        [0.7241],\n",
      "        [0.8288],\n",
      "        [0.8076],\n",
      "        [0.5981],\n",
      "        [0.6420],\n",
      "        [0.9026],\n",
      "        [0.5947],\n",
      "        [0.6959],\n",
      "        [0.6146],\n",
      "        [0.8608],\n",
      "        [0.7000],\n",
      "        [0.6385],\n",
      "        [0.8789],\n",
      "        [0.6548],\n",
      "        [0.6787],\n",
      "        [0.9023],\n",
      "        [0.8959],\n",
      "        [0.5623],\n",
      "        [0.9487],\n",
      "        [0.6870],\n",
      "        [0.9078],\n",
      "        [0.6553],\n",
      "        [0.8906],\n",
      "        [0.6283],\n",
      "        [0.9573],\n",
      "        [0.9244],\n",
      "        [0.7801],\n",
      "        [0.8228],\n",
      "        [0.8773],\n",
      "        [0.8707],\n",
      "        [0.7126],\n",
      "        [0.8330],\n",
      "        [0.9975],\n",
      "        [0.8519],\n",
      "        [0.6852],\n",
      "        [0.6819],\n",
      "        [0.9568],\n",
      "        [0.8311],\n",
      "        [0.6674],\n",
      "        [1.0187],\n",
      "        [0.9087],\n",
      "        [0.5915],\n",
      "        [0.5577],\n",
      "        [0.6611],\n",
      "        [0.8213],\n",
      "        [0.8320],\n",
      "        [0.8725],\n",
      "        [0.6474],\n",
      "        [0.6295],\n",
      "        [0.8439],\n",
      "        [0.9241],\n",
      "        [0.6393],\n",
      "        [0.8026],\n",
      "        [0.8467],\n",
      "        [0.5983],\n",
      "        [0.6811],\n",
      "        [0.7589],\n",
      "        [0.8415],\n",
      "        [0.8784],\n",
      "        [0.6737],\n",
      "        [0.6557],\n",
      "        [0.8957],\n",
      "        [0.7990],\n",
      "        [0.6593],\n",
      "        [0.8900],\n",
      "        [0.6470],\n",
      "        [1.0259],\n",
      "        [0.5996],\n",
      "        [0.8789],\n",
      "        [0.8927],\n",
      "        [0.6192],\n",
      "        [0.8488],\n",
      "        [0.8743],\n",
      "        [0.8481],\n",
      "        [0.6339],\n",
      "        [0.5589],\n",
      "        [0.7030],\n",
      "        [0.8545],\n",
      "        [0.9127],\n",
      "        [0.8007],\n",
      "        [0.9356],\n",
      "        [0.5914],\n",
      "        [0.8129],\n",
      "        [0.8870],\n",
      "        [0.8827],\n",
      "        [0.7384],\n",
      "        [0.8312],\n",
      "        [0.6005],\n",
      "        [0.5300],\n",
      "        [0.7218],\n",
      "        [0.7499],\n",
      "        [0.6891]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0015],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0050],\n",
      "        [0.0058],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0110],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0121],\n",
      "        [0.0124],\n",
      "        [0.0127],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0226],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0245],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0268],\n",
      "        [0.0272],\n",
      "        [0.0274],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0292],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0331],\n",
      "        [0.0331],\n",
      "        [0.0346],\n",
      "        [0.0357],\n",
      "        [0.0371],\n",
      "        [0.0372],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0409],\n",
      "        [0.0409],\n",
      "        [0.0410],\n",
      "        [0.0427],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0468],\n",
      "        [0.0482],\n",
      "        [0.0493],\n",
      "        [0.0506],\n",
      "        [0.0525],\n",
      "        [0.0530],\n",
      "        [0.0547],\n",
      "        [0.0553],\n",
      "        [0.0553],\n",
      "        [0.0582],\n",
      "        [0.0591],\n",
      "        [0.0609],\n",
      "        [0.0621],\n",
      "        [0.0628],\n",
      "        [0.0638],\n",
      "        [0.0648],\n",
      "        [0.0674],\n",
      "        [0.0724],\n",
      "        [0.0725],\n",
      "        [0.0753],\n",
      "        [0.0850],\n",
      "        [0.0892],\n",
      "        [0.0939],\n",
      "        [0.0940],\n",
      "        [0.0946],\n",
      "        [0.1003]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0030],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0023],\n",
      "        [0.0039],\n",
      "        [0.0046],\n",
      "        [0.0006],\n",
      "        [0.0018],\n",
      "        [0.0051],\n",
      "        [0.0001],\n",
      "        [0.0024],\n",
      "        [0.0015],\n",
      "        [0.0073],\n",
      "        [0.0062],\n",
      "        [0.0077],\n",
      "        [0.0073],\n",
      "        [0.0084],\n",
      "        [0.0088],\n",
      "        [0.0100],\n",
      "        [0.0097],\n",
      "        [0.0064],\n",
      "        [0.0106],\n",
      "        [0.0087],\n",
      "        [0.0113],\n",
      "        [0.0080],\n",
      "        [0.0124],\n",
      "        [0.0099],\n",
      "        [0.0114],\n",
      "        [0.0142],\n",
      "        [0.0149],\n",
      "        [0.0100],\n",
      "        [0.0128],\n",
      "        [0.0152],\n",
      "        [0.0112],\n",
      "        [0.0113],\n",
      "        [0.0186],\n",
      "        [0.0190],\n",
      "        [0.0181],\n",
      "        [0.0159],\n",
      "        [0.0198],\n",
      "        [0.0143],\n",
      "        [0.0157],\n",
      "        [0.0196],\n",
      "        [0.0144],\n",
      "        [0.0203],\n",
      "        [0.0191],\n",
      "        [0.0202],\n",
      "        [0.0200],\n",
      "        [0.0212],\n",
      "        [0.0176],\n",
      "        [0.0159],\n",
      "        [0.0201],\n",
      "        [0.0211],\n",
      "        [0.0207],\n",
      "        [0.0206],\n",
      "        [0.0163],\n",
      "        [0.0208],\n",
      "        [0.0219],\n",
      "        [0.0198],\n",
      "        [0.0251],\n",
      "        [0.0229],\n",
      "        [0.0207],\n",
      "        [0.0210],\n",
      "        [0.0254],\n",
      "        [0.0258],\n",
      "        [0.0253],\n",
      "        [0.0234],\n",
      "        [0.0231],\n",
      "        [0.0245],\n",
      "        [0.0288],\n",
      "        [0.0295],\n",
      "        [0.0294],\n",
      "        [0.0254],\n",
      "        [0.0291],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0320],\n",
      "        [0.0320],\n",
      "        [0.0319],\n",
      "        [0.0301],\n",
      "        [0.0300],\n",
      "        [0.0347],\n",
      "        [0.0329],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0344],\n",
      "        [0.0362],\n",
      "        [0.0386],\n",
      "        [0.0345],\n",
      "        [0.0434],\n",
      "        [0.0422],\n",
      "        [0.0419],\n",
      "        [0.0396],\n",
      "        [0.0423],\n",
      "        [0.0461],\n",
      "        [0.0451],\n",
      "        [0.0444],\n",
      "        [0.0483],\n",
      "        [0.0502],\n",
      "        [0.0551],\n",
      "        [0.0526],\n",
      "        [0.0559],\n",
      "        [0.0546],\n",
      "        [0.0575],\n",
      "        [0.0603],\n",
      "        [0.0554],\n",
      "        [0.0583],\n",
      "        [0.0634],\n",
      "        [0.0637],\n",
      "        [0.0653],\n",
      "        [0.0609],\n",
      "        [0.0643],\n",
      "        [0.0743],\n",
      "        [0.0744],\n",
      "        [0.0731],\n",
      "        [0.0823],\n",
      "        [0.0854],\n",
      "        [0.0956],\n",
      "        [0.0902],\n",
      "        [0.0897],\n",
      "        [0.0970]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.167080879211426\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 122\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6483582498949545e-08, 75)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [75, 98, 57, 62, 90, 53, 52, 23, 105, 96, 99, 97, 13, 134, 76, 56, 104, 66, 100, 27, 49, 12, 34, 68, 91, 21, 137, 50, 26, 32, 102, 33, 93, 135, 85, 87, 128, 63, 3, 112, 74, 17, 131, 123, 89, 18, 109, 30, 86, 15, 55, 130, 61, 16, 9, 8, 36, 29, 101, 92, 51, 22, 110, 25, 10, 88, 14, 136, 94, 95, 0, 46, 58, 28, 124, 127, 138, 54, 64, 103, 132, 129, 111, 1, 77, 113, 65, 31, 20, 139, 35, 19, 48, 106, 122, 11, 107, 47, 2, 44, 24, 133, 140, 4, 108, 67, 126, 121, 6, 37, 73, 7, 78, 59, 60, 72, 120, 83, 141, 125, 114, 84] 數值 torch.Size([122, 1])\n",
      "目前模型的Data狀態 torch.Size([122, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7462],\n",
      "        [0.6778],\n",
      "        [0.9214],\n",
      "        [0.8818],\n",
      "        [0.7017],\n",
      "        [0.8692],\n",
      "        [0.8812],\n",
      "        [0.8478],\n",
      "        [0.6764],\n",
      "        [0.6912],\n",
      "        [0.6551],\n",
      "        [0.6784],\n",
      "        [0.9771],\n",
      "        [0.6494],\n",
      "        [0.7513],\n",
      "        [0.8986],\n",
      "        [0.6513],\n",
      "        [0.8435],\n",
      "        [0.6269],\n",
      "        [0.8124],\n",
      "        [0.8437],\n",
      "        [0.9875],\n",
      "        [0.8244],\n",
      "        [0.8266],\n",
      "        [0.6814],\n",
      "        [0.8427],\n",
      "        [0.6399],\n",
      "        [0.9005],\n",
      "        [0.8202],\n",
      "        [0.7981],\n",
      "        [0.6288],\n",
      "        [0.8071],\n",
      "        [0.7019],\n",
      "        [0.6359],\n",
      "        [0.6754],\n",
      "        [0.7206],\n",
      "        [0.5956],\n",
      "        [0.8773],\n",
      "        [0.8606],\n",
      "        [0.6523],\n",
      "        [0.7769],\n",
      "        [0.9065],\n",
      "        [0.6125],\n",
      "        [0.5919],\n",
      "        [0.6928],\n",
      "        [0.8950],\n",
      "        [0.6524],\n",
      "        [0.8694],\n",
      "        [0.6969],\n",
      "        [0.9473],\n",
      "        [0.8895],\n",
      "        [0.5604],\n",
      "        [0.9000],\n",
      "        [0.9232],\n",
      "        [0.9962],\n",
      "        [0.9560],\n",
      "        [0.8218],\n",
      "        [0.8505],\n",
      "        [0.6264],\n",
      "        [0.6845],\n",
      "        [0.8759],\n",
      "        [0.8319],\n",
      "        [0.6647],\n",
      "        [0.8300],\n",
      "        [1.0172],\n",
      "        [0.7091],\n",
      "        [0.9548],\n",
      "        [0.6588],\n",
      "        [0.6823],\n",
      "        [0.6792],\n",
      "        [0.8316],\n",
      "        [0.8716],\n",
      "        [0.9067],\n",
      "        [0.8201],\n",
      "        [0.5557],\n",
      "        [0.5893],\n",
      "        [0.6369],\n",
      "        [0.9227],\n",
      "        [0.8424],\n",
      "        [0.6272],\n",
      "        [0.6447],\n",
      "        [0.5959],\n",
      "        [0.6782],\n",
      "        [0.8464],\n",
      "        [0.7561],\n",
      "        [0.6705],\n",
      "        [0.8009],\n",
      "        [0.8405],\n",
      "        [0.8773],\n",
      "        [0.6562],\n",
      "        [0.7981],\n",
      "        [0.8945],\n",
      "        [0.8886],\n",
      "        [0.6532],\n",
      "        [0.5958],\n",
      "        [1.0242],\n",
      "        [0.6446],\n",
      "        [0.8779],\n",
      "        [0.8924],\n",
      "        [0.8493],\n",
      "        [0.8474],\n",
      "        [0.6166],\n",
      "        [0.6994],\n",
      "        [0.8731],\n",
      "        [0.6317],\n",
      "        [0.8519],\n",
      "        [0.5568],\n",
      "        [0.5875],\n",
      "        [0.9114],\n",
      "        [0.7998],\n",
      "        [0.8098],\n",
      "        [0.9341],\n",
      "        [0.7361],\n",
      "        [0.8851],\n",
      "        [0.8808],\n",
      "        [0.8285],\n",
      "        [0.5967],\n",
      "        [0.7450],\n",
      "        [0.7180],\n",
      "        [0.5282],\n",
      "        [0.6859],\n",
      "        [0.7232]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0001],\n",
      "        [0.0006],\n",
      "        [0.0015],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0018],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0084],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0100],\n",
      "        [0.0106],\n",
      "        [0.0112],\n",
      "        [0.0113],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0124],\n",
      "        [0.0128],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0157],\n",
      "        [0.0159],\n",
      "        [0.0159],\n",
      "        [0.0163],\n",
      "        [0.0176],\n",
      "        [0.0181],\n",
      "        [0.0186],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0212],\n",
      "        [0.0219],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0234],\n",
      "        [0.0245],\n",
      "        [0.0251],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0258],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0294],\n",
      "        [0.0295],\n",
      "        [0.0300],\n",
      "        [0.0301],\n",
      "        [0.0319],\n",
      "        [0.0320],\n",
      "        [0.0320],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0329],\n",
      "        [0.0344],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0362],\n",
      "        [0.0386],\n",
      "        [0.0396],\n",
      "        [0.0419],\n",
      "        [0.0422],\n",
      "        [0.0423],\n",
      "        [0.0434],\n",
      "        [0.0444],\n",
      "        [0.0451],\n",
      "        [0.0461],\n",
      "        [0.0483],\n",
      "        [0.0502],\n",
      "        [0.0526],\n",
      "        [0.0546],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0559],\n",
      "        [0.0575],\n",
      "        [0.0583],\n",
      "        [0.0603],\n",
      "        [0.0609],\n",
      "        [0.0634],\n",
      "        [0.0637],\n",
      "        [0.0643],\n",
      "        [0.0653],\n",
      "        [0.0731],\n",
      "        [0.0743],\n",
      "        [0.0744],\n",
      "        [0.0823],\n",
      "        [0.0854],\n",
      "        [0.0897],\n",
      "        [0.0902],\n",
      "        [0.0956],\n",
      "        [0.0970],\n",
      "        [0.0996]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0031],\n",
      "        [0.0009],\n",
      "        [0.0008],\n",
      "        [0.0022],\n",
      "        [0.0042],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0029],\n",
      "        [0.0050],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0067],\n",
      "        [0.0070],\n",
      "        [0.0052],\n",
      "        [0.0101],\n",
      "        [0.0075],\n",
      "        [0.0095],\n",
      "        [0.0071],\n",
      "        [0.0096],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0088],\n",
      "        [0.0119],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0104],\n",
      "        [0.0137],\n",
      "        [0.0120],\n",
      "        [0.0166],\n",
      "        [0.0136],\n",
      "        [0.0113],\n",
      "        [0.0183],\n",
      "        [0.0159],\n",
      "        [0.0154],\n",
      "        [0.0166],\n",
      "        [0.0143],\n",
      "        [0.0135],\n",
      "        [0.0177],\n",
      "        [0.0184],\n",
      "        [0.0200],\n",
      "        [0.0220],\n",
      "        [0.0186],\n",
      "        [0.0210],\n",
      "        [0.0198],\n",
      "        [0.0228],\n",
      "        [0.0202],\n",
      "        [0.0199],\n",
      "        [0.0202],\n",
      "        [0.0212],\n",
      "        [0.0205],\n",
      "        [0.0203],\n",
      "        [0.0213],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0222],\n",
      "        [0.0231],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0216],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0285],\n",
      "        [0.0259],\n",
      "        [0.0250],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0290],\n",
      "        [0.0294],\n",
      "        [0.0288],\n",
      "        [0.0299],\n",
      "        [0.0301],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0320],\n",
      "        [0.0333],\n",
      "        [0.0328],\n",
      "        [0.0314],\n",
      "        [0.0311],\n",
      "        [0.0333],\n",
      "        [0.0320],\n",
      "        [0.0326],\n",
      "        [0.0349],\n",
      "        [0.0365],\n",
      "        [0.0383],\n",
      "        [0.0386],\n",
      "        [0.0414],\n",
      "        [0.0419],\n",
      "        [0.0425],\n",
      "        [0.0450],\n",
      "        [0.0419],\n",
      "        [0.0444],\n",
      "        [0.0475],\n",
      "        [0.0488],\n",
      "        [0.0506],\n",
      "        [0.0506],\n",
      "        [0.0553],\n",
      "        [0.0557],\n",
      "        [0.0538],\n",
      "        [0.0563],\n",
      "        [0.0587],\n",
      "        [0.0568],\n",
      "        [0.0609],\n",
      "        [0.0584],\n",
      "        [0.0640],\n",
      "        [0.0633],\n",
      "        [0.0617],\n",
      "        [0.0659],\n",
      "        [0.0712],\n",
      "        [0.0749],\n",
      "        [0.0750],\n",
      "        [0.0801],\n",
      "        [0.0830],\n",
      "        [0.0851],\n",
      "        [0.0883],\n",
      "        [0.0958],\n",
      "        [0.0950],\n",
      "        [0.0956]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.44587779045105\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 123\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.981937647447921e-07, 57)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [57, 98, 53, 52, 62, 23, 75, 90, 105, 134, 96, 99, 97, 13, 66, 56, 68, 27, 49, 104, 100, 76, 50, 21, 12, 32, 34, 137, 26, 85, 91, 33, 74, 135, 102, 112, 63, 128, 3, 93, 17, 87, 131, 18, 30, 55, 123, 130, 15, 9, 16, 36, 29, 109, 61, 8, 110, 89, 51, 101, 22, 86, 92, 25, 10, 136, 14, 94, 95, 0, 88, 28, 46, 58, 138, 54, 124, 127, 111, 129, 64, 77, 113, 132, 103, 1, 65, 31, 20, 139, 35, 122, 19, 48, 11, 106, 107, 47, 2, 44, 140, 24, 133, 4, 67, 121, 108, 126, 73, 37, 6, 7, 78, 59, 60, 72, 120, 83, 141, 114, 84, 125, 5] 數值 torch.Size([123, 1])\n",
      "目前模型的Data狀態 torch.Size([123, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9207],\n",
      "        [0.6762],\n",
      "        [0.8694],\n",
      "        [0.8814],\n",
      "        [0.8812],\n",
      "        [0.8484],\n",
      "        [0.7432],\n",
      "        [0.6991],\n",
      "        [0.6745],\n",
      "        [0.6482],\n",
      "        [0.6892],\n",
      "        [0.6537],\n",
      "        [0.6768],\n",
      "        [0.9764],\n",
      "        [0.8426],\n",
      "        [0.8984],\n",
      "        [0.8255],\n",
      "        [0.8127],\n",
      "        [0.8432],\n",
      "        [0.6494],\n",
      "        [0.6258],\n",
      "        [0.7485],\n",
      "        [0.8994],\n",
      "        [0.8431],\n",
      "        [0.9869],\n",
      "        [0.7990],\n",
      "        [0.8252],\n",
      "        [0.6398],\n",
      "        [0.8203],\n",
      "        [0.6724],\n",
      "        [0.6795],\n",
      "        [0.8080],\n",
      "        [0.7741],\n",
      "        [0.6351],\n",
      "        [0.6274],\n",
      "        [0.6507],\n",
      "        [0.8770],\n",
      "        [0.5948],\n",
      "        [0.8612],\n",
      "        [0.6995],\n",
      "        [0.9066],\n",
      "        [0.7172],\n",
      "        [0.6122],\n",
      "        [0.8955],\n",
      "        [0.8694],\n",
      "        [0.8897],\n",
      "        [0.5905],\n",
      "        [0.5605],\n",
      "        [0.9471],\n",
      "        [0.9959],\n",
      "        [0.9233],\n",
      "        [0.8220],\n",
      "        [0.8504],\n",
      "        [0.6510],\n",
      "        [0.8990],\n",
      "        [0.9554],\n",
      "        [0.6631],\n",
      "        [0.6897],\n",
      "        [0.8758],\n",
      "        [0.6252],\n",
      "        [0.8323],\n",
      "        [0.6940],\n",
      "        [0.6825],\n",
      "        [0.8303],\n",
      "        [1.0167],\n",
      "        [0.6583],\n",
      "        [0.9541],\n",
      "        [0.6801],\n",
      "        [0.6771],\n",
      "        [0.8320],\n",
      "        [0.7057],\n",
      "        [0.8204],\n",
      "        [0.8722],\n",
      "        [0.9060],\n",
      "        [0.6366],\n",
      "        [0.9223],\n",
      "        [0.5552],\n",
      "        [0.5886],\n",
      "        [0.6765],\n",
      "        [0.5952],\n",
      "        [0.8424],\n",
      "        [0.7538],\n",
      "        [0.6686],\n",
      "        [0.6440],\n",
      "        [0.6258],\n",
      "        [0.8469],\n",
      "        [0.8007],\n",
      "        [0.8407],\n",
      "        [0.8776],\n",
      "        [0.6551],\n",
      "        [0.7986],\n",
      "        [0.5934],\n",
      "        [0.8947],\n",
      "        [0.8887],\n",
      "        [1.0234],\n",
      "        [0.6516],\n",
      "        [0.6432],\n",
      "        [0.8785],\n",
      "        [0.8927],\n",
      "        [0.8512],\n",
      "        [0.6977],\n",
      "        [0.8481],\n",
      "        [0.6160],\n",
      "        [0.8727],\n",
      "        [0.8504],\n",
      "        [0.5849],\n",
      "        [0.6305],\n",
      "        [0.5561],\n",
      "        [0.8072],\n",
      "        [0.8002],\n",
      "        [0.9108],\n",
      "        [0.9335],\n",
      "        [0.7342],\n",
      "        [0.8845],\n",
      "        [0.8802],\n",
      "        [0.8263],\n",
      "        [0.5944],\n",
      "        [0.7404],\n",
      "        [0.7161],\n",
      "        [0.6839],\n",
      "        [0.7192],\n",
      "        [0.5281],\n",
      "        [0.8699]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0009],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0029],\n",
      "        [0.0031],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0067],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0104],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0120],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0143],\n",
      "        [0.0154],\n",
      "        [0.0159],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0177],\n",
      "        [0.0183],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0225],\n",
      "        [0.0228],\n",
      "        [0.0231],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0250],\n",
      "        [0.0259],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0285],\n",
      "        [0.0288],\n",
      "        [0.0290],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0301],\n",
      "        [0.0311],\n",
      "        [0.0314],\n",
      "        [0.0320],\n",
      "        [0.0320],\n",
      "        [0.0326],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0333],\n",
      "        [0.0349],\n",
      "        [0.0365],\n",
      "        [0.0383],\n",
      "        [0.0386],\n",
      "        [0.0414],\n",
      "        [0.0419],\n",
      "        [0.0419],\n",
      "        [0.0425],\n",
      "        [0.0444],\n",
      "        [0.0450],\n",
      "        [0.0475],\n",
      "        [0.0488],\n",
      "        [0.0506],\n",
      "        [0.0506],\n",
      "        [0.0538],\n",
      "        [0.0553],\n",
      "        [0.0557],\n",
      "        [0.0563],\n",
      "        [0.0568],\n",
      "        [0.0584],\n",
      "        [0.0587],\n",
      "        [0.0609],\n",
      "        [0.0617],\n",
      "        [0.0633],\n",
      "        [0.0640],\n",
      "        [0.0659],\n",
      "        [0.0712],\n",
      "        [0.0749],\n",
      "        [0.0750],\n",
      "        [0.0801],\n",
      "        [0.0830],\n",
      "        [0.0851],\n",
      "        [0.0883],\n",
      "        [0.0950],\n",
      "        [0.0956],\n",
      "        [0.0958],\n",
      "        [0.1122]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0014],\n",
      "        [0.0005],\n",
      "        [0.0034],\n",
      "        [0.0008],\n",
      "        [0.0016],\n",
      "        [0.0049],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0054],\n",
      "        [0.0063],\n",
      "        [0.0054],\n",
      "        [0.0080],\n",
      "        [0.0065],\n",
      "        [0.0095],\n",
      "        [0.0105],\n",
      "        [0.0085],\n",
      "        [0.0095],\n",
      "        [0.0089],\n",
      "        [0.0108],\n",
      "        [0.0104],\n",
      "        [0.0082],\n",
      "        [0.0086],\n",
      "        [0.0085],\n",
      "        [0.0125],\n",
      "        [0.0121],\n",
      "        [0.0099],\n",
      "        [0.0108],\n",
      "        [0.0116],\n",
      "        [0.0101],\n",
      "        [0.0126],\n",
      "        [0.0139],\n",
      "        [0.0133],\n",
      "        [0.0140],\n",
      "        [0.0163],\n",
      "        [0.0155],\n",
      "        [0.0201],\n",
      "        [0.0170],\n",
      "        [0.0197],\n",
      "        [0.0192],\n",
      "        [0.0174],\n",
      "        [0.0164],\n",
      "        [0.0209],\n",
      "        [0.0186],\n",
      "        [0.0200],\n",
      "        [0.0191],\n",
      "        [0.0185],\n",
      "        [0.0225],\n",
      "        [0.0185],\n",
      "        [0.0196],\n",
      "        [0.0220],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0193],\n",
      "        [0.0216],\n",
      "        [0.0227],\n",
      "        [0.0209],\n",
      "        [0.0216],\n",
      "        [0.0207],\n",
      "        [0.0232],\n",
      "        [0.0231],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0256],\n",
      "        [0.0246],\n",
      "        [0.0278],\n",
      "        [0.0280],\n",
      "        [0.0316],\n",
      "        [0.0294],\n",
      "        [0.0274],\n",
      "        [0.0306],\n",
      "        [0.0287],\n",
      "        [0.0303],\n",
      "        [0.0306],\n",
      "        [0.0290],\n",
      "        [0.0296],\n",
      "        [0.0308],\n",
      "        [0.0318],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0323],\n",
      "        [0.0329],\n",
      "        [0.0369],\n",
      "        [0.0338],\n",
      "        [0.0377],\n",
      "        [0.0363],\n",
      "        [0.0385],\n",
      "        [0.0402],\n",
      "        [0.0410],\n",
      "        [0.0401],\n",
      "        [0.0436],\n",
      "        [0.0461],\n",
      "        [0.0449],\n",
      "        [0.0474],\n",
      "        [0.0503],\n",
      "        [0.0540],\n",
      "        [0.0480],\n",
      "        [0.0532],\n",
      "        [0.0574],\n",
      "        [0.0552],\n",
      "        [0.0537],\n",
      "        [0.0572],\n",
      "        [0.0571],\n",
      "        [0.0584],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0623],\n",
      "        [0.0618],\n",
      "        [0.0639],\n",
      "        [0.0712],\n",
      "        [0.0741],\n",
      "        [0.0743],\n",
      "        [0.0798],\n",
      "        [0.0818],\n",
      "        [0.0831],\n",
      "        [0.0876],\n",
      "        [0.0944],\n",
      "        [0.0942],\n",
      "        [0.0947],\n",
      "        [0.1101]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.724385738372803\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 124\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1937354688361665e-07, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 52, 57, 62, 53, 75, 90, 23, 105, 134, 99, 13, 96, 97, 56, 66, 21, 32, 49, 12, 100, 68, 104, 26, 33, 50, 27, 76, 85, 91, 137, 34, 74, 102, 135, 112, 128, 63, 18, 93, 131, 16, 15, 55, 130, 87, 8, 36, 17, 123, 3, 22, 109, 30, 51, 61, 101, 110, 29, 9, 89, 92, 86, 14, 25, 136, 10, 28, 94, 95, 58, 124, 88, 127, 138, 46, 54, 111, 64, 0, 77, 129, 113, 132, 103, 65, 20, 1, 31, 139, 19, 35, 122, 48, 106, 11, 107, 44, 47, 140, 4, 2, 133, 121, 67, 24, 108, 126, 73, 6, 37, 7, 78, 59, 60, 72, 120, 83, 141, 84, 114, 125, 5, 142] 數值 torch.Size([124, 1])\n",
      "目前模型的Data狀態 torch.Size([124, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6767],\n",
      "        [0.8826],\n",
      "        [0.9214],\n",
      "        [0.8817],\n",
      "        [0.8708],\n",
      "        [0.7424],\n",
      "        [0.6989],\n",
      "        [0.8503],\n",
      "        [0.6744],\n",
      "        [0.6482],\n",
      "        [0.6543],\n",
      "        [0.9779],\n",
      "        [0.6894],\n",
      "        [0.6773],\n",
      "        [0.8994],\n",
      "        [0.8435],\n",
      "        [0.8452],\n",
      "        [0.8009],\n",
      "        [0.8439],\n",
      "        [0.9887],\n",
      "        [0.6265],\n",
      "        [0.8262],\n",
      "        [0.6495],\n",
      "        [0.8216],\n",
      "        [0.8098],\n",
      "        [0.8997],\n",
      "        [0.8141],\n",
      "        [0.7478],\n",
      "        [0.6719],\n",
      "        [0.6798],\n",
      "        [0.6407],\n",
      "        [0.8270],\n",
      "        [0.7732],\n",
      "        [0.6278],\n",
      "        [0.6355],\n",
      "        [0.6504],\n",
      "        [0.5952],\n",
      "        [0.8779],\n",
      "        [0.8977],\n",
      "        [0.6991],\n",
      "        [0.6132],\n",
      "        [0.9253],\n",
      "        [0.9488],\n",
      "        [0.8910],\n",
      "        [0.5616],\n",
      "        [0.7163],\n",
      "        [0.9573],\n",
      "        [0.8229],\n",
      "        [0.9086],\n",
      "        [0.5905],\n",
      "        [0.8648],\n",
      "        [0.8341],\n",
      "        [0.6512],\n",
      "        [0.8705],\n",
      "        [0.8769],\n",
      "        [0.8993],\n",
      "        [0.6259],\n",
      "        [0.6631],\n",
      "        [0.8515],\n",
      "        [0.9980],\n",
      "        [0.6891],\n",
      "        [0.6825],\n",
      "        [0.6935],\n",
      "        [0.9554],\n",
      "        [0.8321],\n",
      "        [0.6589],\n",
      "        [1.0186],\n",
      "        [0.8218],\n",
      "        [0.6799],\n",
      "        [0.6770],\n",
      "        [0.9067],\n",
      "        [0.5561],\n",
      "        [0.7049],\n",
      "        [0.5891],\n",
      "        [0.6373],\n",
      "        [0.8738],\n",
      "        [0.9232],\n",
      "        [0.6762],\n",
      "        [0.8435],\n",
      "        [0.8356],\n",
      "        [0.7534],\n",
      "        [0.5956],\n",
      "        [0.6680],\n",
      "        [0.6444],\n",
      "        [0.6263],\n",
      "        [0.8018],\n",
      "        [0.8796],\n",
      "        [0.8505],\n",
      "        [0.8420],\n",
      "        [0.6551],\n",
      "        [0.8966],\n",
      "        [0.7998],\n",
      "        [0.5925],\n",
      "        [0.8899],\n",
      "        [0.6517],\n",
      "        [1.0251],\n",
      "        [0.6434],\n",
      "        [0.8538],\n",
      "        [0.8799],\n",
      "        [0.6972],\n",
      "        [0.8752],\n",
      "        [0.8961],\n",
      "        [0.6165],\n",
      "        [0.5836],\n",
      "        [0.8508],\n",
      "        [0.8503],\n",
      "        [0.6309],\n",
      "        [0.5568],\n",
      "        [0.8065],\n",
      "        [0.9129],\n",
      "        [0.8012],\n",
      "        [0.9355],\n",
      "        [0.7342],\n",
      "        [0.8852],\n",
      "        [0.8810],\n",
      "        [0.8259],\n",
      "        [0.5932],\n",
      "        [0.7384],\n",
      "        [0.7154],\n",
      "        [0.7178],\n",
      "        [0.6833],\n",
      "        [0.5291],\n",
      "        [0.8720],\n",
      "        [0.7275]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0034],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0089],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0116],\n",
      "        [0.0121],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0140],\n",
      "        [0.0155],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0170],\n",
      "        [0.0174],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0186],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0201],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0216],\n",
      "        [0.0216],\n",
      "        [0.0220],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0246],\n",
      "        [0.0256],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0274],\n",
      "        [0.0278],\n",
      "        [0.0280],\n",
      "        [0.0287],\n",
      "        [0.0290],\n",
      "        [0.0294],\n",
      "        [0.0296],\n",
      "        [0.0303],\n",
      "        [0.0306],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0320],\n",
      "        [0.0323],\n",
      "        [0.0329],\n",
      "        [0.0338],\n",
      "        [0.0363],\n",
      "        [0.0369],\n",
      "        [0.0377],\n",
      "        [0.0385],\n",
      "        [0.0401],\n",
      "        [0.0402],\n",
      "        [0.0410],\n",
      "        [0.0436],\n",
      "        [0.0449],\n",
      "        [0.0461],\n",
      "        [0.0474],\n",
      "        [0.0480],\n",
      "        [0.0503],\n",
      "        [0.0532],\n",
      "        [0.0537],\n",
      "        [0.0540],\n",
      "        [0.0552],\n",
      "        [0.0571],\n",
      "        [0.0572],\n",
      "        [0.0574],\n",
      "        [0.0584],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0618],\n",
      "        [0.0623],\n",
      "        [0.0639],\n",
      "        [0.0712],\n",
      "        [0.0741],\n",
      "        [0.0743],\n",
      "        [0.0798],\n",
      "        [0.0818],\n",
      "        [0.0831],\n",
      "        [0.0876],\n",
      "        [0.0942],\n",
      "        [0.0944],\n",
      "        [0.0947],\n",
      "        [0.1101],\n",
      "        [0.1202]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0003],\n",
      "        [0.0016],\n",
      "        [0.0011],\n",
      "        [0.0047],\n",
      "        [0.0032],\n",
      "        [0.0036],\n",
      "        [0.0063],\n",
      "        [0.0047],\n",
      "        [0.0029],\n",
      "        [0.0038],\n",
      "        [0.0048],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0058],\n",
      "        [0.0089],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0082],\n",
      "        [0.0077],\n",
      "        [0.0072],\n",
      "        [0.0104],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0086],\n",
      "        [0.0104],\n",
      "        [0.0113],\n",
      "        [0.0103],\n",
      "        [0.0115],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0140],\n",
      "        [0.0132],\n",
      "        [0.0122],\n",
      "        [0.0119],\n",
      "        [0.0134],\n",
      "        [0.0170],\n",
      "        [0.0173],\n",
      "        [0.0149],\n",
      "        [0.0165],\n",
      "        [0.0184],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0197],\n",
      "        [0.0189],\n",
      "        [0.0180],\n",
      "        [0.0189],\n",
      "        [0.0208],\n",
      "        [0.0213],\n",
      "        [0.0233],\n",
      "        [0.0195],\n",
      "        [0.0206],\n",
      "        [0.0214],\n",
      "        [0.0201],\n",
      "        [0.0209],\n",
      "        [0.0201],\n",
      "        [0.0215],\n",
      "        [0.0223],\n",
      "        [0.0240],\n",
      "        [0.0222],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0242],\n",
      "        [0.0268],\n",
      "        [0.0238],\n",
      "        [0.0270],\n",
      "        [0.0268],\n",
      "        [0.0271],\n",
      "        [0.0269],\n",
      "        [0.0285],\n",
      "        [0.0295],\n",
      "        [0.0291],\n",
      "        [0.0307],\n",
      "        [0.0286],\n",
      "        [0.0317],\n",
      "        [0.0313],\n",
      "        [0.0303],\n",
      "        [0.0296],\n",
      "        [0.0349],\n",
      "        [0.0326],\n",
      "        [0.0303],\n",
      "        [0.0312],\n",
      "        [0.0340],\n",
      "        [0.0318],\n",
      "        [0.0326],\n",
      "        [0.0352],\n",
      "        [0.0402],\n",
      "        [0.0385],\n",
      "        [0.0361],\n",
      "        [0.0391],\n",
      "        [0.0392],\n",
      "        [0.0390],\n",
      "        [0.0443],\n",
      "        [0.0444],\n",
      "        [0.0470],\n",
      "        [0.0468],\n",
      "        [0.0456],\n",
      "        [0.0512],\n",
      "        [0.0502],\n",
      "        [0.0516],\n",
      "        [0.0570],\n",
      "        [0.0570],\n",
      "        [0.0550],\n",
      "        [0.0576],\n",
      "        [0.0590],\n",
      "        [0.0577],\n",
      "        [0.0611],\n",
      "        [0.0616],\n",
      "        [0.0602],\n",
      "        [0.0614],\n",
      "        [0.0626],\n",
      "        [0.0727],\n",
      "        [0.0736],\n",
      "        [0.0737],\n",
      "        [0.0806],\n",
      "        [0.0801],\n",
      "        [0.0820],\n",
      "        [0.0843],\n",
      "        [0.0939],\n",
      "        [0.0935],\n",
      "        [0.0949],\n",
      "        [0.1084],\n",
      "        [0.1164]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.001484632492065\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 125\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.005687845979992e-08, 52)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [52, 62, 98, 57, 134, 75, 90, 99, 96, 97, 53, 105, 13, 56, 23, 21, 32, 100, 12, 49, 33, 66, 104, 26, 76, 91, 68, 50, 137, 27, 85, 135, 102, 74, 112, 34, 18, 93, 128, 16, 63, 55, 15, 8, 131, 36, 87, 22, 130, 101, 51, 109, 17, 61, 123, 30, 110, 92, 89, 29, 86, 3, 136, 9, 14, 28, 25, 95, 10, 94, 58, 138, 88, 124, 64, 129, 111, 127, 113, 54, 46, 103, 77, 65, 132, 0, 20, 139, 31, 122, 19, 35, 1, 48, 106, 44, 107, 11, 140, 47, 4, 121, 133, 2, 67, 108, 24, 6, 126, 37, 73, 7, 78, 59, 60, 120, 72, 83, 141, 114, 84, 125, 5, 142, 43] 數值 torch.Size([125, 1])\n",
      "目前模型的Data狀態 torch.Size([125, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8838],\n",
      "        [0.8822],\n",
      "        [0.6783],\n",
      "        [0.9216],\n",
      "        [0.6459],\n",
      "        [0.7431],\n",
      "        [0.6997],\n",
      "        [0.6560],\n",
      "        [0.6911],\n",
      "        [0.6790],\n",
      "        [0.8721],\n",
      "        [0.6747],\n",
      "        [0.9785],\n",
      "        [0.9001],\n",
      "        [0.8517],\n",
      "        [0.8466],\n",
      "        [0.8024],\n",
      "        [0.6282],\n",
      "        [0.9895],\n",
      "        [0.8442],\n",
      "        [0.8114],\n",
      "        [0.8444],\n",
      "        [0.6499],\n",
      "        [0.8224],\n",
      "        [0.7483],\n",
      "        [0.6811],\n",
      "        [0.8270],\n",
      "        [0.8996],\n",
      "        [0.6392],\n",
      "        [0.8149],\n",
      "        [0.6726],\n",
      "        [0.6334],\n",
      "        [0.6289],\n",
      "        [0.7738],\n",
      "        [0.6498],\n",
      "        [0.8285],\n",
      "        [0.8992],\n",
      "        [0.6996],\n",
      "        [0.5937],\n",
      "        [0.9266],\n",
      "        [0.8790],\n",
      "        [0.8922],\n",
      "        [0.9499],\n",
      "        [0.9587],\n",
      "        [0.6121],\n",
      "        [0.8237],\n",
      "        [0.7166],\n",
      "        [0.8353],\n",
      "        [0.5609],\n",
      "        [0.6274],\n",
      "        [0.8777],\n",
      "        [0.6514],\n",
      "        [0.9098],\n",
      "        [0.8994],\n",
      "        [0.5892],\n",
      "        [0.8710],\n",
      "        [0.6630],\n",
      "        [0.6836],\n",
      "        [0.6896],\n",
      "        [0.8519],\n",
      "        [0.6942],\n",
      "        [0.8680],\n",
      "        [0.6571],\n",
      "        [0.9995],\n",
      "        [0.9558],\n",
      "        [0.8224],\n",
      "        [0.8333],\n",
      "        [0.6781],\n",
      "        [1.0198],\n",
      "        [0.6807],\n",
      "        [0.9070],\n",
      "        [0.6356],\n",
      "        [0.7051],\n",
      "        [0.5556],\n",
      "        [0.8447],\n",
      "        [0.5940],\n",
      "        [0.6757],\n",
      "        [0.5880],\n",
      "        [0.6672],\n",
      "        [0.9239],\n",
      "        [0.8749],\n",
      "        [0.6274],\n",
      "        [0.7544],\n",
      "        [0.8030],\n",
      "        [0.6427],\n",
      "        [0.8388],\n",
      "        [0.8807],\n",
      "        [0.6527],\n",
      "        [0.8427],\n",
      "        [0.5904],\n",
      "        [0.8976],\n",
      "        [0.8008],\n",
      "        [0.8537],\n",
      "        [0.8905],\n",
      "        [0.6522],\n",
      "        [0.8562],\n",
      "        [0.6439],\n",
      "        [1.0261],\n",
      "        [0.6941],\n",
      "        [0.8809],\n",
      "        [0.8774],\n",
      "        [0.5815],\n",
      "        [0.6148],\n",
      "        [0.8991],\n",
      "        [0.8512],\n",
      "        [0.6316],\n",
      "        [0.8519],\n",
      "        [0.9145],\n",
      "        [0.5560],\n",
      "        [0.8021],\n",
      "        [0.8072],\n",
      "        [0.9368],\n",
      "        [0.7357],\n",
      "        [0.8857],\n",
      "        [0.8815],\n",
      "        [0.5914],\n",
      "        [0.8268],\n",
      "        [0.7373],\n",
      "        [0.7121],\n",
      "        [0.6824],\n",
      "        [0.7175],\n",
      "        [0.5289],\n",
      "        [0.8737],\n",
      "        [0.7236],\n",
      "        [0.8330]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0016],\n",
      "        [0.0029],\n",
      "        [0.0032],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0047],\n",
      "        [0.0047],\n",
      "        [0.0048],\n",
      "        [0.0058],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0086],\n",
      "        [0.0089],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0103],\n",
      "        [0.0103],\n",
      "        [0.0104],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0119],\n",
      "        [0.0122],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0140],\n",
      "        [0.0149],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0233],\n",
      "        [0.0238],\n",
      "        [0.0240],\n",
      "        [0.0242],\n",
      "        [0.0268],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0270],\n",
      "        [0.0271],\n",
      "        [0.0285],\n",
      "        [0.0286],\n",
      "        [0.0291],\n",
      "        [0.0295],\n",
      "        [0.0296],\n",
      "        [0.0303],\n",
      "        [0.0303],\n",
      "        [0.0307],\n",
      "        [0.0312],\n",
      "        [0.0313],\n",
      "        [0.0317],\n",
      "        [0.0318],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0340],\n",
      "        [0.0349],\n",
      "        [0.0352],\n",
      "        [0.0361],\n",
      "        [0.0385],\n",
      "        [0.0390],\n",
      "        [0.0391],\n",
      "        [0.0392],\n",
      "        [0.0402],\n",
      "        [0.0443],\n",
      "        [0.0444],\n",
      "        [0.0456],\n",
      "        [0.0468],\n",
      "        [0.0470],\n",
      "        [0.0502],\n",
      "        [0.0512],\n",
      "        [0.0516],\n",
      "        [0.0550],\n",
      "        [0.0570],\n",
      "        [0.0570],\n",
      "        [0.0576],\n",
      "        [0.0577],\n",
      "        [0.0590],\n",
      "        [0.0602],\n",
      "        [0.0611],\n",
      "        [0.0614],\n",
      "        [0.0616],\n",
      "        [0.0626],\n",
      "        [0.0727],\n",
      "        [0.0736],\n",
      "        [0.0737],\n",
      "        [0.0801],\n",
      "        [0.0806],\n",
      "        [0.0820],\n",
      "        [0.0843],\n",
      "        [0.0935],\n",
      "        [0.0939],\n",
      "        [0.0949],\n",
      "        [0.1084],\n",
      "        [0.1164],\n",
      "        [0.1191]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0014],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0014],\n",
      "        [0.0005],\n",
      "        [0.0038],\n",
      "        [0.0042],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0041],\n",
      "        [0.0060],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0053],\n",
      "        [0.0073],\n",
      "        [0.0061],\n",
      "        [0.0055],\n",
      "        [0.0068],\n",
      "        [0.0084],\n",
      "        [0.0080],\n",
      "        [0.0070],\n",
      "        [0.0088],\n",
      "        [0.0098],\n",
      "        [0.0088],\n",
      "        [0.0110],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0093],\n",
      "        [0.0119],\n",
      "        [0.0106],\n",
      "        [0.0099],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0123],\n",
      "        [0.0157],\n",
      "        [0.0142],\n",
      "        [0.0173],\n",
      "        [0.0185],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0162],\n",
      "        [0.0173],\n",
      "        [0.0180],\n",
      "        [0.0196],\n",
      "        [0.0177],\n",
      "        [0.0203],\n",
      "        [0.0188],\n",
      "        [0.0205],\n",
      "        [0.0198],\n",
      "        [0.0193],\n",
      "        [0.0211],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0228],\n",
      "        [0.0219],\n",
      "        [0.0206],\n",
      "        [0.0223],\n",
      "        [0.0232],\n",
      "        [0.0226],\n",
      "        [0.0234],\n",
      "        [0.0248],\n",
      "        [0.0222],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0263],\n",
      "        [0.0275],\n",
      "        [0.0271],\n",
      "        [0.0269],\n",
      "        [0.0277],\n",
      "        [0.0287],\n",
      "        [0.0271],\n",
      "        [0.0304],\n",
      "        [0.0301],\n",
      "        [0.0288],\n",
      "        [0.0287],\n",
      "        [0.0292],\n",
      "        [0.0320],\n",
      "        [0.0297],\n",
      "        [0.0321],\n",
      "        [0.0327],\n",
      "        [0.0319],\n",
      "        [0.0324],\n",
      "        [0.0321],\n",
      "        [0.0359],\n",
      "        [0.0360],\n",
      "        [0.0349],\n",
      "        [0.0337],\n",
      "        [0.0393],\n",
      "        [0.0366],\n",
      "        [0.0389],\n",
      "        [0.0380],\n",
      "        [0.0415],\n",
      "        [0.0448],\n",
      "        [0.0448],\n",
      "        [0.0428],\n",
      "        [0.0472],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0523],\n",
      "        [0.0512],\n",
      "        [0.0527],\n",
      "        [0.0587],\n",
      "        [0.0582],\n",
      "        [0.0568],\n",
      "        [0.0579],\n",
      "        [0.0602],\n",
      "        [0.0600],\n",
      "        [0.0621],\n",
      "        [0.0601],\n",
      "        [0.0613],\n",
      "        [0.0626],\n",
      "        [0.0730],\n",
      "        [0.0736],\n",
      "        [0.0736],\n",
      "        [0.0780],\n",
      "        [0.0806],\n",
      "        [0.0791],\n",
      "        [0.0810],\n",
      "        [0.0920],\n",
      "        [0.0919],\n",
      "        [0.0953],\n",
      "        [0.1085],\n",
      "        [0.1124],\n",
      "        [0.1159]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.28001093864441\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 126\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.522935744764254e-07, 134)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [134, 62, 52, 57, 98, 99, 96, 75, 97, 90, 56, 13, 105, 32, 53, 21, 100, 33, 23, 49, 12, 26, 66, 137, 104, 135, 50, 68, 91, 85, 76, 27, 112, 102, 74, 18, 34, 55, 16, 93, 15, 36, 8, 63, 128, 22, 51, 131, 101, 87, 130, 110, 17, 109, 61, 30, 136, 92, 29, 123, 89, 86, 9, 3, 14, 28, 10, 138, 95, 25, 94, 129, 58, 64, 111, 113, 124, 88, 103, 127, 65, 54, 77, 46, 139, 20, 132, 0, 122, 35, 19, 31, 1, 44, 106, 48, 11, 140, 107, 4, 47, 121, 67, 108, 2, 133, 6, 37, 24, 73, 126, 7, 78, 60, 59, 120, 83, 72, 141, 84, 114, 125, 5, 142, 43, 69] 數值 torch.Size([126, 1])\n",
      "目前模型的Data狀態 torch.Size([126, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6435],\n",
      "        [0.8824],\n",
      "        [0.8849],\n",
      "        [0.9214],\n",
      "        [0.6786],\n",
      "        [0.6563],\n",
      "        [0.6913],\n",
      "        [0.7425],\n",
      "        [0.6794],\n",
      "        [0.6991],\n",
      "        [0.9006],\n",
      "        [0.9779],\n",
      "        [0.6740],\n",
      "        [0.8039],\n",
      "        [0.8734],\n",
      "        [0.8472],\n",
      "        [0.6286],\n",
      "        [0.8130],\n",
      "        [0.8527],\n",
      "        [0.8444],\n",
      "        [0.9889],\n",
      "        [0.8227],\n",
      "        [0.8443],\n",
      "        [0.6380],\n",
      "        [0.6492],\n",
      "        [0.6314],\n",
      "        [0.8994],\n",
      "        [0.8268],\n",
      "        [0.6812],\n",
      "        [0.6716],\n",
      "        [0.7476],\n",
      "        [0.8156],\n",
      "        [0.6487],\n",
      "        [0.6289],\n",
      "        [0.7733],\n",
      "        [0.8999],\n",
      "        [0.8301],\n",
      "        [0.8934],\n",
      "        [0.9269],\n",
      "        [0.6988],\n",
      "        [0.9500],\n",
      "        [0.8248],\n",
      "        [0.9587],\n",
      "        [0.8797],\n",
      "        [0.5922],\n",
      "        [0.8360],\n",
      "        [0.8785],\n",
      "        [0.6109],\n",
      "        [0.6277],\n",
      "        [0.7152],\n",
      "        [0.5602],\n",
      "        [0.6621],\n",
      "        [0.9099],\n",
      "        [0.6509],\n",
      "        [0.8991],\n",
      "        [0.8715],\n",
      "        [0.6555],\n",
      "        [0.6834],\n",
      "        [0.8521],\n",
      "        [0.5877],\n",
      "        [0.6886],\n",
      "        [0.6934],\n",
      "        [0.9997],\n",
      "        [0.8695],\n",
      "        [0.9551],\n",
      "        [0.8229],\n",
      "        [1.0196],\n",
      "        [0.6341],\n",
      "        [0.6779],\n",
      "        [0.8340],\n",
      "        [0.6801],\n",
      "        [0.5925],\n",
      "        [0.9068],\n",
      "        [0.8456],\n",
      "        [0.6746],\n",
      "        [0.6657],\n",
      "        [0.5550],\n",
      "        [0.7038],\n",
      "        [0.6272],\n",
      "        [0.5868],\n",
      "        [0.8036],\n",
      "        [0.9246],\n",
      "        [0.7541],\n",
      "        [0.8760],\n",
      "        [0.6503],\n",
      "        [0.8810],\n",
      "        [0.6409],\n",
      "        [0.8400],\n",
      "        [0.5880],\n",
      "        [0.8020],\n",
      "        [0.8978],\n",
      "        [0.8436],\n",
      "        [0.8551],\n",
      "        [0.8591],\n",
      "        [0.6518],\n",
      "        [0.8911],\n",
      "        [1.0256],\n",
      "        [0.6911],\n",
      "        [0.6435],\n",
      "        [0.8778],\n",
      "        [0.8819],\n",
      "        [0.5792],\n",
      "        [0.8504],\n",
      "        [0.6314],\n",
      "        [0.9004],\n",
      "        [0.6130],\n",
      "        [0.9147],\n",
      "        [0.8034],\n",
      "        [0.8530],\n",
      "        [0.8068],\n",
      "        [0.5550],\n",
      "        [0.9368],\n",
      "        [0.7360],\n",
      "        [0.8816],\n",
      "        [0.8857],\n",
      "        [0.5893],\n",
      "        [0.7344],\n",
      "        [0.8267],\n",
      "        [0.7088],\n",
      "        [0.7154],\n",
      "        [0.6809],\n",
      "        [0.5286],\n",
      "        [0.8737],\n",
      "        [0.7197],\n",
      "        [0.8363],\n",
      "        [0.8270]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0084],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0106],\n",
      "        [0.0110],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0142],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0169],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0185],\n",
      "        [0.0188],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0211],\n",
      "        [0.0219],\n",
      "        [0.0222],\n",
      "        [0.0223],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0232],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0263],\n",
      "        [0.0269],\n",
      "        [0.0271],\n",
      "        [0.0271],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0288],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0301],\n",
      "        [0.0304],\n",
      "        [0.0319],\n",
      "        [0.0320],\n",
      "        [0.0321],\n",
      "        [0.0321],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0337],\n",
      "        [0.0349],\n",
      "        [0.0359],\n",
      "        [0.0360],\n",
      "        [0.0366],\n",
      "        [0.0380],\n",
      "        [0.0389],\n",
      "        [0.0393],\n",
      "        [0.0415],\n",
      "        [0.0428],\n",
      "        [0.0448],\n",
      "        [0.0448],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0472],\n",
      "        [0.0512],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0568],\n",
      "        [0.0579],\n",
      "        [0.0582],\n",
      "        [0.0587],\n",
      "        [0.0600],\n",
      "        [0.0601],\n",
      "        [0.0602],\n",
      "        [0.0613],\n",
      "        [0.0621],\n",
      "        [0.0626],\n",
      "        [0.0730],\n",
      "        [0.0736],\n",
      "        [0.0736],\n",
      "        [0.0780],\n",
      "        [0.0791],\n",
      "        [0.0806],\n",
      "        [0.0810],\n",
      "        [0.0919],\n",
      "        [0.0920],\n",
      "        [0.0953],\n",
      "        [0.1085],\n",
      "        [0.1124],\n",
      "        [0.1159],\n",
      "        [0.1232]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0010],\n",
      "        [0.0024],\n",
      "        [0.0009],\n",
      "        [0.0025],\n",
      "        [0.0023],\n",
      "        [0.0029],\n",
      "        [0.0044],\n",
      "        [0.0030],\n",
      "        [0.0040],\n",
      "        [0.0050],\n",
      "        [0.0063],\n",
      "        [0.0051],\n",
      "        [0.0038],\n",
      "        [0.0071],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0052],\n",
      "        [0.0083],\n",
      "        [0.0080],\n",
      "        [0.0092],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0104],\n",
      "        [0.0094],\n",
      "        [0.0103],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0093],\n",
      "        [0.0106],\n",
      "        [0.0115],\n",
      "        [0.0128],\n",
      "        [0.0126],\n",
      "        [0.0113],\n",
      "        [0.0121],\n",
      "        [0.0137],\n",
      "        [0.0175],\n",
      "        [0.0153],\n",
      "        [0.0167],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0166],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0185],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0207],\n",
      "        [0.0188],\n",
      "        [0.0212],\n",
      "        [0.0210],\n",
      "        [0.0201],\n",
      "        [0.0217],\n",
      "        [0.0226],\n",
      "        [0.0230],\n",
      "        [0.0216],\n",
      "        [0.0231],\n",
      "        [0.0219],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0244],\n",
      "        [0.0265],\n",
      "        [0.0256],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0278],\n",
      "        [0.0266],\n",
      "        [0.0283],\n",
      "        [0.0274],\n",
      "        [0.0295],\n",
      "        [0.0291],\n",
      "        [0.0282],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0283],\n",
      "        [0.0308],\n",
      "        [0.0310],\n",
      "        [0.0308],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0323],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0373],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0386],\n",
      "        [0.0404],\n",
      "        [0.0430],\n",
      "        [0.0404],\n",
      "        [0.0440],\n",
      "        [0.0451],\n",
      "        [0.0459],\n",
      "        [0.0463],\n",
      "        [0.0462],\n",
      "        [0.0506],\n",
      "        [0.0530],\n",
      "        [0.0524],\n",
      "        [0.0560],\n",
      "        [0.0566],\n",
      "        [0.0595],\n",
      "        [0.0580],\n",
      "        [0.0596],\n",
      "        [0.0588],\n",
      "        [0.0614],\n",
      "        [0.0607],\n",
      "        [0.0607],\n",
      "        [0.0623],\n",
      "        [0.0735],\n",
      "        [0.0737],\n",
      "        [0.0738],\n",
      "        [0.0778],\n",
      "        [0.0769],\n",
      "        [0.0802],\n",
      "        [0.0799],\n",
      "        [0.0907],\n",
      "        [0.0920],\n",
      "        [0.0933],\n",
      "        [0.1083],\n",
      "        [0.1107],\n",
      "        [0.1130],\n",
      "        [0.1231]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.557061195373535\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 127\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.3427483003833913e-07, 134)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [134, 57, 62, 99, 52, 98, 96, 97, 32, 90, 75, 56, 105, 33, 21, 100, 13, 53, 49, 26, 23, 66, 12, 91, 104, 50, 68, 135, 137, 85, 102, 76, 74, 112, 27, 18, 55, 36, 16, 93, 15, 34, 8, 128, 22, 63, 131, 101, 51, 130, 109, 87, 17, 110, 92, 61, 123, 30, 136, 29, 89, 86, 9, 28, 14, 3, 10, 95, 94, 138, 64, 25, 124, 58, 111, 129, 113, 88, 127, 103, 65, 77, 54, 46, 139, 20, 132, 122, 35, 0, 19, 31, 44, 1, 106, 48, 11, 107, 140, 4, 121, 47, 67, 108, 133, 37, 2, 6, 73, 126, 24, 7, 78, 60, 59, 83, 120, 141, 72, 84, 114, 125, 5, 142, 43, 69, 115] 數值 torch.Size([127, 1])\n",
      "目前模型的Data狀態 torch.Size([127, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6435],\n",
      "        [0.9209],\n",
      "        [0.8823],\n",
      "        [0.6575],\n",
      "        [0.8859],\n",
      "        [0.6797],\n",
      "        [0.6922],\n",
      "        [0.6805],\n",
      "        [0.8057],\n",
      "        [0.6994],\n",
      "        [0.7419],\n",
      "        [0.9009],\n",
      "        [0.6744],\n",
      "        [0.8147],\n",
      "        [0.8480],\n",
      "        [0.6299],\n",
      "        [0.9771],\n",
      "        [0.8745],\n",
      "        [0.8444],\n",
      "        [0.8233],\n",
      "        [0.8537],\n",
      "        [0.8440],\n",
      "        [0.9881],\n",
      "        [0.6822],\n",
      "        [0.6496],\n",
      "        [0.8991],\n",
      "        [0.8266],\n",
      "        [0.6319],\n",
      "        [0.6390],\n",
      "        [0.6717],\n",
      "        [0.6299],\n",
      "        [0.7471],\n",
      "        [0.7727],\n",
      "        [0.6490],\n",
      "        [0.8164],\n",
      "        [0.9004],\n",
      "        [0.8943],\n",
      "        [0.8260],\n",
      "        [0.9270],\n",
      "        [0.6988],\n",
      "        [0.9500],\n",
      "        [0.8319],\n",
      "        [0.9591],\n",
      "        [0.5931],\n",
      "        [0.8368],\n",
      "        [0.8800],\n",
      "        [0.6122],\n",
      "        [0.6289],\n",
      "        [0.8793],\n",
      "        [0.5619],\n",
      "        [0.6519],\n",
      "        [0.7148],\n",
      "        [0.9099],\n",
      "        [0.6627],\n",
      "        [0.6840],\n",
      "        [0.8986],\n",
      "        [0.5886],\n",
      "        [0.8722],\n",
      "        [0.6563],\n",
      "        [0.8527],\n",
      "        [0.6885],\n",
      "        [0.6935],\n",
      "        [0.9999],\n",
      "        [0.8238],\n",
      "        [0.9544],\n",
      "        [0.8712],\n",
      "        [1.0193],\n",
      "        [0.6784],\n",
      "        [0.6803],\n",
      "        [0.6348],\n",
      "        [0.8461],\n",
      "        [0.8348],\n",
      "        [0.5568],\n",
      "        [0.9063],\n",
      "        [0.6749],\n",
      "        [0.5933],\n",
      "        [0.6657],\n",
      "        [0.7034],\n",
      "        [0.5879],\n",
      "        [0.6281],\n",
      "        [0.8040],\n",
      "        [0.7541],\n",
      "        [0.9250],\n",
      "        [0.8767],\n",
      "        [0.6502],\n",
      "        [0.8815],\n",
      "        [0.6415],\n",
      "        [0.5878],\n",
      "        [0.8033],\n",
      "        [0.8413],\n",
      "        [0.8980],\n",
      "        [0.8446],\n",
      "        [0.8615],\n",
      "        [0.8566],\n",
      "        [0.6526],\n",
      "        [0.8913],\n",
      "        [1.0249],\n",
      "        [0.6445],\n",
      "        [0.6902],\n",
      "        [0.8783],\n",
      "        [0.5789],\n",
      "        [0.8826],\n",
      "        [0.8496],\n",
      "        [0.6327],\n",
      "        [0.6137],\n",
      "        [0.8048],\n",
      "        [0.9016],\n",
      "        [0.9152],\n",
      "        [0.8062],\n",
      "        [0.5564],\n",
      "        [0.8542],\n",
      "        [0.9371],\n",
      "        [0.7365],\n",
      "        [0.8816],\n",
      "        [0.8856],\n",
      "        [0.7322],\n",
      "        [0.5892],\n",
      "        [0.7077],\n",
      "        [0.8264],\n",
      "        [0.7143],\n",
      "        [0.6809],\n",
      "        [0.5306],\n",
      "        [0.8739],\n",
      "        [0.7180],\n",
      "        [0.8392],\n",
      "        [0.8270],\n",
      "        [0.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0063],\n",
      "        [0.0071],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0121],\n",
      "        [0.0126],\n",
      "        [0.0128],\n",
      "        [0.0137],\n",
      "        [0.0153],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0183],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0188],\n",
      "        [0.0201],\n",
      "        [0.0207],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0226],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0244],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0265],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0274],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0283],\n",
      "        [0.0283],\n",
      "        [0.0291],\n",
      "        [0.0295],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0310],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0373],\n",
      "        [0.0386],\n",
      "        [0.0404],\n",
      "        [0.0404],\n",
      "        [0.0430],\n",
      "        [0.0440],\n",
      "        [0.0451],\n",
      "        [0.0459],\n",
      "        [0.0462],\n",
      "        [0.0463],\n",
      "        [0.0506],\n",
      "        [0.0524],\n",
      "        [0.0530],\n",
      "        [0.0560],\n",
      "        [0.0566],\n",
      "        [0.0580],\n",
      "        [0.0588],\n",
      "        [0.0595],\n",
      "        [0.0596],\n",
      "        [0.0607],\n",
      "        [0.0607],\n",
      "        [0.0614],\n",
      "        [0.0623],\n",
      "        [0.0735],\n",
      "        [0.0737],\n",
      "        [0.0738],\n",
      "        [0.0769],\n",
      "        [0.0778],\n",
      "        [0.0799],\n",
      "        [0.0802],\n",
      "        [0.0907],\n",
      "        [0.0920],\n",
      "        [0.0933],\n",
      "        [0.1083],\n",
      "        [0.1107],\n",
      "        [0.1130],\n",
      "        [0.1231],\n",
      "        [0.1300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0018],\n",
      "        [    0.0001],\n",
      "        [    0.0019],\n",
      "        [    0.0034],\n",
      "        [    0.0027],\n",
      "        [    0.0013],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0029],\n",
      "        [    0.0057],\n",
      "        [    0.0061],\n",
      "        [    0.0053],\n",
      "        [    0.0071],\n",
      "        [    0.0045],\n",
      "        [    0.0054],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0074],\n",
      "        [    0.0085],\n",
      "        [    0.0088],\n",
      "        [    0.0085],\n",
      "        [    0.0075],\n",
      "        [    0.0102],\n",
      "        [    0.0105],\n",
      "        [    0.0113],\n",
      "        [    0.0089],\n",
      "        [    0.0088],\n",
      "        [    0.0083],\n",
      "        [    0.0090],\n",
      "        [    0.0087],\n",
      "        [    0.0128],\n",
      "        [    0.0134],\n",
      "        [    0.0105],\n",
      "        [    0.0101],\n",
      "        [    0.0125],\n",
      "        [    0.0136],\n",
      "        [    0.0151],\n",
      "        [    0.0164],\n",
      "        [    0.0168],\n",
      "        [    0.0193],\n",
      "        [    0.0177],\n",
      "        [    0.0182],\n",
      "        [    0.0181],\n",
      "        [    0.0193],\n",
      "        [    0.0180],\n",
      "        [    0.0179],\n",
      "        [    0.0197],\n",
      "        [    0.0196],\n",
      "        [    0.0185],\n",
      "        [    0.0197],\n",
      "        [    0.0220],\n",
      "        [    0.0232],\n",
      "        [    0.0207],\n",
      "        [    0.0189],\n",
      "        [    0.0232],\n",
      "        [    0.0230],\n",
      "        [    0.0236],\n",
      "        [    0.0223],\n",
      "        [    0.0213],\n",
      "        [    0.0227],\n",
      "        [    0.0253],\n",
      "        [    0.0252],\n",
      "        [    0.0241],\n",
      "        [    0.0256],\n",
      "        [    0.0266],\n",
      "        [    0.0275],\n",
      "        [    0.0259],\n",
      "        [    0.0281],\n",
      "        [    0.0292],\n",
      "        [    0.0261],\n",
      "        [    0.0285],\n",
      "        [    0.0281],\n",
      "        [    0.0291],\n",
      "        [    0.0302],\n",
      "        [    0.0270],\n",
      "        [    0.0279],\n",
      "        [    0.0268],\n",
      "        [    0.0333],\n",
      "        [    0.0322],\n",
      "        [    0.0325],\n",
      "        [    0.0321],\n",
      "        [    0.0309],\n",
      "        [    0.0322],\n",
      "        [    0.0337],\n",
      "        [    0.0311],\n",
      "        [    0.0346],\n",
      "        [    0.0371],\n",
      "        [    0.0335],\n",
      "        [    0.0364],\n",
      "        [    0.0382],\n",
      "        [    0.0390],\n",
      "        [    0.0405],\n",
      "        [    0.0385],\n",
      "        [    0.0441],\n",
      "        [    0.0458],\n",
      "        [    0.0450],\n",
      "        [    0.0449],\n",
      "        [    0.0479],\n",
      "        [    0.0431],\n",
      "        [    0.0507],\n",
      "        [    0.0493],\n",
      "        [    0.0532],\n",
      "        [    0.0543],\n",
      "        [    0.0582],\n",
      "        [    0.0598],\n",
      "        [    0.0584],\n",
      "        [    0.0603],\n",
      "        [    0.0598],\n",
      "        [    0.0593],\n",
      "        [    0.0619],\n",
      "        [    0.0617],\n",
      "        [    0.0627],\n",
      "        [    0.0726],\n",
      "        [    0.0746],\n",
      "        [    0.0747],\n",
      "        [    0.0730],\n",
      "        [    0.0748],\n",
      "        [    0.0765],\n",
      "        [    0.0791],\n",
      "        [    0.0877],\n",
      "        [    0.0891],\n",
      "        [    0.0939],\n",
      "        [    0.1087],\n",
      "        [    0.1068],\n",
      "        [    0.1108],\n",
      "        [    0.1221],\n",
      "        [    0.1267]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.835763692855835\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.060872940835907e-09, 57)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [57, 98, 134, 62, 52, 32, 99, 96, 97, 33, 56, 21, 90, 75, 100, 105, 13, 53, 66, 135, 49, 23, 85, 26, 68, 50, 137, 112, 12, 91, 74, 104, 27, 102, 76, 18, 55, 36, 16, 15, 63, 22, 8, 34, 51, 110, 128, 93, 101, 130, 131, 17, 136, 109, 30, 29, 61, 92, 87, 123, 9, 86, 89, 28, 10, 138, 14, 113, 111, 3, 129, 95, 25, 64, 124, 94, 58, 77, 139, 65, 127, 54, 103, 88, 122, 46, 20, 35, 132, 0, 44, 19, 31, 140, 1, 11, 48, 106, 107, 121, 4, 47, 67, 108, 37, 73, 6, 133, 2, 24, 126, 7, 78, 83, 60, 59, 120, 141, 72, 84, 114, 125, 142, 5, 43, 69, 115, 82] 數值 torch.Size([128, 1])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9199],\n",
      "        [0.6785],\n",
      "        [0.6411],\n",
      "        [0.8814],\n",
      "        [0.8862],\n",
      "        [0.8065],\n",
      "        [0.6563],\n",
      "        [0.6910],\n",
      "        [0.6795],\n",
      "        [0.8155],\n",
      "        [0.9006],\n",
      "        [0.8480],\n",
      "        [0.6977],\n",
      "        [0.7402],\n",
      "        [0.6289],\n",
      "        [0.6723],\n",
      "        [0.9761],\n",
      "        [0.8748],\n",
      "        [0.8430],\n",
      "        [0.6299],\n",
      "        [0.8439],\n",
      "        [0.8540],\n",
      "        [0.6698],\n",
      "        [0.8228],\n",
      "        [0.8254],\n",
      "        [0.8981],\n",
      "        [0.6376],\n",
      "        [0.6465],\n",
      "        [0.9870],\n",
      "        [0.6810],\n",
      "        [0.7711],\n",
      "        [0.6476],\n",
      "        [0.8162],\n",
      "        [0.6284],\n",
      "        [0.7452],\n",
      "        [0.9005],\n",
      "        [0.8945],\n",
      "        [0.8262],\n",
      "        [0.9270],\n",
      "        [0.9496],\n",
      "        [0.8796],\n",
      "        [0.8368],\n",
      "        [0.9586],\n",
      "        [0.8327],\n",
      "        [0.8793],\n",
      "        [0.6605],\n",
      "        [0.5915],\n",
      "        [0.6968],\n",
      "        [0.6278],\n",
      "        [0.5610],\n",
      "        [0.6108],\n",
      "        [0.9096],\n",
      "        [0.6546],\n",
      "        [0.6501],\n",
      "        [0.8719],\n",
      "        [0.8522],\n",
      "        [0.8973],\n",
      "        [0.6825],\n",
      "        [0.7123],\n",
      "        [0.5869],\n",
      "        [0.9996],\n",
      "        [0.6915],\n",
      "        [0.6864],\n",
      "        [0.8236],\n",
      "        [1.0187],\n",
      "        [0.6331],\n",
      "        [0.9534],\n",
      "        [0.6628],\n",
      "        [0.6724],\n",
      "        [0.8722],\n",
      "        [0.5917],\n",
      "        [0.6769],\n",
      "        [0.8347],\n",
      "        [0.8458],\n",
      "        [0.5560],\n",
      "        [0.6785],\n",
      "        [0.9053],\n",
      "        [0.7527],\n",
      "        [0.6477],\n",
      "        [0.8036],\n",
      "        [0.5865],\n",
      "        [0.9248],\n",
      "        [0.6267],\n",
      "        [0.7010],\n",
      "        [0.5849],\n",
      "        [0.8770],\n",
      "        [0.8813],\n",
      "        [0.8037],\n",
      "        [0.6396],\n",
      "        [0.8421],\n",
      "        [0.8633],\n",
      "        [0.8977],\n",
      "        [0.8448],\n",
      "        [0.6871],\n",
      "        [0.8576],\n",
      "        [1.0240],\n",
      "        [0.8912],\n",
      "        [0.6508],\n",
      "        [0.6428],\n",
      "        [0.5758],\n",
      "        [0.8783],\n",
      "        [0.8829],\n",
      "        [0.8479],\n",
      "        [0.6311],\n",
      "        [0.8051],\n",
      "        [0.8048],\n",
      "        [0.9149],\n",
      "        [0.6119],\n",
      "        [0.9025],\n",
      "        [0.8546],\n",
      "        [0.5552],\n",
      "        [0.9367],\n",
      "        [0.7357],\n",
      "        [0.7283],\n",
      "        [0.8807],\n",
      "        [0.8847],\n",
      "        [0.5861],\n",
      "        [0.7043],\n",
      "        [0.8252],\n",
      "        [0.7113],\n",
      "        [0.6779],\n",
      "        [0.5299],\n",
      "        [0.7140],\n",
      "        [0.8735],\n",
      "        [0.8414],\n",
      "        [0.8260],\n",
      "        [0.6936],\n",
      "        [0.7676]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0027],\n",
      "        [    0.0029],\n",
      "        [    0.0034],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0045],\n",
      "        [    0.0053],\n",
      "        [    0.0054],\n",
      "        [    0.0057],\n",
      "        [    0.0061],\n",
      "        [    0.0065],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0074],\n",
      "        [    0.0075],\n",
      "        [    0.0083],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0089],\n",
      "        [    0.0090],\n",
      "        [    0.0101],\n",
      "        [    0.0102],\n",
      "        [    0.0105],\n",
      "        [    0.0105],\n",
      "        [    0.0113],\n",
      "        [    0.0125],\n",
      "        [    0.0128],\n",
      "        [    0.0134],\n",
      "        [    0.0136],\n",
      "        [    0.0151],\n",
      "        [    0.0164],\n",
      "        [    0.0168],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0180],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0196],\n",
      "        [    0.0197],\n",
      "        [    0.0197],\n",
      "        [    0.0207],\n",
      "        [    0.0213],\n",
      "        [    0.0220],\n",
      "        [    0.0223],\n",
      "        [    0.0227],\n",
      "        [    0.0230],\n",
      "        [    0.0232],\n",
      "        [    0.0232],\n",
      "        [    0.0236],\n",
      "        [    0.0241],\n",
      "        [    0.0252],\n",
      "        [    0.0253],\n",
      "        [    0.0256],\n",
      "        [    0.0259],\n",
      "        [    0.0261],\n",
      "        [    0.0266],\n",
      "        [    0.0268],\n",
      "        [    0.0270],\n",
      "        [    0.0275],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0281],\n",
      "        [    0.0285],\n",
      "        [    0.0291],\n",
      "        [    0.0292],\n",
      "        [    0.0302],\n",
      "        [    0.0309],\n",
      "        [    0.0311],\n",
      "        [    0.0321],\n",
      "        [    0.0322],\n",
      "        [    0.0322],\n",
      "        [    0.0325],\n",
      "        [    0.0333],\n",
      "        [    0.0335],\n",
      "        [    0.0337],\n",
      "        [    0.0346],\n",
      "        [    0.0364],\n",
      "        [    0.0371],\n",
      "        [    0.0382],\n",
      "        [    0.0385],\n",
      "        [    0.0390],\n",
      "        [    0.0405],\n",
      "        [    0.0431],\n",
      "        [    0.0441],\n",
      "        [    0.0449],\n",
      "        [    0.0450],\n",
      "        [    0.0458],\n",
      "        [    0.0479],\n",
      "        [    0.0493],\n",
      "        [    0.0507],\n",
      "        [    0.0532],\n",
      "        [    0.0543],\n",
      "        [    0.0582],\n",
      "        [    0.0584],\n",
      "        [    0.0593],\n",
      "        [    0.0598],\n",
      "        [    0.0598],\n",
      "        [    0.0603],\n",
      "        [    0.0617],\n",
      "        [    0.0619],\n",
      "        [    0.0627],\n",
      "        [    0.0726],\n",
      "        [    0.0730],\n",
      "        [    0.0746],\n",
      "        [    0.0747],\n",
      "        [    0.0748],\n",
      "        [    0.0765],\n",
      "        [    0.0791],\n",
      "        [    0.0877],\n",
      "        [    0.0891],\n",
      "        [    0.0939],\n",
      "        [    0.1068],\n",
      "        [    0.1087],\n",
      "        [    0.1108],\n",
      "        [    0.1221],\n",
      "        [    0.1267],\n",
      "        [    0.1341]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0033],\n",
      "        [0.0020],\n",
      "        [0.0039],\n",
      "        [0.0052],\n",
      "        [0.0009],\n",
      "        [0.0038],\n",
      "        [0.0066],\n",
      "        [0.0076],\n",
      "        [0.0074],\n",
      "        [0.0056],\n",
      "        [0.0078],\n",
      "        [0.0070],\n",
      "        [0.0098],\n",
      "        [0.0105],\n",
      "        [0.0094],\n",
      "        [0.0108],\n",
      "        [0.0104],\n",
      "        [0.0056],\n",
      "        [0.0043],\n",
      "        [0.0067],\n",
      "        [0.0109],\n",
      "        [0.0071],\n",
      "        [0.0045],\n",
      "        [0.0107],\n",
      "        [0.0054],\n",
      "        [0.0059],\n",
      "        [0.0079],\n",
      "        [0.0065],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0059],\n",
      "        [0.0148],\n",
      "        [0.0108],\n",
      "        [0.0159],\n",
      "        [0.0181],\n",
      "        [0.0154],\n",
      "        [0.0170],\n",
      "        [0.0184],\n",
      "        [0.0190],\n",
      "        [0.0203],\n",
      "        [0.0151],\n",
      "        [0.0196],\n",
      "        [0.0208],\n",
      "        [0.0169],\n",
      "        [0.0205],\n",
      "        [0.0156],\n",
      "        [0.0207],\n",
      "        [0.0237],\n",
      "        [0.0225],\n",
      "        [0.0202],\n",
      "        [0.0208],\n",
      "        [0.0185],\n",
      "        [0.0198],\n",
      "        [0.0250],\n",
      "        [0.0202],\n",
      "        [0.0205],\n",
      "        [0.0266],\n",
      "        [0.0271],\n",
      "        [0.0284],\n",
      "        [0.0252],\n",
      "        [0.0213],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0274],\n",
      "        [0.0227],\n",
      "        [0.0247],\n",
      "        [0.0297],\n",
      "        [0.0229],\n",
      "        [0.0233],\n",
      "        [0.0266],\n",
      "        [0.0265],\n",
      "        [0.0318],\n",
      "        [0.0265],\n",
      "        [0.0311],\n",
      "        [0.0297],\n",
      "        [0.0332],\n",
      "        [0.0333],\n",
      "        [0.0266],\n",
      "        [0.0289],\n",
      "        [0.0346],\n",
      "        [0.0335],\n",
      "        [0.0296],\n",
      "        [0.0354],\n",
      "        [0.0383],\n",
      "        [0.0308],\n",
      "        [0.0320],\n",
      "        [0.0366],\n",
      "        [0.0380],\n",
      "        [0.0388],\n",
      "        [0.0371],\n",
      "        [0.0389],\n",
      "        [0.0411],\n",
      "        [0.0387],\n",
      "        [0.0402],\n",
      "        [0.0433],\n",
      "        [0.0414],\n",
      "        [0.0428],\n",
      "        [0.0491],\n",
      "        [0.0510],\n",
      "        [0.0461],\n",
      "        [0.0528],\n",
      "        [0.0515],\n",
      "        [0.0505],\n",
      "        [0.0610],\n",
      "        [0.0601],\n",
      "        [0.0548],\n",
      "        [0.0622],\n",
      "        [0.0612],\n",
      "        [0.0591],\n",
      "        [0.0604],\n",
      "        [0.0629],\n",
      "        [0.0653],\n",
      "        [0.0688],\n",
      "        [0.0666],\n",
      "        [0.0777],\n",
      "        [0.0778],\n",
      "        [0.0714],\n",
      "        [0.0733],\n",
      "        [0.0748],\n",
      "        [0.0824],\n",
      "        [0.0851],\n",
      "        [0.0944],\n",
      "        [0.1030],\n",
      "        [0.1109],\n",
      "        [0.1107],\n",
      "        [0.1187],\n",
      "        [0.1225],\n",
      "        [0.1283]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.114126682281494\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.700054993620142e-07, 52)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [52, 98, 57, 32, 134, 66, 85, 62, 68, 33, 53, 74, 50, 112, 99, 135, 21, 23, 97, 96, 56, 137, 100, 90, 13, 75, 26, 105, 27, 49, 12, 91, 104, 63, 18, 110, 102, 34, 55, 76, 36, 17, 16, 22, 136, 30, 130, 15, 51, 29, 128, 131, 8, 9, 101, 10, 113, 111, 93, 138, 109, 123, 25, 129, 77, 3, 61, 92, 28, 87, 139, 54, 124, 14, 86, 89, 122, 64, 95, 46, 94, 58, 127, 65, 103, 20, 0, 35, 88, 31, 132, 44, 140, 19, 11, 48, 1, 121, 106, 67, 107, 47, 4, 73, 2, 37, 24, 108, 133, 6, 126, 7, 83, 78, 120, 141, 72, 60, 59, 84, 114, 125, 142, 43, 5, 69, 115, 82, 42] 數值 torch.Size([129, 1])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8843],\n",
      "        [0.6751],\n",
      "        [0.9167],\n",
      "        [0.8056],\n",
      "        [0.6391],\n",
      "        [0.8398],\n",
      "        [0.6655],\n",
      "        [0.8781],\n",
      "        [0.8221],\n",
      "        [0.8144],\n",
      "        [0.8730],\n",
      "        [0.7665],\n",
      "        [0.8952],\n",
      "        [0.6429],\n",
      "        [0.6531],\n",
      "        [0.6283],\n",
      "        [0.8464],\n",
      "        [0.8525],\n",
      "        [0.6761],\n",
      "        [0.6874],\n",
      "        [0.8981],\n",
      "        [0.6366],\n",
      "        [0.6260],\n",
      "        [0.6935],\n",
      "        [0.9729],\n",
      "        [0.7358],\n",
      "        [0.8208],\n",
      "        [0.6687],\n",
      "        [0.8144],\n",
      "        [0.8416],\n",
      "        [0.9838],\n",
      "        [0.6774],\n",
      "        [0.6442],\n",
      "        [0.8767],\n",
      "        [0.8987],\n",
      "        [0.6572],\n",
      "        [0.6253],\n",
      "        [0.8314],\n",
      "        [0.8926],\n",
      "        [0.7405],\n",
      "        [0.8242],\n",
      "        [0.9075],\n",
      "        [0.9248],\n",
      "        [0.8351],\n",
      "        [0.6531],\n",
      "        [0.8697],\n",
      "        [0.5605],\n",
      "        [0.9471],\n",
      "        [0.8774],\n",
      "        [0.8500],\n",
      "        [0.5901],\n",
      "        [0.6098],\n",
      "        [0.9559],\n",
      "        [0.9968],\n",
      "        [0.6249],\n",
      "        [1.0155],\n",
      "        [0.6589],\n",
      "        [0.6687],\n",
      "        [0.6924],\n",
      "        [0.6317],\n",
      "        [0.6470],\n",
      "        [0.5853],\n",
      "        [0.8330],\n",
      "        [0.5903],\n",
      "        [0.7483],\n",
      "        [0.8713],\n",
      "        [0.8937],\n",
      "        [0.6785],\n",
      "        [0.8218],\n",
      "        [0.7071],\n",
      "        [0.6455],\n",
      "        [0.9222],\n",
      "        [0.5554],\n",
      "        [0.9503],\n",
      "        [0.6870],\n",
      "        [0.6818],\n",
      "        [0.5822],\n",
      "        [0.8433],\n",
      "        [0.6732],\n",
      "        [0.8753],\n",
      "        [0.6746],\n",
      "        [0.9022],\n",
      "        [0.5853],\n",
      "        [0.8011],\n",
      "        [0.6237],\n",
      "        [0.8793],\n",
      "        [0.8411],\n",
      "        [0.8020],\n",
      "        [0.6959],\n",
      "        [0.8430],\n",
      "        [0.6380],\n",
      "        [0.8629],\n",
      "        [0.6841],\n",
      "        [0.8956],\n",
      "        [1.0204],\n",
      "        [0.8891],\n",
      "        [0.8568],\n",
      "        [0.5726],\n",
      "        [0.6475],\n",
      "        [0.8441],\n",
      "        [0.6398],\n",
      "        [0.8811],\n",
      "        [0.8762],\n",
      "        [0.8003],\n",
      "        [0.9012],\n",
      "        [0.8034],\n",
      "        [0.8532],\n",
      "        [0.6283],\n",
      "        [0.6105],\n",
      "        [0.9126],\n",
      "        [0.5542],\n",
      "        [0.9341],\n",
      "        [0.7219],\n",
      "        [0.7318],\n",
      "        [0.5828],\n",
      "        [0.7010],\n",
      "        [0.8210],\n",
      "        [0.8776],\n",
      "        [0.8816],\n",
      "        [0.7060],\n",
      "        [0.6739],\n",
      "        [0.5295],\n",
      "        [0.7102],\n",
      "        [0.8414],\n",
      "        [0.8712],\n",
      "        [0.8225],\n",
      "        [0.6894],\n",
      "        [0.7617],\n",
      "        [0.8311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0020],\n",
      "        [0.0033],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0059],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0067],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0148],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0159],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0208],\n",
      "        [0.0213],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0229],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0265],\n",
      "        [0.0265],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0271],\n",
      "        [0.0274],\n",
      "        [0.0284],\n",
      "        [0.0289],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0318],\n",
      "        [0.0320],\n",
      "        [0.0332],\n",
      "        [0.0333],\n",
      "        [0.0335],\n",
      "        [0.0346],\n",
      "        [0.0354],\n",
      "        [0.0366],\n",
      "        [0.0371],\n",
      "        [0.0380],\n",
      "        [0.0383],\n",
      "        [0.0387],\n",
      "        [0.0388],\n",
      "        [0.0389],\n",
      "        [0.0402],\n",
      "        [0.0411],\n",
      "        [0.0414],\n",
      "        [0.0428],\n",
      "        [0.0433],\n",
      "        [0.0461],\n",
      "        [0.0491],\n",
      "        [0.0505],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0528],\n",
      "        [0.0548],\n",
      "        [0.0591],\n",
      "        [0.0601],\n",
      "        [0.0604],\n",
      "        [0.0610],\n",
      "        [0.0612],\n",
      "        [0.0622],\n",
      "        [0.0629],\n",
      "        [0.0653],\n",
      "        [0.0666],\n",
      "        [0.0688],\n",
      "        [0.0714],\n",
      "        [0.0733],\n",
      "        [0.0748],\n",
      "        [0.0777],\n",
      "        [0.0778],\n",
      "        [0.0824],\n",
      "        [0.0851],\n",
      "        [0.0944],\n",
      "        [0.1030],\n",
      "        [0.1107],\n",
      "        [0.1109],\n",
      "        [0.1187],\n",
      "        [0.1225],\n",
      "        [0.1283],\n",
      "        [0.1358]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0056],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0014],\n",
      "        [    0.0019],\n",
      "        [    0.0072],\n",
      "        [    0.0053],\n",
      "        [    0.0018],\n",
      "        [    0.0081],\n",
      "        [    0.0005],\n",
      "        [    0.0104],\n",
      "        [    0.0075],\n",
      "        [    0.0095],\n",
      "        [    0.0078],\n",
      "        [    0.0046],\n",
      "        [    0.0091],\n",
      "        [    0.0025],\n",
      "        [    0.0119],\n",
      "        [    0.0053],\n",
      "        [    0.0056],\n",
      "        [    0.0036],\n",
      "        [    0.0111],\n",
      "        [    0.0072],\n",
      "        [    0.0085],\n",
      "        [    0.0073],\n",
      "        [    0.0091],\n",
      "        [    0.0067],\n",
      "        [    0.0094],\n",
      "        [    0.0151],\n",
      "        [    0.0070],\n",
      "        [    0.0105],\n",
      "        [    0.0121],\n",
      "        [    0.0134],\n",
      "        [    0.0189],\n",
      "        [    0.0108],\n",
      "        [    0.0171],\n",
      "        [    0.0141],\n",
      "        [    0.0220],\n",
      "        [    0.0122],\n",
      "        [    0.0169],\n",
      "        [    0.0137],\n",
      "        [    0.0226],\n",
      "        [    0.0148],\n",
      "        [    0.0151],\n",
      "        [    0.0226],\n",
      "        [    0.0244],\n",
      "        [    0.0169],\n",
      "        [    0.0164],\n",
      "        [    0.0160],\n",
      "        [    0.0245],\n",
      "        [    0.0182],\n",
      "        [    0.0179],\n",
      "        [    0.0174],\n",
      "        [    0.0249],\n",
      "        [    0.0204],\n",
      "        [    0.0260],\n",
      "        [    0.0238],\n",
      "        [    0.0246],\n",
      "        [    0.0226],\n",
      "        [    0.0275],\n",
      "        [    0.0232],\n",
      "        [    0.0228],\n",
      "        [    0.0308],\n",
      "        [    0.0290],\n",
      "        [    0.0282],\n",
      "        [    0.0312],\n",
      "        [    0.0236],\n",
      "        [    0.0256],\n",
      "        [    0.0232],\n",
      "        [    0.0282],\n",
      "        [    0.0309],\n",
      "        [    0.0339],\n",
      "        [    0.0265],\n",
      "        [    0.0265],\n",
      "        [    0.0291],\n",
      "        [    0.0291],\n",
      "        [    0.0320],\n",
      "        [    0.0271],\n",
      "        [    0.0300],\n",
      "        [    0.0369],\n",
      "        [    0.0318],\n",
      "        [    0.0300],\n",
      "        [    0.0307],\n",
      "        [    0.0309],\n",
      "        [    0.0336],\n",
      "        [    0.0323],\n",
      "        [    0.0414],\n",
      "        [    0.0333],\n",
      "        [    0.0380],\n",
      "        [    0.0432],\n",
      "        [    0.0363],\n",
      "        [    0.0324],\n",
      "        [    0.0415],\n",
      "        [    0.0369],\n",
      "        [    0.0444],\n",
      "        [    0.0473],\n",
      "        [    0.0479],\n",
      "        [    0.0469],\n",
      "        [    0.0474],\n",
      "        [    0.0528],\n",
      "        [    0.0491],\n",
      "        [    0.0564],\n",
      "        [    0.0492],\n",
      "        [    0.0567],\n",
      "        [    0.0635],\n",
      "        [    0.0553],\n",
      "        [    0.0652],\n",
      "        [    0.0589],\n",
      "        [    0.0587],\n",
      "        [    0.0587],\n",
      "        [    0.0602],\n",
      "        [    0.0619],\n",
      "        [    0.0654],\n",
      "        [    0.0709],\n",
      "        [    0.0724],\n",
      "        [    0.0743],\n",
      "        [    0.0772],\n",
      "        [    0.0744],\n",
      "        [    0.0745],\n",
      "        [    0.0823],\n",
      "        [    0.0860],\n",
      "        [    0.0911],\n",
      "        [    0.1034],\n",
      "        [    0.1037],\n",
      "        [    0.1077],\n",
      "        [    0.1214],\n",
      "        [    0.1231],\n",
      "        [    0.1284],\n",
      "        [    0.1294]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.39236330986023\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.2228654472655762e-09, 57)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [57, 98, 33, 32, 62, 134, 21, 56, 99, 85, 97, 52, 96, 26, 49, 66, 100, 13, 74, 112, 68, 90, 75, 135, 105, 50, 53, 12, 18, 137, 23, 91, 55, 104, 36, 102, 16, 27, 22, 51, 15, 76, 130, 110, 8, 131, 128, 63, 101, 34, 17, 93, 136, 123, 109, 28, 61, 113, 30, 29, 111, 9, 92, 10, 124, 14, 64, 138, 77, 87, 129, 89, 86, 58, 95, 127, 25, 139, 65, 3, 94, 122, 20, 44, 35, 103, 54, 132, 46, 19, 88, 0, 140, 31, 11, 121, 48, 106, 1, 107, 4, 67, 37, 47, 73, 6, 133, 108, 126, 7, 2, 24, 83, 78, 120, 141, 60, 59, 72, 84, 114, 125, 142, 43, 5, 69, 115, 82, 42, 71] 數值 torch.Size([130, 1])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9200],\n",
      "        [0.6771],\n",
      "        [0.8195],\n",
      "        [0.8108],\n",
      "        [0.8815],\n",
      "        [0.6411],\n",
      "        [0.8508],\n",
      "        [0.9023],\n",
      "        [0.6552],\n",
      "        [0.6664],\n",
      "        [0.6782],\n",
      "        [0.8891],\n",
      "        [0.6894],\n",
      "        [0.8248],\n",
      "        [0.8455],\n",
      "        [0.8427],\n",
      "        [0.6281],\n",
      "        [0.9760],\n",
      "        [0.7681],\n",
      "        [0.6442],\n",
      "        [0.8248],\n",
      "        [0.6948],\n",
      "        [0.7372],\n",
      "        [0.6307],\n",
      "        [0.6700],\n",
      "        [0.8987],\n",
      "        [0.8778],\n",
      "        [0.9868],\n",
      "        [0.9033],\n",
      "        [0.6397],\n",
      "        [0.8573],\n",
      "        [0.6793],\n",
      "        [0.8974],\n",
      "        [0.6455],\n",
      "        [0.8288],\n",
      "        [0.6271],\n",
      "        [0.9290],\n",
      "        [0.8187],\n",
      "        [0.8397],\n",
      "        [0.8819],\n",
      "        [0.9510],\n",
      "        [0.7417],\n",
      "        [0.5638],\n",
      "        [0.6586],\n",
      "        [0.9593],\n",
      "        [0.6127],\n",
      "        [0.5926],\n",
      "        [0.8805],\n",
      "        [0.6270],\n",
      "        [0.8365],\n",
      "        [0.9115],\n",
      "        [0.6935],\n",
      "        [0.6559],\n",
      "        [0.5877],\n",
      "        [0.6488],\n",
      "        [0.8260],\n",
      "        [0.8966],\n",
      "        [0.6598],\n",
      "        [0.8740],\n",
      "        [0.8541],\n",
      "        [0.6699],\n",
      "        [1.0004],\n",
      "        [0.6801],\n",
      "        [1.0187],\n",
      "        [0.5586],\n",
      "        [0.9535],\n",
      "        [0.8472],\n",
      "        [0.6345],\n",
      "        [0.7499],\n",
      "        [0.7073],\n",
      "        [0.5928],\n",
      "        [0.6826],\n",
      "        [0.6876],\n",
      "        [0.9055],\n",
      "        [0.6750],\n",
      "        [0.5880],\n",
      "        [0.8373],\n",
      "        [0.6475],\n",
      "        [0.8047],\n",
      "        [0.8759],\n",
      "        [0.6760],\n",
      "        [0.5834],\n",
      "        [0.8836],\n",
      "        [0.8694],\n",
      "        [0.8067],\n",
      "        [0.6256],\n",
      "        [0.9264],\n",
      "        [0.6404],\n",
      "        [0.8801],\n",
      "        [0.8997],\n",
      "        [0.6963],\n",
      "        [0.8454],\n",
      "        [0.6854],\n",
      "        [0.8475],\n",
      "        [1.0234],\n",
      "        [0.5735],\n",
      "        [0.8936],\n",
      "        [0.6492],\n",
      "        [0.8615],\n",
      "        [0.6416],\n",
      "        [0.8797],\n",
      "        [0.8464],\n",
      "        [0.8083],\n",
      "        [0.8861],\n",
      "        [0.8022],\n",
      "        [0.9160],\n",
      "        [0.6130],\n",
      "        [0.6303],\n",
      "        [0.5569],\n",
      "        [0.9375],\n",
      "        [0.9057],\n",
      "        [0.8581],\n",
      "        [0.7207],\n",
      "        [0.7340],\n",
      "        [0.5837],\n",
      "        [0.7021],\n",
      "        [0.8809],\n",
      "        [0.8849],\n",
      "        [0.8233],\n",
      "        [0.7059],\n",
      "        [0.6749],\n",
      "        [0.5328],\n",
      "        [0.7106],\n",
      "        [0.8484],\n",
      "        [0.8744],\n",
      "        [0.8253],\n",
      "        [0.6900],\n",
      "        [0.7619],\n",
      "        [0.8376],\n",
      "        [0.7991]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0014],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0025],\n",
      "        [    0.0036],\n",
      "        [    0.0046],\n",
      "        [    0.0053],\n",
      "        [    0.0053],\n",
      "        [    0.0056],\n",
      "        [    0.0056],\n",
      "        [    0.0067],\n",
      "        [    0.0070],\n",
      "        [    0.0072],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0075],\n",
      "        [    0.0078],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0091],\n",
      "        [    0.0091],\n",
      "        [    0.0094],\n",
      "        [    0.0095],\n",
      "        [    0.0104],\n",
      "        [    0.0105],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0119],\n",
      "        [    0.0121],\n",
      "        [    0.0122],\n",
      "        [    0.0134],\n",
      "        [    0.0137],\n",
      "        [    0.0141],\n",
      "        [    0.0148],\n",
      "        [    0.0151],\n",
      "        [    0.0151],\n",
      "        [    0.0160],\n",
      "        [    0.0164],\n",
      "        [    0.0169],\n",
      "        [    0.0169],\n",
      "        [    0.0171],\n",
      "        [    0.0174],\n",
      "        [    0.0179],\n",
      "        [    0.0182],\n",
      "        [    0.0189],\n",
      "        [    0.0204],\n",
      "        [    0.0220],\n",
      "        [    0.0226],\n",
      "        [    0.0226],\n",
      "        [    0.0226],\n",
      "        [    0.0228],\n",
      "        [    0.0232],\n",
      "        [    0.0232],\n",
      "        [    0.0236],\n",
      "        [    0.0238],\n",
      "        [    0.0244],\n",
      "        [    0.0245],\n",
      "        [    0.0246],\n",
      "        [    0.0249],\n",
      "        [    0.0256],\n",
      "        [    0.0260],\n",
      "        [    0.0265],\n",
      "        [    0.0265],\n",
      "        [    0.0271],\n",
      "        [    0.0275],\n",
      "        [    0.0282],\n",
      "        [    0.0282],\n",
      "        [    0.0290],\n",
      "        [    0.0291],\n",
      "        [    0.0291],\n",
      "        [    0.0300],\n",
      "        [    0.0300],\n",
      "        [    0.0307],\n",
      "        [    0.0308],\n",
      "        [    0.0309],\n",
      "        [    0.0309],\n",
      "        [    0.0312],\n",
      "        [    0.0318],\n",
      "        [    0.0320],\n",
      "        [    0.0323],\n",
      "        [    0.0324],\n",
      "        [    0.0333],\n",
      "        [    0.0336],\n",
      "        [    0.0339],\n",
      "        [    0.0363],\n",
      "        [    0.0369],\n",
      "        [    0.0369],\n",
      "        [    0.0380],\n",
      "        [    0.0414],\n",
      "        [    0.0415],\n",
      "        [    0.0432],\n",
      "        [    0.0444],\n",
      "        [    0.0469],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0479],\n",
      "        [    0.0491],\n",
      "        [    0.0492],\n",
      "        [    0.0528],\n",
      "        [    0.0553],\n",
      "        [    0.0564],\n",
      "        [    0.0567],\n",
      "        [    0.0587],\n",
      "        [    0.0587],\n",
      "        [    0.0589],\n",
      "        [    0.0602],\n",
      "        [    0.0619],\n",
      "        [    0.0635],\n",
      "        [    0.0652],\n",
      "        [    0.0654],\n",
      "        [    0.0709],\n",
      "        [    0.0724],\n",
      "        [    0.0743],\n",
      "        [    0.0744],\n",
      "        [    0.0745],\n",
      "        [    0.0772],\n",
      "        [    0.0823],\n",
      "        [    0.0860],\n",
      "        [    0.0911],\n",
      "        [    0.1034],\n",
      "        [    0.1037],\n",
      "        [    0.1077],\n",
      "        [    0.1214],\n",
      "        [    0.1231],\n",
      "        [    0.1284],\n",
      "        [    0.1294],\n",
      "        [    0.1406]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0040],\n",
      "        [    0.0030],\n",
      "        [    0.0018],\n",
      "        [    0.0001],\n",
      "        [    0.0060],\n",
      "        [    0.0036],\n",
      "        [    0.0053],\n",
      "        [    0.0070],\n",
      "        [    0.0073],\n",
      "        [    0.0016],\n",
      "        [    0.0082],\n",
      "        [    0.0034],\n",
      "        [    0.0089],\n",
      "        [    0.0096],\n",
      "        [    0.0106],\n",
      "        [    0.0028],\n",
      "        [    0.0098],\n",
      "        [    0.0120],\n",
      "        [    0.0024],\n",
      "        [    0.0051],\n",
      "        [    0.0040],\n",
      "        [    0.0121],\n",
      "        [    0.0140],\n",
      "        [    0.0078],\n",
      "        [    0.0125],\n",
      "        [    0.0058],\n",
      "        [    0.0081],\n",
      "        [    0.0151],\n",
      "        [    0.0140],\n",
      "        [    0.0103],\n",
      "        [    0.0095],\n",
      "        [    0.0150],\n",
      "        [    0.0149],\n",
      "        [    0.0166],\n",
      "        [    0.0158],\n",
      "        [    0.0168],\n",
      "        [    0.0185],\n",
      "        [    0.0127],\n",
      "        [    0.0178],\n",
      "        [    0.0185],\n",
      "        [    0.0203],\n",
      "        [    0.0215],\n",
      "        [    0.0173],\n",
      "        [    0.0145],\n",
      "        [    0.0203],\n",
      "        [    0.0186],\n",
      "        [    0.0193],\n",
      "        [    0.0151],\n",
      "        [    0.0229],\n",
      "        [    0.0207],\n",
      "        [    0.0187],\n",
      "        [    0.0267],\n",
      "        [    0.0216],\n",
      "        [    0.0237],\n",
      "        [    0.0253],\n",
      "        [    0.0254],\n",
      "        [    0.0281],\n",
      "        [    0.0207],\n",
      "        [    0.0222],\n",
      "        [    0.0221],\n",
      "        [    0.0217],\n",
      "        [    0.0218],\n",
      "        [    0.0289],\n",
      "        [    0.0222],\n",
      "        [    0.0265],\n",
      "        [    0.0312],\n",
      "        [    0.0308],\n",
      "        [    0.0263],\n",
      "        [    0.0239],\n",
      "        [    0.0325],\n",
      "        [    0.0278],\n",
      "        [    0.0330],\n",
      "        [    0.0327],\n",
      "        [    0.0341],\n",
      "        [    0.0336],\n",
      "        [    0.0316],\n",
      "        [    0.0281],\n",
      "        [    0.0288],\n",
      "        [    0.0347],\n",
      "        [    0.0293],\n",
      "        [    0.0356],\n",
      "        [    0.0298],\n",
      "        [    0.0354],\n",
      "        [    0.0335],\n",
      "        [    0.0353],\n",
      "        [    0.0363],\n",
      "        [    0.0308],\n",
      "        [    0.0377],\n",
      "        [    0.0338],\n",
      "        [    0.0404],\n",
      "        [    0.0422],\n",
      "        [    0.0388],\n",
      "        [    0.0386],\n",
      "        [    0.0412],\n",
      "        [    0.0400],\n",
      "        [    0.0443],\n",
      "        [    0.0438],\n",
      "        [    0.0499],\n",
      "        [    0.0457],\n",
      "        [    0.0514],\n",
      "        [    0.0523],\n",
      "        [    0.0479],\n",
      "        [    0.0571],\n",
      "        [    0.0534],\n",
      "        [    0.0515],\n",
      "        [    0.0615],\n",
      "        [    0.0599],\n",
      "        [    0.0609],\n",
      "        [    0.0607],\n",
      "        [    0.0648],\n",
      "        [    0.0612],\n",
      "        [    0.0630],\n",
      "        [    0.0591],\n",
      "        [    0.0671],\n",
      "        [    0.0695],\n",
      "        [    0.0712],\n",
      "        [    0.0782],\n",
      "        [    0.0784],\n",
      "        [    0.0724],\n",
      "        [    0.0774],\n",
      "        [    0.0830],\n",
      "        [    0.0909],\n",
      "        [    0.0998],\n",
      "        [    0.1041],\n",
      "        [    0.1111],\n",
      "        [    0.1173],\n",
      "        [    0.1197],\n",
      "        [    0.1229],\n",
      "        [    0.1301],\n",
      "        [    0.1358]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.67050790786743\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7148270359257367e-08, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [32, 85, 33, 74, 66, 98, 52, 134, 68, 57, 112, 21, 50, 62, 56, 99, 135, 53, 97, 96, 23, 26, 100, 137, 49, 13, 90, 105, 27, 18, 75, 110, 55, 91, 63, 12, 36, 104, 102, 130, 22, 51, 16, 131, 17, 128, 15, 8, 113, 34, 76, 136, 111, 9, 29, 30, 10, 101, 123, 77, 109, 28, 138, 124, 93, 129, 61, 25, 139, 92, 3, 122, 64, 54, 14, 127, 87, 86, 89, 44, 95, 46, 58, 65, 35, 20, 94, 103, 132, 140, 0, 11, 19, 31, 88, 48, 121, 1, 67, 106, 107, 73, 4, 47, 37, 83, 133, 126, 108, 2, 6, 24, 7, 78, 120, 141, 72, 84, 60, 59, 114, 125, 142, 43, 5, 69, 115, 82, 42, 71, 70] 數值 torch.Size([131, 1])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8096],\n",
      "        [0.6627],\n",
      "        [0.8181],\n",
      "        [0.7630],\n",
      "        [0.8383],\n",
      "        [0.6742],\n",
      "        [0.8868],\n",
      "        [0.6394],\n",
      "        [0.8206],\n",
      "        [0.9160],\n",
      "        [0.6415],\n",
      "        [0.8481],\n",
      "        [0.8950],\n",
      "        [0.8774],\n",
      "        [0.8989],\n",
      "        [0.6524],\n",
      "        [0.6294],\n",
      "        [0.8755],\n",
      "        [0.6753],\n",
      "        [0.6861],\n",
      "        [0.8549],\n",
      "        [0.8219],\n",
      "        [0.6256],\n",
      "        [0.6390],\n",
      "        [0.8418],\n",
      "        [0.9713],\n",
      "        [0.6913],\n",
      "        [0.6669],\n",
      "        [0.8163],\n",
      "        [0.9001],\n",
      "        [0.7323],\n",
      "        [0.6561],\n",
      "        [0.8947],\n",
      "        [0.6764],\n",
      "        [0.8767],\n",
      "        [0.9822],\n",
      "        [0.8267],\n",
      "        [0.6424],\n",
      "        [0.6243],\n",
      "        [0.5634],\n",
      "        [0.8369],\n",
      "        [0.8794],\n",
      "        [0.9253],\n",
      "        [0.6119],\n",
      "        [0.9077],\n",
      "        [0.5914],\n",
      "        [0.9471],\n",
      "        [0.9564],\n",
      "        [0.6567],\n",
      "        [0.8352],\n",
      "        [0.7371],\n",
      "        [0.6550],\n",
      "        [0.6671],\n",
      "        [0.9973],\n",
      "        [0.8516],\n",
      "        [0.8718],\n",
      "        [1.0150],\n",
      "        [0.6245],\n",
      "        [0.5868],\n",
      "        [0.7457],\n",
      "        [0.6467],\n",
      "        [0.8238],\n",
      "        [0.6333],\n",
      "        [0.5586],\n",
      "        [0.6894],\n",
      "        [0.5916],\n",
      "        [0.8922],\n",
      "        [0.8347],\n",
      "        [0.6453],\n",
      "        [0.6767],\n",
      "        [0.8740],\n",
      "        [0.5812],\n",
      "        [0.8436],\n",
      "        [0.9234],\n",
      "        [0.9488],\n",
      "        [0.5871],\n",
      "        [0.7030],\n",
      "        [0.6840],\n",
      "        [0.6788],\n",
      "        [0.8684],\n",
      "        [0.6715],\n",
      "        [0.8770],\n",
      "        [0.9013],\n",
      "        [0.8009],\n",
      "        [0.8048],\n",
      "        [0.8805],\n",
      "        [0.6722],\n",
      "        [0.6228],\n",
      "        [0.6391],\n",
      "        [0.6826],\n",
      "        [0.8428],\n",
      "        [1.0191],\n",
      "        [0.8963],\n",
      "        [0.8455],\n",
      "        [0.6920],\n",
      "        [0.8900],\n",
      "        [0.5708],\n",
      "        [0.8592],\n",
      "        [0.8415],\n",
      "        [0.6467],\n",
      "        [0.6393],\n",
      "        [0.7971],\n",
      "        [0.8766],\n",
      "        [0.8830],\n",
      "        [0.8064],\n",
      "        [0.7144],\n",
      "        [0.6118],\n",
      "        [0.5564],\n",
      "        [0.6284],\n",
      "        [0.9033],\n",
      "        [0.9132],\n",
      "        [0.8559],\n",
      "        [0.9346],\n",
      "        [0.7302],\n",
      "        [0.5808],\n",
      "        [0.6990],\n",
      "        [0.8185],\n",
      "        [0.7010],\n",
      "        [0.8770],\n",
      "        [0.8810],\n",
      "        [0.6718],\n",
      "        [0.5329],\n",
      "        [0.7071],\n",
      "        [0.8480],\n",
      "        [0.8710],\n",
      "        [0.8212],\n",
      "        [0.6866],\n",
      "        [0.7564],\n",
      "        [0.8369],\n",
      "        [0.7943],\n",
      "        [0.7890]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0024],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0040],\n",
      "        [    0.0051],\n",
      "        [    0.0053],\n",
      "        [    0.0058],\n",
      "        [    0.0060],\n",
      "        [    0.0070],\n",
      "        [    0.0073],\n",
      "        [    0.0078],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0095],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0103],\n",
      "        [    0.0106],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0125],\n",
      "        [    0.0127],\n",
      "        [    0.0140],\n",
      "        [    0.0140],\n",
      "        [    0.0145],\n",
      "        [    0.0149],\n",
      "        [    0.0150],\n",
      "        [    0.0151],\n",
      "        [    0.0151],\n",
      "        [    0.0158],\n",
      "        [    0.0166],\n",
      "        [    0.0168],\n",
      "        [    0.0173],\n",
      "        [    0.0178],\n",
      "        [    0.0185],\n",
      "        [    0.0185],\n",
      "        [    0.0186],\n",
      "        [    0.0187],\n",
      "        [    0.0193],\n",
      "        [    0.0203],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0207],\n",
      "        [    0.0215],\n",
      "        [    0.0216],\n",
      "        [    0.0217],\n",
      "        [    0.0218],\n",
      "        [    0.0221],\n",
      "        [    0.0222],\n",
      "        [    0.0222],\n",
      "        [    0.0229],\n",
      "        [    0.0237],\n",
      "        [    0.0239],\n",
      "        [    0.0253],\n",
      "        [    0.0254],\n",
      "        [    0.0263],\n",
      "        [    0.0265],\n",
      "        [    0.0267],\n",
      "        [    0.0278],\n",
      "        [    0.0281],\n",
      "        [    0.0281],\n",
      "        [    0.0288],\n",
      "        [    0.0289],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0308],\n",
      "        [    0.0308],\n",
      "        [    0.0312],\n",
      "        [    0.0316],\n",
      "        [    0.0325],\n",
      "        [    0.0327],\n",
      "        [    0.0330],\n",
      "        [    0.0335],\n",
      "        [    0.0336],\n",
      "        [    0.0338],\n",
      "        [    0.0341],\n",
      "        [    0.0347],\n",
      "        [    0.0353],\n",
      "        [    0.0354],\n",
      "        [    0.0356],\n",
      "        [    0.0363],\n",
      "        [    0.0377],\n",
      "        [    0.0386],\n",
      "        [    0.0388],\n",
      "        [    0.0400],\n",
      "        [    0.0404],\n",
      "        [    0.0412],\n",
      "        [    0.0422],\n",
      "        [    0.0438],\n",
      "        [    0.0443],\n",
      "        [    0.0457],\n",
      "        [    0.0479],\n",
      "        [    0.0499],\n",
      "        [    0.0514],\n",
      "        [    0.0515],\n",
      "        [    0.0523],\n",
      "        [    0.0534],\n",
      "        [    0.0571],\n",
      "        [    0.0591],\n",
      "        [    0.0599],\n",
      "        [    0.0607],\n",
      "        [    0.0609],\n",
      "        [    0.0612],\n",
      "        [    0.0615],\n",
      "        [    0.0630],\n",
      "        [    0.0648],\n",
      "        [    0.0671],\n",
      "        [    0.0695],\n",
      "        [    0.0712],\n",
      "        [    0.0724],\n",
      "        [    0.0774],\n",
      "        [    0.0782],\n",
      "        [    0.0784],\n",
      "        [    0.0830],\n",
      "        [    0.0909],\n",
      "        [    0.0998],\n",
      "        [    0.1041],\n",
      "        [    0.1111],\n",
      "        [    0.1173],\n",
      "        [    0.1197],\n",
      "        [    0.1229],\n",
      "        [    0.1301],\n",
      "        [    0.1358],\n",
      "        [    0.1398]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0007],\n",
      "        [    0.0001],\n",
      "        [    0.0013],\n",
      "        [    0.0011],\n",
      "        [    0.0003],\n",
      "        [    0.0041],\n",
      "        [    0.0032],\n",
      "        [    0.0032],\n",
      "        [    0.0018],\n",
      "        [    0.0059],\n",
      "        [    0.0042],\n",
      "        [    0.0058],\n",
      "        [    0.0041],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0083],\n",
      "        [    0.0087],\n",
      "        [    0.0078],\n",
      "        [    0.0094],\n",
      "        [    0.0103],\n",
      "        [    0.0092],\n",
      "        [    0.0106],\n",
      "        [    0.0106],\n",
      "        [    0.0115],\n",
      "        [    0.0125],\n",
      "        [    0.0139],\n",
      "        [    0.0136],\n",
      "        [    0.0136],\n",
      "        [    0.0123],\n",
      "        [    0.0147],\n",
      "        [    0.0174],\n",
      "        [    0.0140],\n",
      "        [    0.0155],\n",
      "        [    0.0160],\n",
      "        [    0.0132],\n",
      "        [    0.0167],\n",
      "        [    0.0162],\n",
      "        [    0.0180],\n",
      "        [    0.0179],\n",
      "        [    0.0159],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0198],\n",
      "        [    0.0174],\n",
      "        [    0.0172],\n",
      "        [    0.0187],\n",
      "        [    0.0216],\n",
      "        [    0.0201],\n",
      "        [    0.0196],\n",
      "        [    0.0214],\n",
      "        [    0.0243],\n",
      "        [    0.0228],\n",
      "        [    0.0209],\n",
      "        [    0.0219],\n",
      "        [    0.0219],\n",
      "        [    0.0222],\n",
      "        [    0.0217],\n",
      "        [    0.0238],\n",
      "        [    0.0226],\n",
      "        [    0.0214],\n",
      "        [    0.0255],\n",
      "        [    0.0255],\n",
      "        [    0.0270],\n",
      "        [    0.0248],\n",
      "        [    0.0290],\n",
      "        [    0.0284],\n",
      "        [    0.0305],\n",
      "        [    0.0276],\n",
      "        [    0.0286],\n",
      "        [    0.0304],\n",
      "        [    0.0303],\n",
      "        [    0.0295],\n",
      "        [    0.0327],\n",
      "        [    0.0299],\n",
      "        [    0.0333],\n",
      "        [    0.0307],\n",
      "        [    0.0346],\n",
      "        [    0.0343],\n",
      "        [    0.0349],\n",
      "        [    0.0328],\n",
      "        [    0.0354],\n",
      "        [    0.0325],\n",
      "        [    0.0360],\n",
      "        [    0.0369],\n",
      "        [    0.0356],\n",
      "        [    0.0362],\n",
      "        [    0.0377],\n",
      "        [    0.0374],\n",
      "        [    0.0370],\n",
      "        [    0.0380],\n",
      "        [    0.0389],\n",
      "        [    0.0389],\n",
      "        [    0.0414],\n",
      "        [    0.0413],\n",
      "        [    0.0444],\n",
      "        [    0.0421],\n",
      "        [    0.0433],\n",
      "        [    0.0461],\n",
      "        [    0.0450],\n",
      "        [    0.0506],\n",
      "        [    0.0518],\n",
      "        [    0.0480],\n",
      "        [    0.0526],\n",
      "        [    0.0521],\n",
      "        [    0.0574],\n",
      "        [    0.0548],\n",
      "        [    0.0591],\n",
      "        [    0.0595],\n",
      "        [    0.0610],\n",
      "        [    0.0617],\n",
      "        [    0.0614],\n",
      "        [    0.0630],\n",
      "        [    0.0647],\n",
      "        [    0.0650],\n",
      "        [    0.0682],\n",
      "        [    0.0705],\n",
      "        [    0.0692],\n",
      "        [    0.0745],\n",
      "        [    0.0800],\n",
      "        [    0.0801],\n",
      "        [    0.0820],\n",
      "        [    0.0892],\n",
      "        [    0.0986],\n",
      "        [    0.1030],\n",
      "        [    0.1117],\n",
      "        [    0.1152],\n",
      "        [    0.1184],\n",
      "        [    0.1192],\n",
      "        [    0.1291],\n",
      "        [    0.1326],\n",
      "        [    0.1371]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.950130701065063\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4698130712531565e-08, 85)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [85, 66, 32, 74, 33, 68, 134, 52, 98, 50, 112, 21, 57, 53, 62, 99, 56, 135, 23, 97, 96, 26, 100, 137, 27, 49, 63, 105, 90, 13, 110, 18, 55, 130, 91, 36, 12, 17, 131, 75, 102, 104, 22, 128, 51, 113, 16, 8, 111, 34, 77, 15, 10, 29, 9, 30, 123, 136, 101, 76, 124, 28, 109, 138, 25, 129, 139, 93, 122, 54, 3, 92, 61, 127, 46, 64, 44, 14, 86, 87, 89, 95, 35, 58, 20, 65, 132, 103, 94, 140, 11, 0, 31, 19, 48, 121, 88, 67, 1, 73, 106, 107, 47, 4, 83, 37, 133, 126, 108, 6, 2, 24, 7, 78, 120, 72, 141, 84, 60, 59, 114, 125, 142, 43, 5, 69, 115, 82, 42, 71, 70, 143] 數值 torch.Size([132, 1])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6609],\n",
      "        [0.8358],\n",
      "        [0.8101],\n",
      "        [0.7595],\n",
      "        [0.8187],\n",
      "        [0.8185],\n",
      "        [0.6397],\n",
      "        [0.8867],\n",
      "        [0.6730],\n",
      "        [0.8934],\n",
      "        [0.6406],\n",
      "        [0.8476],\n",
      "        [0.9141],\n",
      "        [0.8752],\n",
      "        [0.8752],\n",
      "        [0.6515],\n",
      "        [0.8976],\n",
      "        [0.6303],\n",
      "        [0.8546],\n",
      "        [0.6742],\n",
      "        [0.6847],\n",
      "        [0.8210],\n",
      "        [0.6248],\n",
      "        [0.6402],\n",
      "        [0.8159],\n",
      "        [0.8399],\n",
      "        [0.8748],\n",
      "        [0.6658],\n",
      "        [0.6897],\n",
      "        [0.9694],\n",
      "        [0.6555],\n",
      "        [0.8994],\n",
      "        [0.8941],\n",
      "        [0.5648],\n",
      "        [0.6755],\n",
      "        [0.8264],\n",
      "        [0.9806],\n",
      "        [0.9062],\n",
      "        [0.6132],\n",
      "        [0.7289],\n",
      "        [0.6233],\n",
      "        [0.6410],\n",
      "        [0.8363],\n",
      "        [0.5920],\n",
      "        [0.8790],\n",
      "        [0.6556],\n",
      "        [0.9240],\n",
      "        [0.9566],\n",
      "        [0.6663],\n",
      "        [0.8358],\n",
      "        [0.7432],\n",
      "        [0.9457],\n",
      "        [1.0144],\n",
      "        [0.8514],\n",
      "        [0.9974],\n",
      "        [0.8718],\n",
      "        [0.5879],\n",
      "        [0.6561],\n",
      "        [0.6236],\n",
      "        [0.7343],\n",
      "        [0.5603],\n",
      "        [0.8237],\n",
      "        [0.6465],\n",
      "        [0.6340],\n",
      "        [0.8342],\n",
      "        [0.5922],\n",
      "        [0.6452],\n",
      "        [0.6871],\n",
      "        [0.5810],\n",
      "        [0.9225],\n",
      "        [0.8750],\n",
      "        [0.6753],\n",
      "        [0.8897],\n",
      "        [0.5880],\n",
      "        [0.8757],\n",
      "        [0.8416],\n",
      "        [0.8690],\n",
      "        [0.9467],\n",
      "        [0.6825],\n",
      "        [0.7008],\n",
      "        [0.6768],\n",
      "        [0.6696],\n",
      "        [0.8044],\n",
      "        [0.8994],\n",
      "        [0.8797],\n",
      "        [0.7987],\n",
      "        [0.6397],\n",
      "        [0.6218],\n",
      "        [0.6701],\n",
      "        [0.6819],\n",
      "        [1.0179],\n",
      "        [0.8429],\n",
      "        [0.8456],\n",
      "        [0.8952],\n",
      "        [0.8884],\n",
      "        [0.5698],\n",
      "        [0.6899],\n",
      "        [0.8386],\n",
      "        [0.8596],\n",
      "        [0.7935],\n",
      "        [0.6460],\n",
      "        [0.6390],\n",
      "        [0.8817],\n",
      "        [0.8764],\n",
      "        [0.7101],\n",
      "        [0.8061],\n",
      "        [0.6126],\n",
      "        [0.5575],\n",
      "        [0.6283],\n",
      "        [0.9134],\n",
      "        [0.9038],\n",
      "        [0.8558],\n",
      "        [0.9347],\n",
      "        [0.7280],\n",
      "        [0.5796],\n",
      "        [0.8153],\n",
      "        [0.6983],\n",
      "        [0.6981],\n",
      "        [0.8753],\n",
      "        [0.8792],\n",
      "        [0.6709],\n",
      "        [0.5347],\n",
      "        [0.7058],\n",
      "        [0.8492],\n",
      "        [0.8704],\n",
      "        [0.8190],\n",
      "        [0.6853],\n",
      "        [0.7527],\n",
      "        [0.8378],\n",
      "        [0.7910],\n",
      "        [0.7864],\n",
      "        [0.7306]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0007],\n",
      "        [    0.0011],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0032],\n",
      "        [    0.0032],\n",
      "        [    0.0041],\n",
      "        [    0.0041],\n",
      "        [    0.0042],\n",
      "        [    0.0058],\n",
      "        [    0.0059],\n",
      "        [    0.0078],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0083],\n",
      "        [    0.0087],\n",
      "        [    0.0092],\n",
      "        [    0.0094],\n",
      "        [    0.0103],\n",
      "        [    0.0106],\n",
      "        [    0.0106],\n",
      "        [    0.0115],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0132],\n",
      "        [    0.0136],\n",
      "        [    0.0136],\n",
      "        [    0.0139],\n",
      "        [    0.0140],\n",
      "        [    0.0147],\n",
      "        [    0.0155],\n",
      "        [    0.0159],\n",
      "        [    0.0160],\n",
      "        [    0.0162],\n",
      "        [    0.0167],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0174],\n",
      "        [    0.0179],\n",
      "        [    0.0180],\n",
      "        [    0.0185],\n",
      "        [    0.0187],\n",
      "        [    0.0189],\n",
      "        [    0.0196],\n",
      "        [    0.0198],\n",
      "        [    0.0201],\n",
      "        [    0.0209],\n",
      "        [    0.0214],\n",
      "        [    0.0214],\n",
      "        [    0.0216],\n",
      "        [    0.0217],\n",
      "        [    0.0219],\n",
      "        [    0.0219],\n",
      "        [    0.0222],\n",
      "        [    0.0226],\n",
      "        [    0.0228],\n",
      "        [    0.0238],\n",
      "        [    0.0243],\n",
      "        [    0.0248],\n",
      "        [    0.0255],\n",
      "        [    0.0255],\n",
      "        [    0.0270],\n",
      "        [    0.0276],\n",
      "        [    0.0284],\n",
      "        [    0.0286],\n",
      "        [    0.0290],\n",
      "        [    0.0295],\n",
      "        [    0.0299],\n",
      "        [    0.0303],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0307],\n",
      "        [    0.0325],\n",
      "        [    0.0327],\n",
      "        [    0.0328],\n",
      "        [    0.0333],\n",
      "        [    0.0343],\n",
      "        [    0.0346],\n",
      "        [    0.0349],\n",
      "        [    0.0354],\n",
      "        [    0.0356],\n",
      "        [    0.0360],\n",
      "        [    0.0362],\n",
      "        [    0.0369],\n",
      "        [    0.0370],\n",
      "        [    0.0374],\n",
      "        [    0.0377],\n",
      "        [    0.0380],\n",
      "        [    0.0389],\n",
      "        [    0.0389],\n",
      "        [    0.0413],\n",
      "        [    0.0414],\n",
      "        [    0.0421],\n",
      "        [    0.0433],\n",
      "        [    0.0444],\n",
      "        [    0.0450],\n",
      "        [    0.0461],\n",
      "        [    0.0480],\n",
      "        [    0.0506],\n",
      "        [    0.0518],\n",
      "        [    0.0521],\n",
      "        [    0.0526],\n",
      "        [    0.0548],\n",
      "        [    0.0574],\n",
      "        [    0.0591],\n",
      "        [    0.0595],\n",
      "        [    0.0610],\n",
      "        [    0.0614],\n",
      "        [    0.0617],\n",
      "        [    0.0630],\n",
      "        [    0.0647],\n",
      "        [    0.0650],\n",
      "        [    0.0682],\n",
      "        [    0.0692],\n",
      "        [    0.0705],\n",
      "        [    0.0745],\n",
      "        [    0.0800],\n",
      "        [    0.0801],\n",
      "        [    0.0820],\n",
      "        [    0.0892],\n",
      "        [    0.0986],\n",
      "        [    0.1030],\n",
      "        [    0.1117],\n",
      "        [    0.1152],\n",
      "        [    0.1184],\n",
      "        [    0.1192],\n",
      "        [    0.1291],\n",
      "        [    0.1326],\n",
      "        [    0.1371],\n",
      "        [    0.1399]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0005],\n",
      "        [0.0025],\n",
      "        [0.0011],\n",
      "        [0.0006],\n",
      "        [0.0021],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0028],\n",
      "        [0.0045],\n",
      "        [0.0038],\n",
      "        [0.0046],\n",
      "        [0.0056],\n",
      "        [0.0096],\n",
      "        [0.0078],\n",
      "        [0.0070],\n",
      "        [0.0075],\n",
      "        [0.0074],\n",
      "        [0.0107],\n",
      "        [0.0079],\n",
      "        [0.0090],\n",
      "        [0.0099],\n",
      "        [0.0094],\n",
      "        [0.0108],\n",
      "        [0.0132],\n",
      "        [0.0124],\n",
      "        [0.0140],\n",
      "        [0.0133],\n",
      "        [0.0129],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0162],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0162],\n",
      "        [0.0179],\n",
      "        [0.0178],\n",
      "        [0.0174],\n",
      "        [0.0172],\n",
      "        [0.0178],\n",
      "        [0.0173],\n",
      "        [0.0197],\n",
      "        [0.0175],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0182],\n",
      "        [0.0206],\n",
      "        [0.0234],\n",
      "        [0.0220],\n",
      "        [0.0208],\n",
      "        [0.0232],\n",
      "        [0.0228],\n",
      "        [0.0240],\n",
      "        [0.0235],\n",
      "        [0.0229],\n",
      "        [0.0220],\n",
      "        [0.0227],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0244],\n",
      "        [0.0250],\n",
      "        [0.0259],\n",
      "        [0.0288],\n",
      "        [0.0273],\n",
      "        [0.0266],\n",
      "        [0.0289],\n",
      "        [0.0282],\n",
      "        [0.0312],\n",
      "        [0.0334],\n",
      "        [0.0296],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0333],\n",
      "        [0.0320],\n",
      "        [0.0301],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0344],\n",
      "        [0.0359],\n",
      "        [0.0352],\n",
      "        [0.0366],\n",
      "        [0.0381],\n",
      "        [0.0367],\n",
      "        [0.0374],\n",
      "        [0.0355],\n",
      "        [0.0399],\n",
      "        [0.0414],\n",
      "        [0.0427],\n",
      "        [0.0407],\n",
      "        [0.0425],\n",
      "        [0.0417],\n",
      "        [0.0442],\n",
      "        [0.0447],\n",
      "        [0.0489],\n",
      "        [0.0480],\n",
      "        [0.0500],\n",
      "        [0.0511],\n",
      "        [0.0529],\n",
      "        [0.0507],\n",
      "        [0.0530],\n",
      "        [0.0561],\n",
      "        [0.0603],\n",
      "        [0.0597],\n",
      "        [0.0602],\n",
      "        [0.0595],\n",
      "        [0.0645],\n",
      "        [0.0646],\n",
      "        [0.0630],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0695],\n",
      "        [0.0679],\n",
      "        [0.0740],\n",
      "        [0.0795],\n",
      "        [0.0798],\n",
      "        [0.0815],\n",
      "        [0.0887],\n",
      "        [0.0954],\n",
      "        [0.0998],\n",
      "        [0.1103],\n",
      "        [0.1158],\n",
      "        [0.1174],\n",
      "        [0.1185],\n",
      "        [0.1263],\n",
      "        [0.1326],\n",
      "        [0.1376],\n",
      "        [0.1364]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.2282497882843\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.632162280884586e-07, 85)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [85, 66, 33, 74, 68, 32, 98, 112, 50, 21, 134, 52, 57, 99, 135, 56, 62, 97, 96, 100, 53, 26, 23, 137, 49, 90, 27, 105, 18, 13, 63, 55, 110, 91, 36, 12, 130, 102, 22, 75, 51, 131, 104, 17, 8, 16, 113, 128, 111, 15, 136, 77, 101, 29, 123, 10, 34, 30, 9, 76, 124, 28, 109, 138, 139, 129, 122, 25, 93, 92, 44, 61, 127, 54, 64, 46, 14, 3, 86, 35, 87, 89, 95, 20, 140, 58, 65, 103, 94, 132, 11, 19, 0, 121, 48, 31, 88, 67, 73, 1, 106, 4, 107, 47, 83, 37, 6, 126, 108, 133, 7, 2, 24, 78, 120, 141, 72, 84, 60, 59, 114, 125, 142, 43, 5, 69, 115, 82, 42, 71, 143, 70, 119] 數值 torch.Size([133, 1])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6615],\n",
      "        [0.8360],\n",
      "        [0.8206],\n",
      "        [0.7595],\n",
      "        [0.8188],\n",
      "        [0.8120],\n",
      "        [0.6744],\n",
      "        [0.6402],\n",
      "        [0.8937],\n",
      "        [0.8488],\n",
      "        [0.6381],\n",
      "        [0.8884],\n",
      "        [0.9144],\n",
      "        [0.6527],\n",
      "        [0.6290],\n",
      "        [0.8984],\n",
      "        [0.8755],\n",
      "        [0.6756],\n",
      "        [0.6860],\n",
      "        [0.6260],\n",
      "        [0.8770],\n",
      "        [0.8216],\n",
      "        [0.8561],\n",
      "        [0.6395],\n",
      "        [0.8400],\n",
      "        [0.6905],\n",
      "        [0.8169],\n",
      "        [0.6662],\n",
      "        [0.9006],\n",
      "        [0.9697],\n",
      "        [0.8756],\n",
      "        [0.8956],\n",
      "        [0.6556],\n",
      "        [0.6767],\n",
      "        [0.8276],\n",
      "        [0.9811],\n",
      "        [0.5645],\n",
      "        [0.6240],\n",
      "        [0.8374],\n",
      "        [0.7289],\n",
      "        [0.8804],\n",
      "        [0.6127],\n",
      "        [0.6412],\n",
      "        [0.9068],\n",
      "        [0.9585],\n",
      "        [0.9249],\n",
      "        [0.6549],\n",
      "        [0.5911],\n",
      "        [0.6660],\n",
      "        [0.9466],\n",
      "        [0.6553],\n",
      "        [0.7438],\n",
      "        [0.6247],\n",
      "        [0.8523],\n",
      "        [0.5876],\n",
      "        [1.0159],\n",
      "        [0.8379],\n",
      "        [0.8731],\n",
      "        [0.9995],\n",
      "        [0.7344],\n",
      "        [0.5607],\n",
      "        [0.8247],\n",
      "        [0.6470],\n",
      "        [0.6328],\n",
      "        [0.6432],\n",
      "        [0.5911],\n",
      "        [0.5796],\n",
      "        [0.8353],\n",
      "        [0.6871],\n",
      "        [0.6760],\n",
      "        [0.8718],\n",
      "        [0.8897],\n",
      "        [0.5875],\n",
      "        [0.9238],\n",
      "        [0.8424],\n",
      "        [0.8766],\n",
      "        [0.9466],\n",
      "        [0.8781],\n",
      "        [0.6832],\n",
      "        [0.8056],\n",
      "        [0.7010],\n",
      "        [0.6772],\n",
      "        [0.6702],\n",
      "        [0.8807],\n",
      "        [0.6795],\n",
      "        [0.8996],\n",
      "        [0.7991],\n",
      "        [0.6224],\n",
      "        [0.6703],\n",
      "        [0.6386],\n",
      "        [1.0189],\n",
      "        [0.8960],\n",
      "        [0.8454],\n",
      "        [0.5682],\n",
      "        [0.8888],\n",
      "        [0.8469],\n",
      "        [0.6900],\n",
      "        [0.8383],\n",
      "        [0.7935],\n",
      "        [0.8625],\n",
      "        [0.6466],\n",
      "        [0.8783],\n",
      "        [0.6396],\n",
      "        [0.8826],\n",
      "        [0.7083],\n",
      "        [0.8075],\n",
      "        [0.9153],\n",
      "        [0.5573],\n",
      "        [0.6291],\n",
      "        [0.6115],\n",
      "        [0.9364],\n",
      "        [0.9067],\n",
      "        [0.8575],\n",
      "        [0.7290],\n",
      "        [0.5781],\n",
      "        [0.6956],\n",
      "        [0.8157],\n",
      "        [0.6976],\n",
      "        [0.8757],\n",
      "        [0.8796],\n",
      "        [0.6703],\n",
      "        [0.5352],\n",
      "        [0.7026],\n",
      "        [0.8524],\n",
      "        [0.8718],\n",
      "        [0.8196],\n",
      "        [0.6843],\n",
      "        [0.7520],\n",
      "        [0.8406],\n",
      "        [0.7911],\n",
      "        [0.7271],\n",
      "        [0.7868],\n",
      "        [0.6350]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0011],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0028],\n",
      "        [0.0038],\n",
      "        [0.0045],\n",
      "        [0.0046],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0056],\n",
      "        [0.0070],\n",
      "        [0.0074],\n",
      "        [0.0075],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0124],\n",
      "        [0.0129],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0140],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0162],\n",
      "        [0.0162],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0178],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0182],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0227],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0232],\n",
      "        [0.0234],\n",
      "        [0.0235],\n",
      "        [0.0240],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0244],\n",
      "        [0.0250],\n",
      "        [0.0259],\n",
      "        [0.0266],\n",
      "        [0.0273],\n",
      "        [0.0282],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0296],\n",
      "        [0.0301],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0312],\n",
      "        [0.0320],\n",
      "        [0.0333],\n",
      "        [0.0334],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0344],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0352],\n",
      "        [0.0355],\n",
      "        [0.0359],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0374],\n",
      "        [0.0381],\n",
      "        [0.0399],\n",
      "        [0.0407],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0427],\n",
      "        [0.0442],\n",
      "        [0.0447],\n",
      "        [0.0480],\n",
      "        [0.0489],\n",
      "        [0.0500],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0529],\n",
      "        [0.0530],\n",
      "        [0.0561],\n",
      "        [0.0595],\n",
      "        [0.0597],\n",
      "        [0.0602],\n",
      "        [0.0603],\n",
      "        [0.0630],\n",
      "        [0.0645],\n",
      "        [0.0646],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0679],\n",
      "        [0.0695],\n",
      "        [0.0740],\n",
      "        [0.0795],\n",
      "        [0.0798],\n",
      "        [0.0815],\n",
      "        [0.0887],\n",
      "        [0.0954],\n",
      "        [0.0998],\n",
      "        [0.1103],\n",
      "        [0.1158],\n",
      "        [0.1174],\n",
      "        [0.1185],\n",
      "        [0.1263],\n",
      "        [0.1326],\n",
      "        [0.1364],\n",
      "        [0.1376],\n",
      "        [0.1396]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0005],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0012],\n",
      "        [0.0031],\n",
      "        [0.0036],\n",
      "        [0.0009],\n",
      "        [0.0035],\n",
      "        [0.0044],\n",
      "        [0.0082],\n",
      "        [0.0055],\n",
      "        [0.0064],\n",
      "        [0.0079],\n",
      "        [0.0043],\n",
      "        [0.0078],\n",
      "        [0.0088],\n",
      "        [0.0085],\n",
      "        [0.0097],\n",
      "        [0.0104],\n",
      "        [0.0100],\n",
      "        [0.0106],\n",
      "        [0.0111],\n",
      "        [0.0083],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0128],\n",
      "        [0.0152],\n",
      "        [0.0130],\n",
      "        [0.0138],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0118],\n",
      "        [0.0153],\n",
      "        [0.0150],\n",
      "        [0.0160],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0174],\n",
      "        [0.0191],\n",
      "        [0.0173],\n",
      "        [0.0202],\n",
      "        [0.0199],\n",
      "        [0.0179],\n",
      "        [0.0168],\n",
      "        [0.0186],\n",
      "        [0.0158],\n",
      "        [0.0225],\n",
      "        [0.0181],\n",
      "        [0.0205],\n",
      "        [0.0194],\n",
      "        [0.0210],\n",
      "        [0.0239],\n",
      "        [0.0225],\n",
      "        [0.0255],\n",
      "        [0.0244],\n",
      "        [0.0242],\n",
      "        [0.0235],\n",
      "        [0.0257],\n",
      "        [0.0256],\n",
      "        [0.0263],\n",
      "        [0.0247],\n",
      "        [0.0268],\n",
      "        [0.0229],\n",
      "        [0.0228],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0287],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0285],\n",
      "        [0.0319],\n",
      "        [0.0336],\n",
      "        [0.0314],\n",
      "        [0.0326],\n",
      "        [0.0330],\n",
      "        [0.0339],\n",
      "        [0.0358],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0360],\n",
      "        [0.0359],\n",
      "        [0.0362],\n",
      "        [0.0352],\n",
      "        [0.0312],\n",
      "        [0.0368],\n",
      "        [0.0377],\n",
      "        [0.0384],\n",
      "        [0.0392],\n",
      "        [0.0410],\n",
      "        [0.0406],\n",
      "        [0.0407],\n",
      "        [0.0434],\n",
      "        [0.0374],\n",
      "        [0.0419],\n",
      "        [0.0427],\n",
      "        [0.0458],\n",
      "        [0.0432],\n",
      "        [0.0467],\n",
      "        [0.0512],\n",
      "        [0.0516],\n",
      "        [0.0495],\n",
      "        [0.0527],\n",
      "        [0.0527],\n",
      "        [0.0495],\n",
      "        [0.0561],\n",
      "        [0.0581],\n",
      "        [0.0621],\n",
      "        [0.0617],\n",
      "        [0.0633],\n",
      "        [0.0617],\n",
      "        [0.0669],\n",
      "        [0.0653],\n",
      "        [0.0654],\n",
      "        [0.0625],\n",
      "        [0.0635],\n",
      "        [0.0686],\n",
      "        [0.0718],\n",
      "        [0.0803],\n",
      "        [0.0806],\n",
      "        [0.0785],\n",
      "        [0.0905],\n",
      "        [0.0904],\n",
      "        [0.0977],\n",
      "        [0.1096],\n",
      "        [0.1151],\n",
      "        [0.1141],\n",
      "        [0.1161],\n",
      "        [0.1248],\n",
      "        [0.1313],\n",
      "        [0.1312],\n",
      "        [0.1365],\n",
      "        [0.1360]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.50740623474121\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.782574028969975e-07, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [66, 85, 112, 33, 68, 74, 32, 50, 98, 135, 21, 52, 57, 56, 99, 134, 137, 97, 62, 96, 53, 100, 26, 23, 110, 27, 18, 63, 49, 55, 13, 90, 36, 105, 91, 113, 12, 8, 51, 22, 17, 111, 130, 16, 102, 75, 136, 104, 131, 15, 77, 29, 128, 139, 138, 30, 101, 34, 129, 10, 122, 28, 123, 76, 9, 124, 109, 44, 25, 93, 92, 140, 54, 61, 64, 46, 127, 14, 86, 35, 20, 3, 89, 87, 95, 58, 121, 65, 103, 94, 11, 19, 132, 48, 31, 67, 0, 88, 73, 4, 83, 1, 106, 47, 107, 37, 6, 108, 7, 126, 120, 133, 141, 24, 78, 2, 72, 84, 114, 60, 59, 142, 125, 43, 5, 115, 69, 82, 42, 143, 71, 119, 70, 41] 數值 torch.Size([134, 1])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8350],\n",
      "        [0.6602],\n",
      "        [0.6373],\n",
      "        [0.8212],\n",
      "        [0.8179],\n",
      "        [0.7579],\n",
      "        [0.8125],\n",
      "        [0.8927],\n",
      "        [0.6736],\n",
      "        [0.6259],\n",
      "        [0.8490],\n",
      "        [0.8890],\n",
      "        [0.9135],\n",
      "        [0.8981],\n",
      "        [0.6519],\n",
      "        [0.6347],\n",
      "        [0.6369],\n",
      "        [0.6750],\n",
      "        [0.8745],\n",
      "        [0.6853],\n",
      "        [0.8774],\n",
      "        [0.6249],\n",
      "        [0.8209],\n",
      "        [0.8565],\n",
      "        [0.6533],\n",
      "        [0.8164],\n",
      "        [0.9011],\n",
      "        [0.8751],\n",
      "        [0.8388],\n",
      "        [0.8960],\n",
      "        [0.9695],\n",
      "        [0.6895],\n",
      "        [0.8275],\n",
      "        [0.6643],\n",
      "        [0.6761],\n",
      "        [0.6518],\n",
      "        [0.9812],\n",
      "        [0.9599],\n",
      "        [0.8806],\n",
      "        [0.8374],\n",
      "        [0.9069],\n",
      "        [0.6635],\n",
      "        [0.5622],\n",
      "        [0.9252],\n",
      "        [0.6225],\n",
      "        [0.7272],\n",
      "        [0.6527],\n",
      "        [0.6391],\n",
      "        [0.6104],\n",
      "        [0.9468],\n",
      "        [0.7428],\n",
      "        [0.8520],\n",
      "        [0.5882],\n",
      "        [0.6394],\n",
      "        [0.6299],\n",
      "        [0.8731],\n",
      "        [0.6235],\n",
      "        [0.8386],\n",
      "        [0.5881],\n",
      "        [1.0171],\n",
      "        [0.5759],\n",
      "        [0.8245],\n",
      "        [0.5850],\n",
      "        [0.7330],\n",
      "        [1.0012],\n",
      "        [0.5589],\n",
      "        [0.6452],\n",
      "        [0.8734],\n",
      "        [0.8353],\n",
      "        [0.6853],\n",
      "        [0.6748],\n",
      "        [0.6752],\n",
      "        [0.9239],\n",
      "        [0.8884],\n",
      "        [0.8418],\n",
      "        [0.8763],\n",
      "        [0.5851],\n",
      "        [0.9461],\n",
      "        [0.6822],\n",
      "        [0.8054],\n",
      "        [0.8807],\n",
      "        [0.8805],\n",
      "        [0.6758],\n",
      "        [0.6995],\n",
      "        [0.6689],\n",
      "        [0.8987],\n",
      "        [0.5639],\n",
      "        [0.7980],\n",
      "        [0.6208],\n",
      "        [0.6686],\n",
      "        [1.0197],\n",
      "        [0.8960],\n",
      "        [0.6357],\n",
      "        [0.8882],\n",
      "        [0.8470],\n",
      "        [0.8368],\n",
      "        [0.8473],\n",
      "        [0.6884],\n",
      "        [0.7922],\n",
      "        [0.8794],\n",
      "        [0.7049],\n",
      "        [0.8647],\n",
      "        [0.6450],\n",
      "        [0.8823],\n",
      "        [0.6380],\n",
      "        [0.8074],\n",
      "        [0.9166],\n",
      "        [0.6275],\n",
      "        [0.9377],\n",
      "        [0.5550],\n",
      "        [0.5739],\n",
      "        [0.6084],\n",
      "        [0.6913],\n",
      "        [0.8581],\n",
      "        [0.7284],\n",
      "        [0.9091],\n",
      "        [0.8147],\n",
      "        [0.6953],\n",
      "        [0.6673],\n",
      "        [0.8750],\n",
      "        [0.8788],\n",
      "        [0.6977],\n",
      "        [0.5333],\n",
      "        [0.8544],\n",
      "        [0.8725],\n",
      "        [0.6810],\n",
      "        [0.8190],\n",
      "        [0.7496],\n",
      "        [0.8421],\n",
      "        [0.7218],\n",
      "        [0.7898],\n",
      "        [0.6314],\n",
      "        [0.7858],\n",
      "        [0.8247]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0055],\n",
      "        [0.0064],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0111],\n",
      "        [0.0118],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0150],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0168],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0185],\n",
      "        [0.0186],\n",
      "        [0.0187],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0202],\n",
      "        [0.0205],\n",
      "        [0.0210],\n",
      "        [0.0225],\n",
      "        [0.0225],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0235],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0244],\n",
      "        [0.0245],\n",
      "        [0.0247],\n",
      "        [0.0255],\n",
      "        [0.0256],\n",
      "        [0.0257],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0326],\n",
      "        [0.0330],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0352],\n",
      "        [0.0358],\n",
      "        [0.0359],\n",
      "        [0.0360],\n",
      "        [0.0362],\n",
      "        [0.0368],\n",
      "        [0.0374],\n",
      "        [0.0377],\n",
      "        [0.0384],\n",
      "        [0.0392],\n",
      "        [0.0406],\n",
      "        [0.0407],\n",
      "        [0.0410],\n",
      "        [0.0419],\n",
      "        [0.0427],\n",
      "        [0.0432],\n",
      "        [0.0434],\n",
      "        [0.0458],\n",
      "        [0.0467],\n",
      "        [0.0495],\n",
      "        [0.0495],\n",
      "        [0.0512],\n",
      "        [0.0516],\n",
      "        [0.0527],\n",
      "        [0.0527],\n",
      "        [0.0561],\n",
      "        [0.0581],\n",
      "        [0.0617],\n",
      "        [0.0617],\n",
      "        [0.0621],\n",
      "        [0.0625],\n",
      "        [0.0633],\n",
      "        [0.0635],\n",
      "        [0.0653],\n",
      "        [0.0654],\n",
      "        [0.0669],\n",
      "        [0.0686],\n",
      "        [0.0718],\n",
      "        [0.0785],\n",
      "        [0.0803],\n",
      "        [0.0806],\n",
      "        [0.0904],\n",
      "        [0.0905],\n",
      "        [0.0977],\n",
      "        [0.1096],\n",
      "        [0.1141],\n",
      "        [0.1151],\n",
      "        [0.1161],\n",
      "        [0.1248],\n",
      "        [0.1312],\n",
      "        [0.1313],\n",
      "        [0.1360],\n",
      "        [0.1365],\n",
      "        [0.1435]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0022],\n",
      "        [    0.0019],\n",
      "        [    0.0010],\n",
      "        [    0.0025],\n",
      "        [    0.0000],\n",
      "        [    0.0041],\n",
      "        [    0.0042],\n",
      "        [    0.0028],\n",
      "        [    0.0041],\n",
      "        [    0.0025],\n",
      "        [    0.0047],\n",
      "        [    0.0063],\n",
      "        [    0.0075],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0104],\n",
      "        [    0.0070],\n",
      "        [    0.0089],\n",
      "        [    0.0098],\n",
      "        [    0.0103],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0113],\n",
      "        [    0.0102],\n",
      "        [    0.0126],\n",
      "        [    0.0133],\n",
      "        [    0.0129],\n",
      "        [    0.0145],\n",
      "        [    0.0131],\n",
      "        [    0.0152],\n",
      "        [    0.0146],\n",
      "        [    0.0141],\n",
      "        [    0.0166],\n",
      "        [    0.0156],\n",
      "        [    0.0136],\n",
      "        [    0.0173],\n",
      "        [    0.0162],\n",
      "        [    0.0168],\n",
      "        [    0.0177],\n",
      "        [    0.0168],\n",
      "        [    0.0163],\n",
      "        [    0.0197],\n",
      "        [    0.0193],\n",
      "        [    0.0198],\n",
      "        [    0.0207],\n",
      "        [    0.0181],\n",
      "        [    0.0216],\n",
      "        [    0.0215],\n",
      "        [    0.0211],\n",
      "        [    0.0203],\n",
      "        [    0.0226],\n",
      "        [    0.0242],\n",
      "        [    0.0200],\n",
      "        [    0.0211],\n",
      "        [    0.0241],\n",
      "        [    0.0247],\n",
      "        [    0.0257],\n",
      "        [    0.0225],\n",
      "        [    0.0245],\n",
      "        [    0.0219],\n",
      "        [    0.0246],\n",
      "        [    0.0268],\n",
      "        [    0.0268],\n",
      "        [    0.0264],\n",
      "        [    0.0269],\n",
      "        [    0.0277],\n",
      "        [    0.0264],\n",
      "        [    0.0285],\n",
      "        [    0.0324],\n",
      "        [    0.0316],\n",
      "        [    0.0280],\n",
      "        [    0.0318],\n",
      "        [    0.0332],\n",
      "        [    0.0333],\n",
      "        [    0.0327],\n",
      "        [    0.0349],\n",
      "        [    0.0356],\n",
      "        [    0.0353],\n",
      "        [    0.0340],\n",
      "        [    0.0356],\n",
      "        [    0.0369],\n",
      "        [    0.0369],\n",
      "        [    0.0372],\n",
      "        [    0.0373],\n",
      "        [    0.0380],\n",
      "        [    0.0345],\n",
      "        [    0.0390],\n",
      "        [    0.0398],\n",
      "        [    0.0408],\n",
      "        [    0.0402],\n",
      "        [    0.0414],\n",
      "        [    0.0428],\n",
      "        [    0.0413],\n",
      "        [    0.0434],\n",
      "        [    0.0411],\n",
      "        [    0.0435],\n",
      "        [    0.0471],\n",
      "        [    0.0454],\n",
      "        [    0.0497],\n",
      "        [    0.0461],\n",
      "        [    0.0517],\n",
      "        [    0.0527],\n",
      "        [    0.0525],\n",
      "        [    0.0536],\n",
      "        [    0.0551],\n",
      "        [    0.0577],\n",
      "        [    0.0625],\n",
      "        [    0.0614],\n",
      "        [    0.0632],\n",
      "        [    0.0596],\n",
      "        [    0.0651],\n",
      "        [    0.0602],\n",
      "        [    0.0657],\n",
      "        [    0.0650],\n",
      "        [    0.0676],\n",
      "        [    0.0676],\n",
      "        [    0.0696],\n",
      "        [    0.0765],\n",
      "        [    0.0811],\n",
      "        [    0.0815],\n",
      "        [    0.0865],\n",
      "        [    0.0911],\n",
      "        [    0.0950],\n",
      "        [    0.1103],\n",
      "        [    0.1116],\n",
      "        [    0.1143],\n",
      "        [    0.1141],\n",
      "        [    0.1225],\n",
      "        [    0.1270],\n",
      "        [    0.1300],\n",
      "        [    0.1338],\n",
      "        [    0.1357],\n",
      "        [    0.1412]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.786296367645264\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7826096154749393e-10, 68)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [68, 112, 85, 66, 33, 135, 50, 74, 98, 32, 21, 52, 137, 57, 56, 99, 97, 62, 110, 96, 134, 53, 100, 26, 23, 27, 63, 55, 18, 113, 36, 49, 90, 13, 91, 8, 111, 105, 51, 17, 12, 22, 136, 16, 130, 102, 139, 77, 75, 15, 138, 131, 104, 122, 129, 29, 30, 128, 10, 28, 101, 34, 9, 44, 76, 123, 124, 109, 140, 25, 92, 54, 93, 46, 61, 64, 35, 121, 127, 86, 14, 20, 89, 3, 87, 95, 58, 65, 103, 11, 94, 67, 48, 19, 132, 31, 0, 73, 83, 88, 4, 1, 47, 106, 107, 37, 6, 120, 141, 7, 108, 126, 78, 133, 24, 2, 72, 84, 114, 60, 59, 142, 125, 43, 5, 115, 82, 69, 42, 143, 71, 119, 70, 41, 38] 數值 torch.Size([135, 1])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8167],\n",
      "        [0.6354],\n",
      "        [0.6591],\n",
      "        [0.8333],\n",
      "        [0.8225],\n",
      "        [0.6241],\n",
      "        [0.8920],\n",
      "        [0.7565],\n",
      "        [0.6731],\n",
      "        [0.8137],\n",
      "        [0.8487],\n",
      "        [0.8898],\n",
      "        [0.6357],\n",
      "        [0.9125],\n",
      "        [0.8978],\n",
      "        [0.6512],\n",
      "        [0.6746],\n",
      "        [0.8735],\n",
      "        [0.6518],\n",
      "        [0.6848],\n",
      "        [0.6326],\n",
      "        [0.8783],\n",
      "        [0.6242],\n",
      "        [0.8203],\n",
      "        [0.8567],\n",
      "        [0.8163],\n",
      "        [0.8745],\n",
      "        [0.8965],\n",
      "        [0.9008],\n",
      "        [0.6496],\n",
      "        [0.8285],\n",
      "        [0.8379],\n",
      "        [0.6888],\n",
      "        [0.9681],\n",
      "        [0.6758],\n",
      "        [0.9605],\n",
      "        [0.6617],\n",
      "        [0.6629],\n",
      "        [0.8810],\n",
      "        [0.9058],\n",
      "        [0.9800],\n",
      "        [0.8371],\n",
      "        [0.6514],\n",
      "        [0.9245],\n",
      "        [0.5610],\n",
      "        [0.6213],\n",
      "        [0.6366],\n",
      "        [0.7420],\n",
      "        [0.7256],\n",
      "        [0.9462],\n",
      "        [0.6281],\n",
      "        [0.6090],\n",
      "        [0.6373],\n",
      "        [0.5734],\n",
      "        [0.5863],\n",
      "        [0.8522],\n",
      "        [0.8737],\n",
      "        [0.5865],\n",
      "        [1.0172],\n",
      "        [0.8246],\n",
      "        [0.6227],\n",
      "        [0.8402],\n",
      "        [1.0019],\n",
      "        [0.8754],\n",
      "        [0.7318],\n",
      "        [0.5837],\n",
      "        [0.5583],\n",
      "        [0.6443],\n",
      "        [0.6720],\n",
      "        [0.8350],\n",
      "        [0.6740],\n",
      "        [0.9243],\n",
      "        [0.6837],\n",
      "        [0.8759],\n",
      "        [0.8871],\n",
      "        [0.8410],\n",
      "        [0.8061],\n",
      "        [0.5611],\n",
      "        [0.5838],\n",
      "        [0.6815],\n",
      "        [0.9444],\n",
      "        [0.8803],\n",
      "        [0.6748],\n",
      "        [0.8816],\n",
      "        [0.6983],\n",
      "        [0.6677],\n",
      "        [0.8975],\n",
      "        [0.7967],\n",
      "        [0.6194],\n",
      "        [1.0192],\n",
      "        [0.6670],\n",
      "        [0.8347],\n",
      "        [0.8875],\n",
      "        [0.8953],\n",
      "        [0.6339],\n",
      "        [0.8477],\n",
      "        [0.8474],\n",
      "        [0.7910],\n",
      "        [0.7014],\n",
      "        [0.6872],\n",
      "        [0.8793],\n",
      "        [0.8652],\n",
      "        [0.8821],\n",
      "        [0.6439],\n",
      "        [0.6372],\n",
      "        [0.8084],\n",
      "        [0.9170],\n",
      "        [0.5710],\n",
      "        [0.6879],\n",
      "        [0.9380],\n",
      "        [0.6268],\n",
      "        [0.5539],\n",
      "        [0.7280],\n",
      "        [0.6066],\n",
      "        [0.8585],\n",
      "        [0.9098],\n",
      "        [0.8138],\n",
      "        [0.6932],\n",
      "        [0.6653],\n",
      "        [0.8741],\n",
      "        [0.8779],\n",
      "        [0.6937],\n",
      "        [0.5328],\n",
      "        [0.8571],\n",
      "        [0.8719],\n",
      "        [0.6785],\n",
      "        [0.7476],\n",
      "        [0.8181],\n",
      "        [0.8445],\n",
      "        [0.7177],\n",
      "        [0.7885],\n",
      "        [0.6292],\n",
      "        [0.7849],\n",
      "        [0.8270],\n",
      "        [0.8065]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0010],\n",
      "        [    0.0019],\n",
      "        [    0.0022],\n",
      "        [    0.0025],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0041],\n",
      "        [    0.0041],\n",
      "        [    0.0042],\n",
      "        [    0.0047],\n",
      "        [    0.0063],\n",
      "        [    0.0070],\n",
      "        [    0.0075],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0089],\n",
      "        [    0.0098],\n",
      "        [    0.0102],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0113],\n",
      "        [    0.0126],\n",
      "        [    0.0129],\n",
      "        [    0.0131],\n",
      "        [    0.0133],\n",
      "        [    0.0136],\n",
      "        [    0.0141],\n",
      "        [    0.0145],\n",
      "        [    0.0146],\n",
      "        [    0.0152],\n",
      "        [    0.0156],\n",
      "        [    0.0162],\n",
      "        [    0.0163],\n",
      "        [    0.0166],\n",
      "        [    0.0168],\n",
      "        [    0.0168],\n",
      "        [    0.0173],\n",
      "        [    0.0177],\n",
      "        [    0.0181],\n",
      "        [    0.0193],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0211],\n",
      "        [    0.0211],\n",
      "        [    0.0215],\n",
      "        [    0.0216],\n",
      "        [    0.0219],\n",
      "        [    0.0225],\n",
      "        [    0.0226],\n",
      "        [    0.0241],\n",
      "        [    0.0242],\n",
      "        [    0.0245],\n",
      "        [    0.0246],\n",
      "        [    0.0247],\n",
      "        [    0.0257],\n",
      "        [    0.0264],\n",
      "        [    0.0264],\n",
      "        [    0.0268],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0277],\n",
      "        [    0.0280],\n",
      "        [    0.0285],\n",
      "        [    0.0316],\n",
      "        [    0.0318],\n",
      "        [    0.0324],\n",
      "        [    0.0327],\n",
      "        [    0.0332],\n",
      "        [    0.0333],\n",
      "        [    0.0340],\n",
      "        [    0.0345],\n",
      "        [    0.0349],\n",
      "        [    0.0353],\n",
      "        [    0.0356],\n",
      "        [    0.0356],\n",
      "        [    0.0369],\n",
      "        [    0.0369],\n",
      "        [    0.0372],\n",
      "        [    0.0373],\n",
      "        [    0.0380],\n",
      "        [    0.0390],\n",
      "        [    0.0398],\n",
      "        [    0.0402],\n",
      "        [    0.0408],\n",
      "        [    0.0411],\n",
      "        [    0.0413],\n",
      "        [    0.0414],\n",
      "        [    0.0428],\n",
      "        [    0.0434],\n",
      "        [    0.0435],\n",
      "        [    0.0454],\n",
      "        [    0.0461],\n",
      "        [    0.0471],\n",
      "        [    0.0497],\n",
      "        [    0.0517],\n",
      "        [    0.0525],\n",
      "        [    0.0527],\n",
      "        [    0.0536],\n",
      "        [    0.0551],\n",
      "        [    0.0577],\n",
      "        [    0.0596],\n",
      "        [    0.0602],\n",
      "        [    0.0614],\n",
      "        [    0.0625],\n",
      "        [    0.0632],\n",
      "        [    0.0650],\n",
      "        [    0.0651],\n",
      "        [    0.0657],\n",
      "        [    0.0676],\n",
      "        [    0.0676],\n",
      "        [    0.0696],\n",
      "        [    0.0765],\n",
      "        [    0.0811],\n",
      "        [    0.0815],\n",
      "        [    0.0865],\n",
      "        [    0.0911],\n",
      "        [    0.0950],\n",
      "        [    0.1103],\n",
      "        [    0.1116],\n",
      "        [    0.1141],\n",
      "        [    0.1143],\n",
      "        [    0.1225],\n",
      "        [    0.1270],\n",
      "        [    0.1300],\n",
      "        [    0.1338],\n",
      "        [    0.1357],\n",
      "        [    0.1412],\n",
      "        [    0.1465]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0022],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0005],\n",
      "        [0.0080],\n",
      "        [0.0040],\n",
      "        [0.0062],\n",
      "        [0.0018],\n",
      "        [0.0015],\n",
      "        [0.0096],\n",
      "        [0.0014],\n",
      "        [0.0113],\n",
      "        [0.0092],\n",
      "        [0.0045],\n",
      "        [0.0044],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0068],\n",
      "        [0.0118],\n",
      "        [0.0076],\n",
      "        [0.0092],\n",
      "        [0.0158],\n",
      "        [0.0088],\n",
      "        [0.0082],\n",
      "        [0.0153],\n",
      "        [0.0164],\n",
      "        [0.0162],\n",
      "        [0.0085],\n",
      "        [0.0100],\n",
      "        [0.0146],\n",
      "        [0.0085],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0132],\n",
      "        [0.0127],\n",
      "        [0.0122],\n",
      "        [0.0178],\n",
      "        [0.0149],\n",
      "        [0.0123],\n",
      "        [0.0191],\n",
      "        [0.0152],\n",
      "        [0.0143],\n",
      "        [0.0202],\n",
      "        [0.0166],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0206],\n",
      "        [0.0229],\n",
      "        [0.0189],\n",
      "        [0.0182],\n",
      "        [0.0227],\n",
      "        [0.0197],\n",
      "        [0.0204],\n",
      "        [0.0224],\n",
      "        [0.0240],\n",
      "        [0.0270],\n",
      "        [0.0290],\n",
      "        [0.0227],\n",
      "        [0.0282],\n",
      "        [0.0203],\n",
      "        [0.0224],\n",
      "        [0.0316],\n",
      "        [0.0307],\n",
      "        [0.0199],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0243],\n",
      "        [0.0254],\n",
      "        [0.0281],\n",
      "        [0.0319],\n",
      "        [0.0292],\n",
      "        [0.0364],\n",
      "        [0.0309],\n",
      "        [0.0364],\n",
      "        [0.0305],\n",
      "        [0.0303],\n",
      "        [0.0289],\n",
      "        [0.0348],\n",
      "        [0.0330],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0325],\n",
      "        [0.0349],\n",
      "        [0.0410],\n",
      "        [0.0355],\n",
      "        [0.0353],\n",
      "        [0.0353],\n",
      "        [0.0367],\n",
      "        [0.0382],\n",
      "        [0.0433],\n",
      "        [0.0393],\n",
      "        [0.0424],\n",
      "        [0.0447],\n",
      "        [0.0384],\n",
      "        [0.0413],\n",
      "        [0.0484],\n",
      "        [0.0462],\n",
      "        [0.0479],\n",
      "        [0.0456],\n",
      "        [0.0453],\n",
      "        [0.0470],\n",
      "        [0.0549],\n",
      "        [0.0565],\n",
      "        [0.0506],\n",
      "        [0.0512],\n",
      "        [0.0496],\n",
      "        [0.0540],\n",
      "        [0.0600],\n",
      "        [0.0602],\n",
      "        [0.0576],\n",
      "        [0.0600],\n",
      "        [0.0612],\n",
      "        [0.0680],\n",
      "        [0.0637],\n",
      "        [0.0698],\n",
      "        [0.0713],\n",
      "        [0.0704],\n",
      "        [0.0704],\n",
      "        [0.0777],\n",
      "        [0.0781],\n",
      "        [0.0787],\n",
      "        [0.0859],\n",
      "        [0.0885],\n",
      "        [0.0877],\n",
      "        [0.1079],\n",
      "        [0.1124],\n",
      "        [0.1155],\n",
      "        [0.1169],\n",
      "        [0.1155],\n",
      "        [0.1263],\n",
      "        [0.1324],\n",
      "        [0.1350],\n",
      "        [0.1386],\n",
      "        [0.1342],\n",
      "        [0.1401]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.06515622138977\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.8052582479176635e-08, 85)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [85, 112, 66, 21, 98, 74, 68, 135, 56, 57, 99, 97, 50, 62, 96, 33, 26, 55, 36, 100, 137, 134, 32, 18, 52, 49, 110, 8, 90, 51, 91, 13, 22, 113, 105, 12, 23, 53, 63, 27, 16, 130, 111, 102, 15, 75, 17, 131, 44, 136, 28, 104, 139, 101, 122, 138, 128, 77, 129, 124, 76, 123, 109, 29, 140, 10, 35, 30, 92, 64, 61, 9, 93, 34, 25, 20, 86, 127, 14, 121, 89, 95, 58, 87, 46, 54, 65, 103, 19, 94, 3, 132, 67, 11, 48, 88, 83, 0, 4, 73, 31, 37, 106, 107, 6, 1, 47, 7, 120, 108, 141, 126, 133, 78, 24, 84, 72, 2, 114, 60, 59, 142, 43, 125, 5, 115, 82, 42, 69, 143, 71, 41, 119, 70, 38, 45] 數值 torch.Size([136, 1])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6609],\n",
      "        [0.6367],\n",
      "        [0.8350],\n",
      "        [0.8519],\n",
      "        [0.6757],\n",
      "        [0.7587],\n",
      "        [0.8188],\n",
      "        [0.6256],\n",
      "        [0.9015],\n",
      "        [0.9154],\n",
      "        [0.6537],\n",
      "        [0.6774],\n",
      "        [0.8954],\n",
      "        [0.8765],\n",
      "        [0.6874],\n",
      "        [0.8280],\n",
      "        [0.8234],\n",
      "        [0.9011],\n",
      "        [0.8340],\n",
      "        [0.6265],\n",
      "        [0.6378],\n",
      "        [0.6337],\n",
      "        [0.8190],\n",
      "        [0.9041],\n",
      "        [0.8947],\n",
      "        [0.8410],\n",
      "        [0.6534],\n",
      "        [0.9645],\n",
      "        [0.6911],\n",
      "        [0.8855],\n",
      "        [0.6787],\n",
      "        [0.9702],\n",
      "        [0.8405],\n",
      "        [0.6506],\n",
      "        [0.6646],\n",
      "        [0.9820],\n",
      "        [0.8607],\n",
      "        [0.8832],\n",
      "        [0.8778],\n",
      "        [0.8201],\n",
      "        [0.9272],\n",
      "        [0.5631],\n",
      "        [0.6632],\n",
      "        [0.6232],\n",
      "        [0.9492],\n",
      "        [0.7274],\n",
      "        [0.9081],\n",
      "        [0.6109],\n",
      "        [0.8819],\n",
      "        [0.6536],\n",
      "        [0.8289],\n",
      "        [0.6386],\n",
      "        [0.6372],\n",
      "        [0.6250],\n",
      "        [0.5739],\n",
      "        [0.6297],\n",
      "        [0.5880],\n",
      "        [0.7447],\n",
      "        [0.5877],\n",
      "        [0.5608],\n",
      "        [0.7339],\n",
      "        [0.5854],\n",
      "        [0.6466],\n",
      "        [0.8565],\n",
      "        [0.6721],\n",
      "        [1.0209],\n",
      "        [0.8111],\n",
      "        [0.8786],\n",
      "        [0.6764],\n",
      "        [0.8441],\n",
      "        [0.8898],\n",
      "        [1.0062],\n",
      "        [0.6851],\n",
      "        [0.8461],\n",
      "        [0.8385],\n",
      "        [0.8834],\n",
      "        [0.6838],\n",
      "        [0.5857],\n",
      "        [0.9462],\n",
      "        [0.5614],\n",
      "        [0.6769],\n",
      "        [0.6697],\n",
      "        [0.9001],\n",
      "        [0.7000],\n",
      "        [0.8797],\n",
      "        [0.9290],\n",
      "        [0.7989],\n",
      "        [0.6209],\n",
      "        [0.8982],\n",
      "        [0.6685],\n",
      "        [0.8857],\n",
      "        [0.6354],\n",
      "        [0.8360],\n",
      "        [1.0224],\n",
      "        [0.8910],\n",
      "        [0.6889],\n",
      "        [0.7009],\n",
      "        [0.8501],\n",
      "        [0.8820],\n",
      "        [0.7934],\n",
      "        [0.8526],\n",
      "        [0.8140],\n",
      "        [0.6460],\n",
      "        [0.6395],\n",
      "        [0.9207],\n",
      "        [0.8685],\n",
      "        [0.8861],\n",
      "        [0.9418],\n",
      "        [0.5713],\n",
      "        [0.6292],\n",
      "        [0.6880],\n",
      "        [0.5558],\n",
      "        [0.6080],\n",
      "        [0.7310],\n",
      "        [0.8627],\n",
      "        [0.6940],\n",
      "        [0.8166],\n",
      "        [0.9134],\n",
      "        [0.6665],\n",
      "        [0.8771],\n",
      "        [0.8807],\n",
      "        [0.6931],\n",
      "        [0.8644],\n",
      "        [0.5354],\n",
      "        [0.8742],\n",
      "        [0.6793],\n",
      "        [0.7490],\n",
      "        [0.8514],\n",
      "        [0.8208],\n",
      "        [0.7170],\n",
      "        [0.7909],\n",
      "        [0.8339],\n",
      "        [0.6304],\n",
      "        [0.7878],\n",
      "        [0.8129],\n",
      "        [0.8620]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0045],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0068],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0092],\n",
      "        [0.0096],\n",
      "        [0.0100],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0132],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0176],\n",
      "        [0.0178],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0206],\n",
      "        [0.0224],\n",
      "        [0.0224],\n",
      "        [0.0227],\n",
      "        [0.0227],\n",
      "        [0.0229],\n",
      "        [0.0240],\n",
      "        [0.0243],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0270],\n",
      "        [0.0281],\n",
      "        [0.0282],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0292],\n",
      "        [0.0303],\n",
      "        [0.0305],\n",
      "        [0.0307],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0325],\n",
      "        [0.0330],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0348],\n",
      "        [0.0349],\n",
      "        [0.0353],\n",
      "        [0.0353],\n",
      "        [0.0355],\n",
      "        [0.0364],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0382],\n",
      "        [0.0384],\n",
      "        [0.0393],\n",
      "        [0.0410],\n",
      "        [0.0413],\n",
      "        [0.0424],\n",
      "        [0.0433],\n",
      "        [0.0447],\n",
      "        [0.0453],\n",
      "        [0.0456],\n",
      "        [0.0462],\n",
      "        [0.0470],\n",
      "        [0.0479],\n",
      "        [0.0484],\n",
      "        [0.0496],\n",
      "        [0.0506],\n",
      "        [0.0512],\n",
      "        [0.0540],\n",
      "        [0.0549],\n",
      "        [0.0565],\n",
      "        [0.0576],\n",
      "        [0.0600],\n",
      "        [0.0600],\n",
      "        [0.0602],\n",
      "        [0.0612],\n",
      "        [0.0637],\n",
      "        [0.0680],\n",
      "        [0.0698],\n",
      "        [0.0704],\n",
      "        [0.0704],\n",
      "        [0.0713],\n",
      "        [0.0777],\n",
      "        [0.0781],\n",
      "        [0.0787],\n",
      "        [0.0859],\n",
      "        [0.0877],\n",
      "        [0.0885],\n",
      "        [0.1079],\n",
      "        [0.1124],\n",
      "        [0.1155],\n",
      "        [0.1155],\n",
      "        [0.1169],\n",
      "        [0.1263],\n",
      "        [0.1324],\n",
      "        [0.1342],\n",
      "        [0.1350],\n",
      "        [0.1386],\n",
      "        [0.1401],\n",
      "        [0.1541]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0002],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0007],\n",
      "        [0.0032],\n",
      "        [0.0003],\n",
      "        [0.0038],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0054],\n",
      "        [0.0053],\n",
      "        [0.0031],\n",
      "        [0.0099],\n",
      "        [0.0070],\n",
      "        [0.0080],\n",
      "        [0.0107],\n",
      "        [0.0104],\n",
      "        [0.0087],\n",
      "        [0.0082],\n",
      "        [0.0090],\n",
      "        [0.0098],\n",
      "        [0.0094],\n",
      "        [0.0131],\n",
      "        [0.0100],\n",
      "        [0.0150],\n",
      "        [0.0117],\n",
      "        [0.0128],\n",
      "        [0.0114],\n",
      "        [0.0140],\n",
      "        [0.0114],\n",
      "        [0.0173],\n",
      "        [0.0167],\n",
      "        [0.0138],\n",
      "        [0.0149],\n",
      "        [0.0190],\n",
      "        [0.0134],\n",
      "        [0.0145],\n",
      "        [0.0134],\n",
      "        [0.0147],\n",
      "        [0.0203],\n",
      "        [0.0171],\n",
      "        [0.0175],\n",
      "        [0.0178],\n",
      "        [0.0215],\n",
      "        [0.0202],\n",
      "        [0.0149],\n",
      "        [0.0194],\n",
      "        [0.0206],\n",
      "        [0.0203],\n",
      "        [0.0214],\n",
      "        [0.0209],\n",
      "        [0.0189],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0219],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0237],\n",
      "        [0.0228],\n",
      "        [0.0249],\n",
      "        [0.0244],\n",
      "        [0.0248],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        [0.0264],\n",
      "        [0.0296],\n",
      "        [0.0284],\n",
      "        [0.0285],\n",
      "        [0.0333],\n",
      "        [0.0338],\n",
      "        [0.0298],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0297],\n",
      "        [0.0355],\n",
      "        [0.0315],\n",
      "        [0.0327],\n",
      "        [0.0382],\n",
      "        [0.0339],\n",
      "        [0.0340],\n",
      "        [0.0355],\n",
      "        [0.0389],\n",
      "        [0.0348],\n",
      "        [0.0328],\n",
      "        [0.0345],\n",
      "        [0.0401],\n",
      "        [0.0384],\n",
      "        [0.0417],\n",
      "        [0.0399],\n",
      "        [0.0403],\n",
      "        [0.0416],\n",
      "        [0.0388],\n",
      "        [0.0406],\n",
      "        [0.0410],\n",
      "        [0.0446],\n",
      "        [0.0435],\n",
      "        [0.0441],\n",
      "        [0.0491],\n",
      "        [0.0462],\n",
      "        [0.0477],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0505],\n",
      "        [0.0549],\n",
      "        [0.0531],\n",
      "        [0.0533],\n",
      "        [0.0584],\n",
      "        [0.0588],\n",
      "        [0.0591],\n",
      "        [0.0579],\n",
      "        [0.0605],\n",
      "        [0.0639],\n",
      "        [0.0681],\n",
      "        [0.0681],\n",
      "        [0.0698],\n",
      "        [0.0688],\n",
      "        [0.0697],\n",
      "        [0.0770],\n",
      "        [0.0808],\n",
      "        [0.0817],\n",
      "        [0.0830],\n",
      "        [0.0872],\n",
      "        [0.0869],\n",
      "        [0.1102],\n",
      "        [0.1113],\n",
      "        [0.1144],\n",
      "        [0.1151],\n",
      "        [0.1150],\n",
      "        [0.1233],\n",
      "        [0.1304],\n",
      "        [0.1334],\n",
      "        [0.1344],\n",
      "        [0.1368],\n",
      "        [0.1396],\n",
      "        [0.1515]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.34449577331543\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.878290044667665e-08, 112)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [112, 68, 85, 98, 50, 74, 135, 66, 21, 97, 99, 96, 56, 57, 33, 100, 36, 137, 32, 134, 62, 52, 55, 26, 91, 90, 110, 8, 18, 23, 63, 113, 51, 53, 27, 105, 17, 49, 22, 130, 13, 111, 102, 139, 12, 131, 75, 136, 16, 44, 104, 28, 15, 138, 101, 122, 128, 124, 77, 129, 123, 109, 76, 29, 140, 10, 30, 92, 35, 25, 9, 86, 93, 34, 127, 46, 64, 61, 121, 89, 54, 87, 20, 95, 14, 103, 67, 58, 94, 65, 3, 11, 48, 132, 19, 83, 0, 88, 73, 31, 4, 37, 106, 107, 1, 47, 6, 141, 7, 120, 108, 126, 133, 24, 78, 72, 2, 84, 114, 60, 59, 142, 125, 43, 5, 115, 82, 69, 42, 143, 71, 41, 119, 70, 38, 45, 81] 數值 torch.Size([137, 1])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6362],\n",
      "        [0.8163],\n",
      "        [0.6617],\n",
      "        [0.6764],\n",
      "        [0.8923],\n",
      "        [0.7574],\n",
      "        [0.6254],\n",
      "        [0.8315],\n",
      "        [0.8492],\n",
      "        [0.6782],\n",
      "        [0.6543],\n",
      "        [0.6880],\n",
      "        [0.8986],\n",
      "        [0.9120],\n",
      "        [0.8280],\n",
      "        [0.6271],\n",
      "        [0.8338],\n",
      "        [0.6377],\n",
      "        [0.8188],\n",
      "        [0.6332],\n",
      "        [0.8735],\n",
      "        [0.8935],\n",
      "        [0.8992],\n",
      "        [0.8208],\n",
      "        [0.6800],\n",
      "        [0.6919],\n",
      "        [0.6533],\n",
      "        [0.9639],\n",
      "        [0.9010],\n",
      "        [0.8588],\n",
      "        [0.8750],\n",
      "        [0.6497],\n",
      "        [0.8838],\n",
      "        [0.8819],\n",
      "        [0.8183],\n",
      "        [0.6645],\n",
      "        [0.9039],\n",
      "        [0.8375],\n",
      "        [0.8380],\n",
      "        [0.5636],\n",
      "        [0.9661],\n",
      "        [0.6629],\n",
      "        [0.6234],\n",
      "        [0.6354],\n",
      "        [0.9783],\n",
      "        [0.6112],\n",
      "        [0.7261],\n",
      "        [0.6536],\n",
      "        [0.9235],\n",
      "        [0.8812],\n",
      "        [0.6381],\n",
      "        [0.8278],\n",
      "        [0.9458],\n",
      "        [0.6289],\n",
      "        [0.6255],\n",
      "        [0.5734],\n",
      "        [0.5879],\n",
      "        [0.5623],\n",
      "        [0.7447],\n",
      "        [0.5875],\n",
      "        [0.5861],\n",
      "        [0.6473],\n",
      "        [0.7337],\n",
      "        [0.8554],\n",
      "        [0.6699],\n",
      "        [1.0192],\n",
      "        [0.8780],\n",
      "        [0.6771],\n",
      "        [0.8105],\n",
      "        [0.8362],\n",
      "        [1.0054],\n",
      "        [0.6853],\n",
      "        [0.6846],\n",
      "        [0.8464],\n",
      "        [0.5860],\n",
      "        [0.8761],\n",
      "        [0.8411],\n",
      "        [0.8865],\n",
      "        [0.5604],\n",
      "        [0.6777],\n",
      "        [0.9271],\n",
      "        [0.7007],\n",
      "        [0.8804],\n",
      "        [0.6695],\n",
      "        [0.9418],\n",
      "        [0.6208],\n",
      "        [0.8324],\n",
      "        [0.8965],\n",
      "        [0.6678],\n",
      "        [0.7956],\n",
      "        [0.8850],\n",
      "        [1.0197],\n",
      "        [0.8873],\n",
      "        [0.6352],\n",
      "        [0.8949],\n",
      "        [0.6988],\n",
      "        [0.8480],\n",
      "        [0.6896],\n",
      "        [0.7917],\n",
      "        [0.8520],\n",
      "        [0.8799],\n",
      "        [0.8138],\n",
      "        [0.6465],\n",
      "        [0.6403],\n",
      "        [0.8667],\n",
      "        [0.8829],\n",
      "        [0.9199],\n",
      "        [0.6857],\n",
      "        [0.9410],\n",
      "        [0.5701],\n",
      "        [0.6301],\n",
      "        [0.5565],\n",
      "        [0.6078],\n",
      "        [0.8610],\n",
      "        [0.7312],\n",
      "        [0.8149],\n",
      "        [0.9119],\n",
      "        [0.6934],\n",
      "        [0.6659],\n",
      "        [0.8745],\n",
      "        [0.8777],\n",
      "        [0.6902],\n",
      "        [0.5369],\n",
      "        [0.8649],\n",
      "        [0.8719],\n",
      "        [0.6782],\n",
      "        [0.7479],\n",
      "        [0.8188],\n",
      "        [0.8519],\n",
      "        [0.7139],\n",
      "        [0.7888],\n",
      "        [0.8348],\n",
      "        [0.6297],\n",
      "        [0.7860],\n",
      "        [0.8134],\n",
      "        [0.8594],\n",
      "        [0.7689]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0104],\n",
      "        [0.0107],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0117],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0134],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0145],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0167],\n",
      "        [0.0171],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0178],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0209],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0228],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0237],\n",
      "        [0.0244],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        [0.0264],\n",
      "        [0.0284],\n",
      "        [0.0285],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0315],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0338],\n",
      "        [0.0339],\n",
      "        [0.0340],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0355],\n",
      "        [0.0355],\n",
      "        [0.0382],\n",
      "        [0.0384],\n",
      "        [0.0388],\n",
      "        [0.0389],\n",
      "        [0.0399],\n",
      "        [0.0401],\n",
      "        [0.0403],\n",
      "        [0.0406],\n",
      "        [0.0410],\n",
      "        [0.0416],\n",
      "        [0.0417],\n",
      "        [0.0435],\n",
      "        [0.0441],\n",
      "        [0.0446],\n",
      "        [0.0462],\n",
      "        [0.0477],\n",
      "        [0.0491],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0505],\n",
      "        [0.0531],\n",
      "        [0.0533],\n",
      "        [0.0549],\n",
      "        [0.0579],\n",
      "        [0.0584],\n",
      "        [0.0588],\n",
      "        [0.0591],\n",
      "        [0.0605],\n",
      "        [0.0639],\n",
      "        [0.0681],\n",
      "        [0.0681],\n",
      "        [0.0688],\n",
      "        [0.0697],\n",
      "        [0.0698],\n",
      "        [0.0770],\n",
      "        [0.0808],\n",
      "        [0.0817],\n",
      "        [0.0830],\n",
      "        [0.0869],\n",
      "        [0.0872],\n",
      "        [0.1102],\n",
      "        [0.1113],\n",
      "        [0.1144],\n",
      "        [0.1150],\n",
      "        [0.1151],\n",
      "        [0.1233],\n",
      "        [0.1304],\n",
      "        [0.1334],\n",
      "        [0.1344],\n",
      "        [0.1368],\n",
      "        [0.1396],\n",
      "        [0.1515],\n",
      "        [0.1546]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0021],\n",
      "        [0.0025],\n",
      "        [0.0016],\n",
      "        [0.0025],\n",
      "        [0.0020],\n",
      "        [0.0062],\n",
      "        [0.0039],\n",
      "        [0.0065],\n",
      "        [0.0048],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0090],\n",
      "        [0.0084],\n",
      "        [0.0096],\n",
      "        [0.0088],\n",
      "        [0.0100],\n",
      "        [0.0084],\n",
      "        [0.0095],\n",
      "        [0.0101],\n",
      "        [0.0099],\n",
      "        [0.0119],\n",
      "        [0.0102],\n",
      "        [0.0107],\n",
      "        [0.0116],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0100],\n",
      "        [0.0125],\n",
      "        [0.0138],\n",
      "        [0.0132],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0141],\n",
      "        [0.0145],\n",
      "        [0.0144],\n",
      "        [0.0168],\n",
      "        [0.0133],\n",
      "        [0.0164],\n",
      "        [0.0173],\n",
      "        [0.0168],\n",
      "        [0.0190],\n",
      "        [0.0157],\n",
      "        [0.0197],\n",
      "        [0.0180],\n",
      "        [0.0205],\n",
      "        [0.0190],\n",
      "        [0.0233],\n",
      "        [0.0207],\n",
      "        [0.0216],\n",
      "        [0.0197],\n",
      "        [0.0231],\n",
      "        [0.0212],\n",
      "        [0.0227],\n",
      "        [0.0218],\n",
      "        [0.0238],\n",
      "        [0.0209],\n",
      "        [0.0231],\n",
      "        [0.0220],\n",
      "        [0.0204],\n",
      "        [0.0234],\n",
      "        [0.0243],\n",
      "        [0.0260],\n",
      "        [0.0277],\n",
      "        [0.0260],\n",
      "        [0.0247],\n",
      "        [0.0261],\n",
      "        [0.0289],\n",
      "        [0.0307],\n",
      "        [0.0295],\n",
      "        [0.0292],\n",
      "        [0.0302],\n",
      "        [0.0335],\n",
      "        [0.0344],\n",
      "        [0.0330],\n",
      "        [0.0328],\n",
      "        [0.0317],\n",
      "        [0.0353],\n",
      "        [0.0361],\n",
      "        [0.0321],\n",
      "        [0.0364],\n",
      "        [0.0340],\n",
      "        [0.0374],\n",
      "        [0.0363],\n",
      "        [0.0380],\n",
      "        [0.0402],\n",
      "        [0.0404],\n",
      "        [0.0359],\n",
      "        [0.0407],\n",
      "        [0.0427],\n",
      "        [0.0425],\n",
      "        [0.0408],\n",
      "        [0.0398],\n",
      "        [0.0397],\n",
      "        [0.0415],\n",
      "        [0.0427],\n",
      "        [0.0391],\n",
      "        [0.0436],\n",
      "        [0.0472],\n",
      "        [0.0432],\n",
      "        [0.0481],\n",
      "        [0.0497],\n",
      "        [0.0494],\n",
      "        [0.0516],\n",
      "        [0.0518],\n",
      "        [0.0530],\n",
      "        [0.0523],\n",
      "        [0.0547],\n",
      "        [0.0566],\n",
      "        [0.0582],\n",
      "        [0.0567],\n",
      "        [0.0603],\n",
      "        [0.0604],\n",
      "        [0.0638],\n",
      "        [0.0681],\n",
      "        [0.0658],\n",
      "        [0.0660],\n",
      "        [0.0698],\n",
      "        [0.0666],\n",
      "        [0.0751],\n",
      "        [0.0825],\n",
      "        [0.0834],\n",
      "        [0.0811],\n",
      "        [0.0862],\n",
      "        [0.0856],\n",
      "        [0.1111],\n",
      "        [0.1091],\n",
      "        [0.1107],\n",
      "        [0.1129],\n",
      "        [0.1136],\n",
      "        [0.1212],\n",
      "        [0.1274],\n",
      "        [0.1319],\n",
      "        [0.1327],\n",
      "        [0.1343],\n",
      "        [0.1385],\n",
      "        [0.1510],\n",
      "        [0.1511]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.6246120929718\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.4915050289564533e-06, 85)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [85, 50, 112, 68, 98, 135, 21, 74, 66, 97, 99, 56, 36, 33, 96, 137, 57, 134, 110, 100, 32, 52, 55, 63, 113, 26, 62, 8, 91, 23, 17, 90, 18, 51, 27, 53, 111, 49, 130, 105, 22, 139, 13, 131, 102, 44, 77, 12, 136, 122, 28, 16, 138, 124, 15, 128, 104, 75, 129, 101, 123, 140, 109, 29, 10, 76, 30, 25, 35, 9, 92, 46, 121, 127, 34, 86, 54, 93, 64, 67, 61, 20, 89, 87, 95, 83, 48, 11, 14, 103, 58, 3, 132, 65, 94, 19, 73, 0, 88, 31, 37, 4, 106, 107, 47, 1, 6, 141, 120, 7, 108, 126, 133, 78, 72, 84, 24, 2, 114, 142, 60, 59, 43, 125, 115, 82, 5, 69, 42, 143, 71, 41, 119, 70, 38, 45, 81, 80] 數值 torch.Size([138, 1])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6595],\n",
      "        [0.8912],\n",
      "        [0.6343],\n",
      "        [0.8141],\n",
      "        [0.6746],\n",
      "        [0.6255],\n",
      "        [0.8486],\n",
      "        [0.7544],\n",
      "        [0.8290],\n",
      "        [0.6765],\n",
      "        [0.6525],\n",
      "        [0.8975],\n",
      "        [0.8341],\n",
      "        [0.8288],\n",
      "        [0.6860],\n",
      "        [0.6381],\n",
      "        [0.9104],\n",
      "        [0.6331],\n",
      "        [0.6516],\n",
      "        [0.6253],\n",
      "        [0.8196],\n",
      "        [0.8936],\n",
      "        [0.8989],\n",
      "        [0.8732],\n",
      "        [0.6476],\n",
      "        [0.8199],\n",
      "        [0.8714],\n",
      "        [0.9642],\n",
      "        [0.6784],\n",
      "        [0.8586],\n",
      "        [0.9023],\n",
      "        [0.6899],\n",
      "        [0.9003],\n",
      "        [0.8837],\n",
      "        [0.8181],\n",
      "        [0.8819],\n",
      "        [0.6611],\n",
      "        [0.8360],\n",
      "        [0.5639],\n",
      "        [0.6626],\n",
      "        [0.8374],\n",
      "        [0.6346],\n",
      "        [0.9644],\n",
      "        [0.6116],\n",
      "        [0.6214],\n",
      "        [0.8821],\n",
      "        [0.7422],\n",
      "        [0.9768],\n",
      "        [0.6541],\n",
      "        [0.5724],\n",
      "        [0.8280],\n",
      "        [0.9222],\n",
      "        [0.6288],\n",
      "        [0.5631],\n",
      "        [0.9446],\n",
      "        [0.5877],\n",
      "        [0.6358],\n",
      "        [0.7230],\n",
      "        [0.5872],\n",
      "        [0.6237],\n",
      "        [0.5862],\n",
      "        [0.6686],\n",
      "        [0.6461],\n",
      "        [0.8556],\n",
      "        [1.0189],\n",
      "        [0.7309],\n",
      "        [0.8785],\n",
      "        [0.8358],\n",
      "        [0.8105],\n",
      "        [1.0057],\n",
      "        [0.6749],\n",
      "        [0.8750],\n",
      "        [0.5586],\n",
      "        [0.5859],\n",
      "        [0.8474],\n",
      "        [0.6833],\n",
      "        [0.9266],\n",
      "        [0.6817],\n",
      "        [0.8390],\n",
      "        [0.8295],\n",
      "        [0.8842],\n",
      "        [0.8796],\n",
      "        [0.6754],\n",
      "        [0.6981],\n",
      "        [0.6670],\n",
      "        [0.6944],\n",
      "        [0.8860],\n",
      "        [1.0188],\n",
      "        [0.9398],\n",
      "        [0.6187],\n",
      "        [0.8948],\n",
      "        [0.8855],\n",
      "        [0.6353],\n",
      "        [0.7931],\n",
      "        [0.6651],\n",
      "        [0.8939],\n",
      "        [0.7887],\n",
      "        [0.8476],\n",
      "        [0.6870],\n",
      "        [0.8523],\n",
      "        [0.8141],\n",
      "        [0.8793],\n",
      "        [0.6450],\n",
      "        [0.6390],\n",
      "        [0.8820],\n",
      "        [0.8666],\n",
      "        [0.9201],\n",
      "        [0.6844],\n",
      "        [0.5680],\n",
      "        [0.9412],\n",
      "        [0.6290],\n",
      "        [0.5567],\n",
      "        [0.6080],\n",
      "        [0.7289],\n",
      "        [0.8121],\n",
      "        [0.6902],\n",
      "        [0.8610],\n",
      "        [0.9120],\n",
      "        [0.6639],\n",
      "        [0.6884],\n",
      "        [0.8728],\n",
      "        [0.8760],\n",
      "        [0.8665],\n",
      "        [0.5377],\n",
      "        [0.6760],\n",
      "        [0.7442],\n",
      "        [0.8710],\n",
      "        [0.8168],\n",
      "        [0.8533],\n",
      "        [0.7119],\n",
      "        [0.7859],\n",
      "        [0.8363],\n",
      "        [0.6280],\n",
      "        [0.7835],\n",
      "        [0.8145],\n",
      "        [0.8589],\n",
      "        [0.7654],\n",
      "        [0.7734]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0039],\n",
      "        [0.0048],\n",
      "        [0.0062],\n",
      "        [0.0065],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0084],\n",
      "        [0.0084],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0107],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0157],\n",
      "        [0.0164],\n",
      "        [0.0168],\n",
      "        [0.0168],\n",
      "        [0.0173],\n",
      "        [0.0180],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0220],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0238],\n",
      "        [0.0243],\n",
      "        [0.0247],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0261],\n",
      "        [0.0277],\n",
      "        [0.0289],\n",
      "        [0.0292],\n",
      "        [0.0295],\n",
      "        [0.0302],\n",
      "        [0.0307],\n",
      "        [0.0317],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0330],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0359],\n",
      "        [0.0361],\n",
      "        [0.0363],\n",
      "        [0.0364],\n",
      "        [0.0374],\n",
      "        [0.0380],\n",
      "        [0.0391],\n",
      "        [0.0397],\n",
      "        [0.0398],\n",
      "        [0.0402],\n",
      "        [0.0404],\n",
      "        [0.0407],\n",
      "        [0.0408],\n",
      "        [0.0415],\n",
      "        [0.0425],\n",
      "        [0.0427],\n",
      "        [0.0427],\n",
      "        [0.0432],\n",
      "        [0.0436],\n",
      "        [0.0472],\n",
      "        [0.0481],\n",
      "        [0.0494],\n",
      "        [0.0497],\n",
      "        [0.0516],\n",
      "        [0.0518],\n",
      "        [0.0523],\n",
      "        [0.0530],\n",
      "        [0.0547],\n",
      "        [0.0566],\n",
      "        [0.0567],\n",
      "        [0.0582],\n",
      "        [0.0603],\n",
      "        [0.0604],\n",
      "        [0.0638],\n",
      "        [0.0658],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0681],\n",
      "        [0.0698],\n",
      "        [0.0751],\n",
      "        [0.0811],\n",
      "        [0.0825],\n",
      "        [0.0834],\n",
      "        [0.0856],\n",
      "        [0.0862],\n",
      "        [0.1091],\n",
      "        [0.1107],\n",
      "        [0.1111],\n",
      "        [0.1129],\n",
      "        [0.1136],\n",
      "        [0.1212],\n",
      "        [0.1274],\n",
      "        [0.1319],\n",
      "        [0.1327],\n",
      "        [0.1343],\n",
      "        [0.1385],\n",
      "        [0.1510],\n",
      "        [0.1511],\n",
      "        [0.1520]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0059],\n",
      "        [0.0006],\n",
      "        [0.0052],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0036],\n",
      "        [0.0063],\n",
      "        [0.0114],\n",
      "        [0.0103],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0100],\n",
      "        [0.0080],\n",
      "        [0.0130],\n",
      "        [0.0094],\n",
      "        [0.0126],\n",
      "        [0.0105],\n",
      "        [0.0070],\n",
      "        [0.0134],\n",
      "        [0.0094],\n",
      "        [0.0086],\n",
      "        [0.0127],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0135],\n",
      "        [0.0157],\n",
      "        [0.0142],\n",
      "        [0.0169],\n",
      "        [0.0119],\n",
      "        [0.0107],\n",
      "        [0.0178],\n",
      "        [0.0158],\n",
      "        [0.0157],\n",
      "        [0.0130],\n",
      "        [0.0129],\n",
      "        [0.0125],\n",
      "        [0.0189],\n",
      "        [0.0168],\n",
      "        [0.0203],\n",
      "        [0.0189],\n",
      "        [0.0169],\n",
      "        [0.0221],\n",
      "        [0.0191],\n",
      "        [0.0232],\n",
      "        [0.0205],\n",
      "        [0.0154],\n",
      "        [0.0233],\n",
      "        [0.0206],\n",
      "        [0.0195],\n",
      "        [0.0223],\n",
      "        [0.0241],\n",
      "        [0.0214],\n",
      "        [0.0218],\n",
      "        [0.0254],\n",
      "        [0.0239],\n",
      "        [0.0267],\n",
      "        [0.0284],\n",
      "        [0.0225],\n",
      "        [0.0271],\n",
      "        [0.0246],\n",
      "        [0.0230],\n",
      "        [0.0287],\n",
      "        [0.0248],\n",
      "        [0.0239],\n",
      "        [0.0329],\n",
      "        [0.0278],\n",
      "        [0.0277],\n",
      "        [0.0310],\n",
      "        [0.0285],\n",
      "        [0.0351],\n",
      "        [0.0293],\n",
      "        [0.0299],\n",
      "        [0.0334],\n",
      "        [0.0321],\n",
      "        [0.0380],\n",
      "        [0.0318],\n",
      "        [0.0393],\n",
      "        [0.0389],\n",
      "        [0.0317],\n",
      "        [0.0400],\n",
      "        [0.0381],\n",
      "        [0.0409],\n",
      "        [0.0424],\n",
      "        [0.0423],\n",
      "        [0.0328],\n",
      "        [0.0371],\n",
      "        [0.0371],\n",
      "        [0.0435],\n",
      "        [0.0438],\n",
      "        [0.0438],\n",
      "        [0.0399],\n",
      "        [0.0419],\n",
      "        [0.0462],\n",
      "        [0.0472],\n",
      "        [0.0448],\n",
      "        [0.0379],\n",
      "        [0.0422],\n",
      "        [0.0521],\n",
      "        [0.0469],\n",
      "        [0.0508],\n",
      "        [0.0515],\n",
      "        [0.0546],\n",
      "        [0.0545],\n",
      "        [0.0501],\n",
      "        [0.0518],\n",
      "        [0.0562],\n",
      "        [0.0548],\n",
      "        [0.0541],\n",
      "        [0.0597],\n",
      "        [0.0628],\n",
      "        [0.0608],\n",
      "        [0.0639],\n",
      "        [0.0611],\n",
      "        [0.0609],\n",
      "        [0.0615],\n",
      "        [0.0669],\n",
      "        [0.0686],\n",
      "        [0.0719],\n",
      "        [0.0789],\n",
      "        [0.0857],\n",
      "        [0.0866],\n",
      "        [0.0858],\n",
      "        [0.0859],\n",
      "        [0.1058],\n",
      "        [0.1046],\n",
      "        [0.1131],\n",
      "        [0.1091],\n",
      "        [0.1140],\n",
      "        [0.1187],\n",
      "        [0.1224],\n",
      "        [0.1323],\n",
      "        [0.1300],\n",
      "        [0.1299],\n",
      "        [0.1393],\n",
      "        [0.1493],\n",
      "        [0.1450],\n",
      "        [0.1462]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.90412902832031\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.266000589974283e-07, 50)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [50, 135, 112, 85, 68, 98, 21, 110, 63, 33, 113, 52, 32, 137, 36, 66, 134, 17, 99, 97, 56, 74, 23, 111, 57, 55, 53, 27, 96, 100, 26, 8, 77, 62, 51, 18, 130, 139, 91, 90, 22, 49, 131, 122, 105, 44, 136, 138, 124, 13, 28, 129, 140, 102, 12, 10, 128, 16, 123, 29, 15, 104, 101, 25, 30, 75, 9, 109, 46, 121, 35, 67, 54, 34, 83, 76, 127, 92, 11, 48, 73, 86, 20, 64, 93, 3, 61, 89, 132, 0, 95, 87, 14, 103, 58, 19, 65, 31, 94, 47, 37, 4, 1, 88, 120, 107, 106, 141, 6, 7, 126, 72, 78, 84, 108, 133, 24, 2, 114, 142, 60, 43, 125, 59, 82, 115, 69, 5, 42, 143, 71, 70, 119, 41, 38, 81, 80, 45, 40] 數值 torch.Size([139, 1])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8887],\n",
      "        [0.6252],\n",
      "        [0.6312],\n",
      "        [0.6552],\n",
      "        [0.8104],\n",
      "        [0.6709],\n",
      "        [0.8471],\n",
      "        [0.6486],\n",
      "        [0.8696],\n",
      "        [0.8280],\n",
      "        [0.6442],\n",
      "        [0.8921],\n",
      "        [0.8189],\n",
      "        [0.6381],\n",
      "        [0.8325],\n",
      "        [0.8252],\n",
      "        [0.6325],\n",
      "        [0.8997],\n",
      "        [0.6490],\n",
      "        [0.6727],\n",
      "        [0.8949],\n",
      "        [0.7492],\n",
      "        [0.8573],\n",
      "        [0.6579],\n",
      "        [0.9074],\n",
      "        [0.8969],\n",
      "        [0.8803],\n",
      "        [0.8167],\n",
      "        [0.6820],\n",
      "        [0.6219],\n",
      "        [0.8180],\n",
      "        [0.9625],\n",
      "        [0.7372],\n",
      "        [0.8676],\n",
      "        [0.8822],\n",
      "        [0.8983],\n",
      "        [0.5638],\n",
      "        [0.6335],\n",
      "        [0.6745],\n",
      "        [0.6856],\n",
      "        [0.8358],\n",
      "        [0.8335],\n",
      "        [0.6114],\n",
      "        [0.5709],\n",
      "        [0.6592],\n",
      "        [0.8814],\n",
      "        [0.6539],\n",
      "        [0.6283],\n",
      "        [0.5634],\n",
      "        [0.9613],\n",
      "        [0.8269],\n",
      "        [0.5863],\n",
      "        [0.6670],\n",
      "        [0.6180],\n",
      "        [0.9740],\n",
      "        [1.0166],\n",
      "        [0.5868],\n",
      "        [0.9197],\n",
      "        [0.5859],\n",
      "        [0.8543],\n",
      "        [0.9420],\n",
      "        [0.6323],\n",
      "        [0.6203],\n",
      "        [0.8342],\n",
      "        [0.8773],\n",
      "        [0.7179],\n",
      "        [1.0040],\n",
      "        [0.6434],\n",
      "        [0.8726],\n",
      "        [0.5564],\n",
      "        [0.8090],\n",
      "        [0.8253],\n",
      "        [0.9243],\n",
      "        [0.8466],\n",
      "        [0.6881],\n",
      "        [0.7257],\n",
      "        [0.5853],\n",
      "        [0.6706],\n",
      "        [1.0161],\n",
      "        [0.8834],\n",
      "        [0.7834],\n",
      "        [0.6787],\n",
      "        [0.8778],\n",
      "        [0.8355],\n",
      "        [0.6768],\n",
      "        [0.8846],\n",
      "        [0.8803],\n",
      "        [0.6708],\n",
      "        [0.6348],\n",
      "        [0.8461],\n",
      "        [0.6628],\n",
      "        [0.6931],\n",
      "        [0.9366],\n",
      "        [0.6154],\n",
      "        [0.8916],\n",
      "        [0.8918],\n",
      "        [0.7894],\n",
      "        [0.8511],\n",
      "        [0.6605],\n",
      "        [0.8797],\n",
      "        [0.8127],\n",
      "        [0.8775],\n",
      "        [0.8654],\n",
      "        [0.6821],\n",
      "        [0.5654],\n",
      "        [0.6362],\n",
      "        [0.6420],\n",
      "        [0.6826],\n",
      "        [0.9185],\n",
      "        [0.9397],\n",
      "        [0.5563],\n",
      "        [0.8071],\n",
      "        [0.7241],\n",
      "        [0.6851],\n",
      "        [0.6265],\n",
      "        [0.6078],\n",
      "        [0.8597],\n",
      "        [0.9107],\n",
      "        [0.6608],\n",
      "        [0.6861],\n",
      "        [0.8695],\n",
      "        [0.8663],\n",
      "        [0.5380],\n",
      "        [0.8728],\n",
      "        [0.7380],\n",
      "        [0.6727],\n",
      "        [0.8130],\n",
      "        [0.8690],\n",
      "        [0.8530],\n",
      "        [0.7093],\n",
      "        [0.7809],\n",
      "        [0.7791],\n",
      "        [0.6253],\n",
      "        [0.8359],\n",
      "        [0.8137],\n",
      "        [0.7593],\n",
      "        [0.7676],\n",
      "        [0.8572],\n",
      "        [0.8096]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0036],\n",
      "        [0.0052],\n",
      "        [0.0059],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0063],\n",
      "        [0.0070],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0086],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0127],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0142],\n",
      "        [0.0154],\n",
      "        [0.0157],\n",
      "        [0.0157],\n",
      "        [0.0158],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0178],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0214],\n",
      "        [0.0218],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0239],\n",
      "        [0.0239],\n",
      "        [0.0241],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0254],\n",
      "        [0.0267],\n",
      "        [0.0271],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0284],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.0293],\n",
      "        [0.0299],\n",
      "        [0.0310],\n",
      "        [0.0317],\n",
      "        [0.0318],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0329],\n",
      "        [0.0334],\n",
      "        [0.0351],\n",
      "        [0.0371],\n",
      "        [0.0371],\n",
      "        [0.0379],\n",
      "        [0.0380],\n",
      "        [0.0381],\n",
      "        [0.0389],\n",
      "        [0.0393],\n",
      "        [0.0399],\n",
      "        [0.0400],\n",
      "        [0.0409],\n",
      "        [0.0419],\n",
      "        [0.0422],\n",
      "        [0.0423],\n",
      "        [0.0424],\n",
      "        [0.0435],\n",
      "        [0.0438],\n",
      "        [0.0438],\n",
      "        [0.0448],\n",
      "        [0.0462],\n",
      "        [0.0469],\n",
      "        [0.0472],\n",
      "        [0.0501],\n",
      "        [0.0508],\n",
      "        [0.0515],\n",
      "        [0.0518],\n",
      "        [0.0521],\n",
      "        [0.0541],\n",
      "        [0.0545],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0562],\n",
      "        [0.0597],\n",
      "        [0.0608],\n",
      "        [0.0609],\n",
      "        [0.0611],\n",
      "        [0.0615],\n",
      "        [0.0628],\n",
      "        [0.0639],\n",
      "        [0.0669],\n",
      "        [0.0686],\n",
      "        [0.0719],\n",
      "        [0.0789],\n",
      "        [0.0857],\n",
      "        [0.0858],\n",
      "        [0.0859],\n",
      "        [0.0866],\n",
      "        [0.1046],\n",
      "        [0.1058],\n",
      "        [0.1091],\n",
      "        [0.1131],\n",
      "        [0.1140],\n",
      "        [0.1187],\n",
      "        [0.1224],\n",
      "        [0.1299],\n",
      "        [0.1300],\n",
      "        [0.1323],\n",
      "        [0.1393],\n",
      "        [0.1450],\n",
      "        [0.1462],\n",
      "        [0.1493],\n",
      "        [0.1588]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0027],\n",
      "        [0.0065],\n",
      "        [0.0041],\n",
      "        [0.0053],\n",
      "        [0.0044],\n",
      "        [0.0049],\n",
      "        [0.0026],\n",
      "        [0.0083],\n",
      "        [0.0103],\n",
      "        [0.0129],\n",
      "        [0.0092],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0127],\n",
      "        [0.0055],\n",
      "        [0.0087],\n",
      "        [0.0078],\n",
      "        [0.0135],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0077],\n",
      "        [0.0107],\n",
      "        [0.0160],\n",
      "        [0.0137],\n",
      "        [0.0098],\n",
      "        [0.0087],\n",
      "        [0.0172],\n",
      "        [0.0170],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0101],\n",
      "        [0.0103],\n",
      "        [0.0161],\n",
      "        [0.0135],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0139],\n",
      "        [0.0190],\n",
      "        [0.0157],\n",
      "        [0.0169],\n",
      "        [0.0152],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0214],\n",
      "        [0.0191],\n",
      "        [0.0150],\n",
      "        [0.0238],\n",
      "        [0.0243],\n",
      "        [0.0183],\n",
      "        [0.0197],\n",
      "        [0.0181],\n",
      "        [0.0248],\n",
      "        [0.0248],\n",
      "        [0.0221],\n",
      "        [0.0207],\n",
      "        [0.0275],\n",
      "        [0.0215],\n",
      "        [0.0212],\n",
      "        [0.0217],\n",
      "        [0.0291],\n",
      "        [0.0224],\n",
      "        [0.0259],\n",
      "        [0.0258],\n",
      "        [0.0314],\n",
      "        [0.0324],\n",
      "        [0.0279],\n",
      "        [0.0326],\n",
      "        [0.0269],\n",
      "        [0.0328],\n",
      "        [0.0312],\n",
      "        [0.0268],\n",
      "        [0.0329],\n",
      "        [0.0357],\n",
      "        [0.0371],\n",
      "        [0.0315],\n",
      "        [0.0325],\n",
      "        [0.0308],\n",
      "        [0.0343],\n",
      "        [0.0403],\n",
      "        [0.0403],\n",
      "        [0.0387],\n",
      "        [0.0375],\n",
      "        [0.0346],\n",
      "        [0.0367],\n",
      "        [0.0390],\n",
      "        [0.0440],\n",
      "        [0.0380],\n",
      "        [0.0404],\n",
      "        [0.0391],\n",
      "        [0.0454],\n",
      "        [0.0415],\n",
      "        [0.0422],\n",
      "        [0.0413],\n",
      "        [0.0427],\n",
      "        [0.0413],\n",
      "        [0.0415],\n",
      "        [0.0445],\n",
      "        [0.0513],\n",
      "        [0.0468],\n",
      "        [0.0537],\n",
      "        [0.0463],\n",
      "        [0.0483],\n",
      "        [0.0554],\n",
      "        [0.0518],\n",
      "        [0.0552],\n",
      "        [0.0528],\n",
      "        [0.0531],\n",
      "        [0.0564],\n",
      "        [0.0525],\n",
      "        [0.0560],\n",
      "        [0.0580],\n",
      "        [0.0620],\n",
      "        [0.0621],\n",
      "        [0.0614],\n",
      "        [0.0609],\n",
      "        [0.0610],\n",
      "        [0.0711],\n",
      "        [0.0724],\n",
      "        [0.0732],\n",
      "        [0.0800],\n",
      "        [0.0832],\n",
      "        [0.0798],\n",
      "        [0.0825],\n",
      "        [0.0841],\n",
      "        [0.1042],\n",
      "        [0.1068],\n",
      "        [0.1110],\n",
      "        [0.1102],\n",
      "        [0.1082],\n",
      "        [0.1197],\n",
      "        [0.1234],\n",
      "        [0.1314],\n",
      "        [0.1315],\n",
      "        [0.1265],\n",
      "        [0.1341],\n",
      "        [0.1448],\n",
      "        [0.1465],\n",
      "        [0.1534],\n",
      "        [0.1535]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.18391513824463\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.721077170368517e-06, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [21, 50, 112, 68, 98, 85, 36, 135, 56, 134, 110, 66, 55, 113, 99, 97, 57, 26, 8, 63, 74, 51, 96, 100, 18, 137, 33, 52, 17, 62, 111, 130, 32, 44, 22, 91, 49, 23, 131, 77, 90, 27, 53, 28, 124, 139, 105, 13, 12, 16, 122, 128, 123, 102, 15, 136, 138, 140, 129, 101, 104, 35, 109, 10, 75, 29, 127, 121, 25, 83, 30, 76, 9, 46, 67, 92, 20, 54, 64, 34, 86, 61, 73, 93, 132, 11, 48, 89, 58, 14, 95, 19, 87, 103, 3, 65, 0, 37, 94, 4, 31, 88, 6, 107, 106, 47, 120, 1, 7, 141, 126, 108, 133, 84, 72, 78, 24, 2, 114, 43, 142, 125, 60, 59, 82, 115, 42, 5, 69, 143, 71, 41, 70, 119, 38, 81, 80, 45, 40, 144] 數值 torch.Size([140, 1])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8508],\n",
      "        [0.8919],\n",
      "        [0.6323],\n",
      "        [0.8122],\n",
      "        [0.6722],\n",
      "        [0.6558],\n",
      "        [0.8371],\n",
      "        [0.6281],\n",
      "        [0.8982],\n",
      "        [0.6352],\n",
      "        [0.6499],\n",
      "        [0.8268],\n",
      "        [0.9009],\n",
      "        [0.6452],\n",
      "        [0.6503],\n",
      "        [0.6741],\n",
      "        [0.9102],\n",
      "        [0.8214],\n",
      "        [0.9664],\n",
      "        [0.8720],\n",
      "        [0.7499],\n",
      "        [0.8863],\n",
      "        [0.6832],\n",
      "        [0.6232],\n",
      "        [0.9019],\n",
      "        [0.6414],\n",
      "        [0.8328],\n",
      "        [0.8964],\n",
      "        [0.9024],\n",
      "        [0.8698],\n",
      "        [0.6591],\n",
      "        [0.5668],\n",
      "        [0.8237],\n",
      "        [0.8868],\n",
      "        [0.8395],\n",
      "        [0.6758],\n",
      "        [0.8365],\n",
      "        [0.8614],\n",
      "        [0.6144],\n",
      "        [0.7379],\n",
      "        [0.6864],\n",
      "        [0.8206],\n",
      "        [0.8847],\n",
      "        [0.8311],\n",
      "        [0.5668],\n",
      "        [0.6356],\n",
      "        [0.6603],\n",
      "        [0.9637],\n",
      "        [0.9766],\n",
      "        [0.9226],\n",
      "        [0.5728],\n",
      "        [0.5892],\n",
      "        [0.5888],\n",
      "        [0.6191],\n",
      "        [0.9450],\n",
      "        [0.6572],\n",
      "        [0.6313],\n",
      "        [0.6687],\n",
      "        [0.5886],\n",
      "        [0.6217],\n",
      "        [0.6331],\n",
      "        [0.8132],\n",
      "        [0.6451],\n",
      "        [1.0202],\n",
      "        [0.7184],\n",
      "        [0.8586],\n",
      "        [0.5880],\n",
      "        [0.5577],\n",
      "        [0.8380],\n",
      "        [0.6868],\n",
      "        [0.8820],\n",
      "        [0.7261],\n",
      "        [1.0081],\n",
      "        [0.8761],\n",
      "        [0.8265],\n",
      "        [0.6714],\n",
      "        [0.8813],\n",
      "        [0.9283],\n",
      "        [0.8377],\n",
      "        [0.8516],\n",
      "        [0.6793],\n",
      "        [0.8823],\n",
      "        [0.7842],\n",
      "        [0.6770],\n",
      "        [0.6377],\n",
      "        [1.0193],\n",
      "        [0.8866],\n",
      "        [0.6713],\n",
      "        [0.8942],\n",
      "        [0.9387],\n",
      "        [0.6636],\n",
      "        [0.8952],\n",
      "        [0.6933],\n",
      "        [0.6164],\n",
      "        [0.8887],\n",
      "        [0.7911],\n",
      "        [0.8493],\n",
      "        [0.8173],\n",
      "        [0.6609],\n",
      "        [0.8806],\n",
      "        [0.8556],\n",
      "        [0.6824],\n",
      "        [0.9223],\n",
      "        [0.6379],\n",
      "        [0.6435],\n",
      "        [0.8834],\n",
      "        [0.5665],\n",
      "        [0.8689],\n",
      "        [0.9434],\n",
      "        [0.6842],\n",
      "        [0.5590],\n",
      "        [0.6284],\n",
      "        [0.6107],\n",
      "        [0.6850],\n",
      "        [0.8081],\n",
      "        [0.7251],\n",
      "        [0.8640],\n",
      "        [0.9145],\n",
      "        [0.6620],\n",
      "        [0.8723],\n",
      "        [0.6873],\n",
      "        [0.5414],\n",
      "        [0.8720],\n",
      "        [0.8753],\n",
      "        [0.7377],\n",
      "        [0.6737],\n",
      "        [0.8587],\n",
      "        [0.8719],\n",
      "        [0.8148],\n",
      "        [0.7104],\n",
      "        [0.7819],\n",
      "        [0.8417],\n",
      "        [0.7806],\n",
      "        [0.6268],\n",
      "        [0.8189],\n",
      "        [0.7592],\n",
      "        [0.7678],\n",
      "        [0.8614],\n",
      "        [0.8149],\n",
      "        [0.7194]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0026],\n",
      "        [0.0027],\n",
      "        [0.0041],\n",
      "        [0.0044],\n",
      "        [0.0049],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0065],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0103],\n",
      "        [0.0103],\n",
      "        [0.0107],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0127],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0135],\n",
      "        [0.0137],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0150],\n",
      "        [0.0152],\n",
      "        [0.0157],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0197],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0238],\n",
      "        [0.0243],\n",
      "        [0.0248],\n",
      "        [0.0248],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0275],\n",
      "        [0.0279],\n",
      "        [0.0291],\n",
      "        [0.0308],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0324],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0328],\n",
      "        [0.0329],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0357],\n",
      "        [0.0367],\n",
      "        [0.0371],\n",
      "        [0.0375],\n",
      "        [0.0380],\n",
      "        [0.0387],\n",
      "        [0.0390],\n",
      "        [0.0391],\n",
      "        [0.0403],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0413],\n",
      "        [0.0413],\n",
      "        [0.0415],\n",
      "        [0.0415],\n",
      "        [0.0422],\n",
      "        [0.0427],\n",
      "        [0.0440],\n",
      "        [0.0445],\n",
      "        [0.0454],\n",
      "        [0.0463],\n",
      "        [0.0468],\n",
      "        [0.0483],\n",
      "        [0.0513],\n",
      "        [0.0518],\n",
      "        [0.0525],\n",
      "        [0.0528],\n",
      "        [0.0531],\n",
      "        [0.0537],\n",
      "        [0.0552],\n",
      "        [0.0554],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0580],\n",
      "        [0.0609],\n",
      "        [0.0610],\n",
      "        [0.0614],\n",
      "        [0.0620],\n",
      "        [0.0621],\n",
      "        [0.0711],\n",
      "        [0.0724],\n",
      "        [0.0732],\n",
      "        [0.0798],\n",
      "        [0.0800],\n",
      "        [0.0825],\n",
      "        [0.0832],\n",
      "        [0.0841],\n",
      "        [0.1042],\n",
      "        [0.1068],\n",
      "        [0.1082],\n",
      "        [0.1102],\n",
      "        [0.1110],\n",
      "        [0.1197],\n",
      "        [0.1234],\n",
      "        [0.1265],\n",
      "        [0.1314],\n",
      "        [0.1315],\n",
      "        [0.1341],\n",
      "        [0.1448],\n",
      "        [0.1465],\n",
      "        [0.1534],\n",
      "        [0.1535],\n",
      "        [0.1604]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0046],\n",
      "        [0.0002],\n",
      "        [0.0076],\n",
      "        [0.0073],\n",
      "        [0.0066],\n",
      "        [0.0075],\n",
      "        [0.0064],\n",
      "        [0.0024],\n",
      "        [0.0101],\n",
      "        [0.0121],\n",
      "        [0.0051],\n",
      "        [0.0119],\n",
      "        [0.0102],\n",
      "        [0.0054],\n",
      "        [0.0112],\n",
      "        [0.0109],\n",
      "        [0.0128],\n",
      "        [0.0124],\n",
      "        [0.0122],\n",
      "        [0.0076],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0147],\n",
      "        [0.0090],\n",
      "        [0.0121],\n",
      "        [0.0118],\n",
      "        [0.0102],\n",
      "        [0.0166],\n",
      "        [0.0103],\n",
      "        [0.0171],\n",
      "        [0.0133],\n",
      "        [0.0152],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0184],\n",
      "        [0.0143],\n",
      "        [0.0196],\n",
      "        [0.0139],\n",
      "        [0.0191],\n",
      "        [0.0151],\n",
      "        [0.0162],\n",
      "        [0.0197],\n",
      "        [0.0204],\n",
      "        [0.0141],\n",
      "        [0.0218],\n",
      "        [0.0235],\n",
      "        [0.0243],\n",
      "        [0.0243],\n",
      "        [0.0178],\n",
      "        [0.0254],\n",
      "        [0.0246],\n",
      "        [0.0243],\n",
      "        [0.0254],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0193],\n",
      "        [0.0208],\n",
      "        [0.0277],\n",
      "        [0.0288],\n",
      "        [0.0281],\n",
      "        [0.0294],\n",
      "        [0.0250],\n",
      "        [0.0303],\n",
      "        [0.0273],\n",
      "        [0.0341],\n",
      "        [0.0274],\n",
      "        [0.0295],\n",
      "        [0.0271],\n",
      "        [0.0310],\n",
      "        [0.0351],\n",
      "        [0.0307],\n",
      "        [0.0303],\n",
      "        [0.0292],\n",
      "        [0.0366],\n",
      "        [0.0371],\n",
      "        [0.0340],\n",
      "        [0.0394],\n",
      "        [0.0366],\n",
      "        [0.0396],\n",
      "        [0.0414],\n",
      "        [0.0362],\n",
      "        [0.0421],\n",
      "        [0.0430],\n",
      "        [0.0373],\n",
      "        [0.0376],\n",
      "        [0.0427],\n",
      "        [0.0445],\n",
      "        [0.0454],\n",
      "        [0.0437],\n",
      "        [0.0443],\n",
      "        [0.0449],\n",
      "        [0.0452],\n",
      "        [0.0431],\n",
      "        [0.0477],\n",
      "        [0.0438],\n",
      "        [0.0471],\n",
      "        [0.0497],\n",
      "        [0.0502],\n",
      "        [0.0499],\n",
      "        [0.0544],\n",
      "        [0.0541],\n",
      "        [0.0550],\n",
      "        [0.0554],\n",
      "        [0.0514],\n",
      "        [0.0513],\n",
      "        [0.0541],\n",
      "        [0.0578],\n",
      "        [0.0506],\n",
      "        [0.0610],\n",
      "        [0.0630],\n",
      "        [0.0649],\n",
      "        [0.0582],\n",
      "        [0.0596],\n",
      "        [0.0603],\n",
      "        [0.0696],\n",
      "        [0.0711],\n",
      "        [0.0696],\n",
      "        [0.0792],\n",
      "        [0.0735],\n",
      "        [0.0845],\n",
      "        [0.0861],\n",
      "        [0.0871],\n",
      "        [0.1008],\n",
      "        [0.1027],\n",
      "        [0.1078],\n",
      "        [0.1125],\n",
      "        [0.1086],\n",
      "        [0.1130],\n",
      "        [0.1207],\n",
      "        [0.1260],\n",
      "        [0.1290],\n",
      "        [0.1279],\n",
      "        [0.1342],\n",
      "        [0.1417],\n",
      "        [0.1437],\n",
      "        [0.1518],\n",
      "        [0.1535],\n",
      "        [0.1535]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.46564817428589\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.253166707712808e-08, 50)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [50, 135, 21, 110, 113, 36, 98, 68, 85, 112, 63, 137, 56, 17, 55, 111, 97, 99, 52, 66, 134, 33, 8, 26, 57, 51, 74, 96, 32, 77, 100, 139, 23, 18, 27, 44, 53, 62, 130, 22, 91, 122, 49, 90, 140, 131, 28, 136, 138, 124, 129, 105, 13, 16, 12, 102, 123, 10, 15, 128, 83, 29, 121, 101, 35, 104, 67, 109, 25, 75, 46, 9, 30, 54, 127, 76, 73, 92, 34, 20, 11, 48, 64, 86, 61, 93, 89, 132, 3, 95, 0, 19, 58, 87, 103, 14, 37, 65, 94, 31, 4, 141, 120, 47, 1, 6, 88, 107, 106, 7, 84, 72, 78, 126, 108, 133, 24, 114, 2, 142, 43, 125, 60, 59, 82, 115, 42, 69, 5, 143, 71, 41, 119, 70, 38, 81, 80, 45, 40, 144, 145] 數值 torch.Size([141, 1])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8894],\n",
      "        [0.6239],\n",
      "        [0.8487],\n",
      "        [0.6467],\n",
      "        [0.6414],\n",
      "        [0.8361],\n",
      "        [0.6706],\n",
      "        [0.8093],\n",
      "        [0.6536],\n",
      "        [0.6288],\n",
      "        [0.8693],\n",
      "        [0.6376],\n",
      "        [0.8958],\n",
      "        [0.8991],\n",
      "        [0.8994],\n",
      "        [0.6557],\n",
      "        [0.6727],\n",
      "        [0.6486],\n",
      "        [0.8952],\n",
      "        [0.8235],\n",
      "        [0.6309],\n",
      "        [0.8321],\n",
      "        [0.9645],\n",
      "        [0.8191],\n",
      "        [0.9071],\n",
      "        [0.8849],\n",
      "        [0.7476],\n",
      "        [0.6818],\n",
      "        [0.8228],\n",
      "        [0.7357],\n",
      "        [0.6214],\n",
      "        [0.6306],\n",
      "        [0.8597],\n",
      "        [0.8994],\n",
      "        [0.8187],\n",
      "        [0.8866],\n",
      "        [0.8836],\n",
      "        [0.8667],\n",
      "        [0.5636],\n",
      "        [0.8375],\n",
      "        [0.6740],\n",
      "        [0.5692],\n",
      "        [0.8340],\n",
      "        [0.6843],\n",
      "        [0.6632],\n",
      "        [0.6110],\n",
      "        [0.8295],\n",
      "        [0.6534],\n",
      "        [0.6272],\n",
      "        [0.5647],\n",
      "        [0.5845],\n",
      "        [0.6576],\n",
      "        [0.9599],\n",
      "        [0.9195],\n",
      "        [0.9730],\n",
      "        [0.6168],\n",
      "        [0.5859],\n",
      "        [1.0177],\n",
      "        [0.9420],\n",
      "        [0.5853],\n",
      "        [0.6824],\n",
      "        [0.8569],\n",
      "        [0.5539],\n",
      "        [0.6198],\n",
      "        [0.8119],\n",
      "        [0.6302],\n",
      "        [0.8229],\n",
      "        [0.6426],\n",
      "        [0.8360],\n",
      "        [0.7160],\n",
      "        [0.8736],\n",
      "        [1.0063],\n",
      "        [0.8806],\n",
      "        [0.9266],\n",
      "        [0.5846],\n",
      "        [0.7235],\n",
      "        [0.7818],\n",
      "        [0.6691],\n",
      "        [0.8510],\n",
      "        [0.8788],\n",
      "        [1.0164],\n",
      "        [0.8838],\n",
      "        [0.8349],\n",
      "        [0.6772],\n",
      "        [0.8789],\n",
      "        [0.6740],\n",
      "        [0.6691],\n",
      "        [0.6337],\n",
      "        [0.8878],\n",
      "        [0.6613],\n",
      "        [0.8478],\n",
      "        [0.8924],\n",
      "        [0.8910],\n",
      "        [0.6906],\n",
      "        [0.6139],\n",
      "        [0.9346],\n",
      "        [0.8164],\n",
      "        [0.7880],\n",
      "        [0.6581],\n",
      "        [0.8542],\n",
      "        [0.8787],\n",
      "        [0.6784],\n",
      "        [0.5627],\n",
      "        [0.8811],\n",
      "        [0.8677],\n",
      "        [0.9206],\n",
      "        [0.6798],\n",
      "        [0.6357],\n",
      "        [0.6412],\n",
      "        [0.9416],\n",
      "        [0.6818],\n",
      "        [0.8058],\n",
      "        [0.7233],\n",
      "        [0.5561],\n",
      "        [0.6263],\n",
      "        [0.6068],\n",
      "        [0.8625],\n",
      "        [0.6585],\n",
      "        [0.9132],\n",
      "        [0.6808],\n",
      "        [0.8729],\n",
      "        [0.5394],\n",
      "        [0.8691],\n",
      "        [0.8722],\n",
      "        [0.7343],\n",
      "        [0.6696],\n",
      "        [0.8591],\n",
      "        [0.8125],\n",
      "        [0.8696],\n",
      "        [0.7037],\n",
      "        [0.7792],\n",
      "        [0.8421],\n",
      "        [0.6232],\n",
      "        [0.7783],\n",
      "        [0.8188],\n",
      "        [0.7560],\n",
      "        [0.7651],\n",
      "        [0.8597],\n",
      "        [0.8150],\n",
      "        [0.7125],\n",
      "        [0.7309]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0024],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0073],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0090],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0118],\n",
      "        [0.0119],\n",
      "        [0.0121],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0124],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0139],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0162],\n",
      "        [0.0166],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0178],\n",
      "        [0.0184],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0218],\n",
      "        [0.0235],\n",
      "        [0.0243],\n",
      "        [0.0243],\n",
      "        [0.0243],\n",
      "        [0.0246],\n",
      "        [0.0250],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0271],\n",
      "        [0.0273],\n",
      "        [0.0274],\n",
      "        [0.0277],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0292],\n",
      "        [0.0294],\n",
      "        [0.0295],\n",
      "        [0.0303],\n",
      "        [0.0303],\n",
      "        [0.0307],\n",
      "        [0.0310],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0351],\n",
      "        [0.0362],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0371],\n",
      "        [0.0373],\n",
      "        [0.0376],\n",
      "        [0.0394],\n",
      "        [0.0396],\n",
      "        [0.0414],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0430],\n",
      "        [0.0431],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0445],\n",
      "        [0.0449],\n",
      "        [0.0452],\n",
      "        [0.0454],\n",
      "        [0.0471],\n",
      "        [0.0477],\n",
      "        [0.0497],\n",
      "        [0.0499],\n",
      "        [0.0502],\n",
      "        [0.0506],\n",
      "        [0.0513],\n",
      "        [0.0514],\n",
      "        [0.0541],\n",
      "        [0.0541],\n",
      "        [0.0544],\n",
      "        [0.0550],\n",
      "        [0.0554],\n",
      "        [0.0578],\n",
      "        [0.0582],\n",
      "        [0.0596],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0630],\n",
      "        [0.0649],\n",
      "        [0.0696],\n",
      "        [0.0696],\n",
      "        [0.0711],\n",
      "        [0.0735],\n",
      "        [0.0792],\n",
      "        [0.0845],\n",
      "        [0.0861],\n",
      "        [0.0871],\n",
      "        [0.1008],\n",
      "        [0.1027],\n",
      "        [0.1078],\n",
      "        [0.1086],\n",
      "        [0.1125],\n",
      "        [0.1130],\n",
      "        [0.1207],\n",
      "        [0.1260],\n",
      "        [0.1279],\n",
      "        [0.1290],\n",
      "        [0.1342],\n",
      "        [0.1417],\n",
      "        [0.1437],\n",
      "        [0.1518],\n",
      "        [0.1535],\n",
      "        [0.1535],\n",
      "        [0.1699]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0007],\n",
      "        [0.0032],\n",
      "        [0.0046],\n",
      "        [0.0041],\n",
      "        [0.0047],\n",
      "        [0.0044],\n",
      "        [0.0065],\n",
      "        [0.0060],\n",
      "        [0.0085],\n",
      "        [0.0083],\n",
      "        [0.0065],\n",
      "        [0.0094],\n",
      "        [0.0103],\n",
      "        [0.0085],\n",
      "        [0.0095],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0135],\n",
      "        [0.0115],\n",
      "        [0.0152],\n",
      "        [0.0141],\n",
      "        [0.0109],\n",
      "        [0.0116],\n",
      "        [0.0129],\n",
      "        [0.0115],\n",
      "        [0.0112],\n",
      "        [0.0108],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0122],\n",
      "        [0.0104],\n",
      "        [0.0159],\n",
      "        [0.0136],\n",
      "        [0.0161],\n",
      "        [0.0122],\n",
      "        [0.0182],\n",
      "        [0.0165],\n",
      "        [0.0187],\n",
      "        [0.0158],\n",
      "        [0.0155],\n",
      "        [0.0160],\n",
      "        [0.0177],\n",
      "        [0.0175],\n",
      "        [0.0149],\n",
      "        [0.0215],\n",
      "        [0.0188],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0208],\n",
      "        [0.0182],\n",
      "        [0.0214],\n",
      "        [0.0239],\n",
      "        [0.0239],\n",
      "        [0.0245],\n",
      "        [0.0232],\n",
      "        [0.0258],\n",
      "        [0.0256],\n",
      "        [0.0250],\n",
      "        [0.0278],\n",
      "        [0.0265],\n",
      "        [0.0282],\n",
      "        [0.0255],\n",
      "        [0.0261],\n",
      "        [0.0267],\n",
      "        [0.0285],\n",
      "        [0.0292],\n",
      "        [0.0291],\n",
      "        [0.0308],\n",
      "        [0.0285],\n",
      "        [0.0310],\n",
      "        [0.0321],\n",
      "        [0.0320],\n",
      "        [0.0354],\n",
      "        [0.0359],\n",
      "        [0.0340],\n",
      "        [0.0380],\n",
      "        [0.0352],\n",
      "        [0.0388],\n",
      "        [0.0364],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0386],\n",
      "        [0.0381],\n",
      "        [0.0417],\n",
      "        [0.0415],\n",
      "        [0.0412],\n",
      "        [0.0456],\n",
      "        [0.0459],\n",
      "        [0.0421],\n",
      "        [0.0463],\n",
      "        [0.0438],\n",
      "        [0.0447],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0463],\n",
      "        [0.0452],\n",
      "        [0.0472],\n",
      "        [0.0489],\n",
      "        [0.0510],\n",
      "        [0.0484],\n",
      "        [0.0458],\n",
      "        [0.0496],\n",
      "        [0.0522],\n",
      "        [0.0568],\n",
      "        [0.0525],\n",
      "        [0.0533],\n",
      "        [0.0543],\n",
      "        [0.0547],\n",
      "        [0.0566],\n",
      "        [0.0588],\n",
      "        [0.0613],\n",
      "        [0.0624],\n",
      "        [0.0622],\n",
      "        [0.0621],\n",
      "        [0.0674],\n",
      "        [0.0714],\n",
      "        [0.0686],\n",
      "        [0.0735],\n",
      "        [0.0679],\n",
      "        [0.0755],\n",
      "        [0.0847],\n",
      "        [0.0859],\n",
      "        [0.0871],\n",
      "        [0.1011],\n",
      "        [0.1010],\n",
      "        [0.1044],\n",
      "        [0.1100],\n",
      "        [0.1112],\n",
      "        [0.1070],\n",
      "        [0.1221],\n",
      "        [0.1226],\n",
      "        [0.1264],\n",
      "        [0.1305],\n",
      "        [0.1314],\n",
      "        [0.1424],\n",
      "        [0.1449],\n",
      "        [0.1534],\n",
      "        [0.1504],\n",
      "        [0.1471],\n",
      "        [0.1635]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.746469497680664\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.7418711801583413e-07, 135)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [135, 50, 21, 113, 98, 110, 36, 85, 137, 68, 63, 97, 112, 55, 99, 56, 111, 17, 139, 96, 8, 74, 51, 66, 26, 100, 44, 57, 52, 18, 33, 140, 32, 134, 77, 91, 22, 23, 122, 27, 62, 136, 90, 138, 49, 129, 53, 130, 28, 124, 105, 131, 102, 16, 13, 12, 15, 121, 10, 123, 101, 83, 35, 128, 29, 104, 75, 109, 67, 25, 46, 30, 9, 76, 92, 54, 127, 20, 11, 73, 48, 86, 64, 34, 89, 93, 61, 95, 19, 87, 103, 58, 37, 132, 141, 3, 14, 0, 65, 4, 94, 120, 31, 47, 6, 88, 107, 106, 7, 1, 84, 72, 108, 126, 78, 133, 142, 114, 24, 2, 43, 125, 60, 59, 115, 82, 42, 143, 69, 5, 71, 41, 119, 70, 38, 81, 80, 144, 40, 45, 145, 39] 數值 torch.Size([142, 1])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6209],\n",
      "        [0.8901],\n",
      "        [0.8502],\n",
      "        [0.6401],\n",
      "        [0.6727],\n",
      "        [0.6462],\n",
      "        [0.8379],\n",
      "        [0.6551],\n",
      "        [0.6351],\n",
      "        [0.8101],\n",
      "        [0.8700],\n",
      "        [0.6751],\n",
      "        [0.6279],\n",
      "        [0.9011],\n",
      "        [0.6505],\n",
      "        [0.8965],\n",
      "        [0.6549],\n",
      "        [0.8993],\n",
      "        [0.6269],\n",
      "        [0.6842],\n",
      "        [0.9658],\n",
      "        [0.7494],\n",
      "        [0.8864],\n",
      "        [0.8240],\n",
      "        [0.8200],\n",
      "        [0.6232],\n",
      "        [0.8896],\n",
      "        [0.9071],\n",
      "        [0.8970],\n",
      "        [0.9005],\n",
      "        [0.8341],\n",
      "        [0.6588],\n",
      "        [0.8245],\n",
      "        [0.6278],\n",
      "        [0.7372],\n",
      "        [0.6759],\n",
      "        [0.8389],\n",
      "        [0.8613],\n",
      "        [0.5675],\n",
      "        [0.8198],\n",
      "        [0.8669],\n",
      "        [0.6506],\n",
      "        [0.6859],\n",
      "        [0.6245],\n",
      "        [0.8347],\n",
      "        [0.5819],\n",
      "        [0.8856],\n",
      "        [0.5620],\n",
      "        [0.8304],\n",
      "        [0.5644],\n",
      "        [0.6581],\n",
      "        [0.6090],\n",
      "        [0.6180],\n",
      "        [0.9199],\n",
      "        [0.9594],\n",
      "        [0.9727],\n",
      "        [0.9423],\n",
      "        [0.5521],\n",
      "        [1.0183],\n",
      "        [0.5847],\n",
      "        [0.6213],\n",
      "        [0.6818],\n",
      "        [0.8134],\n",
      "        [0.5829],\n",
      "        [0.8577],\n",
      "        [0.6305],\n",
      "        [0.7178],\n",
      "        [0.6429],\n",
      "        [0.8228],\n",
      "        [0.8374],\n",
      "        [0.8742],\n",
      "        [0.8816],\n",
      "        [1.0076],\n",
      "        [0.7246],\n",
      "        [0.6704],\n",
      "        [0.9279],\n",
      "        [0.5829],\n",
      "        [0.8795],\n",
      "        [1.0166],\n",
      "        [0.7835],\n",
      "        [0.8842],\n",
      "        [0.6787],\n",
      "        [0.8357],\n",
      "        [0.8532],\n",
      "        [0.6706],\n",
      "        [0.6746],\n",
      "        [0.8786],\n",
      "        [0.6629],\n",
      "        [0.8929],\n",
      "        [0.6916],\n",
      "        [0.6148],\n",
      "        [0.8907],\n",
      "        [0.8184],\n",
      "        [0.6311],\n",
      "        [0.6736],\n",
      "        [0.8906],\n",
      "        [0.9338],\n",
      "        [0.8503],\n",
      "        [0.7884],\n",
      "        [0.8806],\n",
      "        [0.6589],\n",
      "        [0.5610],\n",
      "        [0.8553],\n",
      "        [0.8819],\n",
      "        [0.9222],\n",
      "        [0.6809],\n",
      "        [0.6365],\n",
      "        [0.6419],\n",
      "        [0.9429],\n",
      "        [0.8704],\n",
      "        [0.6824],\n",
      "        [0.8075],\n",
      "        [0.6271],\n",
      "        [0.5548],\n",
      "        [0.7254],\n",
      "        [0.6043],\n",
      "        [0.6752],\n",
      "        [0.6574],\n",
      "        [0.8643],\n",
      "        [0.9157],\n",
      "        [0.8766],\n",
      "        [0.5391],\n",
      "        [0.8693],\n",
      "        [0.8723],\n",
      "        [0.6679],\n",
      "        [0.7346],\n",
      "        [0.8625],\n",
      "        [0.6977],\n",
      "        [0.8138],\n",
      "        [0.8709],\n",
      "        [0.7806],\n",
      "        [0.8456],\n",
      "        [0.6218],\n",
      "        [0.7798],\n",
      "        [0.8216],\n",
      "        [0.7567],\n",
      "        [0.7663],\n",
      "        [0.7061],\n",
      "        [0.8180],\n",
      "        [0.8613],\n",
      "        [0.7246],\n",
      "        [0.8302]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0009],\n",
      "        [0.0032],\n",
      "        [0.0041],\n",
      "        [0.0044],\n",
      "        [0.0046],\n",
      "        [0.0047],\n",
      "        [0.0060],\n",
      "        [0.0065],\n",
      "        [0.0065],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0103],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0136],\n",
      "        [0.0141],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0161],\n",
      "        [0.0165],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0175],\n",
      "        [0.0177],\n",
      "        [0.0182],\n",
      "        [0.0182],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0208],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0232],\n",
      "        [0.0239],\n",
      "        [0.0239],\n",
      "        [0.0245],\n",
      "        [0.0250],\n",
      "        [0.0255],\n",
      "        [0.0256],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0265],\n",
      "        [0.0267],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0285],\n",
      "        [0.0285],\n",
      "        [0.0291],\n",
      "        [0.0292],\n",
      "        [0.0308],\n",
      "        [0.0310],\n",
      "        [0.0320],\n",
      "        [0.0321],\n",
      "        [0.0340],\n",
      "        [0.0352],\n",
      "        [0.0354],\n",
      "        [0.0359],\n",
      "        [0.0364],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0381],\n",
      "        [0.0386],\n",
      "        [0.0388],\n",
      "        [0.0412],\n",
      "        [0.0415],\n",
      "        [0.0417],\n",
      "        [0.0421],\n",
      "        [0.0438],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0447],\n",
      "        [0.0452],\n",
      "        [0.0456],\n",
      "        [0.0458],\n",
      "        [0.0459],\n",
      "        [0.0463],\n",
      "        [0.0463],\n",
      "        [0.0472],\n",
      "        [0.0484],\n",
      "        [0.0489],\n",
      "        [0.0496],\n",
      "        [0.0510],\n",
      "        [0.0522],\n",
      "        [0.0525],\n",
      "        [0.0533],\n",
      "        [0.0543],\n",
      "        [0.0547],\n",
      "        [0.0566],\n",
      "        [0.0568],\n",
      "        [0.0588],\n",
      "        [0.0613],\n",
      "        [0.0621],\n",
      "        [0.0622],\n",
      "        [0.0624],\n",
      "        [0.0674],\n",
      "        [0.0679],\n",
      "        [0.0686],\n",
      "        [0.0714],\n",
      "        [0.0735],\n",
      "        [0.0755],\n",
      "        [0.0847],\n",
      "        [0.0859],\n",
      "        [0.0871],\n",
      "        [0.1010],\n",
      "        [0.1011],\n",
      "        [0.1044],\n",
      "        [0.1070],\n",
      "        [0.1100],\n",
      "        [0.1112],\n",
      "        [0.1221],\n",
      "        [0.1226],\n",
      "        [0.1264],\n",
      "        [0.1305],\n",
      "        [0.1314],\n",
      "        [0.1424],\n",
      "        [0.1449],\n",
      "        [0.1471],\n",
      "        [0.1504],\n",
      "        [0.1534],\n",
      "        [0.1635],\n",
      "        [0.1698]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0034],\n",
      "        [0.0011],\n",
      "        [0.0028],\n",
      "        [0.0020],\n",
      "        [0.0040],\n",
      "        [0.0032],\n",
      "        [0.0027],\n",
      "        [0.0064],\n",
      "        [0.0043],\n",
      "        [0.0072],\n",
      "        [0.0081],\n",
      "        [0.0079],\n",
      "        [0.0102],\n",
      "        [0.0073],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0078],\n",
      "        [0.0091],\n",
      "        [0.0068],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0103],\n",
      "        [0.0125],\n",
      "        [0.0115],\n",
      "        [0.0121],\n",
      "        [0.0094],\n",
      "        [0.0137],\n",
      "        [0.0150],\n",
      "        [0.0138],\n",
      "        [0.0160],\n",
      "        [0.0107],\n",
      "        [0.0168],\n",
      "        [0.0180],\n",
      "        [0.0152],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0167],\n",
      "        [0.0140],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0150],\n",
      "        [0.0176],\n",
      "        [0.0150],\n",
      "        [0.0175],\n",
      "        [0.0156],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0181],\n",
      "        [0.0213],\n",
      "        [0.0223],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0248],\n",
      "        [0.0258],\n",
      "        [0.0263],\n",
      "        [0.0258],\n",
      "        [0.0234],\n",
      "        [0.0250],\n",
      "        [0.0271],\n",
      "        [0.0261],\n",
      "        [0.0239],\n",
      "        [0.0252],\n",
      "        [0.0302],\n",
      "        [0.0289],\n",
      "        [0.0297],\n",
      "        [0.0285],\n",
      "        [0.0298],\n",
      "        [0.0277],\n",
      "        [0.0313],\n",
      "        [0.0310],\n",
      "        [0.0331],\n",
      "        [0.0322],\n",
      "        [0.0346],\n",
      "        [0.0356],\n",
      "        [0.0363],\n",
      "        [0.0377],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0381],\n",
      "        [0.0378],\n",
      "        [0.0384],\n",
      "        [0.0390],\n",
      "        [0.0410],\n",
      "        [0.0414],\n",
      "        [0.0426],\n",
      "        [0.0428],\n",
      "        [0.0422],\n",
      "        [0.0444],\n",
      "        [0.0446],\n",
      "        [0.0451],\n",
      "        [0.0458],\n",
      "        [0.0431],\n",
      "        [0.0480],\n",
      "        [0.0412],\n",
      "        [0.0469],\n",
      "        [0.0484],\n",
      "        [0.0466],\n",
      "        [0.0481],\n",
      "        [0.0484],\n",
      "        [0.0498],\n",
      "        [0.0475],\n",
      "        [0.0521],\n",
      "        [0.0526],\n",
      "        [0.0522],\n",
      "        [0.0540],\n",
      "        [0.0546],\n",
      "        [0.0552],\n",
      "        [0.0565],\n",
      "        [0.0575],\n",
      "        [0.0575],\n",
      "        [0.0615],\n",
      "        [0.0624],\n",
      "        [0.0636],\n",
      "        [0.0628],\n",
      "        [0.0697],\n",
      "        [0.0625],\n",
      "        [0.0668],\n",
      "        [0.0724],\n",
      "        [0.0741],\n",
      "        [0.0718],\n",
      "        [0.0851],\n",
      "        [0.0865],\n",
      "        [0.0879],\n",
      "        [0.0986],\n",
      "        [0.0999],\n",
      "        [0.1009],\n",
      "        [0.1014],\n",
      "        [0.1100],\n",
      "        [0.1116],\n",
      "        [0.1219],\n",
      "        [0.1190],\n",
      "        [0.1247],\n",
      "        [0.1307],\n",
      "        [0.1285],\n",
      "        [0.1414],\n",
      "        [0.1445],\n",
      "        [0.1413],\n",
      "        [0.1474],\n",
      "        [0.1545],\n",
      "        [0.1576],\n",
      "        [0.1668]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.02714657783508\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.2842853038819158e-06, 50)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [50, 113, 36, 21, 110, 135, 98, 137, 85, 139, 68, 55, 111, 97, 63, 99, 17, 56, 44, 96, 112, 51, 140, 8, 74, 26, 100, 66, 57, 18, 122, 52, 136, 138, 77, 91, 22, 129, 33, 23, 27, 32, 62, 49, 90, 134, 28, 53, 130, 124, 105, 121, 131, 102, 83, 16, 10, 35, 13, 15, 101, 12, 123, 67, 75, 29, 104, 109, 128, 46, 25, 9, 30, 76, 92, 54, 11, 20, 127, 48, 73, 86, 64, 34, 141, 89, 95, 93, 61, 37, 19, 87, 103, 58, 0, 3, 120, 132, 65, 14, 4, 94, 31, 6, 47, 88, 107, 106, 7, 1, 84, 72, 108, 142, 78, 126, 114, 133, 43, 24, 2, 125, 60, 59, 115, 82, 42, 143, 69, 5, 41, 71, 119, 38, 70, 144, 81, 80, 40, 45, 145, 39, 146] 數值 torch.Size([143, 1])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8904],\n",
      "        [0.6380],\n",
      "        [0.8398],\n",
      "        [0.8506],\n",
      "        [0.6447],\n",
      "        [0.6182],\n",
      "        [0.6731],\n",
      "        [0.6329],\n",
      "        [0.6547],\n",
      "        [0.6234],\n",
      "        [0.8094],\n",
      "        [0.9023],\n",
      "        [0.6532],\n",
      "        [0.6757],\n",
      "        [0.8697],\n",
      "        [0.6508],\n",
      "        [0.8980],\n",
      "        [0.8966],\n",
      "        [0.8925],\n",
      "        [0.6849],\n",
      "        [0.6262],\n",
      "        [0.8876],\n",
      "        [0.6546],\n",
      "        [0.9659],\n",
      "        [0.7496],\n",
      "        [0.8201],\n",
      "        [0.6232],\n",
      "        [0.8229],\n",
      "        [0.9063],\n",
      "        [0.9003],\n",
      "        [0.5654],\n",
      "        [0.8984],\n",
      "        [0.6483],\n",
      "        [0.6220],\n",
      "        [0.7370],\n",
      "        [0.6762],\n",
      "        [0.8394],\n",
      "        [0.5794],\n",
      "        [0.8360],\n",
      "        [0.8621],\n",
      "        [0.8204],\n",
      "        [0.8262],\n",
      "        [0.8661],\n",
      "        [0.8349],\n",
      "        [0.6858],\n",
      "        [0.6250],\n",
      "        [0.8311],\n",
      "        [0.8873],\n",
      "        [0.5604],\n",
      "        [0.5639],\n",
      "        [0.6572],\n",
      "        [0.5499],\n",
      "        [0.6071],\n",
      "        [0.6176],\n",
      "        [0.6792],\n",
      "        [0.9190],\n",
      "        [1.0177],\n",
      "        [0.8148],\n",
      "        [0.9576],\n",
      "        [0.9416],\n",
      "        [0.6213],\n",
      "        [0.9710],\n",
      "        [0.5834],\n",
      "        [0.8213],\n",
      "        [0.7178],\n",
      "        [0.8584],\n",
      "        [0.6293],\n",
      "        [0.6422],\n",
      "        [0.5805],\n",
      "        [0.8743],\n",
      "        [0.8379],\n",
      "        [1.0077],\n",
      "        [0.8827],\n",
      "        [0.7240],\n",
      "        [0.6701],\n",
      "        [0.9289],\n",
      "        [1.0157],\n",
      "        [0.8793],\n",
      "        [0.5810],\n",
      "        [0.8841],\n",
      "        [0.7836],\n",
      "        [0.6784],\n",
      "        [0.8354],\n",
      "        [0.8554],\n",
      "        [0.6690],\n",
      "        [0.6703],\n",
      "        [0.6628],\n",
      "        [0.6735],\n",
      "        [0.8775],\n",
      "        [0.8204],\n",
      "        [0.8923],\n",
      "        [0.6909],\n",
      "        [0.6140],\n",
      "        [0.8897],\n",
      "        [0.8506],\n",
      "        [0.8916],\n",
      "        [0.5589],\n",
      "        [0.6288],\n",
      "        [0.7876],\n",
      "        [0.9317],\n",
      "        [0.8806],\n",
      "        [0.6580],\n",
      "        [0.8564],\n",
      "        [0.9225],\n",
      "        [0.8822],\n",
      "        [0.6803],\n",
      "        [0.6361],\n",
      "        [0.6414],\n",
      "        [0.9430],\n",
      "        [0.8710],\n",
      "        [0.6811],\n",
      "        [0.8077],\n",
      "        [0.6269],\n",
      "        [0.6698],\n",
      "        [0.7258],\n",
      "        [0.5534],\n",
      "        [0.6556],\n",
      "        [0.6020],\n",
      "        [0.8803],\n",
      "        [0.8652],\n",
      "        [0.9162],\n",
      "        [0.5387],\n",
      "        [0.8687],\n",
      "        [0.8715],\n",
      "        [0.6655],\n",
      "        [0.7334],\n",
      "        [0.8660],\n",
      "        [0.6921],\n",
      "        [0.8138],\n",
      "        [0.8705],\n",
      "        [0.8492],\n",
      "        [0.7804],\n",
      "        [0.6200],\n",
      "        [0.8245],\n",
      "        [0.7800],\n",
      "        [0.7003],\n",
      "        [0.7558],\n",
      "        [0.7659],\n",
      "        [0.8211],\n",
      "        [0.8625],\n",
      "        [0.7187],\n",
      "        [0.8332],\n",
      "        [0.7263]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0011],\n",
      "        [0.0020],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0064],\n",
      "        [0.0068],\n",
      "        [0.0072],\n",
      "        [0.0073],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0103],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.0121],\n",
      "        [0.0125],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0150],\n",
      "        [0.0150],\n",
      "        [0.0150],\n",
      "        [0.0152],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0160],\n",
      "        [0.0167],\n",
      "        [0.0167],\n",
      "        [0.0168],\n",
      "        [0.0172],\n",
      "        [0.0175],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0213],\n",
      "        [0.0223],\n",
      "        [0.0234],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0239],\n",
      "        [0.0248],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0258],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0271],\n",
      "        [0.0277],\n",
      "        [0.0285],\n",
      "        [0.0289],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0322],\n",
      "        [0.0331],\n",
      "        [0.0346],\n",
      "        [0.0356],\n",
      "        [0.0363],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0377],\n",
      "        [0.0378],\n",
      "        [0.0381],\n",
      "        [0.0384],\n",
      "        [0.0390],\n",
      "        [0.0410],\n",
      "        [0.0412],\n",
      "        [0.0414],\n",
      "        [0.0422],\n",
      "        [0.0426],\n",
      "        [0.0428],\n",
      "        [0.0431],\n",
      "        [0.0444],\n",
      "        [0.0446],\n",
      "        [0.0451],\n",
      "        [0.0458],\n",
      "        [0.0466],\n",
      "        [0.0469],\n",
      "        [0.0475],\n",
      "        [0.0480],\n",
      "        [0.0481],\n",
      "        [0.0484],\n",
      "        [0.0484],\n",
      "        [0.0498],\n",
      "        [0.0521],\n",
      "        [0.0522],\n",
      "        [0.0526],\n",
      "        [0.0540],\n",
      "        [0.0546],\n",
      "        [0.0552],\n",
      "        [0.0565],\n",
      "        [0.0575],\n",
      "        [0.0575],\n",
      "        [0.0615],\n",
      "        [0.0624],\n",
      "        [0.0625],\n",
      "        [0.0628],\n",
      "        [0.0636],\n",
      "        [0.0668],\n",
      "        [0.0697],\n",
      "        [0.0718],\n",
      "        [0.0724],\n",
      "        [0.0741],\n",
      "        [0.0851],\n",
      "        [0.0865],\n",
      "        [0.0879],\n",
      "        [0.0986],\n",
      "        [0.0999],\n",
      "        [0.1009],\n",
      "        [0.1014],\n",
      "        [0.1100],\n",
      "        [0.1116],\n",
      "        [0.1190],\n",
      "        [0.1219],\n",
      "        [0.1247],\n",
      "        [0.1285],\n",
      "        [0.1307],\n",
      "        [0.1413],\n",
      "        [0.1414],\n",
      "        [0.1445],\n",
      "        [0.1474],\n",
      "        [0.1545],\n",
      "        [0.1576],\n",
      "        [0.1668],\n",
      "        [0.1705]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0024],\n",
      "        [0.0010],\n",
      "        [0.0002],\n",
      "        [0.0012],\n",
      "        [0.0028],\n",
      "        [0.0062],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0048],\n",
      "        [0.0033],\n",
      "        [0.0063],\n",
      "        [0.0052],\n",
      "        [0.0072],\n",
      "        [0.0054],\n",
      "        [0.0091],\n",
      "        [0.0069],\n",
      "        [0.0091],\n",
      "        [0.0083],\n",
      "        [0.0058],\n",
      "        [0.0076],\n",
      "        [0.0110],\n",
      "        [0.0082],\n",
      "        [0.0064],\n",
      "        [0.0095],\n",
      "        [0.0087],\n",
      "        [0.0104],\n",
      "        [0.0103],\n",
      "        [0.0120],\n",
      "        [0.0135],\n",
      "        [0.0128],\n",
      "        [0.0126],\n",
      "        [0.0173],\n",
      "        [0.0125],\n",
      "        [0.0125],\n",
      "        [0.0168],\n",
      "        [0.0131],\n",
      "        [0.0137],\n",
      "        [0.0132],\n",
      "        [0.0186],\n",
      "        [0.0185],\n",
      "        [0.0181],\n",
      "        [0.0190],\n",
      "        [0.0167],\n",
      "        [0.0162],\n",
      "        [0.0159],\n",
      "        [0.0208],\n",
      "        [0.0168],\n",
      "        [0.0225],\n",
      "        [0.0216],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0220],\n",
      "        [0.0252],\n",
      "        [0.0224],\n",
      "        [0.0232],\n",
      "        [0.0245],\n",
      "        [0.0254],\n",
      "        [0.0232],\n",
      "        [0.0264],\n",
      "        [0.0254],\n",
      "        [0.0246],\n",
      "        [0.0268],\n",
      "        [0.0281],\n",
      "        [0.0277],\n",
      "        [0.0264],\n",
      "        [0.0301],\n",
      "        [0.0294],\n",
      "        [0.0293],\n",
      "        [0.0324],\n",
      "        [0.0320],\n",
      "        [0.0329],\n",
      "        [0.0335],\n",
      "        [0.0346],\n",
      "        [0.0333],\n",
      "        [0.0341],\n",
      "        [0.0382],\n",
      "        [0.0367],\n",
      "        [0.0358],\n",
      "        [0.0392],\n",
      "        [0.0386],\n",
      "        [0.0402],\n",
      "        [0.0368],\n",
      "        [0.0379],\n",
      "        [0.0438],\n",
      "        [0.0365],\n",
      "        [0.0398],\n",
      "        [0.0405],\n",
      "        [0.0419],\n",
      "        [0.0428],\n",
      "        [0.0405],\n",
      "        [0.0438],\n",
      "        [0.0435],\n",
      "        [0.0443],\n",
      "        [0.0459],\n",
      "        [0.0488],\n",
      "        [0.0495],\n",
      "        [0.0462],\n",
      "        [0.0502],\n",
      "        [0.0474],\n",
      "        [0.0493],\n",
      "        [0.0468],\n",
      "        [0.0490],\n",
      "        [0.0536],\n",
      "        [0.0507],\n",
      "        [0.0538],\n",
      "        [0.0527],\n",
      "        [0.0538],\n",
      "        [0.0543],\n",
      "        [0.0552],\n",
      "        [0.0600],\n",
      "        [0.0582],\n",
      "        [0.0636],\n",
      "        [0.0613],\n",
      "        [0.0569],\n",
      "        [0.0651],\n",
      "        [0.0647],\n",
      "        [0.0660],\n",
      "        [0.0719],\n",
      "        [0.0674],\n",
      "        [0.0744],\n",
      "        [0.0763],\n",
      "        [0.0851],\n",
      "        [0.0861],\n",
      "        [0.0877],\n",
      "        [0.0971],\n",
      "        [0.1005],\n",
      "        [0.0967],\n",
      "        [0.0955],\n",
      "        [0.1116],\n",
      "        [0.1105],\n",
      "        [0.1147],\n",
      "        [0.1236],\n",
      "        [0.1236],\n",
      "        [0.1250],\n",
      "        [0.1326],\n",
      "        [0.1349],\n",
      "        [0.1424],\n",
      "        [0.1460],\n",
      "        [0.1436],\n",
      "        [0.1565],\n",
      "        [0.1513],\n",
      "        [0.1631],\n",
      "        [0.1632]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.30674505233765\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.92961227394062e-08, 36)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [36, 113, 21, 98, 137, 50, 110, 139, 85, 55, 97, 44, 135, 68, 140, 99, 111, 96, 51, 56, 74, 63, 17, 8, 100, 26, 112, 66, 136, 138, 122, 18, 91, 129, 57, 22, 90, 49, 62, 28, 77, 52, 27, 23, 33, 32, 134, 124, 130, 105, 121, 102, 53, 83, 35, 16, 101, 131, 15, 10, 13, 75, 12, 67, 123, 109, 104, 29, 46, 128, 25, 76, 9, 92, 30, 20, 141, 11, 86, 64, 54, 48, 127, 89, 73, 37, 95, 93, 61, 87, 34, 19, 103, 58, 120, 4, 65, 0, 94, 14, 3, 132, 6, 88, 31, 107, 47, 106, 7, 142, 84, 1, 108, 72, 126, 78, 114, 43, 133, 24, 2, 125, 60, 59, 143, 42, 115, 82, 5, 69, 41, 119, 71, 38, 70, 144, 81, 40, 80, 145, 45, 39, 146, 147] 數值 torch.Size([144, 1])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8423],\n",
      "        [0.6370],\n",
      "        [0.8521],\n",
      "        [0.6754],\n",
      "        [0.6306],\n",
      "        [0.8916],\n",
      "        [0.6443],\n",
      "        [0.6198],\n",
      "        [0.6563],\n",
      "        [0.9044],\n",
      "        [0.6781],\n",
      "        [0.8961],\n",
      "        [0.6154],\n",
      "        [0.8103],\n",
      "        [0.6504],\n",
      "        [0.6528],\n",
      "        [0.6525],\n",
      "        [0.6874],\n",
      "        [0.8897],\n",
      "        [0.8976],\n",
      "        [0.7519],\n",
      "        [0.8707],\n",
      "        [0.8980],\n",
      "        [0.9671],\n",
      "        [0.6251],\n",
      "        [0.8211],\n",
      "        [0.6254],\n",
      "        [0.8235],\n",
      "        [0.6458],\n",
      "        [0.6195],\n",
      "        [0.5640],\n",
      "        [0.9013],\n",
      "        [0.6783],\n",
      "        [0.5770],\n",
      "        [0.9064],\n",
      "        [0.8410],\n",
      "        [0.6875],\n",
      "        [0.8362],\n",
      "        [0.8666],\n",
      "        [0.8324],\n",
      "        [0.7386],\n",
      "        [0.9008],\n",
      "        [0.8218],\n",
      "        [0.8640],\n",
      "        [0.8386],\n",
      "        [0.8285],\n",
      "        [0.6222],\n",
      "        [0.5638],\n",
      "        [0.5590],\n",
      "        [0.6576],\n",
      "        [0.5485],\n",
      "        [0.6188],\n",
      "        [0.8899],\n",
      "        [0.6785],\n",
      "        [0.8168],\n",
      "        [0.9193],\n",
      "        [0.6229],\n",
      "        [0.6054],\n",
      "        [0.9420],\n",
      "        [1.0182],\n",
      "        [0.9570],\n",
      "        [0.7199],\n",
      "        [0.9704],\n",
      "        [0.8213],\n",
      "        [0.5824],\n",
      "        [0.6427],\n",
      "        [0.6296],\n",
      "        [0.8596],\n",
      "        [0.8753],\n",
      "        [0.5783],\n",
      "        [0.8394],\n",
      "        [0.7253],\n",
      "        [1.0090],\n",
      "        [0.6715],\n",
      "        [0.8842],\n",
      "        [0.8801],\n",
      "        [0.6642],\n",
      "        [1.0158],\n",
      "        [0.6799],\n",
      "        [0.8364],\n",
      "        [0.9308],\n",
      "        [0.8849],\n",
      "        [0.5795],\n",
      "        [0.6720],\n",
      "        [0.7857],\n",
      "        [0.8231],\n",
      "        [0.6645],\n",
      "        [0.6741],\n",
      "        [0.8775],\n",
      "        [0.6920],\n",
      "        [0.8582],\n",
      "        [0.8928],\n",
      "        [0.6149],\n",
      "        [0.8896],\n",
      "        [0.5575],\n",
      "        [0.8822],\n",
      "        [0.7882],\n",
      "        [0.8528],\n",
      "        [0.6588],\n",
      "        [0.9307],\n",
      "        [0.8942],\n",
      "        [0.6265],\n",
      "        [0.9241],\n",
      "        [0.6815],\n",
      "        [0.8579],\n",
      "        [0.6370],\n",
      "        [0.8835],\n",
      "        [0.6423],\n",
      "        [0.9442],\n",
      "        [0.6641],\n",
      "        [0.6818],\n",
      "        [0.8735],\n",
      "        [0.6279],\n",
      "        [0.8098],\n",
      "        [0.5524],\n",
      "        [0.7282],\n",
      "        [0.6548],\n",
      "        [0.8847],\n",
      "        [0.5998],\n",
      "        [0.8673],\n",
      "        [0.9184],\n",
      "        [0.5388],\n",
      "        [0.8692],\n",
      "        [0.8717],\n",
      "        [0.6862],\n",
      "        [0.8702],\n",
      "        [0.6640],\n",
      "        [0.7339],\n",
      "        [0.8716],\n",
      "        [0.8154],\n",
      "        [0.8535],\n",
      "        [0.6189],\n",
      "        [0.7821],\n",
      "        [0.8280],\n",
      "        [0.7819],\n",
      "        [0.6939],\n",
      "        [0.7568],\n",
      "        [0.8249],\n",
      "        [0.7674],\n",
      "        [0.7123],\n",
      "        [0.8645],\n",
      "        [0.8369],\n",
      "        [0.7191],\n",
      "        [0.7239]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0024],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0048],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0058],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0064],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0095],\n",
      "        [0.0103],\n",
      "        [0.0104],\n",
      "        [0.0110],\n",
      "        [0.0120],\n",
      "        [0.0125],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0135],\n",
      "        [0.0137],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0167],\n",
      "        [0.0168],\n",
      "        [0.0168],\n",
      "        [0.0173],\n",
      "        [0.0181],\n",
      "        [0.0185],\n",
      "        [0.0186],\n",
      "        [0.0190],\n",
      "        [0.0208],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0232],\n",
      "        [0.0245],\n",
      "        [0.0246],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0264],\n",
      "        [0.0264],\n",
      "        [0.0268],\n",
      "        [0.0277],\n",
      "        [0.0281],\n",
      "        [0.0293],\n",
      "        [0.0294],\n",
      "        [0.0301],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0333],\n",
      "        [0.0335],\n",
      "        [0.0341],\n",
      "        [0.0346],\n",
      "        [0.0358],\n",
      "        [0.0365],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0379],\n",
      "        [0.0382],\n",
      "        [0.0386],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0402],\n",
      "        [0.0405],\n",
      "        [0.0405],\n",
      "        [0.0419],\n",
      "        [0.0428],\n",
      "        [0.0435],\n",
      "        [0.0438],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0459],\n",
      "        [0.0462],\n",
      "        [0.0468],\n",
      "        [0.0474],\n",
      "        [0.0488],\n",
      "        [0.0490],\n",
      "        [0.0493],\n",
      "        [0.0495],\n",
      "        [0.0502],\n",
      "        [0.0507],\n",
      "        [0.0527],\n",
      "        [0.0536],\n",
      "        [0.0538],\n",
      "        [0.0538],\n",
      "        [0.0543],\n",
      "        [0.0552],\n",
      "        [0.0569],\n",
      "        [0.0582],\n",
      "        [0.0600],\n",
      "        [0.0613],\n",
      "        [0.0636],\n",
      "        [0.0647],\n",
      "        [0.0651],\n",
      "        [0.0660],\n",
      "        [0.0674],\n",
      "        [0.0719],\n",
      "        [0.0744],\n",
      "        [0.0763],\n",
      "        [0.0851],\n",
      "        [0.0861],\n",
      "        [0.0877],\n",
      "        [0.0955],\n",
      "        [0.0967],\n",
      "        [0.0971],\n",
      "        [0.1005],\n",
      "        [0.1105],\n",
      "        [0.1116],\n",
      "        [0.1147],\n",
      "        [0.1236],\n",
      "        [0.1236],\n",
      "        [0.1250],\n",
      "        [0.1326],\n",
      "        [0.1349],\n",
      "        [0.1424],\n",
      "        [0.1436],\n",
      "        [0.1460],\n",
      "        [0.1513],\n",
      "        [0.1565],\n",
      "        [0.1631],\n",
      "        [0.1632],\n",
      "        [0.1759]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0026],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0029],\n",
      "        [0.0046],\n",
      "        [0.0010],\n",
      "        [0.0013],\n",
      "        [0.0046],\n",
      "        [0.0064],\n",
      "        [0.0078],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0133],\n",
      "        [0.0096],\n",
      "        [0.0024],\n",
      "        [0.0082],\n",
      "        [0.0027],\n",
      "        [0.0084],\n",
      "        [0.0107],\n",
      "        [0.0121],\n",
      "        [0.0101],\n",
      "        [0.0056],\n",
      "        [0.0043],\n",
      "        [0.0134],\n",
      "        [0.0117],\n",
      "        [0.0139],\n",
      "        [0.0153],\n",
      "        [0.0156],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0076],\n",
      "        [0.0166],\n",
      "        [0.0145],\n",
      "        [0.0069],\n",
      "        [0.0184],\n",
      "        [0.0166],\n",
      "        [0.0175],\n",
      "        [0.0195],\n",
      "        [0.0210],\n",
      "        [0.0204],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0278],\n",
      "        [0.0249],\n",
      "        [0.0267],\n",
      "        [0.0249],\n",
      "        [0.0170],\n",
      "        [0.0245],\n",
      "        [0.0203],\n",
      "        [0.0189],\n",
      "        [0.0259],\n",
      "        [0.0291],\n",
      "        [0.0263],\n",
      "        [0.0308],\n",
      "        [0.0299],\n",
      "        [0.0203],\n",
      "        [0.0322],\n",
      "        [0.0277],\n",
      "        [0.0326],\n",
      "        [0.0234],\n",
      "        [0.0327],\n",
      "        [0.0325],\n",
      "        [0.0325],\n",
      "        [0.0263],\n",
      "        [0.0282],\n",
      "        [0.0385],\n",
      "        [0.0300],\n",
      "        [0.0356],\n",
      "        [0.0294],\n",
      "        [0.0361],\n",
      "        [0.0308],\n",
      "        [0.0398],\n",
      "        [0.0269],\n",
      "        [0.0313],\n",
      "        [0.0387],\n",
      "        [0.0412],\n",
      "        [0.0350],\n",
      "        [0.0346],\n",
      "        [0.0445],\n",
      "        [0.0414],\n",
      "        [0.0385],\n",
      "        [0.0425],\n",
      "        [0.0421],\n",
      "        [0.0448],\n",
      "        [0.0476],\n",
      "        [0.0458],\n",
      "        [0.0417],\n",
      "        [0.0482],\n",
      "        [0.0466],\n",
      "        [0.0509],\n",
      "        [0.0413],\n",
      "        [0.0495],\n",
      "        [0.0508],\n",
      "        [0.0470],\n",
      "        [0.0515],\n",
      "        [0.0553],\n",
      "        [0.0477],\n",
      "        [0.0566],\n",
      "        [0.0539],\n",
      "        [0.0549],\n",
      "        [0.0502],\n",
      "        [0.0564],\n",
      "        [0.0503],\n",
      "        [0.0570],\n",
      "        [0.0589],\n",
      "        [0.0463],\n",
      "        [0.0555],\n",
      "        [0.0584],\n",
      "        [0.0638],\n",
      "        [0.0618],\n",
      "        [0.0692],\n",
      "        [0.0640],\n",
      "        [0.0613],\n",
      "        [0.0677],\n",
      "        [0.0781],\n",
      "        [0.0718],\n",
      "        [0.0741],\n",
      "        [0.0884],\n",
      "        [0.0903],\n",
      "        [0.0922],\n",
      "        [0.0845],\n",
      "        [0.0972],\n",
      "        [0.0915],\n",
      "        [0.0972],\n",
      "        [0.1138],\n",
      "        [0.1091],\n",
      "        [0.1151],\n",
      "        [0.1187],\n",
      "        [0.1215],\n",
      "        [0.1262],\n",
      "        [0.1306],\n",
      "        [0.1234],\n",
      "        [0.1395],\n",
      "        [0.1444],\n",
      "        [0.1437],\n",
      "        [0.1396],\n",
      "        [0.1538],\n",
      "        [0.1641],\n",
      "        [0.1505],\n",
      "        [0.1624]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.588626861572266\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0814383131219074e-06, 50)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [50, 110, 140, 36, 111, 98, 113, 21, 17, 139, 137, 136, 63, 138, 97, 85, 129, 44, 122, 55, 99, 96, 68, 74, 51, 100, 56, 135, 8, 26, 91, 77, 27, 52, 112, 66, 23, 33, 32, 18, 22, 121, 90, 57, 83, 49, 53, 10, 28, 62, 67, 102, 105, 124, 35, 101, 29, 130, 141, 75, 134, 46, 16, 9, 15, 25, 131, 30, 11, 13, 109, 104, 12, 123, 48, 54, 76, 92, 128, 73, 86, 20, 64, 120, 89, 34, 95, 37, 127, 93, 87, 142, 103, 0, 61, 3, 19, 4, 31, 47, 65, 58, 94, 6, 88, 14, 84, 107, 132, 106, 1, 7, 114, 72, 108, 78, 43, 126, 24, 2, 133, 143, 125, 60, 115, 59, 42, 82, 69, 5, 41, 119, 71, 144, 38, 70, 81, 145, 80, 40, 146, 45, 147, 39, 118] 數值 torch.Size([145, 1])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8882],\n",
      "        [0.6403],\n",
      "        [0.6415],\n",
      "        [0.8399],\n",
      "        [0.6481],\n",
      "        [0.6743],\n",
      "        [0.6321],\n",
      "        [0.8492],\n",
      "        [0.8932],\n",
      "        [0.6120],\n",
      "        [0.6240],\n",
      "        [0.6389],\n",
      "        [0.8672],\n",
      "        [0.6128],\n",
      "        [0.6773],\n",
      "        [0.6547],\n",
      "        [0.5707],\n",
      "        [0.8949],\n",
      "        [0.5590],\n",
      "        [0.9018],\n",
      "        [0.6516],\n",
      "        [0.6866],\n",
      "        [0.8071],\n",
      "        [0.7505],\n",
      "        [0.8871],\n",
      "        [0.6237],\n",
      "        [0.8938],\n",
      "        [0.6083],\n",
      "        [0.9633],\n",
      "        [0.8177],\n",
      "        [0.6770],\n",
      "        [0.7366],\n",
      "        [0.8185],\n",
      "        [0.8984],\n",
      "        [0.6211],\n",
      "        [0.8198],\n",
      "        [0.8612],\n",
      "        [0.8365],\n",
      "        [0.8260],\n",
      "        [0.8975],\n",
      "        [0.8382],\n",
      "        [0.5435],\n",
      "        [0.6858],\n",
      "        [0.9015],\n",
      "        [0.6742],\n",
      "        [0.8329],\n",
      "        [0.8877],\n",
      "        [1.0131],\n",
      "        [0.8288],\n",
      "        [0.8623],\n",
      "        [0.8170],\n",
      "        [0.6167],\n",
      "        [0.6545],\n",
      "        [0.5602],\n",
      "        [0.8142],\n",
      "        [0.6212],\n",
      "        [0.8558],\n",
      "        [0.5540],\n",
      "        [0.6547],\n",
      "        [0.7186],\n",
      "        [0.6152],\n",
      "        [0.8715],\n",
      "        [0.9147],\n",
      "        [1.0049],\n",
      "        [0.9374],\n",
      "        [0.8365],\n",
      "        [0.5998],\n",
      "        [0.8804],\n",
      "        [1.0104],\n",
      "        [0.9512],\n",
      "        [0.6395],\n",
      "        [0.6264],\n",
      "        [0.9646],\n",
      "        [0.5778],\n",
      "        [0.8809],\n",
      "        [0.9276],\n",
      "        [0.7230],\n",
      "        [0.6695],\n",
      "        [0.5723],\n",
      "        [0.7840],\n",
      "        [0.6781],\n",
      "        [0.8761],\n",
      "        [0.8331],\n",
      "        [0.5527],\n",
      "        [0.6703],\n",
      "        [0.8562],\n",
      "        [0.6629],\n",
      "        [0.8210],\n",
      "        [0.5742],\n",
      "        [0.6713],\n",
      "        [0.6897],\n",
      "        [0.6536],\n",
      "        [0.6125],\n",
      "        [0.8510],\n",
      "        [0.8727],\n",
      "        [0.8924],\n",
      "        [0.8885],\n",
      "        [0.8795],\n",
      "        [0.8545],\n",
      "        [0.8799],\n",
      "        [0.7848],\n",
      "        [0.8845],\n",
      "        [0.6562],\n",
      "        [0.9208],\n",
      "        [0.6793],\n",
      "        [0.9248],\n",
      "        [0.6791],\n",
      "        [0.6343],\n",
      "        [0.6201],\n",
      "        [0.6396],\n",
      "        [0.8719],\n",
      "        [0.9405],\n",
      "        [0.6501],\n",
      "        [0.8079],\n",
      "        [0.6254],\n",
      "        [0.7270],\n",
      "        [0.8844],\n",
      "        [0.5478],\n",
      "        [0.8647],\n",
      "        [0.9162],\n",
      "        [0.5936],\n",
      "        [0.6752],\n",
      "        [0.5355],\n",
      "        [0.8649],\n",
      "        [0.6584],\n",
      "        [0.8672],\n",
      "        [0.8698],\n",
      "        [0.7307],\n",
      "        [0.8129],\n",
      "        [0.8683],\n",
      "        [0.8531],\n",
      "        [0.6140],\n",
      "        [0.7800],\n",
      "        [0.6823],\n",
      "        [0.8268],\n",
      "        [0.7798],\n",
      "        [0.7539],\n",
      "        [0.7006],\n",
      "        [0.7650],\n",
      "        [0.8241],\n",
      "        [0.7063],\n",
      "        [0.8618],\n",
      "        [0.7103],\n",
      "        [0.8359],\n",
      "        [0.6339]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0013],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0046],\n",
      "        [0.0046],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0063],\n",
      "        [0.0064],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0096],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0117],\n",
      "        [0.0121],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0139],\n",
      "        [0.0145],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0170],\n",
      "        [0.0175],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0195],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0210],\n",
      "        [0.0234],\n",
      "        [0.0245],\n",
      "        [0.0249],\n",
      "        [0.0249],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0267],\n",
      "        [0.0269],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0291],\n",
      "        [0.0294],\n",
      "        [0.0299],\n",
      "        [0.0300],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0313],\n",
      "        [0.0322],\n",
      "        [0.0325],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0327],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0361],\n",
      "        [0.0385],\n",
      "        [0.0385],\n",
      "        [0.0387],\n",
      "        [0.0398],\n",
      "        [0.0412],\n",
      "        [0.0413],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.0421],\n",
      "        [0.0425],\n",
      "        [0.0445],\n",
      "        [0.0448],\n",
      "        [0.0458],\n",
      "        [0.0463],\n",
      "        [0.0466],\n",
      "        [0.0470],\n",
      "        [0.0476],\n",
      "        [0.0477],\n",
      "        [0.0482],\n",
      "        [0.0495],\n",
      "        [0.0502],\n",
      "        [0.0503],\n",
      "        [0.0508],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0539],\n",
      "        [0.0549],\n",
      "        [0.0553],\n",
      "        [0.0555],\n",
      "        [0.0564],\n",
      "        [0.0566],\n",
      "        [0.0570],\n",
      "        [0.0584],\n",
      "        [0.0589],\n",
      "        [0.0613],\n",
      "        [0.0618],\n",
      "        [0.0638],\n",
      "        [0.0640],\n",
      "        [0.0677],\n",
      "        [0.0692],\n",
      "        [0.0718],\n",
      "        [0.0741],\n",
      "        [0.0781],\n",
      "        [0.0845],\n",
      "        [0.0884],\n",
      "        [0.0903],\n",
      "        [0.0915],\n",
      "        [0.0922],\n",
      "        [0.0972],\n",
      "        [0.0972],\n",
      "        [0.1091],\n",
      "        [0.1138],\n",
      "        [0.1151],\n",
      "        [0.1187],\n",
      "        [0.1215],\n",
      "        [0.1234],\n",
      "        [0.1262],\n",
      "        [0.1306],\n",
      "        [0.1395],\n",
      "        [0.1396],\n",
      "        [0.1437],\n",
      "        [0.1444],\n",
      "        [0.1505],\n",
      "        [0.1538],\n",
      "        [0.1624],\n",
      "        [0.1641],\n",
      "        [0.1770]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0011],\n",
      "        [0.0041],\n",
      "        [0.0077],\n",
      "        [0.0024],\n",
      "        [0.0004],\n",
      "        [0.0033],\n",
      "        [0.0075],\n",
      "        [0.0032],\n",
      "        [0.0042],\n",
      "        [0.0091],\n",
      "        [0.0081],\n",
      "        [0.0018],\n",
      "        [0.0051],\n",
      "        [0.0022],\n",
      "        [0.0065],\n",
      "        [0.0070],\n",
      "        [0.0033],\n",
      "        [0.0050],\n",
      "        [0.0042],\n",
      "        [0.0072],\n",
      "        [0.0088],\n",
      "        [0.0085],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0101],\n",
      "        [0.0125],\n",
      "        [0.0125],\n",
      "        [0.0172],\n",
      "        [0.0131],\n",
      "        [0.0140],\n",
      "        [0.0149],\n",
      "        [0.0141],\n",
      "        [0.0148],\n",
      "        [0.0156],\n",
      "        [0.0185],\n",
      "        [0.0160],\n",
      "        [0.0168],\n",
      "        [0.0171],\n",
      "        [0.0170],\n",
      "        [0.0160],\n",
      "        [0.0158],\n",
      "        [0.0131],\n",
      "        [0.0181],\n",
      "        [0.0197],\n",
      "        [0.0162],\n",
      "        [0.0193],\n",
      "        [0.0211],\n",
      "        [0.0201],\n",
      "        [0.0208],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0259],\n",
      "        [0.0270],\n",
      "        [0.0268],\n",
      "        [0.0259],\n",
      "        [0.0273],\n",
      "        [0.0258],\n",
      "        [0.0292],\n",
      "        [0.0210],\n",
      "        [0.0276],\n",
      "        [0.0317],\n",
      "        [0.0281],\n",
      "        [0.0289],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0306],\n",
      "        [0.0336],\n",
      "        [0.0304],\n",
      "        [0.0309],\n",
      "        [0.0329],\n",
      "        [0.0347],\n",
      "        [0.0347],\n",
      "        [0.0333],\n",
      "        [0.0355],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0366],\n",
      "        [0.0373],\n",
      "        [0.0419],\n",
      "        [0.0387],\n",
      "        [0.0397],\n",
      "        [0.0395],\n",
      "        [0.0416],\n",
      "        [0.0375],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0420],\n",
      "        [0.0474],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0396],\n",
      "        [0.0482],\n",
      "        [0.0493],\n",
      "        [0.0491],\n",
      "        [0.0499],\n",
      "        [0.0481],\n",
      "        [0.0483],\n",
      "        [0.0498],\n",
      "        [0.0504],\n",
      "        [0.0513],\n",
      "        [0.0522],\n",
      "        [0.0530],\n",
      "        [0.0531],\n",
      "        [0.0561],\n",
      "        [0.0562],\n",
      "        [0.0541],\n",
      "        [0.0583],\n",
      "        [0.0599],\n",
      "        [0.0589],\n",
      "        [0.0609],\n",
      "        [0.0583],\n",
      "        [0.0578],\n",
      "        [0.0620],\n",
      "        [0.0656],\n",
      "        [0.0640],\n",
      "        [0.0652],\n",
      "        [0.0718],\n",
      "        [0.0729],\n",
      "        [0.0761],\n",
      "        [0.0813],\n",
      "        [0.0773],\n",
      "        [0.0901],\n",
      "        [0.0913],\n",
      "        [0.0875],\n",
      "        [0.0933],\n",
      "        [0.0949],\n",
      "        [0.0952],\n",
      "        [0.1092],\n",
      "        [0.1131],\n",
      "        [0.1129],\n",
      "        [0.1148],\n",
      "        [0.1214],\n",
      "        [0.1155],\n",
      "        [0.1250],\n",
      "        [0.1306],\n",
      "        [0.1379],\n",
      "        [0.1317],\n",
      "        [0.1427],\n",
      "        [0.1426],\n",
      "        [0.1416],\n",
      "        [0.1546],\n",
      "        [0.1529],\n",
      "        [0.1626],\n",
      "        [0.1731]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.87047457695007\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.673355427556089e-07, 111)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [111, 50, 136, 138, 36, 21, 98, 129, 110, 17, 122, 44, 63, 97, 85, 55, 113, 140, 137, 96, 99, 139, 68, 74, 51, 100, 56, 8, 121, 26, 77, 27, 91, 52, 22, 66, 18, 83, 23, 32, 33, 135, 90, 112, 49, 57, 10, 28, 141, 53, 62, 67, 29, 102, 35, 124, 105, 101, 75, 46, 16, 130, 9, 15, 30, 25, 11, 134, 13, 12, 131, 48, 104, 109, 54, 123, 76, 92, 120, 73, 20, 142, 86, 64, 128, 37, 89, 34, 95, 93, 87, 127, 19, 103, 4, 61, 0, 31, 3, 47, 65, 58, 94, 6, 84, 88, 14, 114, 107, 7, 106, 132, 1, 72, 78, 43, 108, 126, 24, 2, 143, 133, 115, 125, 60, 59, 42, 82, 69, 41, 5, 119, 144, 71, 38, 70, 145, 81, 146, 40, 80, 147, 45, 39, 118, 150] 數值 torch.Size([146, 1])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6450],\n",
      "        [0.8882],\n",
      "        [0.6352],\n",
      "        [0.6092],\n",
      "        [0.8402],\n",
      "        [0.8502],\n",
      "        [0.6739],\n",
      "        [0.5671],\n",
      "        [0.6374],\n",
      "        [0.8931],\n",
      "        [0.5557],\n",
      "        [0.8969],\n",
      "        [0.8667],\n",
      "        [0.6771],\n",
      "        [0.6541],\n",
      "        [0.9024],\n",
      "        [0.6285],\n",
      "        [0.6362],\n",
      "        [0.6206],\n",
      "        [0.6865],\n",
      "        [0.6510],\n",
      "        [0.6074],\n",
      "        [0.8068],\n",
      "        [0.7505],\n",
      "        [0.8877],\n",
      "        [0.6228],\n",
      "        [0.8934],\n",
      "        [0.9636],\n",
      "        [0.5396],\n",
      "        [0.8175],\n",
      "        [0.7359],\n",
      "        [0.8184],\n",
      "        [0.6766],\n",
      "        [0.8991],\n",
      "        [0.8390],\n",
      "        [0.8195],\n",
      "        [0.8981],\n",
      "        [0.6715],\n",
      "        [0.8622],\n",
      "        [0.8265],\n",
      "        [0.8371],\n",
      "        [0.6044],\n",
      "        [0.6853],\n",
      "        [0.6179],\n",
      "        [0.8331],\n",
      "        [0.9003],\n",
      "        [1.0129],\n",
      "        [0.8284],\n",
      "        [0.6488],\n",
      "        [0.8885],\n",
      "        [0.8612],\n",
      "        [0.8162],\n",
      "        [0.8553],\n",
      "        [0.6153],\n",
      "        [0.8141],\n",
      "        [0.5583],\n",
      "        [0.6524],\n",
      "        [0.6201],\n",
      "        [0.7187],\n",
      "        [0.8714],\n",
      "        [0.9149],\n",
      "        [0.5515],\n",
      "        [1.0053],\n",
      "        [0.9374],\n",
      "        [0.8799],\n",
      "        [0.8371],\n",
      "        [1.0099],\n",
      "        [0.6113],\n",
      "        [0.9504],\n",
      "        [0.9640],\n",
      "        [0.5969],\n",
      "        [0.8807],\n",
      "        [0.6243],\n",
      "        [0.6373],\n",
      "        [0.9276],\n",
      "        [0.5750],\n",
      "        [0.7220],\n",
      "        [0.6683],\n",
      "        [0.5488],\n",
      "        [0.7842],\n",
      "        [0.8764],\n",
      "        [0.6468],\n",
      "        [0.6771],\n",
      "        [0.8327],\n",
      "        [0.5689],\n",
      "        [0.8215],\n",
      "        [0.6697],\n",
      "        [0.8569],\n",
      "        [0.6622],\n",
      "        [0.6695],\n",
      "        [0.6883],\n",
      "        [0.5713],\n",
      "        [0.8886],\n",
      "        [0.6109],\n",
      "        [0.8807],\n",
      "        [0.8711],\n",
      "        [0.8533],\n",
      "        [0.8541],\n",
      "        [0.8946],\n",
      "        [0.8801],\n",
      "        [0.7843],\n",
      "        [0.8832],\n",
      "        [0.6547],\n",
      "        [0.9216],\n",
      "        [0.6776],\n",
      "        [0.6781],\n",
      "        [0.9238],\n",
      "        [0.6467],\n",
      "        [0.6324],\n",
      "        [0.9411],\n",
      "        [0.6377],\n",
      "        [0.6168],\n",
      "        [0.8744],\n",
      "        [0.8081],\n",
      "        [0.7270],\n",
      "        [0.8870],\n",
      "        [0.6237],\n",
      "        [0.5453],\n",
      "        [0.8657],\n",
      "        [0.9182],\n",
      "        [0.6680],\n",
      "        [0.5904],\n",
      "        [0.6544],\n",
      "        [0.5337],\n",
      "        [0.8640],\n",
      "        [0.8661],\n",
      "        [0.8721],\n",
      "        [0.7287],\n",
      "        [0.8131],\n",
      "        [0.8552],\n",
      "        [0.8690],\n",
      "        [0.6102],\n",
      "        [0.6745],\n",
      "        [0.7799],\n",
      "        [0.8280],\n",
      "        [0.7799],\n",
      "        [0.6927],\n",
      "        [0.7523],\n",
      "        [0.6975],\n",
      "        [0.8259],\n",
      "        [0.7640],\n",
      "        [0.7009],\n",
      "        [0.8625],\n",
      "        [0.8374],\n",
      "        [0.6300],\n",
      "        [0.7148]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0011],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0065],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0101],\n",
      "        [0.0125],\n",
      "        [0.0125],\n",
      "        [0.0131],\n",
      "        [0.0131],\n",
      "        [0.0140],\n",
      "        [0.0141],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0168],\n",
      "        [0.0170],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0181],\n",
      "        [0.0185],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0201],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        [0.0268],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0276],\n",
      "        [0.0281],\n",
      "        [0.0289],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0317],\n",
      "        [0.0329],\n",
      "        [0.0333],\n",
      "        [0.0336],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0347],\n",
      "        [0.0350],\n",
      "        [0.0355],\n",
      "        [0.0366],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0387],\n",
      "        [0.0395],\n",
      "        [0.0396],\n",
      "        [0.0397],\n",
      "        [0.0416],\n",
      "        [0.0419],\n",
      "        [0.0420],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0474],\n",
      "        [0.0481],\n",
      "        [0.0482],\n",
      "        [0.0483],\n",
      "        [0.0491],\n",
      "        [0.0493],\n",
      "        [0.0498],\n",
      "        [0.0499],\n",
      "        [0.0504],\n",
      "        [0.0513],\n",
      "        [0.0522],\n",
      "        [0.0530],\n",
      "        [0.0531],\n",
      "        [0.0541],\n",
      "        [0.0561],\n",
      "        [0.0562],\n",
      "        [0.0578],\n",
      "        [0.0583],\n",
      "        [0.0583],\n",
      "        [0.0589],\n",
      "        [0.0599],\n",
      "        [0.0609],\n",
      "        [0.0620],\n",
      "        [0.0640],\n",
      "        [0.0652],\n",
      "        [0.0656],\n",
      "        [0.0718],\n",
      "        [0.0729],\n",
      "        [0.0761],\n",
      "        [0.0773],\n",
      "        [0.0813],\n",
      "        [0.0875],\n",
      "        [0.0901],\n",
      "        [0.0913],\n",
      "        [0.0933],\n",
      "        [0.0949],\n",
      "        [0.0952],\n",
      "        [0.1092],\n",
      "        [0.1129],\n",
      "        [0.1131],\n",
      "        [0.1148],\n",
      "        [0.1155],\n",
      "        [0.1214],\n",
      "        [0.1250],\n",
      "        [0.1306],\n",
      "        [0.1317],\n",
      "        [0.1379],\n",
      "        [0.1416],\n",
      "        [0.1426],\n",
      "        [0.1427],\n",
      "        [0.1529],\n",
      "        [0.1546],\n",
      "        [0.1626],\n",
      "        [0.1731],\n",
      "        [0.1781]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0020],\n",
      "        [    0.0007],\n",
      "        [    0.0014],\n",
      "        [    0.0005],\n",
      "        [    0.0015],\n",
      "        [    0.0005],\n",
      "        [    0.0028],\n",
      "        [    0.0033],\n",
      "        [    0.0077],\n",
      "        [    0.0042],\n",
      "        [    0.0001],\n",
      "        [    0.0082],\n",
      "        [    0.0025],\n",
      "        [    0.0037],\n",
      "        [    0.0036],\n",
      "        [    0.0075],\n",
      "        [    0.0104],\n",
      "        [    0.0088],\n",
      "        [    0.0045],\n",
      "        [    0.0051],\n",
      "        [    0.0108],\n",
      "        [    0.0062],\n",
      "        [    0.0058],\n",
      "        [    0.0065],\n",
      "        [    0.0091],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0126],\n",
      "        [    0.0106],\n",
      "        [    0.0173],\n",
      "        [    0.0180],\n",
      "        [    0.0112],\n",
      "        [    0.0193],\n",
      "        [    0.0113],\n",
      "        [    0.0122],\n",
      "        [    0.0119],\n",
      "        [    0.0173],\n",
      "        [    0.0213],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0184],\n",
      "        [    0.0146],\n",
      "        [    0.0181],\n",
      "        [    0.0159],\n",
      "        [    0.0178],\n",
      "        [    0.0228],\n",
      "        [    0.0182],\n",
      "        [    0.0176],\n",
      "        [    0.0249],\n",
      "        [    0.0197],\n",
      "        [    0.0257],\n",
      "        [    0.0281],\n",
      "        [    0.0231],\n",
      "        [    0.0231],\n",
      "        [    0.0255],\n",
      "        [    0.0254],\n",
      "        [    0.0242],\n",
      "        [    0.0233],\n",
      "        [    0.0314],\n",
      "        [    0.0252],\n",
      "        [    0.0286],\n",
      "        [    0.0332],\n",
      "        [    0.0265],\n",
      "        [    0.0324],\n",
      "        [    0.0348],\n",
      "        [    0.0334],\n",
      "        [    0.0327],\n",
      "        [    0.0303],\n",
      "        [    0.0306],\n",
      "        [    0.0334],\n",
      "        [    0.0375],\n",
      "        [    0.0328],\n",
      "        [    0.0333],\n",
      "        [    0.0379],\n",
      "        [    0.0351],\n",
      "        [    0.0336],\n",
      "        [    0.0345],\n",
      "        [    0.0371],\n",
      "        [    0.0430],\n",
      "        [    0.0356],\n",
      "        [    0.0353],\n",
      "        [    0.0369],\n",
      "        [    0.0382],\n",
      "        [    0.0423],\n",
      "        [    0.0387],\n",
      "        [    0.0387],\n",
      "        [    0.0459],\n",
      "        [    0.0393],\n",
      "        [    0.0442],\n",
      "        [    0.0447],\n",
      "        [    0.0472],\n",
      "        [    0.0444],\n",
      "        [    0.0456],\n",
      "        [    0.0435],\n",
      "        [    0.0473],\n",
      "        [    0.0557],\n",
      "        [    0.0520],\n",
      "        [    0.0556],\n",
      "        [    0.0539],\n",
      "        [    0.0477],\n",
      "        [    0.0504],\n",
      "        [    0.0504],\n",
      "        [    0.0492],\n",
      "        [    0.0566],\n",
      "        [    0.0535],\n",
      "        [    0.0538],\n",
      "        [    0.0578],\n",
      "        [    0.0564],\n",
      "        [    0.0548],\n",
      "        [    0.0570],\n",
      "        [    0.0603],\n",
      "        [    0.0674],\n",
      "        [    0.0662],\n",
      "        [    0.0682],\n",
      "        [    0.0596],\n",
      "        [    0.0636],\n",
      "        [    0.0712],\n",
      "        [    0.0774],\n",
      "        [    0.0817],\n",
      "        [    0.0724],\n",
      "        [    0.0816],\n",
      "        [    0.0869],\n",
      "        [    0.0885],\n",
      "        [    0.0888],\n",
      "        [    0.0909],\n",
      "        [    0.0896],\n",
      "        [    0.0971],\n",
      "        [    0.1133],\n",
      "        [    0.1079],\n",
      "        [    0.1087],\n",
      "        [    0.1143],\n",
      "        [    0.1098],\n",
      "        [    0.1254],\n",
      "        [    0.1210],\n",
      "        [    0.1347],\n",
      "        [    0.1258],\n",
      "        [    0.1402],\n",
      "        [    0.1347],\n",
      "        [    0.1378],\n",
      "        [    0.1456],\n",
      "        [    0.1453],\n",
      "        [    0.1587],\n",
      "        [    0.1582],\n",
      "        [    0.1724],\n",
      "        [    0.1703]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.15118145942688\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.879385536085465e-10, 111)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [111, 44, 98, 36, 136, 138, 21, 50, 97, 129, 110, 55, 85, 122, 96, 99, 74, 68, 51, 113, 17, 63, 137, 100, 56, 8, 140, 26, 139, 91, 22, 18, 66, 121, 90, 49, 77, 83, 141, 57, 27, 112, 28, 135, 52, 62, 32, 33, 23, 10, 35, 102, 75, 101, 53, 16, 105, 124, 67, 15, 29, 130, 13, 12, 46, 30, 134, 104, 9, 109, 131, 11, 76, 92, 25, 123, 142, 20, 86, 120, 48, 54, 64, 89, 37, 95, 128, 73, 4, 93, 19, 87, 103, 34, 127, 61, 65, 6, 58, 94, 31, 88, 14, 47, 7, 3, 0, 107, 84, 106, 114, 43, 132, 108, 72, 1, 78, 126, 143, 24, 133, 2, 115, 125, 60, 42, 59, 82, 41, 5, 144, 69, 119, 38, 71, 145, 70, 146, 40, 81, 147, 80, 39, 45, 148, 150, 151] 數值 torch.Size([147, 1])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6454],\n",
      "        [0.9019],\n",
      "        [0.6776],\n",
      "        [0.8431],\n",
      "        [0.6340],\n",
      "        [0.6084],\n",
      "        [0.8548],\n",
      "        [0.8912],\n",
      "        [0.6810],\n",
      "        [0.5666],\n",
      "        [0.6382],\n",
      "        [0.9060],\n",
      "        [0.6574],\n",
      "        [0.5556],\n",
      "        [0.6906],\n",
      "        [0.6546],\n",
      "        [0.7548],\n",
      "        [0.8105],\n",
      "        [0.8913],\n",
      "        [0.6285],\n",
      "        [0.8966],\n",
      "        [0.8699],\n",
      "        [0.6199],\n",
      "        [0.6262],\n",
      "        [0.8962],\n",
      "        [0.9669],\n",
      "        [0.6336],\n",
      "        [0.8209],\n",
      "        [0.6058],\n",
      "        [0.6802],\n",
      "        [0.8434],\n",
      "        [0.9022],\n",
      "        [0.8233],\n",
      "        [0.5391],\n",
      "        [0.6888],\n",
      "        [0.8366],\n",
      "        [0.7391],\n",
      "        [0.6727],\n",
      "        [0.6454],\n",
      "        [0.9021],\n",
      "        [0.8216],\n",
      "        [0.6183],\n",
      "        [0.8310],\n",
      "        [0.6032],\n",
      "        [0.9027],\n",
      "        [0.8636],\n",
      "        [0.8298],\n",
      "        [0.8406],\n",
      "        [0.8667],\n",
      "        [1.0155],\n",
      "        [0.8170],\n",
      "        [0.6181],\n",
      "        [0.7230],\n",
      "        [0.6232],\n",
      "        [0.8923],\n",
      "        [0.9186],\n",
      "        [0.6540],\n",
      "        [0.5597],\n",
      "        [0.8193],\n",
      "        [0.9408],\n",
      "        [0.8576],\n",
      "        [0.5521],\n",
      "        [0.9530],\n",
      "        [0.9667],\n",
      "        [0.8746],\n",
      "        [0.8820],\n",
      "        [0.6102],\n",
      "        [0.6262],\n",
      "        [1.0087],\n",
      "        [0.6387],\n",
      "        [0.5972],\n",
      "        [1.0125],\n",
      "        [0.7250],\n",
      "        [0.6712],\n",
      "        [0.8414],\n",
      "        [0.5754],\n",
      "        [0.6425],\n",
      "        [0.8803],\n",
      "        [0.6799],\n",
      "        [0.5485],\n",
      "        [0.8838],\n",
      "        [0.9305],\n",
      "        [0.8362],\n",
      "        [0.6731],\n",
      "        [0.8248],\n",
      "        [0.6657],\n",
      "        [0.5685],\n",
      "        [0.7885],\n",
      "        [0.8855],\n",
      "        [0.6719],\n",
      "        [0.8922],\n",
      "        [0.6908],\n",
      "        [0.6136],\n",
      "        [0.8604],\n",
      "        [0.5715],\n",
      "        [0.8730],\n",
      "        [0.7880],\n",
      "        [0.9255],\n",
      "        [0.8851],\n",
      "        [0.6574],\n",
      "        [0.8563],\n",
      "        [0.6808],\n",
      "        [0.9262],\n",
      "        [0.8835],\n",
      "        [0.9446],\n",
      "        [0.9003],\n",
      "        [0.8597],\n",
      "        [0.6343],\n",
      "        [0.6802],\n",
      "        [0.6396],\n",
      "        [0.6466],\n",
      "        [0.8925],\n",
      "        [0.6164],\n",
      "        [0.6257],\n",
      "        [0.8124],\n",
      "        [0.8809],\n",
      "        [0.7312],\n",
      "        [0.5458],\n",
      "        [0.6630],\n",
      "        [0.8703],\n",
      "        [0.5901],\n",
      "        [0.9238],\n",
      "        [0.6538],\n",
      "        [0.5353],\n",
      "        [0.8665],\n",
      "        [0.8774],\n",
      "        [0.8684],\n",
      "        [0.7306],\n",
      "        [0.8603],\n",
      "        [0.8734],\n",
      "        [0.6688],\n",
      "        [0.8171],\n",
      "        [0.6097],\n",
      "        [0.8320],\n",
      "        [0.7839],\n",
      "        [0.6869],\n",
      "        [0.7839],\n",
      "        [0.6906],\n",
      "        [0.8307],\n",
      "        [0.7545],\n",
      "        [0.6933],\n",
      "        [0.7669],\n",
      "        [0.8418],\n",
      "        [0.8666],\n",
      "        [0.6961],\n",
      "        [0.7069],\n",
      "        [0.7049]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0014],\n",
      "        [    0.0015],\n",
      "        [    0.0020],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0033],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0042],\n",
      "        [    0.0045],\n",
      "        [    0.0051],\n",
      "        [    0.0058],\n",
      "        [    0.0062],\n",
      "        [    0.0065],\n",
      "        [    0.0075],\n",
      "        [    0.0077],\n",
      "        [    0.0082],\n",
      "        [    0.0088],\n",
      "        [    0.0091],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0104],\n",
      "        [    0.0106],\n",
      "        [    0.0108],\n",
      "        [    0.0112],\n",
      "        [    0.0113],\n",
      "        [    0.0119],\n",
      "        [    0.0122],\n",
      "        [    0.0126],\n",
      "        [    0.0146],\n",
      "        [    0.0159],\n",
      "        [    0.0173],\n",
      "        [    0.0173],\n",
      "        [    0.0176],\n",
      "        [    0.0178],\n",
      "        [    0.0180],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0184],\n",
      "        [    0.0193],\n",
      "        [    0.0197],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0213],\n",
      "        [    0.0228],\n",
      "        [    0.0231],\n",
      "        [    0.0231],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0249],\n",
      "        [    0.0252],\n",
      "        [    0.0254],\n",
      "        [    0.0255],\n",
      "        [    0.0257],\n",
      "        [    0.0265],\n",
      "        [    0.0281],\n",
      "        [    0.0286],\n",
      "        [    0.0303],\n",
      "        [    0.0306],\n",
      "        [    0.0314],\n",
      "        [    0.0324],\n",
      "        [    0.0327],\n",
      "        [    0.0328],\n",
      "        [    0.0332],\n",
      "        [    0.0333],\n",
      "        [    0.0334],\n",
      "        [    0.0334],\n",
      "        [    0.0336],\n",
      "        [    0.0345],\n",
      "        [    0.0348],\n",
      "        [    0.0351],\n",
      "        [    0.0353],\n",
      "        [    0.0356],\n",
      "        [    0.0369],\n",
      "        [    0.0371],\n",
      "        [    0.0375],\n",
      "        [    0.0379],\n",
      "        [    0.0382],\n",
      "        [    0.0387],\n",
      "        [    0.0387],\n",
      "        [    0.0393],\n",
      "        [    0.0423],\n",
      "        [    0.0430],\n",
      "        [    0.0435],\n",
      "        [    0.0442],\n",
      "        [    0.0444],\n",
      "        [    0.0447],\n",
      "        [    0.0456],\n",
      "        [    0.0459],\n",
      "        [    0.0472],\n",
      "        [    0.0473],\n",
      "        [    0.0477],\n",
      "        [    0.0492],\n",
      "        [    0.0504],\n",
      "        [    0.0504],\n",
      "        [    0.0520],\n",
      "        [    0.0535],\n",
      "        [    0.0538],\n",
      "        [    0.0539],\n",
      "        [    0.0548],\n",
      "        [    0.0556],\n",
      "        [    0.0557],\n",
      "        [    0.0564],\n",
      "        [    0.0566],\n",
      "        [    0.0570],\n",
      "        [    0.0578],\n",
      "        [    0.0596],\n",
      "        [    0.0603],\n",
      "        [    0.0636],\n",
      "        [    0.0662],\n",
      "        [    0.0674],\n",
      "        [    0.0682],\n",
      "        [    0.0712],\n",
      "        [    0.0724],\n",
      "        [    0.0774],\n",
      "        [    0.0816],\n",
      "        [    0.0817],\n",
      "        [    0.0869],\n",
      "        [    0.0885],\n",
      "        [    0.0888],\n",
      "        [    0.0896],\n",
      "        [    0.0909],\n",
      "        [    0.0971],\n",
      "        [    0.1079],\n",
      "        [    0.1087],\n",
      "        [    0.1098],\n",
      "        [    0.1133],\n",
      "        [    0.1143],\n",
      "        [    0.1210],\n",
      "        [    0.1254],\n",
      "        [    0.1258],\n",
      "        [    0.1347],\n",
      "        [    0.1347],\n",
      "        [    0.1378],\n",
      "        [    0.1402],\n",
      "        [    0.1453],\n",
      "        [    0.1456],\n",
      "        [    0.1582],\n",
      "        [    0.1587],\n",
      "        [    0.1696],\n",
      "        [    0.1703],\n",
      "        [    0.1709]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0016],\n",
      "        [0.0038],\n",
      "        [0.0002],\n",
      "        [0.0041],\n",
      "        [0.0025],\n",
      "        [0.0031],\n",
      "        [0.0016],\n",
      "        [0.0010],\n",
      "        [0.0004],\n",
      "        [0.0036],\n",
      "        [0.0033],\n",
      "        [0.0012],\n",
      "        [0.0025],\n",
      "        [0.0010],\n",
      "        [0.0018],\n",
      "        [0.0029],\n",
      "        [0.0048],\n",
      "        [0.0064],\n",
      "        [0.0087],\n",
      "        [0.0073],\n",
      "        [0.0087],\n",
      "        [0.0128],\n",
      "        [0.0058],\n",
      "        [0.0105],\n",
      "        [0.0110],\n",
      "        [0.0167],\n",
      "        [0.0101],\n",
      "        [0.0157],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0115],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0123],\n",
      "        [0.0156],\n",
      "        [0.0191],\n",
      "        [0.0171],\n",
      "        [0.0102],\n",
      "        [0.0197],\n",
      "        [0.0181],\n",
      "        [0.0186],\n",
      "        [0.0193],\n",
      "        [0.0230],\n",
      "        [0.0194],\n",
      "        [0.0202],\n",
      "        [0.0201],\n",
      "        [0.0207],\n",
      "        [0.0225],\n",
      "        [0.0203],\n",
      "        [0.0236],\n",
      "        [0.0205],\n",
      "        [0.0199],\n",
      "        [0.0214],\n",
      "        [0.0255],\n",
      "        [0.0255],\n",
      "        [0.0247],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0274],\n",
      "        [0.0264],\n",
      "        [0.0301],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0311],\n",
      "        [0.0302],\n",
      "        [0.0371],\n",
      "        [0.0314],\n",
      "        [0.0317],\n",
      "        [0.0329],\n",
      "        [0.0357],\n",
      "        [0.0309],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0361],\n",
      "        [0.0366],\n",
      "        [0.0266],\n",
      "        [0.0354],\n",
      "        [0.0354],\n",
      "        [0.0358],\n",
      "        [0.0368],\n",
      "        [0.0370],\n",
      "        [0.0370],\n",
      "        [0.0364],\n",
      "        [0.0388],\n",
      "        [0.0365],\n",
      "        [0.0452],\n",
      "        [0.0455],\n",
      "        [0.0420],\n",
      "        [0.0429],\n",
      "        [0.0448],\n",
      "        [0.0436],\n",
      "        [0.0431],\n",
      "        [0.0458],\n",
      "        [0.0494],\n",
      "        [0.0487],\n",
      "        [0.0459],\n",
      "        [0.0495],\n",
      "        [0.0521],\n",
      "        [0.0486],\n",
      "        [0.0503],\n",
      "        [0.0521],\n",
      "        [0.0559],\n",
      "        [0.0537],\n",
      "        [0.0557],\n",
      "        [0.0579],\n",
      "        [0.0593],\n",
      "        [0.0554],\n",
      "        [0.0579],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0573],\n",
      "        [0.0637],\n",
      "        [0.0622],\n",
      "        [0.0686],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0727],\n",
      "        [0.0629],\n",
      "        [0.0787],\n",
      "        [0.0847],\n",
      "        [0.0837],\n",
      "        [0.0845],\n",
      "        [0.0885],\n",
      "        [0.0894],\n",
      "        [0.0875],\n",
      "        [0.0919],\n",
      "        [0.0974],\n",
      "        [0.1062],\n",
      "        [0.1078],\n",
      "        [0.0994],\n",
      "        [0.1151],\n",
      "        [0.1125],\n",
      "        [0.1203],\n",
      "        [0.1277],\n",
      "        [0.1152],\n",
      "        [0.1370],\n",
      "        [0.1227],\n",
      "        [0.1362],\n",
      "        [0.1409],\n",
      "        [0.1323],\n",
      "        [0.1469],\n",
      "        [0.1571],\n",
      "        [0.1594],\n",
      "        [0.1551],\n",
      "        [0.1569],\n",
      "        [0.1577]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.43170475959778\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.937202919246374e-08, 36)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [36, 129, 111, 96, 97, 85, 44, 50, 99, 122, 138, 74, 21, 55, 110, 98, 136, 68, 100, 51, 17, 91, 113, 63, 22, 26, 141, 56, 66, 121, 8, 18, 90, 137, 49, 139, 140, 83, 27, 112, 77, 28, 52, 57, 75, 32, 62, 10, 102, 33, 101, 23, 135, 35, 105, 53, 16, 124, 67, 29, 142, 15, 130, 30, 11, 46, 104, 9, 76, 13, 92, 12, 109, 86, 20, 131, 120, 25, 89, 95, 123, 48, 64, 54, 134, 37, 4, 93, 103, 87, 19, 128, 73, 34, 65, 94, 61, 127, 6, 31, 58, 88, 47, 107, 7, 14, 106, 114, 43, 3, 84, 0, 108, 143, 132, 72, 1, 78, 126, 24, 2, 115, 133, 42, 125, 60, 59, 82, 144, 41, 5, 119, 69, 145, 38, 146, 71, 147, 40, 70, 81, 80, 148, 150, 39, 151, 45, 149] 數值 torch.Size([148, 1])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8423],\n",
      "        [0.5634],\n",
      "        [0.6446],\n",
      "        [0.6941],\n",
      "        [0.6845],\n",
      "        [0.6598],\n",
      "        [0.9034],\n",
      "        [0.8908],\n",
      "        [0.6579],\n",
      "        [0.5539],\n",
      "        [0.6045],\n",
      "        [0.7577],\n",
      "        [0.8564],\n",
      "        [0.9063],\n",
      "        [0.6380],\n",
      "        [0.6810],\n",
      "        [0.6292],\n",
      "        [0.8119],\n",
      "        [0.6296],\n",
      "        [0.8915],\n",
      "        [0.8962],\n",
      "        [0.6829],\n",
      "        [0.6273],\n",
      "        [0.8703],\n",
      "        [0.8449],\n",
      "        [0.8214],\n",
      "        [0.6379],\n",
      "        [0.8954],\n",
      "        [0.8248],\n",
      "        [0.5375],\n",
      "        [0.9657],\n",
      "        [0.9026],\n",
      "        [0.6910],\n",
      "        [0.6158],\n",
      "        [0.8369],\n",
      "        [0.6008],\n",
      "        [0.6273],\n",
      "        [0.6724],\n",
      "        [0.8218],\n",
      "        [0.6178],\n",
      "        [0.7409],\n",
      "        [0.8299],\n",
      "        [0.9029],\n",
      "        [0.9003],\n",
      "        [0.7264],\n",
      "        [0.8295],\n",
      "        [0.8631],\n",
      "        [1.0130],\n",
      "        [0.6206],\n",
      "        [0.8407],\n",
      "        [0.6260],\n",
      "        [0.8679],\n",
      "        [0.5986],\n",
      "        [0.8165],\n",
      "        [0.6547],\n",
      "        [0.8929],\n",
      "        [0.9182],\n",
      "        [0.5592],\n",
      "        [0.8199],\n",
      "        [0.8560],\n",
      "        [0.6339],\n",
      "        [0.9400],\n",
      "        [0.5506],\n",
      "        [0.8798],\n",
      "        [1.0099],\n",
      "        [0.8743],\n",
      "        [0.6275],\n",
      "        [1.0072],\n",
      "        [0.7266],\n",
      "        [0.9509],\n",
      "        [0.6732],\n",
      "        [0.9646],\n",
      "        [0.6391],\n",
      "        [0.6814],\n",
      "        [0.8805],\n",
      "        [0.5949],\n",
      "        [0.5472],\n",
      "        [0.8426],\n",
      "        [0.6753],\n",
      "        [0.6685],\n",
      "        [0.5739],\n",
      "        [0.8831],\n",
      "        [0.8373],\n",
      "        [0.9296],\n",
      "        [0.6058],\n",
      "        [0.8247],\n",
      "        [0.8869],\n",
      "        [0.6732],\n",
      "        [0.6161],\n",
      "        [0.6919],\n",
      "        [0.8919],\n",
      "        [0.5655],\n",
      "        [0.7910],\n",
      "        [0.8603],\n",
      "        [0.7897],\n",
      "        [0.6592],\n",
      "        [0.8716],\n",
      "        [0.5693],\n",
      "        [0.9253],\n",
      "        [0.8545],\n",
      "        [0.8834],\n",
      "        [0.6821],\n",
      "        [0.8833],\n",
      "        [0.6353],\n",
      "        [0.9437],\n",
      "        [0.9241],\n",
      "        [0.6406],\n",
      "        [0.6453],\n",
      "        [0.8948],\n",
      "        [0.9026],\n",
      "        [0.6815],\n",
      "        [0.8633],\n",
      "        [0.6270],\n",
      "        [0.6535],\n",
      "        [0.6130],\n",
      "        [0.8147],\n",
      "        [0.8844],\n",
      "        [0.7343],\n",
      "        [0.5444],\n",
      "        [0.8716],\n",
      "        [0.9259],\n",
      "        [0.6514],\n",
      "        [0.5870],\n",
      "        [0.8794],\n",
      "        [0.5354],\n",
      "        [0.8658],\n",
      "        [0.8675],\n",
      "        [0.7309],\n",
      "        [0.6584],\n",
      "        [0.8620],\n",
      "        [0.8743],\n",
      "        [0.6078],\n",
      "        [0.8190],\n",
      "        [0.6763],\n",
      "        [0.8327],\n",
      "        [0.6786],\n",
      "        [0.7862],\n",
      "        [0.6802],\n",
      "        [0.8323],\n",
      "        [0.7862],\n",
      "        [0.7552],\n",
      "        [0.7683],\n",
      "        [0.6815],\n",
      "        [0.6935],\n",
      "        [0.8429],\n",
      "        [0.6917],\n",
      "        [0.8673],\n",
      "        [0.7040]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0018],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0048],\n",
      "        [0.0058],\n",
      "        [0.0064],\n",
      "        [0.0073],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0128],\n",
      "        [0.0156],\n",
      "        [0.0157],\n",
      "        [0.0167],\n",
      "        [0.0171],\n",
      "        [0.0181],\n",
      "        [0.0186],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0214],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0236],\n",
      "        [0.0247],\n",
      "        [0.0255],\n",
      "        [0.0255],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0264],\n",
      "        [0.0266],\n",
      "        [0.0274],\n",
      "        [0.0301],\n",
      "        [0.0302],\n",
      "        [0.0309],\n",
      "        [0.0311],\n",
      "        [0.0314],\n",
      "        [0.0317],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0329],\n",
      "        [0.0354],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0361],\n",
      "        [0.0364],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0368],\n",
      "        [0.0370],\n",
      "        [0.0370],\n",
      "        [0.0371],\n",
      "        [0.0388],\n",
      "        [0.0420],\n",
      "        [0.0429],\n",
      "        [0.0431],\n",
      "        [0.0436],\n",
      "        [0.0448],\n",
      "        [0.0452],\n",
      "        [0.0455],\n",
      "        [0.0458],\n",
      "        [0.0459],\n",
      "        [0.0486],\n",
      "        [0.0487],\n",
      "        [0.0494],\n",
      "        [0.0495],\n",
      "        [0.0503],\n",
      "        [0.0521],\n",
      "        [0.0521],\n",
      "        [0.0537],\n",
      "        [0.0554],\n",
      "        [0.0557],\n",
      "        [0.0559],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0573],\n",
      "        [0.0579],\n",
      "        [0.0579],\n",
      "        [0.0593],\n",
      "        [0.0622],\n",
      "        [0.0629],\n",
      "        [0.0637],\n",
      "        [0.0686],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0727],\n",
      "        [0.0787],\n",
      "        [0.0837],\n",
      "        [0.0845],\n",
      "        [0.0847],\n",
      "        [0.0875],\n",
      "        [0.0885],\n",
      "        [0.0894],\n",
      "        [0.0919],\n",
      "        [0.0974],\n",
      "        [0.0994],\n",
      "        [0.1062],\n",
      "        [0.1078],\n",
      "        [0.1125],\n",
      "        [0.1151],\n",
      "        [0.1152],\n",
      "        [0.1203],\n",
      "        [0.1227],\n",
      "        [0.1277],\n",
      "        [0.1323],\n",
      "        [0.1362],\n",
      "        [0.1370],\n",
      "        [0.1409],\n",
      "        [0.1469],\n",
      "        [0.1551],\n",
      "        [0.1569],\n",
      "        [0.1571],\n",
      "        [0.1577],\n",
      "        [0.1594],\n",
      "        [0.1634]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0060],\n",
      "        [0.0075],\n",
      "        [0.0062],\n",
      "        [0.0031],\n",
      "        [0.0010],\n",
      "        [0.0041],\n",
      "        [0.0023],\n",
      "        [0.0040],\n",
      "        [0.0038],\n",
      "        [0.0031],\n",
      "        [0.0102],\n",
      "        [0.0057],\n",
      "        [0.0008],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0017],\n",
      "        [0.0126],\n",
      "        [0.0091],\n",
      "        [0.0077],\n",
      "        [0.0113],\n",
      "        [0.0011],\n",
      "        [0.0113],\n",
      "        [0.0144],\n",
      "        [0.0036],\n",
      "        [0.0138],\n",
      "        [0.0148],\n",
      "        [0.0011],\n",
      "        [0.0166],\n",
      "        [0.0149],\n",
      "        [0.0053],\n",
      "        [0.0178],\n",
      "        [0.0167],\n",
      "        [0.0155],\n",
      "        [0.0206],\n",
      "        [0.0205],\n",
      "        [0.0244],\n",
      "        [0.0268],\n",
      "        [0.0113],\n",
      "        [0.0132],\n",
      "        [0.0238],\n",
      "        [0.0153],\n",
      "        [0.0254],\n",
      "        [0.0144],\n",
      "        [0.0269],\n",
      "        [0.0223],\n",
      "        [0.0147],\n",
      "        [0.0262],\n",
      "        [0.0120],\n",
      "        [0.0231],\n",
      "        [0.0157],\n",
      "        [0.0239],\n",
      "        [0.0183],\n",
      "        [0.0312],\n",
      "        [0.0292],\n",
      "        [0.0290],\n",
      "        [0.0207],\n",
      "        [0.0316],\n",
      "        [0.0301],\n",
      "        [0.0213],\n",
      "        [0.0198],\n",
      "        [0.0143],\n",
      "        [0.0339],\n",
      "        [0.0355],\n",
      "        [0.0230],\n",
      "        [0.0225],\n",
      "        [0.0253],\n",
      "        [0.0351],\n",
      "        [0.0246],\n",
      "        [0.0359],\n",
      "        [0.0403],\n",
      "        [0.0359],\n",
      "        [0.0404],\n",
      "        [0.0372],\n",
      "        [0.0392],\n",
      "        [0.0407],\n",
      "        [0.0419],\n",
      "        [0.0304],\n",
      "        [0.0320],\n",
      "        [0.0396],\n",
      "        [0.0392],\n",
      "        [0.0419],\n",
      "        [0.0307],\n",
      "        [0.0415],\n",
      "        [0.0308],\n",
      "        [0.0452],\n",
      "        [0.0440],\n",
      "        [0.0462],\n",
      "        [0.0471],\n",
      "        [0.0457],\n",
      "        [0.0480],\n",
      "        [0.0507],\n",
      "        [0.0520],\n",
      "        [0.0421],\n",
      "        [0.0406],\n",
      "        [0.0498],\n",
      "        [0.0522],\n",
      "        [0.0556],\n",
      "        [0.0555],\n",
      "        [0.0553],\n",
      "        [0.0435],\n",
      "        [0.0592],\n",
      "        [0.0562],\n",
      "        [0.0481],\n",
      "        [0.0592],\n",
      "        [0.0620],\n",
      "        [0.0637],\n",
      "        [0.0598],\n",
      "        [0.0506],\n",
      "        [0.0604],\n",
      "        [0.0545],\n",
      "        [0.0539],\n",
      "        [0.0570],\n",
      "        [0.0657],\n",
      "        [0.0497],\n",
      "        [0.0709],\n",
      "        [0.0649],\n",
      "        [0.0685],\n",
      "        [0.0686],\n",
      "        [0.0779],\n",
      "        [0.0746],\n",
      "        [0.0800],\n",
      "        [0.0777],\n",
      "        [0.0916],\n",
      "        [0.0907],\n",
      "        [0.0922],\n",
      "        [0.0954],\n",
      "        [0.0982],\n",
      "        [0.0921],\n",
      "        [0.0854],\n",
      "        [0.1096],\n",
      "        [0.1126],\n",
      "        [0.1064],\n",
      "        [0.1113],\n",
      "        [0.1009],\n",
      "        [0.1248],\n",
      "        [0.1071],\n",
      "        [0.1241],\n",
      "        [0.1156],\n",
      "        [0.1397],\n",
      "        [0.1335],\n",
      "        [0.1359],\n",
      "        [0.1425],\n",
      "        [0.1369],\n",
      "        [0.1399],\n",
      "        [0.1612],\n",
      "        [0.1411],\n",
      "        [0.1547],\n",
      "        [0.1452]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.71212720870972\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.689521114822128e-07, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [21, 97, 141, 17, 98, 44, 96, 122, 63, 99, 50, 85, 121, 74, 36, 111, 129, 100, 55, 110, 68, 138, 91, 51, 83, 10, 136, 27, 22, 142, 113, 52, 32, 26, 66, 77, 90, 33, 56, 18, 8, 23, 29, 49, 137, 53, 67, 75, 11, 30, 102, 112, 101, 139, 9, 46, 28, 62, 140, 57, 105, 35, 124, 120, 48, 54, 135, 16, 25, 15, 104, 130, 92, 76, 109, 95, 86, 89, 13, 12, 34, 20, 64, 131, 123, 73, 31, 37, 134, 103, 4, 93, 87, 47, 143, 65, 114, 19, 128, 94, 84, 3, 6, 127, 61, 88, 0, 58, 107, 106, 43, 7, 14, 72, 108, 1, 78, 132, 24, 115, 126, 2, 144, 42, 133, 82, 125, 60, 59, 145, 119, 146, 41, 69, 5, 147, 71, 38, 70, 81, 148, 40, 150, 151, 80, 149, 153, 152, 45] 數值 torch.Size([149, 1])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8525],\n",
      "        [0.6825],\n",
      "        [0.6267],\n",
      "        [0.8901],\n",
      "        [0.6789],\n",
      "        [0.8996],\n",
      "        [0.6920],\n",
      "        [0.5483],\n",
      "        [0.8652],\n",
      "        [0.6559],\n",
      "        [0.8852],\n",
      "        [0.6569],\n",
      "        [0.5318],\n",
      "        [0.7549],\n",
      "        [0.8365],\n",
      "        [0.6392],\n",
      "        [0.5563],\n",
      "        [0.6276],\n",
      "        [0.9013],\n",
      "        [0.6331],\n",
      "        [0.8076],\n",
      "        [0.5967],\n",
      "        [0.6802],\n",
      "        [0.8865],\n",
      "        [0.6666],\n",
      "        [1.0048],\n",
      "        [0.6208],\n",
      "        [0.8168],\n",
      "        [0.8410],\n",
      "        [0.6216],\n",
      "        [0.6216],\n",
      "        [0.8979],\n",
      "        [0.8242],\n",
      "        [0.8167],\n",
      "        [0.8206],\n",
      "        [0.7371],\n",
      "        [0.6878],\n",
      "        [0.8357],\n",
      "        [0.8893],\n",
      "        [0.8974],\n",
      "        [0.9589],\n",
      "        [0.8637],\n",
      "        [0.8493],\n",
      "        [0.8319],\n",
      "        [0.6080],\n",
      "        [0.8881],\n",
      "        [0.8149],\n",
      "        [0.7240],\n",
      "        [1.0016],\n",
      "        [0.8725],\n",
      "        [0.6180],\n",
      "        [0.6126],\n",
      "        [0.6236],\n",
      "        [0.5921],\n",
      "        [1.0001],\n",
      "        [0.8686],\n",
      "        [0.8238],\n",
      "        [0.8571],\n",
      "        [0.6171],\n",
      "        [0.8931],\n",
      "        [0.6504],\n",
      "        [0.8109],\n",
      "        [0.5550],\n",
      "        [0.5417],\n",
      "        [0.8769],\n",
      "        [0.9234],\n",
      "        [0.5903],\n",
      "        [0.9122],\n",
      "        [0.8385],\n",
      "        [0.9334],\n",
      "        [0.6239],\n",
      "        [0.5452],\n",
      "        [0.6698],\n",
      "        [0.7227],\n",
      "        [0.6348],\n",
      "        [0.6658],\n",
      "        [0.6775],\n",
      "        [0.6721],\n",
      "        [0.9431],\n",
      "        [0.9568],\n",
      "        [0.8551],\n",
      "        [0.8752],\n",
      "        [0.8329],\n",
      "        [0.5887],\n",
      "        [0.5686],\n",
      "        [0.7876],\n",
      "        [0.8478],\n",
      "        [0.8195],\n",
      "        [0.5978],\n",
      "        [0.6134],\n",
      "        [0.8827],\n",
      "        [0.6690],\n",
      "        [0.6875],\n",
      "        [0.8777],\n",
      "        [0.6404],\n",
      "        [0.7859],\n",
      "        [0.6394],\n",
      "        [0.8860],\n",
      "        [0.5587],\n",
      "        [0.6555],\n",
      "        [0.6775],\n",
      "        [0.8992],\n",
      "        [0.9195],\n",
      "        [0.5632],\n",
      "        [0.8647],\n",
      "        [0.6780],\n",
      "        [0.8610],\n",
      "        [0.8763],\n",
      "        [0.6315],\n",
      "        [0.6368],\n",
      "        [0.8917],\n",
      "        [0.9374],\n",
      "        [0.9163],\n",
      "        [0.8111],\n",
      "        [0.6236],\n",
      "        [0.8820],\n",
      "        [0.7316],\n",
      "        [0.6058],\n",
      "        [0.8675],\n",
      "        [0.6446],\n",
      "        [0.5392],\n",
      "        [0.9221],\n",
      "        [0.6444],\n",
      "        [0.8762],\n",
      "        [0.5801],\n",
      "        [0.7255],\n",
      "        [0.5316],\n",
      "        [0.8598],\n",
      "        [0.8612],\n",
      "        [0.6620],\n",
      "        [0.6017],\n",
      "        [0.6629],\n",
      "        [0.8586],\n",
      "        [0.8151],\n",
      "        [0.8695],\n",
      "        [0.6636],\n",
      "        [0.7826],\n",
      "        [0.8282],\n",
      "        [0.7827],\n",
      "        [0.7502],\n",
      "        [0.6634],\n",
      "        [0.8287],\n",
      "        [0.6765],\n",
      "        [0.6751],\n",
      "        [0.7639],\n",
      "        [0.6858],\n",
      "        [0.6735],\n",
      "        [0.6838],\n",
      "        [0.8626]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0011],\n",
      "        [0.0017],\n",
      "        [0.0023],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0091],\n",
      "        [0.0102],\n",
      "        [0.0113],\n",
      "        [0.0113],\n",
      "        [0.0113],\n",
      "        [0.0120],\n",
      "        [0.0126],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0144],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0178],\n",
      "        [0.0183],\n",
      "        [0.0198],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0244],\n",
      "        [0.0246],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0262],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0290],\n",
      "        [0.0292],\n",
      "        [0.0301],\n",
      "        [0.0304],\n",
      "        [0.0307],\n",
      "        [0.0308],\n",
      "        [0.0312],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0339],\n",
      "        [0.0351],\n",
      "        [0.0355],\n",
      "        [0.0359],\n",
      "        [0.0359],\n",
      "        [0.0372],\n",
      "        [0.0392],\n",
      "        [0.0392],\n",
      "        [0.0396],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0406],\n",
      "        [0.0407],\n",
      "        [0.0415],\n",
      "        [0.0419],\n",
      "        [0.0419],\n",
      "        [0.0421],\n",
      "        [0.0435],\n",
      "        [0.0440],\n",
      "        [0.0452],\n",
      "        [0.0457],\n",
      "        [0.0462],\n",
      "        [0.0471],\n",
      "        [0.0480],\n",
      "        [0.0481],\n",
      "        [0.0497],\n",
      "        [0.0498],\n",
      "        [0.0506],\n",
      "        [0.0507],\n",
      "        [0.0520],\n",
      "        [0.0522],\n",
      "        [0.0539],\n",
      "        [0.0545],\n",
      "        [0.0553],\n",
      "        [0.0555],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0570],\n",
      "        [0.0592],\n",
      "        [0.0592],\n",
      "        [0.0598],\n",
      "        [0.0604],\n",
      "        [0.0620],\n",
      "        [0.0637],\n",
      "        [0.0649],\n",
      "        [0.0657],\n",
      "        [0.0685],\n",
      "        [0.0686],\n",
      "        [0.0709],\n",
      "        [0.0746],\n",
      "        [0.0777],\n",
      "        [0.0779],\n",
      "        [0.0800],\n",
      "        [0.0854],\n",
      "        [0.0907],\n",
      "        [0.0916],\n",
      "        [0.0921],\n",
      "        [0.0922],\n",
      "        [0.0954],\n",
      "        [0.0982],\n",
      "        [0.1009],\n",
      "        [0.1064],\n",
      "        [0.1071],\n",
      "        [0.1096],\n",
      "        [0.1113],\n",
      "        [0.1126],\n",
      "        [0.1156],\n",
      "        [0.1241],\n",
      "        [0.1248],\n",
      "        [0.1335],\n",
      "        [0.1359],\n",
      "        [0.1369],\n",
      "        [0.1397],\n",
      "        [0.1399],\n",
      "        [0.1411],\n",
      "        [0.1425],\n",
      "        [0.1452],\n",
      "        [0.1505],\n",
      "        [0.1508],\n",
      "        [0.1547]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0008],\n",
      "        [0.0085],\n",
      "        [0.0012],\n",
      "        [0.0036],\n",
      "        [0.0025],\n",
      "        [0.0012],\n",
      "        [0.0053],\n",
      "        [0.0034],\n",
      "        [0.0018],\n",
      "        [0.0054],\n",
      "        [0.0031],\n",
      "        [0.0028],\n",
      "        [0.0040],\n",
      "        [0.0092],\n",
      "        [0.0081],\n",
      "        [0.0108],\n",
      "        [0.0056],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0080],\n",
      "        [0.0143],\n",
      "        [0.0102],\n",
      "        [0.0125],\n",
      "        [0.0101],\n",
      "        [0.0089],\n",
      "        [0.0178],\n",
      "        [0.0125],\n",
      "        [0.0124],\n",
      "        [0.0058],\n",
      "        [0.0166],\n",
      "        [0.0131],\n",
      "        [0.0127],\n",
      "        [0.0147],\n",
      "        [0.0132],\n",
      "        [0.0155],\n",
      "        [0.0148],\n",
      "        [0.0138],\n",
      "        [0.0182],\n",
      "        [0.0162],\n",
      "        [0.0200],\n",
      "        [0.0191],\n",
      "        [0.0166],\n",
      "        [0.0209],\n",
      "        [0.0249],\n",
      "        [0.0196],\n",
      "        [0.0221],\n",
      "        [0.0200],\n",
      "        [0.0195],\n",
      "        [0.0188],\n",
      "        [0.0217],\n",
      "        [0.0253],\n",
      "        [0.0223],\n",
      "        [0.0291],\n",
      "        [0.0222],\n",
      "        [0.0244],\n",
      "        [0.0278],\n",
      "        [0.0274],\n",
      "        [0.0330],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0316],\n",
      "        [0.0312],\n",
      "        [0.0282],\n",
      "        [0.0293],\n",
      "        [0.0283],\n",
      "        [0.0360],\n",
      "        [0.0316],\n",
      "        [0.0329],\n",
      "        [0.0348],\n",
      "        [0.0346],\n",
      "        [0.0372],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0383],\n",
      "        [0.0377],\n",
      "        [0.0398],\n",
      "        [0.0390],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0383],\n",
      "        [0.0404],\n",
      "        [0.0407],\n",
      "        [0.0442],\n",
      "        [0.0440],\n",
      "        [0.0433],\n",
      "        [0.0398],\n",
      "        [0.0464],\n",
      "        [0.0497],\n",
      "        [0.0440],\n",
      "        [0.0450],\n",
      "        [0.0470],\n",
      "        [0.0488],\n",
      "        [0.0472],\n",
      "        [0.0401],\n",
      "        [0.0479],\n",
      "        [0.0479],\n",
      "        [0.0508],\n",
      "        [0.0552],\n",
      "        [0.0515],\n",
      "        [0.0540],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0581],\n",
      "        [0.0577],\n",
      "        [0.0567],\n",
      "        [0.0612],\n",
      "        [0.0614],\n",
      "        [0.0597],\n",
      "        [0.0602],\n",
      "        [0.0602],\n",
      "        [0.0637],\n",
      "        [0.0653],\n",
      "        [0.0660],\n",
      "        [0.0658],\n",
      "        [0.0723],\n",
      "        [0.0701],\n",
      "        [0.0744],\n",
      "        [0.0754],\n",
      "        [0.0743],\n",
      "        [0.0799],\n",
      "        [0.0819],\n",
      "        [0.0747],\n",
      "        [0.0909],\n",
      "        [0.0948],\n",
      "        [0.0907],\n",
      "        [0.0928],\n",
      "        [0.0967],\n",
      "        [0.0996],\n",
      "        [0.0900],\n",
      "        [0.1033],\n",
      "        [0.0948],\n",
      "        [0.1103],\n",
      "        [0.1124],\n",
      "        [0.1117],\n",
      "        [0.1023],\n",
      "        [0.1255],\n",
      "        [0.1268],\n",
      "        [0.1347],\n",
      "        [0.1349],\n",
      "        [0.1218],\n",
      "        [0.1403],\n",
      "        [0.1254],\n",
      "        [0.1265],\n",
      "        [0.1421],\n",
      "        [0.1298],\n",
      "        [0.1351],\n",
      "        [0.1355],\n",
      "        [0.1545]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.99259924888611\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.747170573362382e-07, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [21, 97, 17, 96, 99, 44, 121, 85, 63, 98, 74, 122, 50, 100, 142, 68, 111, 141, 10, 36, 55, 110, 83, 91, 129, 22, 51, 27, 32, 52, 66, 33, 138, 26, 90, 77, 18, 29, 113, 136, 56, 30, 23, 11, 53, 75, 8, 49, 102, 67, 9, 101, 46, 137, 112, 62, 28, 120, 54, 139, 48, 57, 105, 124, 16, 35, 25, 140, 104, 15, 92, 76, 135, 130, 95, 34, 109, 89, 31, 86, 143, 20, 64, 13, 12, 73, 123, 103, 131, 4, 37, 93, 47, 65, 114, 87, 134, 19, 94, 84, 128, 3, 6, 88, 61, 127, 107, 106, 43, 0, 58, 7, 14, 108, 72, 78, 1, 115, 132, 144, 24, 126, 2, 145, 82, 42, 125, 133, 146, 60, 59, 147, 119, 41, 5, 69, 148, 150, 71, 151, 38, 149, 70, 81, 153, 152, 40, 80, 45, 118] 數值 torch.Size([150, 1])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8542],\n",
      "        [0.6844],\n",
      "        [0.8902],\n",
      "        [0.6938],\n",
      "        [0.6579],\n",
      "        [0.8993],\n",
      "        [0.5293],\n",
      "        [0.6579],\n",
      "        [0.8651],\n",
      "        [0.6808],\n",
      "        [0.7566],\n",
      "        [0.5461],\n",
      "        [0.8838],\n",
      "        [0.6297],\n",
      "        [0.6131],\n",
      "        [0.8087],\n",
      "        [0.6373],\n",
      "        [0.6192],\n",
      "        [1.0016],\n",
      "        [0.8333],\n",
      "        [0.9003],\n",
      "        [0.6318],\n",
      "        [0.6654],\n",
      "        [0.6812],\n",
      "        [0.5530],\n",
      "        [0.8423],\n",
      "        [0.8854],\n",
      "        [0.8161],\n",
      "        [0.8221],\n",
      "        [0.8966],\n",
      "        [0.8223],\n",
      "        [0.8338],\n",
      "        [0.5927],\n",
      "        [0.8169],\n",
      "        [0.6885],\n",
      "        [0.7373],\n",
      "        [0.8979],\n",
      "        [0.8461],\n",
      "        [0.6194],\n",
      "        [0.6156],\n",
      "        [0.8877],\n",
      "        [0.8684],\n",
      "        [0.8646],\n",
      "        [0.9986],\n",
      "        [0.8870],\n",
      "        [0.7263],\n",
      "        [0.9567],\n",
      "        [0.8315],\n",
      "        [0.6195],\n",
      "        [0.8157],\n",
      "        [0.9977],\n",
      "        [0.6251],\n",
      "        [0.8677],\n",
      "        [0.6038],\n",
      "        [0.6111],\n",
      "        [0.8559],\n",
      "        [0.8214],\n",
      "        [0.5396],\n",
      "        [0.9208],\n",
      "        [0.5874],\n",
      "        [0.8756],\n",
      "        [0.8905],\n",
      "        [0.6498],\n",
      "        [0.5539],\n",
      "        [0.9122],\n",
      "        [0.8084],\n",
      "        [0.8395],\n",
      "        [0.6109],\n",
      "        [0.6244],\n",
      "        [0.9325],\n",
      "        [0.6702],\n",
      "        [0.7229],\n",
      "        [0.5856],\n",
      "        [0.5435],\n",
      "        [0.6673],\n",
      "        [0.8527],\n",
      "        [0.6337],\n",
      "        [0.6727],\n",
      "        [0.8440],\n",
      "        [0.6770],\n",
      "        [0.6308],\n",
      "        [0.8755],\n",
      "        [0.8336],\n",
      "        [0.9413],\n",
      "        [0.9552],\n",
      "        [0.7888],\n",
      "        [0.5665],\n",
      "        [0.6151],\n",
      "        [0.5863],\n",
      "        [0.8840],\n",
      "        [0.8172],\n",
      "        [0.6691],\n",
      "        [0.8769],\n",
      "        [0.7878],\n",
      "        [0.6368],\n",
      "        [0.6867],\n",
      "        [0.5933],\n",
      "        [0.8858],\n",
      "        [0.6562],\n",
      "        [0.6775],\n",
      "        [0.5556],\n",
      "        [0.9010],\n",
      "        [0.9184],\n",
      "        [0.6775],\n",
      "        [0.8626],\n",
      "        [0.5606],\n",
      "        [0.6311],\n",
      "        [0.6364],\n",
      "        [0.8919],\n",
      "        [0.8651],\n",
      "        [0.8740],\n",
      "        [0.9357],\n",
      "        [0.9147],\n",
      "        [0.6235],\n",
      "        [0.8122],\n",
      "        [0.7332],\n",
      "        [0.8859],\n",
      "        [0.6412],\n",
      "        [0.6024],\n",
      "        [0.6337],\n",
      "        [0.8683],\n",
      "        [0.5372],\n",
      "        [0.9240],\n",
      "        [0.6510],\n",
      "        [0.7242],\n",
      "        [0.8761],\n",
      "        [0.5310],\n",
      "        [0.5770],\n",
      "        [0.6506],\n",
      "        [0.8586],\n",
      "        [0.8597],\n",
      "        [0.6502],\n",
      "        [0.5986],\n",
      "        [0.8579],\n",
      "        [0.8705],\n",
      "        [0.8162],\n",
      "        [0.6483],\n",
      "        [0.6620],\n",
      "        [0.7840],\n",
      "        [0.6605],\n",
      "        [0.8262],\n",
      "        [0.6704],\n",
      "        [0.7839],\n",
      "        [0.7492],\n",
      "        [0.6581],\n",
      "        [0.6684],\n",
      "        [0.8281],\n",
      "        [0.7635],\n",
      "        [0.8624],\n",
      "        [0.6170]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0018],\n",
      "        [0.0025],\n",
      "        [0.0028],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0040],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0124],\n",
      "        [0.0125],\n",
      "        [0.0125],\n",
      "        [0.0127],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0155],\n",
      "        [0.0162],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0178],\n",
      "        [0.0182],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0196],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0209],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0223],\n",
      "        [0.0244],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0274],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0283],\n",
      "        [0.0291],\n",
      "        [0.0293],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0312],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0329],\n",
      "        [0.0330],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0360],\n",
      "        [0.0372],\n",
      "        [0.0377],\n",
      "        [0.0383],\n",
      "        [0.0383],\n",
      "        [0.0390],\n",
      "        [0.0398],\n",
      "        [0.0398],\n",
      "        [0.0401],\n",
      "        [0.0404],\n",
      "        [0.0407],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0433],\n",
      "        [0.0440],\n",
      "        [0.0440],\n",
      "        [0.0442],\n",
      "        [0.0450],\n",
      "        [0.0464],\n",
      "        [0.0470],\n",
      "        [0.0472],\n",
      "        [0.0479],\n",
      "        [0.0479],\n",
      "        [0.0488],\n",
      "        [0.0497],\n",
      "        [0.0508],\n",
      "        [0.0515],\n",
      "        [0.0540],\n",
      "        [0.0552],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0567],\n",
      "        [0.0577],\n",
      "        [0.0581],\n",
      "        [0.0597],\n",
      "        [0.0602],\n",
      "        [0.0602],\n",
      "        [0.0612],\n",
      "        [0.0614],\n",
      "        [0.0637],\n",
      "        [0.0653],\n",
      "        [0.0658],\n",
      "        [0.0660],\n",
      "        [0.0701],\n",
      "        [0.0723],\n",
      "        [0.0743],\n",
      "        [0.0744],\n",
      "        [0.0747],\n",
      "        [0.0754],\n",
      "        [0.0799],\n",
      "        [0.0819],\n",
      "        [0.0900],\n",
      "        [0.0907],\n",
      "        [0.0909],\n",
      "        [0.0928],\n",
      "        [0.0948],\n",
      "        [0.0948],\n",
      "        [0.0967],\n",
      "        [0.0996],\n",
      "        [0.1023],\n",
      "        [0.1033],\n",
      "        [0.1103],\n",
      "        [0.1117],\n",
      "        [0.1124],\n",
      "        [0.1218],\n",
      "        [0.1254],\n",
      "        [0.1255],\n",
      "        [0.1265],\n",
      "        [0.1268],\n",
      "        [0.1298],\n",
      "        [0.1347],\n",
      "        [0.1349],\n",
      "        [0.1351],\n",
      "        [0.1355],\n",
      "        [0.1403],\n",
      "        [0.1421],\n",
      "        [0.1545],\n",
      "        [0.1601]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0023],\n",
      "        [0.0002],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0023],\n",
      "        [0.0028],\n",
      "        [0.0006],\n",
      "        [0.0042],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0045],\n",
      "        [0.0079],\n",
      "        [0.0067],\n",
      "        [0.0061],\n",
      "        [0.0008],\n",
      "        [0.0078],\n",
      "        [0.0111],\n",
      "        [0.0143],\n",
      "        [0.0068],\n",
      "        [0.0123],\n",
      "        [0.0105],\n",
      "        [0.0124],\n",
      "        [0.0070],\n",
      "        [0.0113],\n",
      "        [0.0135],\n",
      "        [0.0113],\n",
      "        [0.0135],\n",
      "        [0.0118],\n",
      "        [0.0108],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0120],\n",
      "        [0.0173],\n",
      "        [0.0147],\n",
      "        [0.0161],\n",
      "        [0.0138],\n",
      "        [0.0154],\n",
      "        [0.0139],\n",
      "        [0.0200],\n",
      "        [0.0216],\n",
      "        [0.0199],\n",
      "        [0.0152],\n",
      "        [0.0200],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0200],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0225],\n",
      "        [0.0222],\n",
      "        [0.0207],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0291],\n",
      "        [0.0298],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0327],\n",
      "        [0.0283],\n",
      "        [0.0319],\n",
      "        [0.0320],\n",
      "        [0.0326],\n",
      "        [0.0312],\n",
      "        [0.0342],\n",
      "        [0.0337],\n",
      "        [0.0378],\n",
      "        [0.0361],\n",
      "        [0.0351],\n",
      "        [0.0372],\n",
      "        [0.0373],\n",
      "        [0.0395],\n",
      "        [0.0386],\n",
      "        [0.0386],\n",
      "        [0.0360],\n",
      "        [0.0408],\n",
      "        [0.0404],\n",
      "        [0.0365],\n",
      "        [0.0422],\n",
      "        [0.0325],\n",
      "        [0.0398],\n",
      "        [0.0409],\n",
      "        [0.0430],\n",
      "        [0.0429],\n",
      "        [0.0427],\n",
      "        [0.0462],\n",
      "        [0.0446],\n",
      "        [0.0460],\n",
      "        [0.0440],\n",
      "        [0.0488],\n",
      "        [0.0489],\n",
      "        [0.0465],\n",
      "        [0.0471],\n",
      "        [0.0443],\n",
      "        [0.0514],\n",
      "        [0.0529],\n",
      "        [0.0505],\n",
      "        [0.0530],\n",
      "        [0.0521],\n",
      "        [0.0578],\n",
      "        [0.0580],\n",
      "        [0.0571],\n",
      "        [0.0591],\n",
      "        [0.0600],\n",
      "        [0.0604],\n",
      "        [0.0618],\n",
      "        [0.0623],\n",
      "        [0.0602],\n",
      "        [0.0647],\n",
      "        [0.0635],\n",
      "        [0.0647],\n",
      "        [0.0661],\n",
      "        [0.0676],\n",
      "        [0.0655],\n",
      "        [0.0694],\n",
      "        [0.0757],\n",
      "        [0.0701],\n",
      "        [0.0768],\n",
      "        [0.0662],\n",
      "        [0.0762],\n",
      "        [0.0819],\n",
      "        [0.0838],\n",
      "        [0.0812],\n",
      "        [0.0874],\n",
      "        [0.0911],\n",
      "        [0.0938],\n",
      "        [0.0970],\n",
      "        [0.0849],\n",
      "        [0.0982],\n",
      "        [0.1012],\n",
      "        [0.0915],\n",
      "        [0.0993],\n",
      "        [0.1112],\n",
      "        [0.1108],\n",
      "        [0.1123],\n",
      "        [0.1096],\n",
      "        [0.1135],\n",
      "        [0.1252],\n",
      "        [0.1146],\n",
      "        [0.1289],\n",
      "        [0.1172],\n",
      "        [0.1343],\n",
      "        [0.1319],\n",
      "        [0.1223],\n",
      "        [0.1227],\n",
      "        [0.1412],\n",
      "        [0.1398],\n",
      "        [0.1542],\n",
      "        [0.1556]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.27343773841858\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.376978779419005e-08, 97)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [97, 121, 142, 17, 96, 99, 21, 63, 44, 98, 85, 74, 100, 50, 10, 83, 68, 122, 55, 32, 111, 22, 91, 27, 52, 33, 66, 36, 110, 129, 51, 77, 29, 141, 26, 30, 18, 90, 138, 11, 53, 56, 113, 75, 23, 9, 8, 49, 136, 67, 102, 101, 46, 120, 54, 137, 112, 48, 62, 28, 16, 57, 105, 143, 124, 139, 25, 35, 15, 34, 104, 31, 92, 76, 140, 95, 130, 135, 20, 89, 109, 64, 86, 73, 12, 13, 4, 114, 103, 131, 123, 47, 65, 37, 93, 19, 87, 84, 134, 94, 6, 128, 3, 88, 61, 43, 127, 107, 106, 58, 0, 7, 72, 14, 144, 108, 78, 115, 1, 24, 132, 145, 126, 2, 146, 82, 42, 147, 125, 133, 60, 119, 59, 148, 5, 41, 69, 150, 151, 149, 153, 152, 71, 38, 81, 70, 80, 40, 158, 45, 118] 數值 torch.Size([151, 1])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6838],\n",
      "        [0.5259],\n",
      "        [0.6064],\n",
      "        [0.8907],\n",
      "        [0.6931],\n",
      "        [0.6575],\n",
      "        [0.8557],\n",
      "        [0.8642],\n",
      "        [0.8990],\n",
      "        [0.6802],\n",
      "        [0.6568],\n",
      "        [0.7561],\n",
      "        [0.6292],\n",
      "        [0.8825],\n",
      "        [0.9996],\n",
      "        [0.6623],\n",
      "        [0.8089],\n",
      "        [0.5435],\n",
      "        [0.8991],\n",
      "        [0.8202],\n",
      "        [0.6343],\n",
      "        [0.8435],\n",
      "        [0.6801],\n",
      "        [0.8154],\n",
      "        [0.8953],\n",
      "        [0.8320],\n",
      "        [0.8232],\n",
      "        [0.8302],\n",
      "        [0.6292],\n",
      "        [0.5503],\n",
      "        [0.8843],\n",
      "        [0.7356],\n",
      "        [0.8435],\n",
      "        [0.6135],\n",
      "        [0.8168],\n",
      "        [0.8648],\n",
      "        [0.8987],\n",
      "        [0.6873],\n",
      "        [0.5897],\n",
      "        [0.9966],\n",
      "        [0.8857],\n",
      "        [0.8860],\n",
      "        [0.6160],\n",
      "        [0.7263],\n",
      "        [0.8654],\n",
      "        [0.9963],\n",
      "        [0.9553],\n",
      "        [0.8309],\n",
      "        [0.6117],\n",
      "        [0.8158],\n",
      "        [0.6187],\n",
      "        [0.6242],\n",
      "        [0.8669],\n",
      "        [0.5364],\n",
      "        [0.9184],\n",
      "        [0.6007],\n",
      "        [0.6083],\n",
      "        [0.8745],\n",
      "        [0.8542],\n",
      "        [0.8194],\n",
      "        [0.9126],\n",
      "        [0.8881],\n",
      "        [0.6474],\n",
      "        [0.6231],\n",
      "        [0.5525],\n",
      "        [0.5839],\n",
      "        [0.8403],\n",
      "        [0.8059],\n",
      "        [0.9322],\n",
      "        [0.8505],\n",
      "        [0.6229],\n",
      "        [0.8408],\n",
      "        [0.6685],\n",
      "        [0.7213],\n",
      "        [0.6062],\n",
      "        [0.6664],\n",
      "        [0.5421],\n",
      "        [0.5821],\n",
      "        [0.8761],\n",
      "        [0.6713],\n",
      "        [0.6313],\n",
      "        [0.8334],\n",
      "        [0.6746],\n",
      "        [0.7882],\n",
      "        [0.9544],\n",
      "        [0.9404],\n",
      "        [0.8850],\n",
      "        [0.6331],\n",
      "        [0.6146],\n",
      "        [0.5846],\n",
      "        [0.5643],\n",
      "        [0.8761],\n",
      "        [0.7885],\n",
      "        [0.8148],\n",
      "        [0.6672],\n",
      "        [0.8861],\n",
      "        [0.6841],\n",
      "        [0.6757],\n",
      "        [0.5900],\n",
      "        [0.6548],\n",
      "        [0.9177],\n",
      "        [0.5530],\n",
      "        [0.9027],\n",
      "        [0.6751],\n",
      "        [0.8603],\n",
      "        [0.8919],\n",
      "        [0.5584],\n",
      "        [0.6289],\n",
      "        [0.6342],\n",
      "        [0.8719],\n",
      "        [0.8686],\n",
      "        [0.9347],\n",
      "        [0.8116],\n",
      "        [0.9139],\n",
      "        [0.6252],\n",
      "        [0.6216],\n",
      "        [0.7325],\n",
      "        [0.6370],\n",
      "        [0.8892],\n",
      "        [0.8691],\n",
      "        [0.5999],\n",
      "        [0.6423],\n",
      "        [0.5352],\n",
      "        [0.9259],\n",
      "        [0.6407],\n",
      "        [0.7209],\n",
      "        [0.8758],\n",
      "        [0.6395],\n",
      "        [0.5300],\n",
      "        [0.5747],\n",
      "        [0.8571],\n",
      "        [0.5947],\n",
      "        [0.8581],\n",
      "        [0.6360],\n",
      "        [0.8713],\n",
      "        [0.8569],\n",
      "        [0.8162],\n",
      "        [0.6501],\n",
      "        [0.6486],\n",
      "        [0.6578],\n",
      "        [0.6454],\n",
      "        [0.6557],\n",
      "        [0.7836],\n",
      "        [0.8241],\n",
      "        [0.7463],\n",
      "        [0.7836],\n",
      "        [0.7611],\n",
      "        [0.8273],\n",
      "        [0.6176],\n",
      "        [0.8621],\n",
      "        [0.6125]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0023],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0061],\n",
      "        [0.0067],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0113],\n",
      "        [0.0113],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0135],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0161],\n",
      "        [0.0173],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0207],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0283],\n",
      "        [0.0291],\n",
      "        [0.0298],\n",
      "        [0.0312],\n",
      "        [0.0319],\n",
      "        [0.0320],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0327],\n",
      "        [0.0337],\n",
      "        [0.0342],\n",
      "        [0.0351],\n",
      "        [0.0360],\n",
      "        [0.0361],\n",
      "        [0.0365],\n",
      "        [0.0372],\n",
      "        [0.0373],\n",
      "        [0.0378],\n",
      "        [0.0386],\n",
      "        [0.0386],\n",
      "        [0.0395],\n",
      "        [0.0398],\n",
      "        [0.0404],\n",
      "        [0.0408],\n",
      "        [0.0409],\n",
      "        [0.0422],\n",
      "        [0.0427],\n",
      "        [0.0429],\n",
      "        [0.0430],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0446],\n",
      "        [0.0460],\n",
      "        [0.0462],\n",
      "        [0.0465],\n",
      "        [0.0471],\n",
      "        [0.0488],\n",
      "        [0.0489],\n",
      "        [0.0505],\n",
      "        [0.0514],\n",
      "        [0.0521],\n",
      "        [0.0529],\n",
      "        [0.0530],\n",
      "        [0.0571],\n",
      "        [0.0578],\n",
      "        [0.0580],\n",
      "        [0.0591],\n",
      "        [0.0600],\n",
      "        [0.0602],\n",
      "        [0.0604],\n",
      "        [0.0618],\n",
      "        [0.0623],\n",
      "        [0.0635],\n",
      "        [0.0647],\n",
      "        [0.0647],\n",
      "        [0.0655],\n",
      "        [0.0661],\n",
      "        [0.0662],\n",
      "        [0.0676],\n",
      "        [0.0694],\n",
      "        [0.0701],\n",
      "        [0.0757],\n",
      "        [0.0762],\n",
      "        [0.0768],\n",
      "        [0.0812],\n",
      "        [0.0819],\n",
      "        [0.0838],\n",
      "        [0.0849],\n",
      "        [0.0874],\n",
      "        [0.0911],\n",
      "        [0.0915],\n",
      "        [0.0938],\n",
      "        [0.0970],\n",
      "        [0.0982],\n",
      "        [0.0993],\n",
      "        [0.1012],\n",
      "        [0.1096],\n",
      "        [0.1108],\n",
      "        [0.1112],\n",
      "        [0.1123],\n",
      "        [0.1135],\n",
      "        [0.1146],\n",
      "        [0.1172],\n",
      "        [0.1223],\n",
      "        [0.1227],\n",
      "        [0.1252],\n",
      "        [0.1289],\n",
      "        [0.1319],\n",
      "        [0.1343],\n",
      "        [0.1398],\n",
      "        [0.1412],\n",
      "        [0.1531],\n",
      "        [0.1542],\n",
      "        [0.1556]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0020],\n",
      "        [0.0051],\n",
      "        [0.0042],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0059],\n",
      "        [0.0040],\n",
      "        [0.0010],\n",
      "        [0.0045],\n",
      "        [0.0033],\n",
      "        [0.0029],\n",
      "        [0.0047],\n",
      "        [0.0058],\n",
      "        [0.0073],\n",
      "        [0.0059],\n",
      "        [0.0055],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0109],\n",
      "        [0.0119],\n",
      "        [0.0080],\n",
      "        [0.0102],\n",
      "        [0.0131],\n",
      "        [0.0128],\n",
      "        [0.0123],\n",
      "        [0.0094],\n",
      "        [0.0133],\n",
      "        [0.0128],\n",
      "        [0.0139],\n",
      "        [0.0124],\n",
      "        [0.0143],\n",
      "        [0.0134],\n",
      "        [0.0177],\n",
      "        [0.0128],\n",
      "        [0.0139],\n",
      "        [0.0126],\n",
      "        [0.0151],\n",
      "        [0.0181],\n",
      "        [0.0180],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0211],\n",
      "        [0.0180],\n",
      "        [0.0230],\n",
      "        [0.0219],\n",
      "        [0.0203],\n",
      "        [0.0201],\n",
      "        [0.0231],\n",
      "        [0.0244],\n",
      "        [0.0215],\n",
      "        [0.0223],\n",
      "        [0.0249],\n",
      "        [0.0239],\n",
      "        [0.0257],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0293],\n",
      "        [0.0285],\n",
      "        [0.0298],\n",
      "        [0.0286],\n",
      "        [0.0321],\n",
      "        [0.0324],\n",
      "        [0.0273],\n",
      "        [0.0318],\n",
      "        [0.0340],\n",
      "        [0.0366],\n",
      "        [0.0346],\n",
      "        [0.0332],\n",
      "        [0.0360],\n",
      "        [0.0356],\n",
      "        [0.0353],\n",
      "        [0.0368],\n",
      "        [0.0368],\n",
      "        [0.0401],\n",
      "        [0.0374],\n",
      "        [0.0378],\n",
      "        [0.0407],\n",
      "        [0.0371],\n",
      "        [0.0397],\n",
      "        [0.0411],\n",
      "        [0.0390],\n",
      "        [0.0424],\n",
      "        [0.0443],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0410],\n",
      "        [0.0428],\n",
      "        [0.0433],\n",
      "        [0.0454],\n",
      "        [0.0461],\n",
      "        [0.0478],\n",
      "        [0.0443],\n",
      "        [0.0491],\n",
      "        [0.0488],\n",
      "        [0.0482],\n",
      "        [0.0519],\n",
      "        [0.0524],\n",
      "        [0.0538],\n",
      "        [0.0524],\n",
      "        [0.0555],\n",
      "        [0.0582],\n",
      "        [0.0617],\n",
      "        [0.0593],\n",
      "        [0.0600],\n",
      "        [0.0580],\n",
      "        [0.0605],\n",
      "        [0.0619],\n",
      "        [0.0625],\n",
      "        [0.0635],\n",
      "        [0.0700],\n",
      "        [0.0633],\n",
      "        [0.0672],\n",
      "        [0.0646],\n",
      "        [0.0602],\n",
      "        [0.0674],\n",
      "        [0.0708],\n",
      "        [0.0683],\n",
      "        [0.0809],\n",
      "        [0.0791],\n",
      "        [0.0769],\n",
      "        [0.0751],\n",
      "        [0.0818],\n",
      "        [0.0876],\n",
      "        [0.0776],\n",
      "        [0.0863],\n",
      "        [0.0891],\n",
      "        [0.0834],\n",
      "        [0.0928],\n",
      "        [0.0969],\n",
      "        [0.0974],\n",
      "        [0.0976],\n",
      "        [0.1006],\n",
      "        [0.1000],\n",
      "        [0.1081],\n",
      "        [0.1100],\n",
      "        [0.1144],\n",
      "        [0.1043],\n",
      "        [0.1053],\n",
      "        [0.1074],\n",
      "        [0.1122],\n",
      "        [0.1127],\n",
      "        [0.1270],\n",
      "        [0.1289],\n",
      "        [0.1313],\n",
      "        [0.1361],\n",
      "        [0.1397],\n",
      "        [0.1399],\n",
      "        [0.1401],\n",
      "        [0.1558],\n",
      "        [0.1533]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.554877519607544\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.412070211401442e-07, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 99, 44, 97, 121, 74, 85, 63, 17, 98, 100, 142, 68, 50, 21, 83, 10, 22, 122, 66, 55, 91, 32, 111, 33, 51, 18, 52, 110, 26, 27, 36, 29, 129, 30, 77, 90, 141, 75, 11, 138, 53, 56, 49, 8, 113, 102, 9, 101, 23, 136, 120, 67, 46, 54, 143, 62, 16, 137, 112, 48, 28, 124, 57, 105, 15, 139, 35, 31, 104, 34, 25, 92, 76, 20, 95, 130, 64, 89, 140, 135, 4, 109, 12, 13, 86, 114, 103, 73, 65, 131, 123, 47, 19, 93, 37, 87, 84, 94, 134, 6, 43, 128, 88, 61, 144, 127, 3, 107, 106, 7, 58, 14, 72, 108, 115, 0, 78, 145, 132, 146, 24, 1, 126, 147, 82, 2, 42, 125, 133, 60, 119, 148, 59, 150, 151, 149, 5, 41, 153, 152, 69, 71, 38, 81, 70, 80, 40, 158, 157, 118, 45] 數值 torch.Size([152, 1])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6944],\n",
      "        [0.6589],\n",
      "        [0.9008],\n",
      "        [0.6851],\n",
      "        [0.5246],\n",
      "        [0.7577],\n",
      "        [0.6578],\n",
      "        [0.8657],\n",
      "        [0.8932],\n",
      "        [0.6816],\n",
      "        [0.6306],\n",
      "        [0.6022],\n",
      "        [0.8112],\n",
      "        [0.8834],\n",
      "        [0.8592],\n",
      "        [0.6612],\n",
      "        [1.0000],\n",
      "        [0.8468],\n",
      "        [0.5431],\n",
      "        [0.8261],\n",
      "        [0.9002],\n",
      "        [0.6813],\n",
      "        [0.8204],\n",
      "        [0.6335],\n",
      "        [0.8323],\n",
      "        [0.8855],\n",
      "        [0.9015],\n",
      "        [0.8963],\n",
      "        [0.6288],\n",
      "        [0.8187],\n",
      "        [0.8167],\n",
      "        [0.8292],\n",
      "        [0.8430],\n",
      "        [0.5498],\n",
      "        [0.8635],\n",
      "        [0.7361],\n",
      "        [0.6882],\n",
      "        [0.6101],\n",
      "        [0.7283],\n",
      "        [0.9971],\n",
      "        [0.5889],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8323],\n",
      "        [0.9563],\n",
      "        [0.6149],\n",
      "        [0.6196],\n",
      "        [0.9974],\n",
      "        [0.6252],\n",
      "        [0.8685],\n",
      "        [0.6103],\n",
      "        [0.5352],\n",
      "        [0.8180],\n",
      "        [0.8681],\n",
      "        [0.9183],\n",
      "        [0.6180],\n",
      "        [0.8548],\n",
      "        [0.9152],\n",
      "        [0.5999],\n",
      "        [0.6077],\n",
      "        [0.8755],\n",
      "        [0.8194],\n",
      "        [0.5533],\n",
      "        [0.8878],\n",
      "        [0.6471],\n",
      "        [0.9341],\n",
      "        [0.5826],\n",
      "        [0.8054],\n",
      "        [0.8396],\n",
      "        [0.6234],\n",
      "        [0.8505],\n",
      "        [0.8431],\n",
      "        [0.6689],\n",
      "        [0.7218],\n",
      "        [0.8788],\n",
      "        [0.6676],\n",
      "        [0.5428],\n",
      "        [0.8354],\n",
      "        [0.6721],\n",
      "        [0.6038],\n",
      "        [0.5809],\n",
      "        [0.8880],\n",
      "        [0.6310],\n",
      "        [0.9559],\n",
      "        [0.9417],\n",
      "        [0.6743],\n",
      "        [0.6316],\n",
      "        [0.6159],\n",
      "        [0.7898],\n",
      "        [0.7913],\n",
      "        [0.5851],\n",
      "        [0.5644],\n",
      "        [0.8775],\n",
      "        [0.8885],\n",
      "        [0.6673],\n",
      "        [0.8144],\n",
      "        [0.6836],\n",
      "        [0.6760],\n",
      "        [0.6553],\n",
      "        [0.5891],\n",
      "        [0.9193],\n",
      "        [0.8941],\n",
      "        [0.5526],\n",
      "        [0.6749],\n",
      "        [0.8602],\n",
      "        [0.6192],\n",
      "        [0.5582],\n",
      "        [0.9064],\n",
      "        [0.6288],\n",
      "        [0.6341],\n",
      "        [0.9361],\n",
      "        [0.8719],\n",
      "        [0.9154],\n",
      "        [0.8133],\n",
      "        [0.6218],\n",
      "        [0.6352],\n",
      "        [0.8739],\n",
      "        [0.7339],\n",
      "        [0.6361],\n",
      "        [0.5999],\n",
      "        [0.6334],\n",
      "        [0.8720],\n",
      "        [0.8944],\n",
      "        [0.5352],\n",
      "        [0.6313],\n",
      "        [0.7198],\n",
      "        [0.9298],\n",
      "        [0.8778],\n",
      "        [0.5310],\n",
      "        [0.5748],\n",
      "        [0.8578],\n",
      "        [0.5929],\n",
      "        [0.6264],\n",
      "        [0.8587],\n",
      "        [0.6409],\n",
      "        [0.6393],\n",
      "        [0.6480],\n",
      "        [0.8741],\n",
      "        [0.8582],\n",
      "        [0.6352],\n",
      "        [0.6456],\n",
      "        [0.8183],\n",
      "        [0.7855],\n",
      "        [0.8241],\n",
      "        [0.7456],\n",
      "        [0.7854],\n",
      "        [0.7610],\n",
      "        [0.8285],\n",
      "        [0.6047],\n",
      "        [0.6164],\n",
      "        [0.6102],\n",
      "        [0.8637]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0029],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0047],\n",
      "        [0.0051],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0059],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0102],\n",
      "        [0.0109],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0128],\n",
      "        [0.0128],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0139],\n",
      "        [0.0139],\n",
      "        [0.0143],\n",
      "        [0.0151],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0211],\n",
      "        [0.0215],\n",
      "        [0.0219],\n",
      "        [0.0223],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0239],\n",
      "        [0.0244],\n",
      "        [0.0249],\n",
      "        [0.0257],\n",
      "        [0.0273],\n",
      "        [0.0285],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0293],\n",
      "        [0.0298],\n",
      "        [0.0318],\n",
      "        [0.0321],\n",
      "        [0.0324],\n",
      "        [0.0332],\n",
      "        [0.0340],\n",
      "        [0.0346],\n",
      "        [0.0353],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0366],\n",
      "        [0.0368],\n",
      "        [0.0368],\n",
      "        [0.0371],\n",
      "        [0.0374],\n",
      "        [0.0378],\n",
      "        [0.0390],\n",
      "        [0.0397],\n",
      "        [0.0401],\n",
      "        [0.0407],\n",
      "        [0.0410],\n",
      "        [0.0411],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0428],\n",
      "        [0.0433],\n",
      "        [0.0443],\n",
      "        [0.0443],\n",
      "        [0.0454],\n",
      "        [0.0461],\n",
      "        [0.0478],\n",
      "        [0.0482],\n",
      "        [0.0488],\n",
      "        [0.0491],\n",
      "        [0.0519],\n",
      "        [0.0524],\n",
      "        [0.0524],\n",
      "        [0.0538],\n",
      "        [0.0555],\n",
      "        [0.0580],\n",
      "        [0.0582],\n",
      "        [0.0593],\n",
      "        [0.0600],\n",
      "        [0.0602],\n",
      "        [0.0605],\n",
      "        [0.0617],\n",
      "        [0.0619],\n",
      "        [0.0625],\n",
      "        [0.0633],\n",
      "        [0.0635],\n",
      "        [0.0646],\n",
      "        [0.0672],\n",
      "        [0.0674],\n",
      "        [0.0683],\n",
      "        [0.0700],\n",
      "        [0.0708],\n",
      "        [0.0751],\n",
      "        [0.0769],\n",
      "        [0.0776],\n",
      "        [0.0791],\n",
      "        [0.0809],\n",
      "        [0.0818],\n",
      "        [0.0834],\n",
      "        [0.0863],\n",
      "        [0.0876],\n",
      "        [0.0891],\n",
      "        [0.0928],\n",
      "        [0.0969],\n",
      "        [0.0974],\n",
      "        [0.0976],\n",
      "        [0.1000],\n",
      "        [0.1006],\n",
      "        [0.1043],\n",
      "        [0.1053],\n",
      "        [0.1074],\n",
      "        [0.1081],\n",
      "        [0.1100],\n",
      "        [0.1122],\n",
      "        [0.1127],\n",
      "        [0.1144],\n",
      "        [0.1270],\n",
      "        [0.1289],\n",
      "        [0.1313],\n",
      "        [0.1361],\n",
      "        [0.1397],\n",
      "        [0.1399],\n",
      "        [0.1401],\n",
      "        [0.1445],\n",
      "        [0.1533],\n",
      "        [0.1558]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0021],\n",
      "        [0.0039],\n",
      "        [0.0022],\n",
      "        [0.0031],\n",
      "        [0.0044],\n",
      "        [0.0054],\n",
      "        [0.0050],\n",
      "        [0.0043],\n",
      "        [0.0099],\n",
      "        [0.0043],\n",
      "        [0.0058],\n",
      "        [0.0082],\n",
      "        [0.0039],\n",
      "        [0.0067],\n",
      "        [0.0059],\n",
      "        [0.0092],\n",
      "        [0.0076],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0133],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0109],\n",
      "        [0.0128],\n",
      "        [0.0139],\n",
      "        [0.0120],\n",
      "        [0.0133],\n",
      "        [0.0153],\n",
      "        [0.0120],\n",
      "        [0.0149],\n",
      "        [0.0117],\n",
      "        [0.0139],\n",
      "        [0.0150],\n",
      "        [0.0216],\n",
      "        [0.0169],\n",
      "        [0.0174],\n",
      "        [0.0194],\n",
      "        [0.0191],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0202],\n",
      "        [0.0230],\n",
      "        [0.0214],\n",
      "        [0.0220],\n",
      "        [0.0223],\n",
      "        [0.0250],\n",
      "        [0.0250],\n",
      "        [0.0221],\n",
      "        [0.0254],\n",
      "        [0.0248],\n",
      "        [0.0246],\n",
      "        [0.0217],\n",
      "        [0.0290],\n",
      "        [0.0272],\n",
      "        [0.0299],\n",
      "        [0.0300],\n",
      "        [0.0291],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0324],\n",
      "        [0.0358],\n",
      "        [0.0361],\n",
      "        [0.0332],\n",
      "        [0.0360],\n",
      "        [0.0351],\n",
      "        [0.0383],\n",
      "        [0.0372],\n",
      "        [0.0371],\n",
      "        [0.0356],\n",
      "        [0.0372],\n",
      "        [0.0376],\n",
      "        [0.0381],\n",
      "        [0.0397],\n",
      "        [0.0430],\n",
      "        [0.0423],\n",
      "        [0.0393],\n",
      "        [0.0421],\n",
      "        [0.0411],\n",
      "        [0.0415],\n",
      "        [0.0434],\n",
      "        [0.0407],\n",
      "        [0.0429],\n",
      "        [0.0450],\n",
      "        [0.0427],\n",
      "        [0.0453],\n",
      "        [0.0463],\n",
      "        [0.0480],\n",
      "        [0.0470],\n",
      "        [0.0495],\n",
      "        [0.0504],\n",
      "        [0.0531],\n",
      "        [0.0519],\n",
      "        [0.0528],\n",
      "        [0.0551],\n",
      "        [0.0549],\n",
      "        [0.0569],\n",
      "        [0.0590],\n",
      "        [0.0603],\n",
      "        [0.0612],\n",
      "        [0.0537],\n",
      "        [0.0612],\n",
      "        [0.0642],\n",
      "        [0.0628],\n",
      "        [0.0634],\n",
      "        [0.0628],\n",
      "        [0.0646],\n",
      "        [0.0643],\n",
      "        [0.0679],\n",
      "        [0.0680],\n",
      "        [0.0658],\n",
      "        [0.0739],\n",
      "        [0.0713],\n",
      "        [0.0685],\n",
      "        [0.0773],\n",
      "        [0.0698],\n",
      "        [0.0809],\n",
      "        [0.0848],\n",
      "        [0.0823],\n",
      "        [0.0748],\n",
      "        [0.0843],\n",
      "        [0.0902],\n",
      "        [0.0882],\n",
      "        [0.0924],\n",
      "        [0.0972],\n",
      "        [0.0977],\n",
      "        [0.0953],\n",
      "        [0.0899],\n",
      "        [0.1011],\n",
      "        [0.0948],\n",
      "        [0.0958],\n",
      "        [0.0973],\n",
      "        [0.1064],\n",
      "        [0.1096],\n",
      "        [0.1017],\n",
      "        [0.1022],\n",
      "        [0.1155],\n",
      "        [0.1278],\n",
      "        [0.1299],\n",
      "        [0.1298],\n",
      "        [0.1369],\n",
      "        [0.1388],\n",
      "        [0.1397],\n",
      "        [0.1267],\n",
      "        [0.1311],\n",
      "        [0.1505],\n",
      "        [0.1561]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.836005210876465\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.862430268483877e-08, 99)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [99, 96, 44, 97, 74, 85, 121, 83, 68, 100, 63, 98, 17, 50, 22, 10, 66, 21, 122, 55, 91, 142, 32, 18, 33, 30, 29, 26, 51, 52, 111, 27, 77, 110, 129, 90, 36, 75, 11, 53, 138, 49, 56, 8, 102, 141, 143, 9, 120, 101, 113, 54, 46, 136, 23, 67, 16, 62, 48, 137, 112, 28, 124, 15, 31, 57, 105, 34, 20, 139, 104, 35, 76, 92, 95, 130, 64, 25, 4, 89, 114, 12, 13, 109, 135, 65, 103, 140, 86, 73, 131, 123, 19, 47, 93, 37, 84, 94, 87, 144, 6, 134, 43, 128, 88, 127, 61, 107, 7, 106, 3, 14, 58, 115, 72, 108, 145, 146, 78, 0, 147, 132, 24, 126, 82, 1, 42, 148, 2, 125, 150, 119, 151, 133, 149, 60, 59, 153, 152, 5, 41, 69, 158, 71, 81, 38, 157, 70, 80, 40, 155, 118, 156] 數值 torch.Size([153, 1])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6595],\n",
      "        [0.6947],\n",
      "        [0.9015],\n",
      "        [0.6856],\n",
      "        [0.7584],\n",
      "        [0.6579],\n",
      "        [0.5227],\n",
      "        [0.6592],\n",
      "        [0.8124],\n",
      "        [0.6310],\n",
      "        [0.8660],\n",
      "        [0.6822],\n",
      "        [0.8943],\n",
      "        [0.8834],\n",
      "        [0.8489],\n",
      "        [0.9994],\n",
      "        [0.8279],\n",
      "        [0.8615],\n",
      "        [0.5422],\n",
      "        [0.9003],\n",
      "        [0.6816],\n",
      "        [0.5974],\n",
      "        [0.8195],\n",
      "        [0.9032],\n",
      "        [0.8316],\n",
      "        [0.8613],\n",
      "        [0.8415],\n",
      "        [0.8195],\n",
      "        [0.8857],\n",
      "        [0.8963],\n",
      "        [0.6321],\n",
      "        [0.8170],\n",
      "        [0.7357],\n",
      "        [0.6276],\n",
      "        [0.5489],\n",
      "        [0.6884],\n",
      "        [0.8273],\n",
      "        [0.7294],\n",
      "        [0.9964],\n",
      "        [0.8865],\n",
      "        [0.5876],\n",
      "        [0.8326],\n",
      "        [0.8861],\n",
      "        [0.9565],\n",
      "        [0.6197],\n",
      "        [0.6062],\n",
      "        [0.6124],\n",
      "        [0.9975],\n",
      "        [0.5334],\n",
      "        [0.6252],\n",
      "        [0.6130],\n",
      "        [0.9172],\n",
      "        [0.8681],\n",
      "        [0.6084],\n",
      "        [0.8704],\n",
      "        [0.8191],\n",
      "        [0.9166],\n",
      "        [0.8543],\n",
      "        [0.8753],\n",
      "        [0.5987],\n",
      "        [0.6064],\n",
      "        [0.8183],\n",
      "        [0.5536],\n",
      "        [0.9349],\n",
      "        [0.8375],\n",
      "        [0.8865],\n",
      "        [0.6459],\n",
      "        [0.8495],\n",
      "        [0.8803],\n",
      "        [0.5807],\n",
      "        [0.6230],\n",
      "        [0.8039],\n",
      "        [0.7215],\n",
      "        [0.6685],\n",
      "        [0.6678],\n",
      "        [0.5431],\n",
      "        [0.8362],\n",
      "        [0.8449],\n",
      "        [0.8897],\n",
      "        [0.6721],\n",
      "        [0.6295],\n",
      "        [0.9561],\n",
      "        [0.9419],\n",
      "        [0.6300],\n",
      "        [0.5792],\n",
      "        [0.7930],\n",
      "        [0.6162],\n",
      "        [0.6009],\n",
      "        [0.6733],\n",
      "        [0.7905],\n",
      "        [0.5853],\n",
      "        [0.5642],\n",
      "        [0.8897],\n",
      "        [0.8777],\n",
      "        [0.6665],\n",
      "        [0.8131],\n",
      "        [0.6755],\n",
      "        [0.6549],\n",
      "        [0.6824],\n",
      "        [0.6127],\n",
      "        [0.9198],\n",
      "        [0.5878],\n",
      "        [0.8952],\n",
      "        [0.5517],\n",
      "        [0.6740],\n",
      "        [0.5576],\n",
      "        [0.8591],\n",
      "        [0.6279],\n",
      "        [0.9366],\n",
      "        [0.6332],\n",
      "        [0.9089],\n",
      "        [0.9157],\n",
      "        [0.8708],\n",
      "        [0.6327],\n",
      "        [0.8140],\n",
      "        [0.6213],\n",
      "        [0.6295],\n",
      "        [0.6256],\n",
      "        [0.7344],\n",
      "        [0.8778],\n",
      "        [0.6227],\n",
      "        [0.5995],\n",
      "        [0.8738],\n",
      "        [0.5348],\n",
      "        [0.7178],\n",
      "        [0.8983],\n",
      "        [0.8788],\n",
      "        [0.6164],\n",
      "        [0.9324],\n",
      "        [0.5315],\n",
      "        [0.6314],\n",
      "        [0.5906],\n",
      "        [0.6298],\n",
      "        [0.5745],\n",
      "        [0.6379],\n",
      "        [0.8575],\n",
      "        [0.8583],\n",
      "        [0.6247],\n",
      "        [0.6352],\n",
      "        [0.8757],\n",
      "        [0.8585],\n",
      "        [0.8193],\n",
      "        [0.5913],\n",
      "        [0.7863],\n",
      "        [0.7441],\n",
      "        [0.8231],\n",
      "        [0.6031],\n",
      "        [0.7862],\n",
      "        [0.7601],\n",
      "        [0.8287],\n",
      "        [0.6347],\n",
      "        [0.6074],\n",
      "        [0.6275]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0031],\n",
      "        [0.0039],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0067],\n",
      "        [0.0076],\n",
      "        [0.0082],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0109],\n",
      "        [0.0116],\n",
      "        [0.0117],\n",
      "        [0.0120],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0128],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0139],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0169],\n",
      "        [0.0174],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0202],\n",
      "        [0.0214],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0230],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0250],\n",
      "        [0.0250],\n",
      "        [0.0254],\n",
      "        [0.0272],\n",
      "        [0.0290],\n",
      "        [0.0291],\n",
      "        [0.0299],\n",
      "        [0.0300],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0324],\n",
      "        [0.0332],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0351],\n",
      "        [0.0356],\n",
      "        [0.0358],\n",
      "        [0.0360],\n",
      "        [0.0361],\n",
      "        [0.0371],\n",
      "        [0.0372],\n",
      "        [0.0372],\n",
      "        [0.0376],\n",
      "        [0.0381],\n",
      "        [0.0383],\n",
      "        [0.0393],\n",
      "        [0.0397],\n",
      "        [0.0407],\n",
      "        [0.0411],\n",
      "        [0.0415],\n",
      "        [0.0421],\n",
      "        [0.0423],\n",
      "        [0.0427],\n",
      "        [0.0429],\n",
      "        [0.0430],\n",
      "        [0.0434],\n",
      "        [0.0450],\n",
      "        [0.0453],\n",
      "        [0.0463],\n",
      "        [0.0470],\n",
      "        [0.0480],\n",
      "        [0.0495],\n",
      "        [0.0504],\n",
      "        [0.0519],\n",
      "        [0.0528],\n",
      "        [0.0531],\n",
      "        [0.0537],\n",
      "        [0.0549],\n",
      "        [0.0551],\n",
      "        [0.0569],\n",
      "        [0.0590],\n",
      "        [0.0603],\n",
      "        [0.0612],\n",
      "        [0.0612],\n",
      "        [0.0628],\n",
      "        [0.0628],\n",
      "        [0.0634],\n",
      "        [0.0642],\n",
      "        [0.0643],\n",
      "        [0.0646],\n",
      "        [0.0658],\n",
      "        [0.0679],\n",
      "        [0.0680],\n",
      "        [0.0685],\n",
      "        [0.0698],\n",
      "        [0.0713],\n",
      "        [0.0739],\n",
      "        [0.0748],\n",
      "        [0.0773],\n",
      "        [0.0809],\n",
      "        [0.0823],\n",
      "        [0.0843],\n",
      "        [0.0848],\n",
      "        [0.0882],\n",
      "        [0.0899],\n",
      "        [0.0902],\n",
      "        [0.0924],\n",
      "        [0.0948],\n",
      "        [0.0953],\n",
      "        [0.0958],\n",
      "        [0.0972],\n",
      "        [0.0973],\n",
      "        [0.0977],\n",
      "        [0.1011],\n",
      "        [0.1017],\n",
      "        [0.1022],\n",
      "        [0.1064],\n",
      "        [0.1096],\n",
      "        [0.1155],\n",
      "        [0.1267],\n",
      "        [0.1278],\n",
      "        [0.1298],\n",
      "        [0.1299],\n",
      "        [0.1311],\n",
      "        [0.1369],\n",
      "        [0.1388],\n",
      "        [0.1397],\n",
      "        [0.1474],\n",
      "        [0.1505],\n",
      "        [0.1536]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0010],\n",
      "        [0.0020],\n",
      "        [0.0013],\n",
      "        [0.0012],\n",
      "        [0.0050],\n",
      "        [0.0084],\n",
      "        [0.0003],\n",
      "        [0.0025],\n",
      "        [0.0055],\n",
      "        [0.0062],\n",
      "        [0.0043],\n",
      "        [0.0078],\n",
      "        [0.0044],\n",
      "        [0.0028],\n",
      "        [0.0069],\n",
      "        [0.0048],\n",
      "        [0.0115],\n",
      "        [0.0129],\n",
      "        [0.0077],\n",
      "        [0.0111],\n",
      "        [0.0172],\n",
      "        [0.0097],\n",
      "        [0.0080],\n",
      "        [0.0114],\n",
      "        [0.0098],\n",
      "        [0.0109],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0141],\n",
      "        [0.0168],\n",
      "        [0.0142],\n",
      "        [0.0129],\n",
      "        [0.0171],\n",
      "        [0.0184],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0158],\n",
      "        [0.0177],\n",
      "        [0.0203],\n",
      "        [0.0229],\n",
      "        [0.0181],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0231],\n",
      "        [0.0279],\n",
      "        [0.0135],\n",
      "        [0.0229],\n",
      "        [0.0179],\n",
      "        [0.0240],\n",
      "        [0.0270],\n",
      "        [0.0249],\n",
      "        [0.0264],\n",
      "        [0.0293],\n",
      "        [0.0280],\n",
      "        [0.0273],\n",
      "        [0.0245],\n",
      "        [0.0282],\n",
      "        [0.0306],\n",
      "        [0.0334],\n",
      "        [0.0334],\n",
      "        [0.0316],\n",
      "        [0.0338],\n",
      "        [0.0302],\n",
      "        [0.0315],\n",
      "        [0.0335],\n",
      "        [0.0364],\n",
      "        [0.0347],\n",
      "        [0.0331],\n",
      "        [0.0399],\n",
      "        [0.0382],\n",
      "        [0.0370],\n",
      "        [0.0381],\n",
      "        [0.0392],\n",
      "        [0.0382],\n",
      "        [0.0400],\n",
      "        [0.0359],\n",
      "        [0.0409],\n",
      "        [0.0373],\n",
      "        [0.0416],\n",
      "        [0.0364],\n",
      "        [0.0398],\n",
      "        [0.0401],\n",
      "        [0.0452],\n",
      "        [0.0466],\n",
      "        [0.0399],\n",
      "        [0.0442],\n",
      "        [0.0481],\n",
      "        [0.0466],\n",
      "        [0.0462],\n",
      "        [0.0475],\n",
      "        [0.0493],\n",
      "        [0.0447],\n",
      "        [0.0500],\n",
      "        [0.0517],\n",
      "        [0.0511],\n",
      "        [0.0497],\n",
      "        [0.0546],\n",
      "        [0.0564],\n",
      "        [0.0446],\n",
      "        [0.0542],\n",
      "        [0.0589],\n",
      "        [0.0541],\n",
      "        [0.0625],\n",
      "        [0.0633],\n",
      "        [0.0645],\n",
      "        [0.0611],\n",
      "        [0.0657],\n",
      "        [0.0621],\n",
      "        [0.0662],\n",
      "        [0.0670],\n",
      "        [0.0628],\n",
      "        [0.0646],\n",
      "        [0.0611],\n",
      "        [0.0694],\n",
      "        [0.0706],\n",
      "        [0.0594],\n",
      "        [0.0594],\n",
      "        [0.0714],\n",
      "        [0.0782],\n",
      "        [0.0635],\n",
      "        [0.0800],\n",
      "        [0.0837],\n",
      "        [0.0857],\n",
      "        [0.0815],\n",
      "        [0.0892],\n",
      "        [0.0858],\n",
      "        [0.0770],\n",
      "        [0.0934],\n",
      "        [0.0947],\n",
      "        [0.0823],\n",
      "        [0.0909],\n",
      "        [0.0832],\n",
      "        [0.1000],\n",
      "        [0.0843],\n",
      "        [0.0970],\n",
      "        [0.1005],\n",
      "        [0.0879],\n",
      "        [0.0886],\n",
      "        [0.1046],\n",
      "        [0.1083],\n",
      "        [0.1171],\n",
      "        [0.1095],\n",
      "        [0.1294],\n",
      "        [0.1277],\n",
      "        [0.1302],\n",
      "        [0.1141],\n",
      "        [0.1384],\n",
      "        [0.1375],\n",
      "        [0.1388],\n",
      "        [0.1331],\n",
      "        [0.1456],\n",
      "        [0.1381]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.11692214012146\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 154\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.200641505278327e-08, 83)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [83, 96, 99, 74, 97, 44, 68, 22, 98, 50, 66, 85, 100, 63, 10, 55, 17, 18, 121, 32, 30, 26, 51, 29, 91, 33, 21, 77, 122, 143, 52, 27, 75, 36, 90, 111, 110, 142, 11, 120, 49, 129, 56, 8, 53, 9, 138, 102, 101, 16, 54, 46, 113, 67, 141, 23, 62, 136, 15, 48, 31, 28, 20, 112, 137, 57, 124, 34, 64, 114, 105, 35, 4, 76, 104, 95, 92, 12, 65, 139, 130, 13, 25, 89, 103, 144, 19, 109, 73, 135, 86, 131, 140, 123, 84, 47, 37, 93, 43, 6, 94, 87, 134, 145, 146, 61, 115, 7, 128, 14, 88, 147, 127, 58, 107, 106, 3, 72, 108, 78, 148, 0, 132, 82, 150, 151, 24, 149, 126, 42, 153, 152, 1, 119, 2, 125, 60, 133, 59, 5, 41, 158, 157, 69, 81, 71, 38, 155, 80, 156, 70, 40, 154, 118] 數值 torch.Size([154, 1])\n",
      "目前模型的Data狀態 torch.Size([154, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6556],\n",
      "        [0.6940],\n",
      "        [0.6585],\n",
      "        [0.7594],\n",
      "        [0.6848],\n",
      "        [0.9038],\n",
      "        [0.8141],\n",
      "        [0.8520],\n",
      "        [0.6815],\n",
      "        [0.8848],\n",
      "        [0.8307],\n",
      "        [0.6561],\n",
      "        [0.6298],\n",
      "        [0.8678],\n",
      "        [0.9997],\n",
      "        [0.9019],\n",
      "        [0.8968],\n",
      "        [0.9061],\n",
      "        [0.5181],\n",
      "        [0.8191],\n",
      "        [0.8594],\n",
      "        [0.8212],\n",
      "        [0.8870],\n",
      "        [0.8405],\n",
      "        [0.6803],\n",
      "        [0.8314],\n",
      "        [0.8648],\n",
      "        [0.7347],\n",
      "        [0.5385],\n",
      "        [0.6042],\n",
      "        [0.8976],\n",
      "        [0.8179],\n",
      "        [0.7305],\n",
      "        [0.8260],\n",
      "        [0.6868],\n",
      "        [0.6286],\n",
      "        [0.6245],\n",
      "        [0.5901],\n",
      "        [0.9967],\n",
      "        [0.5293],\n",
      "        [0.8343],\n",
      "        [0.5454],\n",
      "        [0.8870],\n",
      "        [0.9570],\n",
      "        [0.8877],\n",
      "        [0.9984],\n",
      "        [0.5840],\n",
      "        [0.6181],\n",
      "        [0.6234],\n",
      "        [0.9193],\n",
      "        [0.9175],\n",
      "        [0.8697],\n",
      "        [0.6090],\n",
      "        [0.8209],\n",
      "        [0.5999],\n",
      "        [0.8735],\n",
      "        [0.8551],\n",
      "        [0.6040],\n",
      "        [0.9371],\n",
      "        [0.8769],\n",
      "        [0.8358],\n",
      "        [0.8176],\n",
      "        [0.8828],\n",
      "        [0.6030],\n",
      "        [0.5952],\n",
      "        [0.8865],\n",
      "        [0.5513],\n",
      "        [0.8491],\n",
      "        [0.8385],\n",
      "        [0.6253],\n",
      "        [0.6430],\n",
      "        [0.8030],\n",
      "        [0.8917],\n",
      "        [0.7205],\n",
      "        [0.6208],\n",
      "        [0.6668],\n",
      "        [0.6664],\n",
      "        [0.9575],\n",
      "        [0.7958],\n",
      "        [0.5767],\n",
      "        [0.5407],\n",
      "        [0.9432],\n",
      "        [0.8474],\n",
      "        [0.6702],\n",
      "        [0.6150],\n",
      "        [0.6036],\n",
      "        [0.8920],\n",
      "        [0.6269],\n",
      "        [0.7917],\n",
      "        [0.5750],\n",
      "        [0.6702],\n",
      "        [0.5830],\n",
      "        [0.5958],\n",
      "        [0.5612],\n",
      "        [0.6733],\n",
      "        [0.8797],\n",
      "        [0.8124],\n",
      "        [0.6644],\n",
      "        [0.8980],\n",
      "        [0.9205],\n",
      "        [0.6531],\n",
      "        [0.6790],\n",
      "        [0.5840],\n",
      "        [0.6205],\n",
      "        [0.6153],\n",
      "        [0.8592],\n",
      "        [0.6280],\n",
      "        [0.9373],\n",
      "        [0.5482],\n",
      "        [0.9172],\n",
      "        [0.6709],\n",
      "        [0.6114],\n",
      "        [0.5543],\n",
      "        [0.8709],\n",
      "        [0.6251],\n",
      "        [0.6304],\n",
      "        [0.9117],\n",
      "        [0.8156],\n",
      "        [0.6187],\n",
      "        [0.7344],\n",
      "        [0.6035],\n",
      "        [0.8822],\n",
      "        [0.5967],\n",
      "        [0.7150],\n",
      "        [0.6190],\n",
      "        [0.6172],\n",
      "        [0.8766],\n",
      "        [0.6249],\n",
      "        [0.5314],\n",
      "        [0.8811],\n",
      "        [0.6109],\n",
      "        [0.6216],\n",
      "        [0.9028],\n",
      "        [0.5862],\n",
      "        [0.9356],\n",
      "        [0.5292],\n",
      "        [0.8582],\n",
      "        [0.5717],\n",
      "        [0.8588],\n",
      "        [0.8775],\n",
      "        [0.8599],\n",
      "        [0.5740],\n",
      "        [0.5860],\n",
      "        [0.8209],\n",
      "        [0.7420],\n",
      "        [0.7879],\n",
      "        [0.8228],\n",
      "        [0.6203],\n",
      "        [0.7589],\n",
      "        [0.6121],\n",
      "        [0.7877],\n",
      "        [0.8297],\n",
      "        [0.6116],\n",
      "        [0.6025]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0020],\n",
      "        [0.0025],\n",
      "        [0.0028],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0048],\n",
      "        [0.0050],\n",
      "        [0.0055],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0084],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0129],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0141],\n",
      "        [0.0142],\n",
      "        [0.0158],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0177],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0203],\n",
      "        [0.0229],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0240],\n",
      "        [0.0245],\n",
      "        [0.0249],\n",
      "        [0.0264],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0279],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0293],\n",
      "        [0.0302],\n",
      "        [0.0306],\n",
      "        [0.0315],\n",
      "        [0.0316],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0334],\n",
      "        [0.0335],\n",
      "        [0.0338],\n",
      "        [0.0347],\n",
      "        [0.0359],\n",
      "        [0.0364],\n",
      "        [0.0364],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0381],\n",
      "        [0.0382],\n",
      "        [0.0382],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0399],\n",
      "        [0.0400],\n",
      "        [0.0401],\n",
      "        [0.0409],\n",
      "        [0.0416],\n",
      "        [0.0442],\n",
      "        [0.0446],\n",
      "        [0.0447],\n",
      "        [0.0452],\n",
      "        [0.0462],\n",
      "        [0.0466],\n",
      "        [0.0466],\n",
      "        [0.0475],\n",
      "        [0.0481],\n",
      "        [0.0493],\n",
      "        [0.0497],\n",
      "        [0.0500],\n",
      "        [0.0511],\n",
      "        [0.0517],\n",
      "        [0.0541],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0564],\n",
      "        [0.0589],\n",
      "        [0.0594],\n",
      "        [0.0594],\n",
      "        [0.0611],\n",
      "        [0.0611],\n",
      "        [0.0621],\n",
      "        [0.0625],\n",
      "        [0.0628],\n",
      "        [0.0633],\n",
      "        [0.0635],\n",
      "        [0.0645],\n",
      "        [0.0646],\n",
      "        [0.0657],\n",
      "        [0.0662],\n",
      "        [0.0670],\n",
      "        [0.0694],\n",
      "        [0.0706],\n",
      "        [0.0714],\n",
      "        [0.0770],\n",
      "        [0.0782],\n",
      "        [0.0800],\n",
      "        [0.0815],\n",
      "        [0.0823],\n",
      "        [0.0832],\n",
      "        [0.0837],\n",
      "        [0.0843],\n",
      "        [0.0857],\n",
      "        [0.0858],\n",
      "        [0.0879],\n",
      "        [0.0886],\n",
      "        [0.0892],\n",
      "        [0.0909],\n",
      "        [0.0934],\n",
      "        [0.0947],\n",
      "        [0.0970],\n",
      "        [0.1000],\n",
      "        [0.1005],\n",
      "        [0.1046],\n",
      "        [0.1083],\n",
      "        [0.1095],\n",
      "        [0.1141],\n",
      "        [0.1171],\n",
      "        [0.1277],\n",
      "        [0.1294],\n",
      "        [0.1302],\n",
      "        [0.1331],\n",
      "        [0.1375],\n",
      "        [0.1381],\n",
      "        [0.1384],\n",
      "        [0.1388],\n",
      "        [0.1407],\n",
      "        [0.1456]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0017],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0000],\n",
      "        [    0.0018],\n",
      "        [    0.0029],\n",
      "        [    0.0011],\n",
      "        [    0.0004],\n",
      "        [    0.0050],\n",
      "        [    0.0042],\n",
      "        [    0.0026],\n",
      "        [    0.0051],\n",
      "        [    0.0049],\n",
      "        [    0.0071],\n",
      "        [    0.0057],\n",
      "        [    0.0074],\n",
      "        [    0.0090],\n",
      "        [    0.0063],\n",
      "        [    0.0098],\n",
      "        [    0.0090],\n",
      "        [    0.0076],\n",
      "        [    0.0092],\n",
      "        [    0.0106],\n",
      "        [    0.0097],\n",
      "        [    0.0109],\n",
      "        [    0.0109],\n",
      "        [    0.0138],\n",
      "        [    0.0127],\n",
      "        [    0.0136],\n",
      "        [    0.0084],\n",
      "        [    0.0142],\n",
      "        [    0.0148],\n",
      "        [    0.0144],\n",
      "        [    0.0181],\n",
      "        [    0.0166],\n",
      "        [    0.0179],\n",
      "        [    0.0180],\n",
      "        [    0.0215],\n",
      "        [    0.0166],\n",
      "        [    0.0167],\n",
      "        [    0.0174],\n",
      "        [    0.0189],\n",
      "        [    0.0190],\n",
      "        [    0.0203],\n",
      "        [    0.0203],\n",
      "        [    0.0223],\n",
      "        [    0.0238],\n",
      "        [    0.0227],\n",
      "        [    0.0238],\n",
      "        [    0.0232],\n",
      "        [    0.0240],\n",
      "        [    0.0269],\n",
      "        [    0.0285],\n",
      "        [    0.0287],\n",
      "        [    0.0313],\n",
      "        [    0.0301],\n",
      "        [    0.0281],\n",
      "        [    0.0309],\n",
      "        [    0.0295],\n",
      "        [    0.0309],\n",
      "        [    0.0296],\n",
      "        [    0.0324],\n",
      "        [    0.0315],\n",
      "        [    0.0343],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0344],\n",
      "        [    0.0345],\n",
      "        [    0.0375],\n",
      "        [    0.0381],\n",
      "        [    0.0364],\n",
      "        [    0.0381],\n",
      "        [    0.0383],\n",
      "        [    0.0378],\n",
      "        [    0.0395],\n",
      "        [    0.0398],\n",
      "        [    0.0375],\n",
      "        [    0.0411],\n",
      "        [    0.0393],\n",
      "        [    0.0401],\n",
      "        [    0.0427],\n",
      "        [    0.0417],\n",
      "        [    0.0435],\n",
      "        [    0.0387],\n",
      "        [    0.0434],\n",
      "        [    0.0460],\n",
      "        [    0.0474],\n",
      "        [    0.0479],\n",
      "        [    0.0479],\n",
      "        [    0.0471],\n",
      "        [    0.0505],\n",
      "        [    0.0494],\n",
      "        [    0.0491],\n",
      "        [    0.0507],\n",
      "        [    0.0520],\n",
      "        [    0.0522],\n",
      "        [    0.0526],\n",
      "        [    0.0545],\n",
      "        [    0.0548],\n",
      "        [    0.0579],\n",
      "        [    0.0598],\n",
      "        [    0.0534],\n",
      "        [    0.0523],\n",
      "        [    0.0617],\n",
      "        [    0.0589],\n",
      "        [    0.0624],\n",
      "        [    0.0630],\n",
      "        [    0.0625],\n",
      "        [    0.0645],\n",
      "        [    0.0556],\n",
      "        [    0.0649],\n",
      "        [    0.0654],\n",
      "        [    0.0664],\n",
      "        [    0.0670],\n",
      "        [    0.0684],\n",
      "        [    0.0706],\n",
      "        [    0.0709],\n",
      "        [    0.0721],\n",
      "        [    0.0677],\n",
      "        [    0.0812],\n",
      "        [    0.0800],\n",
      "        [    0.0799],\n",
      "        [    0.0734],\n",
      "        [    0.0741],\n",
      "        [    0.0856],\n",
      "        [    0.0748],\n",
      "        [    0.0860],\n",
      "        [    0.0845],\n",
      "        [    0.0777],\n",
      "        [    0.0786],\n",
      "        [    0.0922],\n",
      "        [    0.0892],\n",
      "        [    0.0950],\n",
      "        [    0.0940],\n",
      "        [    0.0969],\n",
      "        [    0.0999],\n",
      "        [    0.1006],\n",
      "        [    0.1037],\n",
      "        [    0.1077],\n",
      "        [    0.0964],\n",
      "        [    0.1011],\n",
      "        [    0.1183],\n",
      "        [    0.1265],\n",
      "        [    0.1308],\n",
      "        [    0.1308],\n",
      "        [    0.1221],\n",
      "        [    0.1370],\n",
      "        [    0.1263],\n",
      "        [    0.1398],\n",
      "        [    0.1383],\n",
      "        [    0.1306],\n",
      "        [    0.1433]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.39942383766174\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 155\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7808474694902543e-09, 74)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [74, 22, 99, 96, 68, 83, 97, 66, 44, 50, 100, 98, 85, 10, 18, 63, 55, 30, 143, 32, 17, 26, 29, 121, 51, 91, 33, 77, 122, 21, 52, 75, 27, 11, 90, 120, 49, 111, 110, 36, 129, 56, 8, 53, 142, 9, 102, 16, 101, 138, 54, 46, 62, 113, 67, 15, 31, 23, 48, 136, 141, 20, 28, 124, 34, 137, 112, 64, 114, 57, 4, 105, 65, 95, 35, 76, 104, 144, 130, 92, 12, 13, 139, 89, 25, 19, 103, 109, 131, 73, 135, 86, 84, 123, 140, 47, 37, 93, 146, 43, 145, 6, 94, 147, 87, 115, 134, 61, 7, 14, 128, 88, 127, 58, 107, 106, 148, 3, 72, 108, 78, 150, 151, 149, 153, 152, 82, 132, 0, 42, 24, 126, 119, 1, 125, 2, 158, 60, 133, 59, 157, 5, 41, 69, 155, 156, 81, 154, 38, 71, 80, 40, 70, 118, 45] 數值 torch.Size([155, 1])\n",
      "目前模型的Data狀態 torch.Size([155, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7606],\n",
      "        [0.8544],\n",
      "        [0.6592],\n",
      "        [0.6944],\n",
      "        [0.8155],\n",
      "        [0.6536],\n",
      "        [0.6853],\n",
      "        [0.8329],\n",
      "        [0.9047],\n",
      "        [0.8851],\n",
      "        [0.6304],\n",
      "        [0.6821],\n",
      "        [0.6560],\n",
      "        [0.9985],\n",
      "        [0.9078],\n",
      "        [0.8688],\n",
      "        [0.9022],\n",
      "        [0.8572],\n",
      "        [0.5991],\n",
      "        [0.8184],\n",
      "        [0.8980],\n",
      "        [0.8223],\n",
      "        [0.8392],\n",
      "        [0.5167],\n",
      "        [0.8872],\n",
      "        [0.6805],\n",
      "        [0.8309],\n",
      "        [0.7345],\n",
      "        [0.5378],\n",
      "        [0.8672],\n",
      "        [0.8977],\n",
      "        [0.7319],\n",
      "        [0.8184],\n",
      "        [0.9956],\n",
      "        [0.6868],\n",
      "        [0.5281],\n",
      "        [0.8350],\n",
      "        [0.6274],\n",
      "        [0.6235],\n",
      "        [0.8244],\n",
      "        [0.5449],\n",
      "        [0.8869],\n",
      "        [0.9564],\n",
      "        [0.8877],\n",
      "        [0.5858],\n",
      "        [0.9979],\n",
      "        [0.6184],\n",
      "        [0.9206],\n",
      "        [0.6236],\n",
      "        [0.5831],\n",
      "        [0.9166],\n",
      "        [0.8702],\n",
      "        [0.8552],\n",
      "        [0.6075],\n",
      "        [0.8223],\n",
      "        [0.9378],\n",
      "        [0.8338],\n",
      "        [0.8756],\n",
      "        [0.8771],\n",
      "        [0.6024],\n",
      "        [0.5964],\n",
      "        [0.8844],\n",
      "        [0.8168],\n",
      "        [0.5518],\n",
      "        [0.8483],\n",
      "        [0.5944],\n",
      "        [0.6021],\n",
      "        [0.8399],\n",
      "        [0.6233],\n",
      "        [0.8855],\n",
      "        [0.8925],\n",
      "        [0.6420],\n",
      "        [0.7981],\n",
      "        [0.6672],\n",
      "        [0.8020],\n",
      "        [0.7205],\n",
      "        [0.6207],\n",
      "        [0.5977],\n",
      "        [0.5414],\n",
      "        [0.6662],\n",
      "        [0.9574],\n",
      "        [0.9432],\n",
      "        [0.5754],\n",
      "        [0.6701],\n",
      "        [0.8493],\n",
      "        [0.8932],\n",
      "        [0.6156],\n",
      "        [0.6260],\n",
      "        [0.5835],\n",
      "        [0.7929],\n",
      "        [0.5737],\n",
      "        [0.6689],\n",
      "        [0.6727],\n",
      "        [0.5611],\n",
      "        [0.5935],\n",
      "        [0.8804],\n",
      "        [0.8115],\n",
      "        [0.6638],\n",
      "        [0.6082],\n",
      "        [0.8995],\n",
      "        [0.6145],\n",
      "        [0.9203],\n",
      "        [0.6530],\n",
      "        [0.6035],\n",
      "        [0.6775],\n",
      "        [0.6258],\n",
      "        [0.5831],\n",
      "        [0.8586],\n",
      "        [0.9370],\n",
      "        [0.9175],\n",
      "        [0.5478],\n",
      "        [0.6698],\n",
      "        [0.5539],\n",
      "        [0.8701],\n",
      "        [0.6244],\n",
      "        [0.6296],\n",
      "        [0.5941],\n",
      "        [0.9131],\n",
      "        [0.8168],\n",
      "        [0.6183],\n",
      "        [0.7352],\n",
      "        [0.6100],\n",
      "        [0.6081],\n",
      "        [0.6153],\n",
      "        [0.6007],\n",
      "        [0.6115],\n",
      "        [0.7134],\n",
      "        [0.5967],\n",
      "        [0.8852],\n",
      "        [0.8824],\n",
      "        [0.8785],\n",
      "        [0.5311],\n",
      "        [0.5845],\n",
      "        [0.9058],\n",
      "        [0.5299],\n",
      "        [0.9371],\n",
      "        [0.5610],\n",
      "        [0.8583],\n",
      "        [0.5718],\n",
      "        [0.8587],\n",
      "        [0.5730],\n",
      "        [0.8784],\n",
      "        [0.8605],\n",
      "        [0.8222],\n",
      "        [0.6094],\n",
      "        [0.6003],\n",
      "        [0.7409],\n",
      "        [0.6015],\n",
      "        [0.8222],\n",
      "        [0.7893],\n",
      "        [0.7583],\n",
      "        [0.8302],\n",
      "        [0.7891],\n",
      "        [0.6002],\n",
      "        [0.8669]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0004],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0011],\n",
      "        [    0.0017],\n",
      "        [    0.0018],\n",
      "        [    0.0026],\n",
      "        [    0.0029],\n",
      "        [    0.0042],\n",
      "        [    0.0049],\n",
      "        [    0.0050],\n",
      "        [    0.0051],\n",
      "        [    0.0057],\n",
      "        [    0.0063],\n",
      "        [    0.0071],\n",
      "        [    0.0074],\n",
      "        [    0.0076],\n",
      "        [    0.0084],\n",
      "        [    0.0090],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0097],\n",
      "        [    0.0098],\n",
      "        [    0.0106],\n",
      "        [    0.0109],\n",
      "        [    0.0109],\n",
      "        [    0.0127],\n",
      "        [    0.0136],\n",
      "        [    0.0138],\n",
      "        [    0.0142],\n",
      "        [    0.0144],\n",
      "        [    0.0148],\n",
      "        [    0.0166],\n",
      "        [    0.0166],\n",
      "        [    0.0167],\n",
      "        [    0.0174],\n",
      "        [    0.0179],\n",
      "        [    0.0180],\n",
      "        [    0.0181],\n",
      "        [    0.0189],\n",
      "        [    0.0190],\n",
      "        [    0.0203],\n",
      "        [    0.0203],\n",
      "        [    0.0215],\n",
      "        [    0.0223],\n",
      "        [    0.0227],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0238],\n",
      "        [    0.0240],\n",
      "        [    0.0269],\n",
      "        [    0.0281],\n",
      "        [    0.0285],\n",
      "        [    0.0287],\n",
      "        [    0.0295],\n",
      "        [    0.0296],\n",
      "        [    0.0301],\n",
      "        [    0.0309],\n",
      "        [    0.0309],\n",
      "        [    0.0313],\n",
      "        [    0.0315],\n",
      "        [    0.0324],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0343],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0345],\n",
      "        [    0.0345],\n",
      "        [    0.0364],\n",
      "        [    0.0375],\n",
      "        [    0.0375],\n",
      "        [    0.0378],\n",
      "        [    0.0381],\n",
      "        [    0.0381],\n",
      "        [    0.0383],\n",
      "        [    0.0387],\n",
      "        [    0.0393],\n",
      "        [    0.0395],\n",
      "        [    0.0398],\n",
      "        [    0.0401],\n",
      "        [    0.0411],\n",
      "        [    0.0417],\n",
      "        [    0.0427],\n",
      "        [    0.0434],\n",
      "        [    0.0435],\n",
      "        [    0.0460],\n",
      "        [    0.0471],\n",
      "        [    0.0474],\n",
      "        [    0.0479],\n",
      "        [    0.0479],\n",
      "        [    0.0491],\n",
      "        [    0.0494],\n",
      "        [    0.0505],\n",
      "        [    0.0507],\n",
      "        [    0.0520],\n",
      "        [    0.0522],\n",
      "        [    0.0523],\n",
      "        [    0.0526],\n",
      "        [    0.0534],\n",
      "        [    0.0545],\n",
      "        [    0.0548],\n",
      "        [    0.0556],\n",
      "        [    0.0579],\n",
      "        [    0.0589],\n",
      "        [    0.0598],\n",
      "        [    0.0617],\n",
      "        [    0.0624],\n",
      "        [    0.0625],\n",
      "        [    0.0630],\n",
      "        [    0.0645],\n",
      "        [    0.0649],\n",
      "        [    0.0654],\n",
      "        [    0.0664],\n",
      "        [    0.0670],\n",
      "        [    0.0677],\n",
      "        [    0.0684],\n",
      "        [    0.0706],\n",
      "        [    0.0709],\n",
      "        [    0.0721],\n",
      "        [    0.0734],\n",
      "        [    0.0741],\n",
      "        [    0.0748],\n",
      "        [    0.0777],\n",
      "        [    0.0786],\n",
      "        [    0.0799],\n",
      "        [    0.0800],\n",
      "        [    0.0812],\n",
      "        [    0.0845],\n",
      "        [    0.0856],\n",
      "        [    0.0860],\n",
      "        [    0.0892],\n",
      "        [    0.0922],\n",
      "        [    0.0940],\n",
      "        [    0.0950],\n",
      "        [    0.0964],\n",
      "        [    0.0969],\n",
      "        [    0.0999],\n",
      "        [    0.1006],\n",
      "        [    0.1011],\n",
      "        [    0.1037],\n",
      "        [    0.1077],\n",
      "        [    0.1183],\n",
      "        [    0.1221],\n",
      "        [    0.1263],\n",
      "        [    0.1265],\n",
      "        [    0.1306],\n",
      "        [    0.1308],\n",
      "        [    0.1308],\n",
      "        [    0.1370],\n",
      "        [    0.1383],\n",
      "        [    0.1398],\n",
      "        [    0.1433],\n",
      "        [    0.1589]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0005],\n",
      "        [0.0015],\n",
      "        [0.0009],\n",
      "        [0.0005],\n",
      "        [0.0020],\n",
      "        [0.0035],\n",
      "        [0.0018],\n",
      "        [0.0017],\n",
      "        [0.0057],\n",
      "        [0.0027],\n",
      "        [0.0069],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0065],\n",
      "        [0.0063],\n",
      "        [0.0089],\n",
      "        [0.0050],\n",
      "        [0.0065],\n",
      "        [0.0076],\n",
      "        [0.0082],\n",
      "        [0.0092],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0119],\n",
      "        [0.0089],\n",
      "        [0.0097],\n",
      "        [0.0132],\n",
      "        [0.0111],\n",
      "        [0.0146],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0145],\n",
      "        [0.0140],\n",
      "        [0.0149],\n",
      "        [0.0182],\n",
      "        [0.0186],\n",
      "        [0.0168],\n",
      "        [0.0166],\n",
      "        [0.0203],\n",
      "        [0.0162],\n",
      "        [0.0210],\n",
      "        [0.0214],\n",
      "        [0.0187],\n",
      "        [0.0226],\n",
      "        [0.0209],\n",
      "        [0.0207],\n",
      "        [0.0238],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0213],\n",
      "        [0.0251],\n",
      "        [0.0294],\n",
      "        [0.0276],\n",
      "        [0.0290],\n",
      "        [0.0306],\n",
      "        [0.0271],\n",
      "        [0.0309],\n",
      "        [0.0290],\n",
      "        [0.0294],\n",
      "        [0.0318],\n",
      "        [0.0314],\n",
      "        [0.0336],\n",
      "        [0.0297],\n",
      "        [0.0324],\n",
      "        [0.0322],\n",
      "        [0.0328],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.0371],\n",
      "        [0.0367],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0363],\n",
      "        [0.0398],\n",
      "        [0.0373],\n",
      "        [0.0367],\n",
      "        [0.0361],\n",
      "        [0.0357],\n",
      "        [0.0380],\n",
      "        [0.0414],\n",
      "        [0.0418],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0434],\n",
      "        [0.0438],\n",
      "        [0.0412],\n",
      "        [0.0445],\n",
      "        [0.0437],\n",
      "        [0.0478],\n",
      "        [0.0459],\n",
      "        [0.0469],\n",
      "        [0.0501],\n",
      "        [0.0464],\n",
      "        [0.0500],\n",
      "        [0.0493],\n",
      "        [0.0536],\n",
      "        [0.0513],\n",
      "        [0.0486],\n",
      "        [0.0530],\n",
      "        [0.0507],\n",
      "        [0.0553],\n",
      "        [0.0536],\n",
      "        [0.0513],\n",
      "        [0.0573],\n",
      "        [0.0590],\n",
      "        [0.0576],\n",
      "        [0.0637],\n",
      "        [0.0632],\n",
      "        [0.0639],\n",
      "        [0.0603],\n",
      "        [0.0636],\n",
      "        [0.0623],\n",
      "        [0.0677],\n",
      "        [0.0649],\n",
      "        [0.0657],\n",
      "        [0.0622],\n",
      "        [0.0688],\n",
      "        [0.0709],\n",
      "        [0.0691],\n",
      "        [0.0733],\n",
      "        [0.0683],\n",
      "        [0.0690],\n",
      "        [0.0691],\n",
      "        [0.0716],\n",
      "        [0.0726],\n",
      "        [0.0792],\n",
      "        [0.0772],\n",
      "        [0.0828],\n",
      "        [0.0848],\n",
      "        [0.0861],\n",
      "        [0.0831],\n",
      "        [0.0900],\n",
      "        [0.0936],\n",
      "        [0.0901],\n",
      "        [0.0951],\n",
      "        [0.0880],\n",
      "        [0.0980],\n",
      "        [0.0968],\n",
      "        [0.1020],\n",
      "        [0.0927],\n",
      "        [0.1038],\n",
      "        [0.1083],\n",
      "        [0.1187],\n",
      "        [0.1152],\n",
      "        [0.1187],\n",
      "        [0.1261],\n",
      "        [0.1245],\n",
      "        [0.1323],\n",
      "        [0.1314],\n",
      "        [0.1369],\n",
      "        [0.1389],\n",
      "        [0.1404],\n",
      "        [0.1436],\n",
      "        [0.1575]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.68171238899231\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.53552570939064e-07, 68)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [68, 22, 74, 96, 99, 44, 66, 83, 100, 85, 10, 97, 30, 50, 63, 18, 143, 98, 32, 29, 121, 17, 91, 55, 26, 33, 122, 51, 52, 75, 77, 11, 27, 21, 90, 129, 110, 111, 120, 49, 53, 36, 102, 9, 56, 54, 8, 101, 138, 142, 16, 46, 31, 113, 48, 67, 62, 136, 124, 15, 23, 20, 141, 137, 34, 112, 28, 64, 114, 130, 144, 95, 105, 65, 104, 4, 57, 76, 92, 139, 35, 89, 103, 12, 13, 25, 131, 19, 109, 135, 123, 86, 73, 146, 47, 140, 84, 145, 147, 93, 43, 94, 37, 6, 87, 134, 115, 128, 148, 127, 7, 88, 61, 14, 107, 106, 58, 150, 3, 151, 108, 149, 72, 153, 152, 78, 132, 82, 0, 126, 42, 24, 158, 119, 125, 157, 1, 2, 133, 60, 59, 5, 41, 155, 69, 156, 154, 81, 71, 38, 80, 40, 70, 118, 45, 39] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8161],\n",
      "        [0.8553],\n",
      "        [0.7614],\n",
      "        [0.6959],\n",
      "        [0.6612],\n",
      "        [0.9035],\n",
      "        [0.8337],\n",
      "        [0.6533],\n",
      "        [0.6326],\n",
      "        [0.6578],\n",
      "        [0.9962],\n",
      "        [0.6871],\n",
      "        [0.8546],\n",
      "        [0.8835],\n",
      "        [0.8680],\n",
      "        [0.9076],\n",
      "        [0.5972],\n",
      "        [0.6840],\n",
      "        [0.8171],\n",
      "        [0.8375],\n",
      "        [0.5183],\n",
      "        [0.8972],\n",
      "        [0.6825],\n",
      "        [0.9007],\n",
      "        [0.8224],\n",
      "        [0.8297],\n",
      "        [0.5404],\n",
      "        [0.8859],\n",
      "        [0.8963],\n",
      "        [0.7333],\n",
      "        [0.7350],\n",
      "        [0.9931],\n",
      "        [0.8182],\n",
      "        [0.8680],\n",
      "        [0.6884],\n",
      "        [0.5475],\n",
      "        [0.6249],\n",
      "        [0.6286],\n",
      "        [0.5296],\n",
      "        [0.8338],\n",
      "        [0.8861],\n",
      "        [0.8222],\n",
      "        [0.6205],\n",
      "        [0.9964],\n",
      "        [0.8849],\n",
      "        [0.9139],\n",
      "        [0.9553],\n",
      "        [0.6254],\n",
      "        [0.5849],\n",
      "        [0.5846],\n",
      "        [0.9200],\n",
      "        [0.8684],\n",
      "        [0.8313],\n",
      "        [0.6084],\n",
      "        [0.8752],\n",
      "        [0.8227],\n",
      "        [0.8539],\n",
      "        [0.6039],\n",
      "        [0.5554],\n",
      "        [0.9367],\n",
      "        [0.8763],\n",
      "        [0.8845],\n",
      "        [0.5960],\n",
      "        [0.5964],\n",
      "        [0.8468],\n",
      "        [0.6036],\n",
      "        [0.8156],\n",
      "        [0.8397],\n",
      "        [0.6238],\n",
      "        [0.5450],\n",
      "        [0.5951],\n",
      "        [0.6687],\n",
      "        [0.6429],\n",
      "        [0.7990],\n",
      "        [0.6223],\n",
      "        [0.8922],\n",
      "        [0.8829],\n",
      "        [0.7213],\n",
      "        [0.6676],\n",
      "        [0.5769],\n",
      "        [0.8003],\n",
      "        [0.6718],\n",
      "        [0.6179],\n",
      "        [0.9558],\n",
      "        [0.9415],\n",
      "        [0.8499],\n",
      "        [0.5869],\n",
      "        [0.8929],\n",
      "        [0.6275],\n",
      "        [0.5757],\n",
      "        [0.5641],\n",
      "        [0.6699],\n",
      "        [0.7933],\n",
      "        [0.6045],\n",
      "        [0.8789],\n",
      "        [0.5939],\n",
      "        [0.6737],\n",
      "        [0.6117],\n",
      "        [0.5992],\n",
      "        [0.6648],\n",
      "        [0.8992],\n",
      "        [0.6542],\n",
      "        [0.8099],\n",
      "        [0.9195],\n",
      "        [0.6781],\n",
      "        [0.5853],\n",
      "        [0.6259],\n",
      "        [0.5504],\n",
      "        [0.5886],\n",
      "        [0.5565],\n",
      "        [0.9362],\n",
      "        [0.6707],\n",
      "        [0.8566],\n",
      "        [0.9162],\n",
      "        [0.6259],\n",
      "        [0.6309],\n",
      "        [0.8678],\n",
      "        [0.6049],\n",
      "        [0.9135],\n",
      "        [0.6030],\n",
      "        [0.6202],\n",
      "        [0.6097],\n",
      "        [0.8171],\n",
      "        [0.5947],\n",
      "        [0.6055],\n",
      "        [0.7363],\n",
      "        [0.5996],\n",
      "        [0.7127],\n",
      "        [0.8867],\n",
      "        [0.5340],\n",
      "        [0.8822],\n",
      "        [0.8790],\n",
      "        [0.5525],\n",
      "        [0.5853],\n",
      "        [0.5337],\n",
      "        [0.5646],\n",
      "        [0.9071],\n",
      "        [0.9373],\n",
      "        [0.5749],\n",
      "        [0.8572],\n",
      "        [0.8574],\n",
      "        [0.8784],\n",
      "        [0.8599],\n",
      "        [0.6025],\n",
      "        [0.8225],\n",
      "        [0.5927],\n",
      "        [0.5954],\n",
      "        [0.7404],\n",
      "        [0.7899],\n",
      "        [0.8207],\n",
      "        [0.7582],\n",
      "        [0.8296],\n",
      "        [0.7896],\n",
      "        [0.6005],\n",
      "        [0.8654],\n",
      "        [0.8352]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0050],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0076],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0082],\n",
      "        [0.0089],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0111],\n",
      "        [0.0119],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0140],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0162],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0182],\n",
      "        [0.0186],\n",
      "        [0.0187],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0226],\n",
      "        [0.0238],\n",
      "        [0.0251],\n",
      "        [0.0271],\n",
      "        [0.0276],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0294],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0318],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0328],\n",
      "        [0.0336],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.0357],\n",
      "        [0.0361],\n",
      "        [0.0363],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0367],\n",
      "        [0.0371],\n",
      "        [0.0373],\n",
      "        [0.0380],\n",
      "        [0.0396],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0412],\n",
      "        [0.0414],\n",
      "        [0.0418],\n",
      "        [0.0434],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0445],\n",
      "        [0.0459],\n",
      "        [0.0464],\n",
      "        [0.0469],\n",
      "        [0.0478],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0500],\n",
      "        [0.0501],\n",
      "        [0.0507],\n",
      "        [0.0513],\n",
      "        [0.0513],\n",
      "        [0.0530],\n",
      "        [0.0536],\n",
      "        [0.0536],\n",
      "        [0.0553],\n",
      "        [0.0573],\n",
      "        [0.0576],\n",
      "        [0.0590],\n",
      "        [0.0603],\n",
      "        [0.0622],\n",
      "        [0.0623],\n",
      "        [0.0632],\n",
      "        [0.0636],\n",
      "        [0.0637],\n",
      "        [0.0639],\n",
      "        [0.0649],\n",
      "        [0.0657],\n",
      "        [0.0677],\n",
      "        [0.0683],\n",
      "        [0.0688],\n",
      "        [0.0690],\n",
      "        [0.0691],\n",
      "        [0.0691],\n",
      "        [0.0709],\n",
      "        [0.0716],\n",
      "        [0.0726],\n",
      "        [0.0733],\n",
      "        [0.0772],\n",
      "        [0.0792],\n",
      "        [0.0828],\n",
      "        [0.0831],\n",
      "        [0.0848],\n",
      "        [0.0861],\n",
      "        [0.0880],\n",
      "        [0.0900],\n",
      "        [0.0901],\n",
      "        [0.0927],\n",
      "        [0.0936],\n",
      "        [0.0951],\n",
      "        [0.0968],\n",
      "        [0.0980],\n",
      "        [0.1020],\n",
      "        [0.1038],\n",
      "        [0.1083],\n",
      "        [0.1152],\n",
      "        [0.1187],\n",
      "        [0.1187],\n",
      "        [0.1245],\n",
      "        [0.1261],\n",
      "        [0.1314],\n",
      "        [0.1323],\n",
      "        [0.1369],\n",
      "        [0.1389],\n",
      "        [0.1404],\n",
      "        [0.1436],\n",
      "        [0.1575],\n",
      "        [0.1648]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0014],\n",
      "        [0.0035],\n",
      "        [0.0024],\n",
      "        [0.0022],\n",
      "        [0.0029],\n",
      "        [0.0042],\n",
      "        [0.0003],\n",
      "        [0.0028],\n",
      "        [0.0015],\n",
      "        [0.0021],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0059],\n",
      "        [0.0039],\n",
      "        [0.0081],\n",
      "        [0.0041],\n",
      "        [0.0050],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0079],\n",
      "        [0.0098],\n",
      "        [0.0073],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0115],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0150],\n",
      "        [0.0113],\n",
      "        [0.0142],\n",
      "        [0.0153],\n",
      "        [0.0166],\n",
      "        [0.0174],\n",
      "        [0.0136],\n",
      "        [0.0147],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0186],\n",
      "        [0.0171],\n",
      "        [0.0207],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0233],\n",
      "        [0.0196],\n",
      "        [0.0227],\n",
      "        [0.0192],\n",
      "        [0.0211],\n",
      "        [0.0206],\n",
      "        [0.0237],\n",
      "        [0.0219],\n",
      "        [0.0264],\n",
      "        [0.0279],\n",
      "        [0.0273],\n",
      "        [0.0303],\n",
      "        [0.0307],\n",
      "        [0.0282],\n",
      "        [0.0280],\n",
      "        [0.0272],\n",
      "        [0.0287],\n",
      "        [0.0339],\n",
      "        [0.0290],\n",
      "        [0.0321],\n",
      "        [0.0305],\n",
      "        [0.0343],\n",
      "        [0.0320],\n",
      "        [0.0322],\n",
      "        [0.0328],\n",
      "        [0.0352],\n",
      "        [0.0333],\n",
      "        [0.0341],\n",
      "        [0.0353],\n",
      "        [0.0360],\n",
      "        [0.0345],\n",
      "        [0.0359],\n",
      "        [0.0349],\n",
      "        [0.0364],\n",
      "        [0.0362],\n",
      "        [0.0371],\n",
      "        [0.0387],\n",
      "        [0.0386],\n",
      "        [0.0387],\n",
      "        [0.0400],\n",
      "        [0.0400],\n",
      "        [0.0406],\n",
      "        [0.0461],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0434],\n",
      "        [0.0445],\n",
      "        [0.0443],\n",
      "        [0.0463],\n",
      "        [0.0494],\n",
      "        [0.0459],\n",
      "        [0.0510],\n",
      "        [0.0497],\n",
      "        [0.0507],\n",
      "        [0.0487],\n",
      "        [0.0479],\n",
      "        [0.0509],\n",
      "        [0.0496],\n",
      "        [0.0530],\n",
      "        [0.0521],\n",
      "        [0.0533],\n",
      "        [0.0570],\n",
      "        [0.0559],\n",
      "        [0.0591],\n",
      "        [0.0588],\n",
      "        [0.0578],\n",
      "        [0.0606],\n",
      "        [0.0610],\n",
      "        [0.0631],\n",
      "        [0.0628],\n",
      "        [0.0625],\n",
      "        [0.0639],\n",
      "        [0.0649],\n",
      "        [0.0670],\n",
      "        [0.0646],\n",
      "        [0.0714],\n",
      "        [0.0654],\n",
      "        [0.0678],\n",
      "        [0.0649],\n",
      "        [0.0728],\n",
      "        [0.0672],\n",
      "        [0.0684],\n",
      "        [0.0748],\n",
      "        [0.0748],\n",
      "        [0.0790],\n",
      "        [0.0856],\n",
      "        [0.0815],\n",
      "        [0.0815],\n",
      "        [0.0891],\n",
      "        [0.0812],\n",
      "        [0.0904],\n",
      "        [0.0876],\n",
      "        [0.0860],\n",
      "        [0.0966],\n",
      "        [0.0976],\n",
      "        [0.0945],\n",
      "        [0.0966],\n",
      "        [0.1008],\n",
      "        [0.1019],\n",
      "        [0.1054],\n",
      "        [0.1102],\n",
      "        [0.1206],\n",
      "        [0.1130],\n",
      "        [0.1200],\n",
      "        [0.1264],\n",
      "        [0.1332],\n",
      "        [0.1305],\n",
      "        [0.1377],\n",
      "        [0.1364],\n",
      "        [0.1422],\n",
      "        [0.1438],\n",
      "        [0.1592],\n",
      "        [0.1625]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.96369028091431\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1988447568000993e-07, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [66, 68, 100, 85, 96, 74, 83, 99, 22, 50, 18, 44, 143, 97, 10, 30, 55, 26, 91, 121, 63, 98, 32, 29, 51, 122, 17, 75, 33, 90, 77, 129, 52, 11, 110, 111, 27, 49, 21, 120, 8, 36, 102, 56, 138, 53, 101, 16, 54, 9, 142, 46, 124, 113, 31, 136, 62, 15, 20, 48, 137, 67, 112, 141, 28, 64, 130, 23, 144, 34, 65, 4, 114, 95, 104, 105, 76, 57, 92, 35, 139, 89, 103, 12, 13, 131, 19, 109, 123, 135, 146, 25, 86, 147, 145, 73, 43, 140, 84, 93, 47, 37, 94, 6, 134, 87, 148, 128, 115, 127, 7, 14, 61, 88, 107, 150, 106, 149, 151, 58, 153, 108, 152, 3, 72, 78, 132, 82, 158, 42, 126, 0, 157, 125, 24, 119, 133, 60, 1, 2, 59, 5, 41, 155, 156, 154, 69, 81, 38, 71, 40, 80, 70, 118, 45, 39, 116] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8358],\n",
      "        [0.8181],\n",
      "        [0.6339],\n",
      "        [0.6589],\n",
      "        [0.6972],\n",
      "        [0.7630],\n",
      "        [0.6525],\n",
      "        [0.6627],\n",
      "        [0.8582],\n",
      "        [0.8854],\n",
      "        [0.9100],\n",
      "        [0.9060],\n",
      "        [0.5957],\n",
      "        [0.6885],\n",
      "        [0.9978],\n",
      "        [0.8555],\n",
      "        [0.9027],\n",
      "        [0.8245],\n",
      "        [0.6841],\n",
      "        [0.5186],\n",
      "        [0.8698],\n",
      "        [0.6856],\n",
      "        [0.8186],\n",
      "        [0.8388],\n",
      "        [0.8881],\n",
      "        [0.5416],\n",
      "        [0.8988],\n",
      "        [0.7350],\n",
      "        [0.8315],\n",
      "        [0.6897],\n",
      "        [0.7360],\n",
      "        [0.5491],\n",
      "        [0.8985],\n",
      "        [0.9943],\n",
      "        [0.6258],\n",
      "        [0.6294],\n",
      "        [0.8202],\n",
      "        [0.8353],\n",
      "        [0.8708],\n",
      "        [0.5299],\n",
      "        [0.9575],\n",
      "        [0.8233],\n",
      "        [0.6216],\n",
      "        [0.8863],\n",
      "        [0.5863],\n",
      "        [0.8881],\n",
      "        [0.6264],\n",
      "        [0.9219],\n",
      "        [0.9153],\n",
      "        [0.9988],\n",
      "        [0.5836],\n",
      "        [0.8696],\n",
      "        [0.5579],\n",
      "        [0.6087],\n",
      "        [0.8321],\n",
      "        [0.6053],\n",
      "        [0.8551],\n",
      "        [0.9386],\n",
      "        [0.8869],\n",
      "        [0.8765],\n",
      "        [0.5982],\n",
      "        [0.8243],\n",
      "        [0.6044],\n",
      "        [0.5957],\n",
      "        [0.8170],\n",
      "        [0.8416],\n",
      "        [0.5474],\n",
      "        [0.8794],\n",
      "        [0.5931],\n",
      "        [0.8487],\n",
      "        [0.8012],\n",
      "        [0.8940],\n",
      "        [0.6240],\n",
      "        [0.6697],\n",
      "        [0.6231],\n",
      "        [0.6434],\n",
      "        [0.7224],\n",
      "        [0.8836],\n",
      "        [0.6686],\n",
      "        [0.8014],\n",
      "        [0.5779],\n",
      "        [0.6730],\n",
      "        [0.6192],\n",
      "        [0.9572],\n",
      "        [0.9428],\n",
      "        [0.5895],\n",
      "        [0.8950],\n",
      "        [0.6286],\n",
      "        [0.5662],\n",
      "        [0.5771],\n",
      "        [0.6017],\n",
      "        [0.8526],\n",
      "        [0.6705],\n",
      "        [0.5958],\n",
      "        [0.6098],\n",
      "        [0.7949],\n",
      "        [0.9025],\n",
      "        [0.5943],\n",
      "        [0.6743],\n",
      "        [0.6652],\n",
      "        [0.8807],\n",
      "        [0.8114],\n",
      "        [0.6548],\n",
      "        [0.9214],\n",
      "        [0.5871],\n",
      "        [0.6785],\n",
      "        [0.5842],\n",
      "        [0.5520],\n",
      "        [0.6260],\n",
      "        [0.5581],\n",
      "        [0.9384],\n",
      "        [0.9175],\n",
      "        [0.8574],\n",
      "        [0.6711],\n",
      "        [0.6269],\n",
      "        [0.6012],\n",
      "        [0.6317],\n",
      "        [0.6055],\n",
      "        [0.5994],\n",
      "        [0.8684],\n",
      "        [0.5902],\n",
      "        [0.6215],\n",
      "        [0.6013],\n",
      "        [0.9161],\n",
      "        [0.8189],\n",
      "        [0.7378],\n",
      "        [0.6019],\n",
      "        [0.7125],\n",
      "        [0.5457],\n",
      "        [0.8855],\n",
      "        [0.5356],\n",
      "        [0.8896],\n",
      "        [0.5579],\n",
      "        [0.5362],\n",
      "        [0.8819],\n",
      "        [0.5858],\n",
      "        [0.5772],\n",
      "        [0.8587],\n",
      "        [0.9102],\n",
      "        [0.9398],\n",
      "        [0.8586],\n",
      "        [0.8802],\n",
      "        [0.8628],\n",
      "        [0.5974],\n",
      "        [0.5869],\n",
      "        [0.5910],\n",
      "        [0.8245],\n",
      "        [0.7408],\n",
      "        [0.8225],\n",
      "        [0.7917],\n",
      "        [0.8320],\n",
      "        [0.7591],\n",
      "        [0.7914],\n",
      "        [0.6007],\n",
      "        [0.8671],\n",
      "        [0.8375],\n",
      "        [0.6335]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0035],\n",
      "        [0.0039],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0059],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0136],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0166],\n",
      "        [0.0171],\n",
      "        [0.0174],\n",
      "        [0.0186],\n",
      "        [0.0192],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0196],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0211],\n",
      "        [0.0219],\n",
      "        [0.0227],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0264],\n",
      "        [0.0272],\n",
      "        [0.0273],\n",
      "        [0.0279],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0290],\n",
      "        [0.0303],\n",
      "        [0.0305],\n",
      "        [0.0307],\n",
      "        [0.0320],\n",
      "        [0.0321],\n",
      "        [0.0322],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0339],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0349],\n",
      "        [0.0352],\n",
      "        [0.0353],\n",
      "        [0.0359],\n",
      "        [0.0360],\n",
      "        [0.0362],\n",
      "        [0.0364],\n",
      "        [0.0371],\n",
      "        [0.0386],\n",
      "        [0.0387],\n",
      "        [0.0387],\n",
      "        [0.0400],\n",
      "        [0.0400],\n",
      "        [0.0406],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0434],\n",
      "        [0.0443],\n",
      "        [0.0445],\n",
      "        [0.0459],\n",
      "        [0.0461],\n",
      "        [0.0463],\n",
      "        [0.0479],\n",
      "        [0.0487],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0497],\n",
      "        [0.0507],\n",
      "        [0.0509],\n",
      "        [0.0510],\n",
      "        [0.0521],\n",
      "        [0.0530],\n",
      "        [0.0533],\n",
      "        [0.0559],\n",
      "        [0.0570],\n",
      "        [0.0578],\n",
      "        [0.0588],\n",
      "        [0.0591],\n",
      "        [0.0606],\n",
      "        [0.0610],\n",
      "        [0.0625],\n",
      "        [0.0628],\n",
      "        [0.0631],\n",
      "        [0.0639],\n",
      "        [0.0646],\n",
      "        [0.0649],\n",
      "        [0.0649],\n",
      "        [0.0654],\n",
      "        [0.0670],\n",
      "        [0.0672],\n",
      "        [0.0678],\n",
      "        [0.0684],\n",
      "        [0.0714],\n",
      "        [0.0728],\n",
      "        [0.0748],\n",
      "        [0.0748],\n",
      "        [0.0790],\n",
      "        [0.0812],\n",
      "        [0.0815],\n",
      "        [0.0815],\n",
      "        [0.0856],\n",
      "        [0.0860],\n",
      "        [0.0876],\n",
      "        [0.0891],\n",
      "        [0.0904],\n",
      "        [0.0945],\n",
      "        [0.0966],\n",
      "        [0.0966],\n",
      "        [0.0976],\n",
      "        [0.1008],\n",
      "        [0.1019],\n",
      "        [0.1054],\n",
      "        [0.1102],\n",
      "        [0.1130],\n",
      "        [0.1200],\n",
      "        [0.1206],\n",
      "        [0.1264],\n",
      "        [0.1305],\n",
      "        [0.1332],\n",
      "        [0.1364],\n",
      "        [0.1377],\n",
      "        [0.1422],\n",
      "        [0.1438],\n",
      "        [0.1592],\n",
      "        [0.1625],\n",
      "        [0.2131]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0010],\n",
      "        [    0.0034],\n",
      "        [    0.0040],\n",
      "        [    0.0006],\n",
      "        [    0.0017],\n",
      "        [    0.0065],\n",
      "        [    0.0013],\n",
      "        [    0.0042],\n",
      "        [    0.0038],\n",
      "        [    0.0037],\n",
      "        [    0.0050],\n",
      "        [    0.0012],\n",
      "        [    0.0035],\n",
      "        [    0.0050],\n",
      "        [    0.0051],\n",
      "        [    0.0067],\n",
      "        [    0.0071],\n",
      "        [    0.0087],\n",
      "        [    0.0105],\n",
      "        [    0.0079],\n",
      "        [    0.0070],\n",
      "        [    0.0089],\n",
      "        [    0.0087],\n",
      "        [    0.0095],\n",
      "        [    0.0115],\n",
      "        [    0.0095],\n",
      "        [    0.0122],\n",
      "        [    0.0115],\n",
      "        [    0.0152],\n",
      "        [    0.0128],\n",
      "        [    0.0158],\n",
      "        [    0.0155],\n",
      "        [    0.0149],\n",
      "        [    0.0181],\n",
      "        [    0.0184],\n",
      "        [    0.0165],\n",
      "        [    0.0174],\n",
      "        [    0.0181],\n",
      "        [    0.0158],\n",
      "        [    0.0188],\n",
      "        [    0.0198],\n",
      "        [    0.0216],\n",
      "        [    0.0200],\n",
      "        [    0.0218],\n",
      "        [    0.0208],\n",
      "        [    0.0232],\n",
      "        [    0.0217],\n",
      "        [    0.0224],\n",
      "        [    0.0239],\n",
      "        [    0.0271],\n",
      "        [    0.0258],\n",
      "        [    0.0275],\n",
      "        [    0.0301],\n",
      "        [    0.0270],\n",
      "        [    0.0290],\n",
      "        [    0.0289],\n",
      "        [    0.0286],\n",
      "        [    0.0286],\n",
      "        [    0.0299],\n",
      "        [    0.0312],\n",
      "        [    0.0301],\n",
      "        [    0.0345],\n",
      "        [    0.0348],\n",
      "        [    0.0327],\n",
      "        [    0.0331],\n",
      "        [    0.0338],\n",
      "        [    0.0350],\n",
      "        [    0.0299],\n",
      "        [    0.0343],\n",
      "        [    0.0347],\n",
      "        [    0.0352],\n",
      "        [    0.0323],\n",
      "        [    0.0372],\n",
      "        [    0.0382],\n",
      "        [    0.0385],\n",
      "        [    0.0376],\n",
      "        [    0.0374],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.0403],\n",
      "        [    0.0404],\n",
      "        [    0.0419],\n",
      "        [    0.0404],\n",
      "        [    0.0410],\n",
      "        [    0.0413],\n",
      "        [    0.0415],\n",
      "        [    0.0454],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0409],\n",
      "        [    0.0466],\n",
      "        [    0.0485],\n",
      "        [    0.0424],\n",
      "        [    0.0446],\n",
      "        [    0.0488],\n",
      "        [    0.0481],\n",
      "        [    0.0518],\n",
      "        [    0.0484],\n",
      "        [    0.0534],\n",
      "        [    0.0510],\n",
      "        [    0.0525],\n",
      "        [    0.0555],\n",
      "        [    0.0532],\n",
      "        [    0.0567],\n",
      "        [    0.0596],\n",
      "        [    0.0514],\n",
      "        [    0.0599],\n",
      "        [    0.0561],\n",
      "        [    0.0617],\n",
      "        [    0.0606],\n",
      "        [    0.0630],\n",
      "        [    0.0640],\n",
      "        [    0.0655],\n",
      "        [    0.0659],\n",
      "        [    0.0589],\n",
      "        [    0.0671],\n",
      "        [    0.0588],\n",
      "        [    0.0598],\n",
      "        [    0.0681],\n",
      "        [    0.0608],\n",
      "        [    0.0697],\n",
      "        [    0.0622],\n",
      "        [    0.0720],\n",
      "        [    0.0724],\n",
      "        [    0.0738],\n",
      "        [    0.0751],\n",
      "        [    0.0762],\n",
      "        [    0.0725],\n",
      "        [    0.0800],\n",
      "        [    0.0827],\n",
      "        [    0.0863],\n",
      "        [    0.0775],\n",
      "        [    0.0881],\n",
      "        [    0.0900],\n",
      "        [    0.0879],\n",
      "        [    0.0948],\n",
      "        [    0.0971],\n",
      "        [    0.0976],\n",
      "        [    0.0981],\n",
      "        [    0.1015],\n",
      "        [    0.1021],\n",
      "        [    0.1043],\n",
      "        [    0.1032],\n",
      "        [    0.1053],\n",
      "        [    0.1135],\n",
      "        [    0.1202],\n",
      "        [    0.1242],\n",
      "        [    0.1304],\n",
      "        [    0.1326],\n",
      "        [    0.1358],\n",
      "        [    0.1360],\n",
      "        [    0.1417],\n",
      "        [    0.1411],\n",
      "        [    0.1591],\n",
      "        [    0.1621],\n",
      "        [    0.2101]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.2455735206604\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.1486028007066125e-08, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [66, 96, 68, 143, 99, 74, 100, 97, 18, 50, 85, 22, 44, 10, 30, 83, 55, 98, 26, 63, 91, 29, 32, 51, 17, 121, 122, 33, 75, 77, 11, 90, 52, 129, 120, 27, 49, 110, 21, 111, 8, 36, 56, 53, 102, 16, 138, 54, 101, 9, 46, 31, 142, 124, 20, 15, 62, 136, 48, 144, 67, 113, 137, 114, 28, 64, 130, 34, 112, 65, 141, 23, 4, 95, 57, 76, 104, 105, 92, 35, 139, 12, 89, 146, 13, 131, 19, 103, 147, 145, 123, 109, 135, 25, 43, 84, 86, 73, 47, 148, 140, 37, 6, 93, 94, 115, 134, 149, 150, 87, 151, 128, 7, 153, 127, 152, 14, 61, 88, 107, 106, 58, 108, 3, 72, 158, 78, 132, 82, 157, 42, 126, 0, 119, 125, 24, 133, 60, 1, 2, 59, 5, 155, 41, 156, 154, 69, 81, 38, 71, 40, 80, 118, 70, 45, 39, 116, 117] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8357],\n",
      "        [0.6956],\n",
      "        [0.8177],\n",
      "        [0.5919],\n",
      "        [0.6610],\n",
      "        [0.7623],\n",
      "        [0.6319],\n",
      "        [0.6870],\n",
      "        [0.9104],\n",
      "        [0.8854],\n",
      "        [0.6571],\n",
      "        [0.8590],\n",
      "        [0.9068],\n",
      "        [0.9978],\n",
      "        [0.8547],\n",
      "        [0.6488],\n",
      "        [0.9029],\n",
      "        [0.6842],\n",
      "        [0.8244],\n",
      "        [0.8695],\n",
      "        [0.6827],\n",
      "        [0.8383],\n",
      "        [0.8184],\n",
      "        [0.8884],\n",
      "        [0.8985],\n",
      "        [0.5160],\n",
      "        [0.5400],\n",
      "        [0.8315],\n",
      "        [0.7341],\n",
      "        [0.7346],\n",
      "        [0.9940],\n",
      "        [0.6881],\n",
      "        [0.8990],\n",
      "        [0.5480],\n",
      "        [0.5272],\n",
      "        [0.8201],\n",
      "        [0.8350],\n",
      "        [0.6235],\n",
      "        [0.8714],\n",
      "        [0.6270],\n",
      "        [0.9579],\n",
      "        [0.8227],\n",
      "        [0.8859],\n",
      "        [0.8883],\n",
      "        [0.6195],\n",
      "        [0.9221],\n",
      "        [0.5851],\n",
      "        [0.9150],\n",
      "        [0.6242],\n",
      "        [0.9994],\n",
      "        [0.8691],\n",
      "        [0.8312],\n",
      "        [0.5801],\n",
      "        [0.5576],\n",
      "        [0.8873],\n",
      "        [0.9387],\n",
      "        [0.8544],\n",
      "        [0.6043],\n",
      "        [0.8761],\n",
      "        [0.5889],\n",
      "        [0.8237],\n",
      "        [0.6059],\n",
      "        [0.5974],\n",
      "        [0.6211],\n",
      "        [0.8165],\n",
      "        [0.8413],\n",
      "        [0.5469],\n",
      "        [0.8488],\n",
      "        [0.6019],\n",
      "        [0.8009],\n",
      "        [0.5929],\n",
      "        [0.8804],\n",
      "        [0.8937],\n",
      "        [0.6678],\n",
      "        [0.8825],\n",
      "        [0.7210],\n",
      "        [0.6208],\n",
      "        [0.6410],\n",
      "        [0.6665],\n",
      "        [0.8007],\n",
      "        [0.5763],\n",
      "        [0.9569],\n",
      "        [0.6713],\n",
      "        [0.5968],\n",
      "        [0.9424],\n",
      "        [0.5893],\n",
      "        [0.8951],\n",
      "        [0.6172],\n",
      "        [0.5903],\n",
      "        [0.6057],\n",
      "        [0.5656],\n",
      "        [0.6266],\n",
      "        [0.5761],\n",
      "        [0.8532],\n",
      "        [0.9041],\n",
      "        [0.6720],\n",
      "        [0.6683],\n",
      "        [0.7943],\n",
      "        [0.8807],\n",
      "        [0.5779],\n",
      "        [0.5921],\n",
      "        [0.8111],\n",
      "        [0.9216],\n",
      "        [0.6627],\n",
      "        [0.6523],\n",
      "        [0.6230],\n",
      "        [0.5863],\n",
      "        [0.5994],\n",
      "        [0.5955],\n",
      "        [0.6759],\n",
      "        [0.5938],\n",
      "        [0.5509],\n",
      "        [0.9388],\n",
      "        [0.5838],\n",
      "        [0.5570],\n",
      "        [0.5951],\n",
      "        [0.9170],\n",
      "        [0.8563],\n",
      "        [0.6687],\n",
      "        [0.6248],\n",
      "        [0.6295],\n",
      "        [0.8673],\n",
      "        [0.6196],\n",
      "        [0.9167],\n",
      "        [0.8185],\n",
      "        [0.5371],\n",
      "        [0.7368],\n",
      "        [0.6017],\n",
      "        [0.7097],\n",
      "        [0.5494],\n",
      "        [0.8869],\n",
      "        [0.5344],\n",
      "        [0.8902],\n",
      "        [0.5832],\n",
      "        [0.5358],\n",
      "        [0.8828],\n",
      "        [0.5769],\n",
      "        [0.8581],\n",
      "        [0.9111],\n",
      "        [0.9403],\n",
      "        [0.8579],\n",
      "        [0.8800],\n",
      "        [0.5904],\n",
      "        [0.8639],\n",
      "        [0.5792],\n",
      "        [0.5844],\n",
      "        [0.8241],\n",
      "        [0.7385],\n",
      "        [0.8226],\n",
      "        [0.7911],\n",
      "        [0.8326],\n",
      "        [0.7574],\n",
      "        [0.5979],\n",
      "        [0.7909],\n",
      "        [0.8670],\n",
      "        [0.8379],\n",
      "        [0.6306],\n",
      "        [0.6349]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0010],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0017],\n",
      "        [    0.0034],\n",
      "        [    0.0035],\n",
      "        [    0.0037],\n",
      "        [    0.0038],\n",
      "        [    0.0040],\n",
      "        [    0.0042],\n",
      "        [    0.0050],\n",
      "        [    0.0050],\n",
      "        [    0.0051],\n",
      "        [    0.0065],\n",
      "        [    0.0067],\n",
      "        [    0.0070],\n",
      "        [    0.0071],\n",
      "        [    0.0079],\n",
      "        [    0.0087],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0095],\n",
      "        [    0.0095],\n",
      "        [    0.0105],\n",
      "        [    0.0115],\n",
      "        [    0.0115],\n",
      "        [    0.0122],\n",
      "        [    0.0128],\n",
      "        [    0.0149],\n",
      "        [    0.0152],\n",
      "        [    0.0155],\n",
      "        [    0.0158],\n",
      "        [    0.0158],\n",
      "        [    0.0165],\n",
      "        [    0.0174],\n",
      "        [    0.0181],\n",
      "        [    0.0181],\n",
      "        [    0.0184],\n",
      "        [    0.0188],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0208],\n",
      "        [    0.0216],\n",
      "        [    0.0217],\n",
      "        [    0.0218],\n",
      "        [    0.0224],\n",
      "        [    0.0232],\n",
      "        [    0.0239],\n",
      "        [    0.0258],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0275],\n",
      "        [    0.0286],\n",
      "        [    0.0286],\n",
      "        [    0.0289],\n",
      "        [    0.0290],\n",
      "        [    0.0299],\n",
      "        [    0.0299],\n",
      "        [    0.0301],\n",
      "        [    0.0301],\n",
      "        [    0.0312],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0331],\n",
      "        [    0.0338],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0347],\n",
      "        [    0.0348],\n",
      "        [    0.0350],\n",
      "        [    0.0352],\n",
      "        [    0.0372],\n",
      "        [    0.0374],\n",
      "        [    0.0376],\n",
      "        [    0.0382],\n",
      "        [    0.0385],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.0403],\n",
      "        [    0.0404],\n",
      "        [    0.0404],\n",
      "        [    0.0409],\n",
      "        [    0.0410],\n",
      "        [    0.0413],\n",
      "        [    0.0415],\n",
      "        [    0.0419],\n",
      "        [    0.0424],\n",
      "        [    0.0446],\n",
      "        [    0.0449],\n",
      "        [    0.0454],\n",
      "        [    0.0455],\n",
      "        [    0.0466],\n",
      "        [    0.0481],\n",
      "        [    0.0484],\n",
      "        [    0.0485],\n",
      "        [    0.0488],\n",
      "        [    0.0510],\n",
      "        [    0.0514],\n",
      "        [    0.0518],\n",
      "        [    0.0525],\n",
      "        [    0.0532],\n",
      "        [    0.0534],\n",
      "        [    0.0555],\n",
      "        [    0.0561],\n",
      "        [    0.0567],\n",
      "        [    0.0588],\n",
      "        [    0.0589],\n",
      "        [    0.0596],\n",
      "        [    0.0598],\n",
      "        [    0.0599],\n",
      "        [    0.0606],\n",
      "        [    0.0608],\n",
      "        [    0.0617],\n",
      "        [    0.0622],\n",
      "        [    0.0630],\n",
      "        [    0.0640],\n",
      "        [    0.0655],\n",
      "        [    0.0659],\n",
      "        [    0.0671],\n",
      "        [    0.0681],\n",
      "        [    0.0697],\n",
      "        [    0.0720],\n",
      "        [    0.0724],\n",
      "        [    0.0725],\n",
      "        [    0.0738],\n",
      "        [    0.0751],\n",
      "        [    0.0762],\n",
      "        [    0.0775],\n",
      "        [    0.0800],\n",
      "        [    0.0827],\n",
      "        [    0.0863],\n",
      "        [    0.0879],\n",
      "        [    0.0881],\n",
      "        [    0.0900],\n",
      "        [    0.0948],\n",
      "        [    0.0971],\n",
      "        [    0.0976],\n",
      "        [    0.0981],\n",
      "        [    0.1015],\n",
      "        [    0.1021],\n",
      "        [    0.1032],\n",
      "        [    0.1043],\n",
      "        [    0.1053],\n",
      "        [    0.1135],\n",
      "        [    0.1202],\n",
      "        [    0.1242],\n",
      "        [    0.1304],\n",
      "        [    0.1326],\n",
      "        [    0.1358],\n",
      "        [    0.1360],\n",
      "        [    0.1411],\n",
      "        [    0.1417],\n",
      "        [    0.1591],\n",
      "        [    0.1621],\n",
      "        [    0.2101],\n",
      "        [    0.2112]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0029],\n",
      "        [    0.0009],\n",
      "        [    0.0034],\n",
      "        [    0.0003],\n",
      "        [    0.0014],\n",
      "        [    0.0036],\n",
      "        [    0.0036],\n",
      "        [    0.0039],\n",
      "        [    0.0000],\n",
      "        [    0.0007],\n",
      "        [    0.0038],\n",
      "        [    0.0079],\n",
      "        [    0.0089],\n",
      "        [    0.0087],\n",
      "        [    0.0074],\n",
      "        [    0.0079],\n",
      "        [    0.0033],\n",
      "        [    0.0075],\n",
      "        [    0.0044],\n",
      "        [    0.0107],\n",
      "        [    0.0082],\n",
      "        [    0.0112],\n",
      "        [    0.0117],\n",
      "        [    0.0061],\n",
      "        [    0.0126],\n",
      "        [    0.0116],\n",
      "        [    0.0115],\n",
      "        [    0.0144],\n",
      "        [    0.0105],\n",
      "        [    0.0139],\n",
      "        [    0.0183],\n",
      "        [    0.0148],\n",
      "        [    0.0190],\n",
      "        [    0.0151],\n",
      "        [    0.0146],\n",
      "        [    0.0193],\n",
      "        [    0.0148],\n",
      "        [    0.0187],\n",
      "        [    0.0216],\n",
      "        [    0.0190],\n",
      "        [    0.0150],\n",
      "        [    0.0174],\n",
      "        [    0.0172],\n",
      "        [    0.0241],\n",
      "        [    0.0220],\n",
      "        [    0.0182],\n",
      "        [    0.0210],\n",
      "        [    0.0254],\n",
      "        [    0.0237],\n",
      "        [    0.0281],\n",
      "        [    0.0285],\n",
      "        [    0.0291],\n",
      "        [    0.0282],\n",
      "        [    0.0261],\n",
      "        [    0.0251],\n",
      "        [    0.0251],\n",
      "        [    0.0266],\n",
      "        [    0.0279],\n",
      "        [    0.0327],\n",
      "        [    0.0281],\n",
      "        [    0.0324],\n",
      "        [    0.0314],\n",
      "        [    0.0300],\n",
      "        [    0.0311],\n",
      "        [    0.0302],\n",
      "        [    0.0305],\n",
      "        [    0.0326],\n",
      "        [    0.0373],\n",
      "        [    0.0353],\n",
      "        [    0.0322],\n",
      "        [    0.0353],\n",
      "        [    0.0390],\n",
      "        [    0.0325],\n",
      "        [    0.0372],\n",
      "        [    0.0352],\n",
      "        [    0.0365],\n",
      "        [    0.0386],\n",
      "        [    0.0391],\n",
      "        [    0.0392],\n",
      "        [    0.0371],\n",
      "        [    0.0398],\n",
      "        [    0.0371],\n",
      "        [    0.0401],\n",
      "        [    0.0385],\n",
      "        [    0.0378],\n",
      "        [    0.0396],\n",
      "        [    0.0381],\n",
      "        [    0.0421],\n",
      "        [    0.0395],\n",
      "        [    0.0430],\n",
      "        [    0.0439],\n",
      "        [    0.0458],\n",
      "        [    0.0445],\n",
      "        [    0.0500],\n",
      "        [    0.0434],\n",
      "        [    0.0483],\n",
      "        [    0.0486],\n",
      "        [    0.0509],\n",
      "        [    0.0542],\n",
      "        [    0.0477],\n",
      "        [    0.0518],\n",
      "        [    0.0499],\n",
      "        [    0.0498],\n",
      "        [    0.0538],\n",
      "        [    0.0559],\n",
      "        [    0.0549],\n",
      "        [    0.0555],\n",
      "        [    0.0554],\n",
      "        [    0.0557],\n",
      "        [    0.0601],\n",
      "        [    0.0567],\n",
      "        [    0.0591],\n",
      "        [    0.0568],\n",
      "        [    0.0568],\n",
      "        [    0.0610],\n",
      "        [    0.0584],\n",
      "        [    0.0600],\n",
      "        [    0.0620],\n",
      "        [    0.0659],\n",
      "        [    0.0663],\n",
      "        [    0.0676],\n",
      "        [    0.0660],\n",
      "        [    0.0700],\n",
      "        [    0.0756],\n",
      "        [    0.0748],\n",
      "        [    0.0663],\n",
      "        [    0.0752],\n",
      "        [    0.0733],\n",
      "        [    0.0758],\n",
      "        [    0.0715],\n",
      "        [    0.0755],\n",
      "        [    0.0823],\n",
      "        [    0.0899],\n",
      "        [    0.0870],\n",
      "        [    0.0869],\n",
      "        [    0.0938],\n",
      "        [    0.0932],\n",
      "        [    0.0946],\n",
      "        [    0.1016],\n",
      "        [    0.1019],\n",
      "        [    0.0991],\n",
      "        [    0.0993],\n",
      "        [    0.0986],\n",
      "        [    0.1002],\n",
      "        [    0.1001],\n",
      "        [    0.1094],\n",
      "        [    0.1226],\n",
      "        [    0.1243],\n",
      "        [    0.1274],\n",
      "        [    0.1348],\n",
      "        [    0.1323],\n",
      "        [    0.1368],\n",
      "        [    0.1399],\n",
      "        [    0.1439],\n",
      "        [    0.1620],\n",
      "        [    0.1587],\n",
      "        [    0.2090],\n",
      "        [    0.2104]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.529439210891724\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.78985781330266e-11, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [18, 143, 50, 96, 99, 66, 55, 68, 74, 100, 85, 97, 26, 51, 30, 98, 83, 22, 91, 10, 44, 75, 63, 29, 122, 121, 32, 17, 77, 33, 120, 49, 90, 8, 129, 56, 36, 16, 11, 110, 111, 52, 27, 138, 21, 102, 101, 53, 15, 20, 54, 124, 62, 136, 144, 9, 142, 46, 31, 137, 28, 64, 114, 113, 65, 67, 4, 130, 48, 57, 141, 112, 76, 35, 12, 95, 34, 13, 19, 146, 104, 23, 105, 92, 147, 131, 139, 89, 103, 145, 43, 123, 135, 109, 148, 84, 86, 6, 37, 25, 73, 140, 93, 47, 115, 149, 134, 150, 94, 151, 153, 7, 152, 128, 14, 87, 127, 61, 88, 58, 158, 107, 106, 108, 157, 132, 72, 78, 42, 3, 82, 126, 125, 119, 0, 133, 24, 60, 155, 59, 5, 156, 41, 1, 2, 154, 69, 81, 38, 40, 71, 80, 118, 70, 39, 45, 116, 117, 79] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9141],\n",
      "        [0.5904],\n",
      "        [0.8885],\n",
      "        [0.6959],\n",
      "        [0.6611],\n",
      "        [0.8384],\n",
      "        [0.9063],\n",
      "        [0.8201],\n",
      "        [0.7642],\n",
      "        [0.6317],\n",
      "        [0.6573],\n",
      "        [0.6874],\n",
      "        [0.8271],\n",
      "        [0.8917],\n",
      "        [0.8570],\n",
      "        [0.6847],\n",
      "        [0.6474],\n",
      "        [0.8627],\n",
      "        [0.6833],\n",
      "        [1.0014],\n",
      "        [0.9108],\n",
      "        [0.7358],\n",
      "        [0.8723],\n",
      "        [0.8408],\n",
      "        [0.5399],\n",
      "        [0.5149],\n",
      "        [0.8211],\n",
      "        [0.9015],\n",
      "        [0.7357],\n",
      "        [0.8344],\n",
      "        [0.5259],\n",
      "        [0.8376],\n",
      "        [0.6886],\n",
      "        [0.9617],\n",
      "        [0.5487],\n",
      "        [0.8887],\n",
      "        [0.8251],\n",
      "        [0.9256],\n",
      "        [0.9973],\n",
      "        [0.6229],\n",
      "        [0.6264],\n",
      "        [0.9025],\n",
      "        [0.8229],\n",
      "        [0.5860],\n",
      "        [0.8750],\n",
      "        [0.6192],\n",
      "        [0.6238],\n",
      "        [0.8915],\n",
      "        [0.9422],\n",
      "        [0.8908],\n",
      "        [0.9179],\n",
      "        [0.5590],\n",
      "        [0.8567],\n",
      "        [0.6054],\n",
      "        [0.5870],\n",
      "        [1.0036],\n",
      "        [0.5790],\n",
      "        [0.8718],\n",
      "        [0.8333],\n",
      "        [0.5987],\n",
      "        [0.8190],\n",
      "        [0.8438],\n",
      "        [0.6199],\n",
      "        [0.6046],\n",
      "        [0.8034],\n",
      "        [0.8260],\n",
      "        [0.8965],\n",
      "        [0.5481],\n",
      "        [0.8790],\n",
      "        [0.8848],\n",
      "        [0.5925],\n",
      "        [0.6011],\n",
      "        [0.7221],\n",
      "        [0.8030],\n",
      "        [0.9601],\n",
      "        [0.6678],\n",
      "        [0.8518],\n",
      "        [0.9455],\n",
      "        [0.8986],\n",
      "        [0.5943],\n",
      "        [0.6203],\n",
      "        [0.8845],\n",
      "        [0.6404],\n",
      "        [0.6664],\n",
      "        [0.5874],\n",
      "        [0.5910],\n",
      "        [0.5768],\n",
      "        [0.6716],\n",
      "        [0.6170],\n",
      "        [0.6041],\n",
      "        [0.9088],\n",
      "        [0.5666],\n",
      "        [0.5770],\n",
      "        [0.6262],\n",
      "        [0.5742],\n",
      "        [0.6719],\n",
      "        [0.6682],\n",
      "        [0.9250],\n",
      "        [0.8137],\n",
      "        [0.8566],\n",
      "        [0.7964],\n",
      "        [0.5922],\n",
      "        [0.6622],\n",
      "        [0.8839],\n",
      "        [0.6218],\n",
      "        [0.5959],\n",
      "        [0.5875],\n",
      "        [0.5923],\n",
      "        [0.6518],\n",
      "        [0.5907],\n",
      "        [0.5798],\n",
      "        [0.9426],\n",
      "        [0.5914],\n",
      "        [0.5516],\n",
      "        [0.9201],\n",
      "        [0.6754],\n",
      "        [0.5578],\n",
      "        [0.8583],\n",
      "        [0.6684],\n",
      "        [0.8694],\n",
      "        [0.5308],\n",
      "        [0.6244],\n",
      "        [0.6290],\n",
      "        [0.6193],\n",
      "        [0.5434],\n",
      "        [0.6034],\n",
      "        [0.8209],\n",
      "        [0.7382],\n",
      "        [0.8915],\n",
      "        [0.9203],\n",
      "        [0.7093],\n",
      "        [0.5348],\n",
      "        [0.5369],\n",
      "        [0.5823],\n",
      "        [0.8938],\n",
      "        [0.5785],\n",
      "        [0.8867],\n",
      "        [0.8606],\n",
      "        [0.5858],\n",
      "        [0.8603],\n",
      "        [0.8829],\n",
      "        [0.5741],\n",
      "        [0.8679],\n",
      "        [0.9151],\n",
      "        [0.9440],\n",
      "        [0.5803],\n",
      "        [0.8265],\n",
      "        [0.7386],\n",
      "        [0.8256],\n",
      "        [0.8361],\n",
      "        [0.7933],\n",
      "        [0.7581],\n",
      "        [0.5968],\n",
      "        [0.7931],\n",
      "        [0.8413],\n",
      "        [0.8699],\n",
      "        [0.6295],\n",
      "        [0.6341],\n",
      "        [0.7454]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0014],\n",
      "        [    0.0029],\n",
      "        [    0.0033],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0036],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0044],\n",
      "        [    0.0061],\n",
      "        [    0.0074],\n",
      "        [    0.0075],\n",
      "        [    0.0079],\n",
      "        [    0.0079],\n",
      "        [    0.0082],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0105],\n",
      "        [    0.0107],\n",
      "        [    0.0112],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0117],\n",
      "        [    0.0126],\n",
      "        [    0.0139],\n",
      "        [    0.0144],\n",
      "        [    0.0146],\n",
      "        [    0.0148],\n",
      "        [    0.0148],\n",
      "        [    0.0150],\n",
      "        [    0.0151],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0182],\n",
      "        [    0.0183],\n",
      "        [    0.0187],\n",
      "        [    0.0190],\n",
      "        [    0.0190],\n",
      "        [    0.0193],\n",
      "        [    0.0210],\n",
      "        [    0.0216],\n",
      "        [    0.0220],\n",
      "        [    0.0237],\n",
      "        [    0.0241],\n",
      "        [    0.0251],\n",
      "        [    0.0251],\n",
      "        [    0.0254],\n",
      "        [    0.0261],\n",
      "        [    0.0266],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0281],\n",
      "        [    0.0282],\n",
      "        [    0.0285],\n",
      "        [    0.0291],\n",
      "        [    0.0300],\n",
      "        [    0.0302],\n",
      "        [    0.0305],\n",
      "        [    0.0311],\n",
      "        [    0.0314],\n",
      "        [    0.0322],\n",
      "        [    0.0324],\n",
      "        [    0.0325],\n",
      "        [    0.0326],\n",
      "        [    0.0327],\n",
      "        [    0.0352],\n",
      "        [    0.0353],\n",
      "        [    0.0353],\n",
      "        [    0.0365],\n",
      "        [    0.0371],\n",
      "        [    0.0371],\n",
      "        [    0.0372],\n",
      "        [    0.0373],\n",
      "        [    0.0378],\n",
      "        [    0.0381],\n",
      "        [    0.0385],\n",
      "        [    0.0386],\n",
      "        [    0.0390],\n",
      "        [    0.0391],\n",
      "        [    0.0392],\n",
      "        [    0.0395],\n",
      "        [    0.0396],\n",
      "        [    0.0398],\n",
      "        [    0.0401],\n",
      "        [    0.0421],\n",
      "        [    0.0430],\n",
      "        [    0.0434],\n",
      "        [    0.0439],\n",
      "        [    0.0445],\n",
      "        [    0.0458],\n",
      "        [    0.0477],\n",
      "        [    0.0483],\n",
      "        [    0.0486],\n",
      "        [    0.0498],\n",
      "        [    0.0499],\n",
      "        [    0.0500],\n",
      "        [    0.0509],\n",
      "        [    0.0518],\n",
      "        [    0.0538],\n",
      "        [    0.0542],\n",
      "        [    0.0549],\n",
      "        [    0.0554],\n",
      "        [    0.0555],\n",
      "        [    0.0557],\n",
      "        [    0.0559],\n",
      "        [    0.0567],\n",
      "        [    0.0568],\n",
      "        [    0.0568],\n",
      "        [    0.0584],\n",
      "        [    0.0591],\n",
      "        [    0.0600],\n",
      "        [    0.0601],\n",
      "        [    0.0610],\n",
      "        [    0.0620],\n",
      "        [    0.0659],\n",
      "        [    0.0660],\n",
      "        [    0.0663],\n",
      "        [    0.0663],\n",
      "        [    0.0676],\n",
      "        [    0.0700],\n",
      "        [    0.0715],\n",
      "        [    0.0733],\n",
      "        [    0.0748],\n",
      "        [    0.0752],\n",
      "        [    0.0755],\n",
      "        [    0.0756],\n",
      "        [    0.0758],\n",
      "        [    0.0823],\n",
      "        [    0.0869],\n",
      "        [    0.0870],\n",
      "        [    0.0899],\n",
      "        [    0.0932],\n",
      "        [    0.0938],\n",
      "        [    0.0946],\n",
      "        [    0.0986],\n",
      "        [    0.0991],\n",
      "        [    0.0993],\n",
      "        [    0.1001],\n",
      "        [    0.1002],\n",
      "        [    0.1016],\n",
      "        [    0.1019],\n",
      "        [    0.1094],\n",
      "        [    0.1226],\n",
      "        [    0.1243],\n",
      "        [    0.1274],\n",
      "        [    0.1323],\n",
      "        [    0.1348],\n",
      "        [    0.1368],\n",
      "        [    0.1399],\n",
      "        [    0.1439],\n",
      "        [    0.1587],\n",
      "        [    0.1620],\n",
      "        [    0.2090],\n",
      "        [    0.2104],\n",
      "        [    0.2843]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0035],\n",
      "        [0.0033],\n",
      "        [0.0048],\n",
      "        [0.0054],\n",
      "        [0.0043],\n",
      "        [0.0016],\n",
      "        [0.0076],\n",
      "        [0.0013],\n",
      "        [0.0023],\n",
      "        [0.0092],\n",
      "        [0.0098],\n",
      "        [0.0022],\n",
      "        [0.0077],\n",
      "        [0.0100],\n",
      "        [0.0032],\n",
      "        [0.0016],\n",
      "        [0.0153],\n",
      "        [0.0051],\n",
      "        [0.0140],\n",
      "        [0.0037],\n",
      "        [0.0053],\n",
      "        [0.0163],\n",
      "        [0.0056],\n",
      "        [0.0075],\n",
      "        [0.0135],\n",
      "        [0.0147],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0074],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0186],\n",
      "        [0.0208],\n",
      "        [0.0194],\n",
      "        [0.0159],\n",
      "        [0.0219],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0130],\n",
      "        [0.0235],\n",
      "        [0.0239],\n",
      "        [0.0150],\n",
      "        [0.0160],\n",
      "        [0.0218],\n",
      "        [0.0186],\n",
      "        [0.0273],\n",
      "        [0.0293],\n",
      "        [0.0198],\n",
      "        [0.0293],\n",
      "        [0.0284],\n",
      "        [0.0204],\n",
      "        [0.0267],\n",
      "        [0.0319],\n",
      "        [0.0287],\n",
      "        [0.0246],\n",
      "        [0.0235],\n",
      "        [0.0308],\n",
      "        [0.0243],\n",
      "        [0.0248],\n",
      "        [0.0304],\n",
      "        [0.0337],\n",
      "        [0.0354],\n",
      "        [0.0259],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0276],\n",
      "        [0.0371],\n",
      "        [0.0328],\n",
      "        [0.0285],\n",
      "        [0.0403],\n",
      "        [0.0374],\n",
      "        [0.0399],\n",
      "        [0.0428],\n",
      "        [0.0411],\n",
      "        [0.0418],\n",
      "        [0.0432],\n",
      "        [0.0335],\n",
      "        [0.0425],\n",
      "        [0.0416],\n",
      "        [0.0343],\n",
      "        [0.0439],\n",
      "        [0.0362],\n",
      "        [0.0447],\n",
      "        [0.0455],\n",
      "        [0.0349],\n",
      "        [0.0398],\n",
      "        [0.0408],\n",
      "        [0.0462],\n",
      "        [0.0472],\n",
      "        [0.0394],\n",
      "        [0.0464],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0506],\n",
      "        [0.0425],\n",
      "        [0.0419],\n",
      "        [0.0553],\n",
      "        [0.0541],\n",
      "        [0.0538],\n",
      "        [0.0470],\n",
      "        [0.0449],\n",
      "        [0.0534],\n",
      "        [0.0603],\n",
      "        [0.0503],\n",
      "        [0.0499],\n",
      "        [0.0500],\n",
      "        [0.0559],\n",
      "        [0.0507],\n",
      "        [0.0622],\n",
      "        [0.0516],\n",
      "        [0.0511],\n",
      "        [0.0609],\n",
      "        [0.0527],\n",
      "        [0.0601],\n",
      "        [0.0643],\n",
      "        [0.0671],\n",
      "        [0.0621],\n",
      "        [0.0674],\n",
      "        [0.0726],\n",
      "        [0.0711],\n",
      "        [0.0587],\n",
      "        [0.0713],\n",
      "        [0.0728],\n",
      "        [0.0747],\n",
      "        [0.0639],\n",
      "        [0.0735],\n",
      "        [0.0689],\n",
      "        [0.0689],\n",
      "        [0.0784],\n",
      "        [0.0716],\n",
      "        [0.0682],\n",
      "        [0.0835],\n",
      "        [0.0875],\n",
      "        [0.0829],\n",
      "        [0.0861],\n",
      "        [0.0931],\n",
      "        [0.0908],\n",
      "        [0.0995],\n",
      "        [0.0920],\n",
      "        [0.1040],\n",
      "        [0.1035],\n",
      "        [0.0930],\n",
      "        [0.1033],\n",
      "        [0.0979],\n",
      "        [0.0976],\n",
      "        [0.1034],\n",
      "        [0.1175],\n",
      "        [0.1168],\n",
      "        [0.1310],\n",
      "        [0.1356],\n",
      "        [0.1292],\n",
      "        [0.1296],\n",
      "        [0.1354],\n",
      "        [0.1386],\n",
      "        [0.1622],\n",
      "        [0.1580],\n",
      "        [0.2041],\n",
      "        [0.2059],\n",
      "        [0.2776]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.81415271759033\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 1 個區塊累積花費時間(s) 39.81357932090759\n",
      "<<The performance of 1 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 39.81357932090759\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1271.56\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1682.84\n",
      "The accuracy(2000) for l = 1: 81.76%\n",
      "The accuracy(3000) for l = 1: 92.45%\n",
      "The maximum error: tensor(7086.0430)\n",
      "The minimum error: tensor(33.6719)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 5659.9\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 5997.5\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 25.0%\n",
      "The maximum error: 8048.015625\n",
      "The minimum error: 2618.62109375\n",
      "------------------------------------------------------------\n",
      "0.8176100628930818\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<2>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7394232827427913e-06, 64)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [64, 62, 94, 93, 70, 26, 139, 14, 6, 95, 46, 18, 40, 92, 59, 73, 25, 51, 22, 28, 13, 96, 81, 47, 29, 116, 7, 118, 87, 117, 48, 79, 125, 23, 71, 17, 45, 4, 49, 50, 86, 32, 134, 52, 12, 5, 106, 107, 42, 140, 27, 110, 120, 98, 63, 16, 44, 132, 97, 11, 133, 138, 58, 126, 30, 24, 142, 143, 60, 19, 109, 61, 0, 137, 141, 127, 108, 53, 135, 31, 15, 8, 80, 9, 144, 72, 91, 100, 101, 69, 119, 131, 88, 85, 39, 21, 99, 111, 145, 43, 105, 146, 149, 147, 148, 136, 33, 2, 82, 130, 154, 124, 89, 3, 123, 90, 153, 10, 83, 57, 78, 68, 74, 54, 103, 84, 102, 128, 104, 38, 115, 122, 121, 20, 151, 152, 129, 56, 155, 37, 150, 1, 55, 77, 65, 67, 76, 34, 114, 36, 66, 41, 35, 112, 113]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0043],\n",
      "        [0.0048],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0074],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0186],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0239],\n",
      "        [0.0243],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0259],\n",
      "        [0.0267],\n",
      "        [0.0273],\n",
      "        [0.0276],\n",
      "        [0.0284],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0304],\n",
      "        [0.0308],\n",
      "        [0.0319],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.0349],\n",
      "        [0.0354],\n",
      "        [0.0362],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0371],\n",
      "        [0.0374],\n",
      "        [0.0394],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0403],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0416],\n",
      "        [0.0418],\n",
      "        [0.0419],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0432],\n",
      "        [0.0439],\n",
      "        [0.0447],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0455],\n",
      "        [0.0462],\n",
      "        [0.0464],\n",
      "        [0.0470],\n",
      "        [0.0472],\n",
      "        [0.0499],\n",
      "        [0.0500],\n",
      "        [0.0503],\n",
      "        [0.0506],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0516],\n",
      "        [0.0527],\n",
      "        [0.0534],\n",
      "        [0.0538],\n",
      "        [0.0541],\n",
      "        [0.0553],\n",
      "        [0.0559],\n",
      "        [0.0587],\n",
      "        [0.0601],\n",
      "        [0.0603],\n",
      "        [0.0609],\n",
      "        [0.0621],\n",
      "        [0.0622],\n",
      "        [0.0639],\n",
      "        [0.0643],\n",
      "        [0.0671],\n",
      "        [0.0674],\n",
      "        [0.0682],\n",
      "        [0.0689],\n",
      "        [0.0689],\n",
      "        [0.0711],\n",
      "        [0.0713],\n",
      "        [0.0726],\n",
      "        [0.0728],\n",
      "        [0.0735],\n",
      "        [0.0747],\n",
      "        [0.0784],\n",
      "        [0.0829],\n",
      "        [0.0835],\n",
      "        [0.0875],\n",
      "        [0.0908],\n",
      "        [0.0920],\n",
      "        [0.0930],\n",
      "        [0.0931],\n",
      "        [0.0995],\n",
      "        [0.1026],\n",
      "        [0.1033],\n",
      "        [0.1034],\n",
      "        [0.1035],\n",
      "        [0.1040],\n",
      "        [0.1168],\n",
      "        [0.1175],\n",
      "        [0.1292],\n",
      "        [0.1296],\n",
      "        [0.1310],\n",
      "        [0.1354],\n",
      "        [0.1356],\n",
      "        [0.1386],\n",
      "        [0.1580],\n",
      "        [0.1622],\n",
      "        [0.2041],\n",
      "        [0.2059]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7394232827427913e-06, 64)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [64, 62, 94, 93, 70, 26, 139, 14, 6, 95, 46, 18, 40, 92, 59, 73, 25, 51, 22, 28, 13, 96, 81, 47, 29, 116, 7, 118, 87, 117, 48, 79, 125, 23, 71, 17, 45, 4, 49, 50, 86, 32, 134, 52, 12, 5, 106, 107, 42, 140, 27, 110, 120, 98, 63, 16, 44, 132, 97, 11, 133, 138, 58, 126, 30, 24, 142, 143, 60, 19, 109, 61, 0, 137, 141, 127, 108, 53, 135, 31, 15, 8, 80, 9, 144, 72, 91, 100, 101, 69, 119, 131, 88, 85, 39, 21, 99, 111, 145, 43, 105, 146, 149, 147, 148, 136, 33, 2, 82, 130, 154, 124, 89, 3, 123, 90, 153, 10, 83, 57, 78, 68, 74, 54, 103, 84, 102, 128, 104, 38, 115, 122, 121, 20, 151, 152, 129, 56, 155, 37, 150, 1, 55, 77, 65, 67, 76, 34, 114, 36, 66, 41, 35, 112, 113, 156] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8153],\n",
      "        [0.8339],\n",
      "        [0.6788],\n",
      "        [0.6814],\n",
      "        [0.7583],\n",
      "        [0.8528],\n",
      "        [0.5873],\n",
      "        [0.9106],\n",
      "        [0.9964],\n",
      "        [0.6554],\n",
      "        [0.8844],\n",
      "        [0.8599],\n",
      "        [0.9071],\n",
      "        [0.6897],\n",
      "        [0.8673],\n",
      "        [0.7291],\n",
      "        [0.8371],\n",
      "        [0.9020],\n",
      "        [0.8238],\n",
      "        [0.8176],\n",
      "        [0.8974],\n",
      "        [0.6261],\n",
      "        [0.6512],\n",
      "        [0.8878],\n",
      "        [0.8308],\n",
      "        [0.5224],\n",
      "        [0.9921],\n",
      "        [0.5379],\n",
      "        [0.6774],\n",
      "        [0.5118],\n",
      "        [0.8985],\n",
      "        [0.6400],\n",
      "        [0.5478],\n",
      "        [0.8197],\n",
      "        [0.7300],\n",
      "        [0.8720],\n",
      "        [0.8338],\n",
      "        [0.9572],\n",
      "        [0.8872],\n",
      "        [0.9129],\n",
      "        [0.6826],\n",
      "        [0.8209],\n",
      "        [0.5852],\n",
      "        [0.8840],\n",
      "        [0.9216],\n",
      "        [0.9990],\n",
      "        [0.6180],\n",
      "        [0.6215],\n",
      "        [0.8676],\n",
      "        [0.5836],\n",
      "        [0.8291],\n",
      "        [0.6148],\n",
      "        [0.5584],\n",
      "        [0.6138],\n",
      "        [0.8213],\n",
      "        [0.8875],\n",
      "        [0.8748],\n",
      "        [0.6046],\n",
      "        [0.6182],\n",
      "        [0.9380],\n",
      "        [0.5982],\n",
      "        [0.5765],\n",
      "        [0.8514],\n",
      "        [0.5478],\n",
      "        [0.8479],\n",
      "        [0.8155],\n",
      "        [0.5901],\n",
      "        [0.5829],\n",
      "        [0.8390],\n",
      "        [0.8817],\n",
      "        [0.5996],\n",
      "        [0.7991],\n",
      "        [0.8919],\n",
      "        [0.5904],\n",
      "        [0.6005],\n",
      "        [0.5908],\n",
      "        [0.5965],\n",
      "        [0.8796],\n",
      "        [0.5758],\n",
      "        [0.7989],\n",
      "        [0.8951],\n",
      "        [0.9555],\n",
      "        [0.6655],\n",
      "        [0.9409],\n",
      "        [0.5689],\n",
      "        [0.7158],\n",
      "        [0.6618],\n",
      "        [0.6151],\n",
      "        [0.6347],\n",
      "        [0.7904],\n",
      "        [0.5656],\n",
      "        [0.5765],\n",
      "        [0.6601],\n",
      "        [0.6655],\n",
      "        [0.9058],\n",
      "        [0.8536],\n",
      "        [0.6120],\n",
      "        [0.6168],\n",
      "        [0.5906],\n",
      "        [0.8800],\n",
      "        [0.6215],\n",
      "        [0.5873],\n",
      "        [0.5741],\n",
      "        [0.5856],\n",
      "        [0.5857],\n",
      "        [0.5906],\n",
      "        [0.8098],\n",
      "        [0.9206],\n",
      "        [0.6615],\n",
      "        [0.5871],\n",
      "        [0.5232],\n",
      "        [0.5507],\n",
      "        [0.6558],\n",
      "        [0.9385],\n",
      "        [0.5566],\n",
      "        [0.6456],\n",
      "        [0.5358],\n",
      "        [0.9157],\n",
      "        [0.6683],\n",
      "        [0.8529],\n",
      "        [0.7017],\n",
      "        [0.8150],\n",
      "        [0.7319],\n",
      "        [0.8644],\n",
      "        [0.6194],\n",
      "        [0.6616],\n",
      "        [0.6238],\n",
      "        [0.6032],\n",
      "        [0.6145],\n",
      "        [0.8886],\n",
      "        [0.5782],\n",
      "        [0.5336],\n",
      "        [0.5363],\n",
      "        [0.8837],\n",
      "        [0.5792],\n",
      "        [0.5670],\n",
      "        [0.5786],\n",
      "        [0.8557],\n",
      "        [0.5430],\n",
      "        [0.8649],\n",
      "        [0.5743],\n",
      "        [0.8786],\n",
      "        [0.8554],\n",
      "        [0.7311],\n",
      "        [0.8214],\n",
      "        [0.7877],\n",
      "        [0.7509],\n",
      "        [0.8220],\n",
      "        [0.5922],\n",
      "        [0.8329],\n",
      "        [0.7878],\n",
      "        [0.8659],\n",
      "        [0.8378],\n",
      "        [0.6246],\n",
      "        [0.6296],\n",
      "        [0.5398]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0043],\n",
      "        [0.0048],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0074],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0186],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0239],\n",
      "        [0.0243],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0259],\n",
      "        [0.0267],\n",
      "        [0.0273],\n",
      "        [0.0276],\n",
      "        [0.0284],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0304],\n",
      "        [0.0308],\n",
      "        [0.0319],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.0349],\n",
      "        [0.0354],\n",
      "        [0.0362],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0371],\n",
      "        [0.0374],\n",
      "        [0.0394],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0403],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0416],\n",
      "        [0.0418],\n",
      "        [0.0419],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0432],\n",
      "        [0.0439],\n",
      "        [0.0447],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0455],\n",
      "        [0.0462],\n",
      "        [0.0464],\n",
      "        [0.0470],\n",
      "        [0.0472],\n",
      "        [0.0499],\n",
      "        [0.0500],\n",
      "        [0.0503],\n",
      "        [0.0506],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0516],\n",
      "        [0.0527],\n",
      "        [0.0534],\n",
      "        [0.0538],\n",
      "        [0.0541],\n",
      "        [0.0553],\n",
      "        [0.0559],\n",
      "        [0.0587],\n",
      "        [0.0601],\n",
      "        [0.0603],\n",
      "        [0.0609],\n",
      "        [0.0621],\n",
      "        [0.0622],\n",
      "        [0.0639],\n",
      "        [0.0643],\n",
      "        [0.0671],\n",
      "        [0.0674],\n",
      "        [0.0682],\n",
      "        [0.0689],\n",
      "        [0.0689],\n",
      "        [0.0711],\n",
      "        [0.0713],\n",
      "        [0.0726],\n",
      "        [0.0728],\n",
      "        [0.0735],\n",
      "        [0.0747],\n",
      "        [0.0784],\n",
      "        [0.0829],\n",
      "        [0.0835],\n",
      "        [0.0875],\n",
      "        [0.0908],\n",
      "        [0.0920],\n",
      "        [0.0930],\n",
      "        [0.0931],\n",
      "        [0.0995],\n",
      "        [0.1026],\n",
      "        [0.1033],\n",
      "        [0.1034],\n",
      "        [0.1035],\n",
      "        [0.1040],\n",
      "        [0.1168],\n",
      "        [0.1175],\n",
      "        [0.1292],\n",
      "        [0.1296],\n",
      "        [0.1310],\n",
      "        [0.1354],\n",
      "        [0.1356],\n",
      "        [0.1386],\n",
      "        [0.1580],\n",
      "        [0.1622],\n",
      "        [0.2041],\n",
      "        [0.2059],\n",
      "        [0.2134]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0053],\n",
      "        [0.0056],\n",
      "        [0.0046],\n",
      "        [0.0009],\n",
      "        [0.0031],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0058],\n",
      "        [0.0155],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0130],\n",
      "        [0.0134],\n",
      "        [0.0021],\n",
      "        [0.0124],\n",
      "        [0.0116],\n",
      "        [0.0126],\n",
      "        [0.0002],\n",
      "        [0.0022],\n",
      "        [0.0132],\n",
      "        [0.0169],\n",
      "        [0.0084],\n",
      "        [0.0068],\n",
      "        [0.0023],\n",
      "        [0.0164],\n",
      "        [0.0067],\n",
      "        [0.0241],\n",
      "        [0.0159],\n",
      "        [0.0105],\n",
      "        [0.0192],\n",
      "        [0.0232],\n",
      "        [0.0149],\n",
      "        [0.0179],\n",
      "        [0.0212],\n",
      "        [0.0114],\n",
      "        [0.0271],\n",
      "        [0.0130],\n",
      "        [0.0077],\n",
      "        [0.0272],\n",
      "        [0.0278],\n",
      "        [0.0171],\n",
      "        [0.0177],\n",
      "        [0.0236],\n",
      "        [0.0152],\n",
      "        [0.0125],\n",
      "        [0.0363],\n",
      "        [0.0243],\n",
      "        [0.0248],\n",
      "        [0.0300],\n",
      "        [0.0185],\n",
      "        [0.0292],\n",
      "        [0.0241],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.0341],\n",
      "        [0.0200],\n",
      "        [0.0348],\n",
      "        [0.0298],\n",
      "        [0.0291],\n",
      "        [0.0195],\n",
      "        [0.0314],\n",
      "        [0.0354],\n",
      "        [0.0263],\n",
      "        [0.0340],\n",
      "        [0.0395],\n",
      "        [0.0292],\n",
      "        [0.0276],\n",
      "        [0.0273],\n",
      "        [0.0293],\n",
      "        [0.0450],\n",
      "        [0.0386],\n",
      "        [0.0310],\n",
      "        [0.0265],\n",
      "        [0.0409],\n",
      "        [0.0340],\n",
      "        [0.0395],\n",
      "        [0.0417],\n",
      "        [0.0345],\n",
      "        [0.0434],\n",
      "        [0.0376],\n",
      "        [0.0332],\n",
      "        [0.0314],\n",
      "        [0.0446],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0390],\n",
      "        [0.0411],\n",
      "        [0.0440],\n",
      "        [0.0446],\n",
      "        [0.0512],\n",
      "        [0.0454],\n",
      "        [0.0468],\n",
      "        [0.0437],\n",
      "        [0.0430],\n",
      "        [0.0377],\n",
      "        [0.0546],\n",
      "        [0.0469],\n",
      "        [0.0479],\n",
      "        [0.0418],\n",
      "        [0.0569],\n",
      "        [0.0508],\n",
      "        [0.0428],\n",
      "        [0.0422],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0562],\n",
      "        [0.0497],\n",
      "        [0.0430],\n",
      "        [0.0524],\n",
      "        [0.0570],\n",
      "        [0.0457],\n",
      "        [0.0618],\n",
      "        [0.0592],\n",
      "        [0.0494],\n",
      "        [0.0635],\n",
      "        [0.0611],\n",
      "        [0.0512],\n",
      "        [0.0557],\n",
      "        [0.0646],\n",
      "        [0.0623],\n",
      "        [0.0700],\n",
      "        [0.0757],\n",
      "        [0.0738],\n",
      "        [0.0655],\n",
      "        [0.0715],\n",
      "        [0.0702],\n",
      "        [0.0729],\n",
      "        [0.0734],\n",
      "        [0.0749],\n",
      "        [0.0701],\n",
      "        [0.0797],\n",
      "        [0.0854],\n",
      "        [0.0886],\n",
      "        [0.0996],\n",
      "        [0.0828],\n",
      "        [0.0826],\n",
      "        [0.0938],\n",
      "        [0.0933],\n",
      "        [0.0902],\n",
      "        [0.0961],\n",
      "        [0.0946],\n",
      "        [0.0935],\n",
      "        [0.0979],\n",
      "        [0.1198],\n",
      "        [0.1244],\n",
      "        [0.1349],\n",
      "        [0.1338],\n",
      "        [0.1261],\n",
      "        [0.1322],\n",
      "        [0.1295],\n",
      "        [0.1440],\n",
      "        [0.1641],\n",
      "        [0.1563],\n",
      "        [0.2023],\n",
      "        [0.2046],\n",
      "        [0.2011]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 40.33662152290344\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.211440168295667e-08, 51)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [51, 93, 92, 22, 95, 47, 46, 70, 94, 64, 62, 14, 116, 81, 4, 26, 96, 139, 87, 71, 73, 59, 12, 25, 45, 18, 28, 40, 79, 52, 6, 118, 29, 13, 86, 32, 125, 140, 117, 11, 16, 23, 48, 134, 110, 7, 106, 107, 58, 0, 120, 17, 49, 98, 143, 142, 50, 97, 24, 27, 60, 132, 42, 61, 8, 133, 9, 15, 144, 126, 141, 63, 53, 44, 138, 5, 31, 39, 109, 72, 30, 127, 137, 91, 108, 145, 149, 146, 85, 2, 135, 88, 147, 100, 148, 80, 101, 19, 119, 154, 131, 99, 111, 3, 33, 105, 69, 153, 82, 21, 10, 136, 43, 130, 89, 90, 124, 57, 123, 83, 54, 78, 38, 84, 103, 102, 128, 74, 104, 68, 115, 152, 151, 122, 121, 155, 56, 1, 129, 150, 37, 55, 20, 77, 65, 34, 36, 114, 76, 67, 66, 35, 41, 156, 112, 113, 157] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9098],\n",
      "        [0.6844],\n",
      "        [0.6929],\n",
      "        [0.8293],\n",
      "        [0.6575],\n",
      "        [0.8956],\n",
      "        [0.8916],\n",
      "        [0.7637],\n",
      "        [0.6818],\n",
      "        [0.8219],\n",
      "        [0.8411],\n",
      "        [0.9199],\n",
      "        [0.5180],\n",
      "        [0.6543],\n",
      "        [0.9690],\n",
      "        [0.8578],\n",
      "        [0.6270],\n",
      "        [0.5822],\n",
      "        [0.6809],\n",
      "        [0.7349],\n",
      "        [0.7333],\n",
      "        [0.8740],\n",
      "        [0.9313],\n",
      "        [0.8421],\n",
      "        [0.8394],\n",
      "        [0.8677],\n",
      "        [0.8227],\n",
      "        [0.9153],\n",
      "        [0.6405],\n",
      "        [0.8907],\n",
      "        [1.0083],\n",
      "        [0.5356],\n",
      "        [0.8364],\n",
      "        [0.9058],\n",
      "        [0.6862],\n",
      "        [0.8249],\n",
      "        [0.5458],\n",
      "        [0.5775],\n",
      "        [0.5073],\n",
      "        [0.9479],\n",
      "        [0.8959],\n",
      "        [0.8248],\n",
      "        [0.9067],\n",
      "        [0.5834],\n",
      "        [0.6129],\n",
      "        [1.0032],\n",
      "        [0.6173],\n",
      "        [0.6205],\n",
      "        [0.8570],\n",
      "        [0.9025],\n",
      "        [0.5582],\n",
      "        [0.8805],\n",
      "        [0.8946],\n",
      "        [0.6139],\n",
      "        [0.5753],\n",
      "        [0.5834],\n",
      "        [0.9204],\n",
      "        [0.6184],\n",
      "        [0.8200],\n",
      "        [0.8335],\n",
      "        [0.8450],\n",
      "        [0.6035],\n",
      "        [0.8733],\n",
      "        [0.8046],\n",
      "        [0.9659],\n",
      "        [0.5972],\n",
      "        [0.9505],\n",
      "        [0.9034],\n",
      "        [0.5599],\n",
      "        [0.5467],\n",
      "        [0.5951],\n",
      "        [0.8277],\n",
      "        [0.8855],\n",
      "        [0.8810],\n",
      "        [0.5718],\n",
      "        [1.0118],\n",
      "        [0.8025],\n",
      "        [0.9145],\n",
      "        [0.5974],\n",
      "        [0.7196],\n",
      "        [0.8539],\n",
      "        [0.5911],\n",
      "        [0.5869],\n",
      "        [0.6639],\n",
      "        [0.5947],\n",
      "        [0.5824],\n",
      "        [0.5652],\n",
      "        [0.5794],\n",
      "        [0.6688],\n",
      "        [0.9317],\n",
      "        [0.5732],\n",
      "        [0.6620],\n",
      "        [0.5778],\n",
      "        [0.6149],\n",
      "        [0.5772],\n",
      "        [0.6682],\n",
      "        [0.6348],\n",
      "        [0.8905],\n",
      "        [0.5651],\n",
      "        [0.5102],\n",
      "        [0.5748],\n",
      "        [0.6123],\n",
      "        [0.6148],\n",
      "        [0.9501],\n",
      "        [0.8138],\n",
      "        [0.6212],\n",
      "        [0.7967],\n",
      "        [0.5232],\n",
      "        [0.6643],\n",
      "        [0.8611],\n",
      "        [0.9244],\n",
      "        [0.5877],\n",
      "        [0.8865],\n",
      "        [0.5859],\n",
      "        [0.6569],\n",
      "        [0.6466],\n",
      "        [0.5489],\n",
      "        [0.8580],\n",
      "        [0.5552],\n",
      "        [0.6708],\n",
      "        [0.8699],\n",
      "        [0.7035],\n",
      "        [0.8968],\n",
      "        [0.6641],\n",
      "        [0.6192],\n",
      "        [0.6237],\n",
      "        [0.6033],\n",
      "        [0.7368],\n",
      "        [0.6143],\n",
      "        [0.8219],\n",
      "        [0.5750],\n",
      "        [0.5566],\n",
      "        [0.5700],\n",
      "        [0.5317],\n",
      "        [0.5353],\n",
      "        [0.5306],\n",
      "        [0.8619],\n",
      "        [0.8886],\n",
      "        [0.5779],\n",
      "        [0.5655],\n",
      "        [0.8721],\n",
      "        [0.8614],\n",
      "        [0.8924],\n",
      "        [0.7341],\n",
      "        [0.8282],\n",
      "        [0.8268],\n",
      "        [0.8390],\n",
      "        [0.5891],\n",
      "        [0.7551],\n",
      "        [0.7934],\n",
      "        [0.7933],\n",
      "        [0.8437],\n",
      "        [0.8721],\n",
      "        [0.5274],\n",
      "        [0.6227],\n",
      "        [0.6283],\n",
      "        [0.5299]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0021],\n",
      "        [    0.0022],\n",
      "        [    0.0022],\n",
      "        [    0.0023],\n",
      "        [    0.0024],\n",
      "        [    0.0031],\n",
      "        [    0.0046],\n",
      "        [    0.0053],\n",
      "        [    0.0056],\n",
      "        [    0.0058],\n",
      "        [    0.0067],\n",
      "        [    0.0068],\n",
      "        [    0.0077],\n",
      "        [    0.0082],\n",
      "        [    0.0084],\n",
      "        [    0.0084],\n",
      "        [    0.0105],\n",
      "        [    0.0114],\n",
      "        [    0.0116],\n",
      "        [    0.0124],\n",
      "        [    0.0125],\n",
      "        [    0.0126],\n",
      "        [    0.0130],\n",
      "        [    0.0130],\n",
      "        [    0.0132],\n",
      "        [    0.0134],\n",
      "        [    0.0149],\n",
      "        [    0.0152],\n",
      "        [    0.0155],\n",
      "        [    0.0159],\n",
      "        [    0.0164],\n",
      "        [    0.0169],\n",
      "        [    0.0171],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0185],\n",
      "        [    0.0192],\n",
      "        [    0.0195],\n",
      "        [    0.0200],\n",
      "        [    0.0212],\n",
      "        [    0.0232],\n",
      "        [    0.0236],\n",
      "        [    0.0241],\n",
      "        [    0.0241],\n",
      "        [    0.0243],\n",
      "        [    0.0248],\n",
      "        [    0.0263],\n",
      "        [    0.0265],\n",
      "        [    0.0269],\n",
      "        [    0.0271],\n",
      "        [    0.0272],\n",
      "        [    0.0272],\n",
      "        [    0.0273],\n",
      "        [    0.0276],\n",
      "        [    0.0278],\n",
      "        [    0.0291],\n",
      "        [    0.0292],\n",
      "        [    0.0292],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0300],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0314],\n",
      "        [    0.0328],\n",
      "        [    0.0332],\n",
      "        [    0.0335],\n",
      "        [    0.0340],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0345],\n",
      "        [    0.0348],\n",
      "        [    0.0354],\n",
      "        [    0.0363],\n",
      "        [    0.0376],\n",
      "        [    0.0377],\n",
      "        [    0.0386],\n",
      "        [    0.0390],\n",
      "        [    0.0395],\n",
      "        [    0.0395],\n",
      "        [    0.0409],\n",
      "        [    0.0411],\n",
      "        [    0.0417],\n",
      "        [    0.0418],\n",
      "        [    0.0422],\n",
      "        [    0.0428],\n",
      "        [    0.0430],\n",
      "        [    0.0430],\n",
      "        [    0.0434],\n",
      "        [    0.0437],\n",
      "        [    0.0438],\n",
      "        [    0.0440],\n",
      "        [    0.0443],\n",
      "        [    0.0446],\n",
      "        [    0.0446],\n",
      "        [    0.0450],\n",
      "        [    0.0454],\n",
      "        [    0.0457],\n",
      "        [    0.0468],\n",
      "        [    0.0469],\n",
      "        [    0.0479],\n",
      "        [    0.0494],\n",
      "        [    0.0497],\n",
      "        [    0.0508],\n",
      "        [    0.0512],\n",
      "        [    0.0512],\n",
      "        [    0.0524],\n",
      "        [    0.0546],\n",
      "        [    0.0557],\n",
      "        [    0.0562],\n",
      "        [    0.0569],\n",
      "        [    0.0570],\n",
      "        [    0.0592],\n",
      "        [    0.0611],\n",
      "        [    0.0618],\n",
      "        [    0.0623],\n",
      "        [    0.0635],\n",
      "        [    0.0646],\n",
      "        [    0.0655],\n",
      "        [    0.0700],\n",
      "        [    0.0701],\n",
      "        [    0.0702],\n",
      "        [    0.0715],\n",
      "        [    0.0729],\n",
      "        [    0.0734],\n",
      "        [    0.0738],\n",
      "        [    0.0749],\n",
      "        [    0.0757],\n",
      "        [    0.0797],\n",
      "        [    0.0826],\n",
      "        [    0.0828],\n",
      "        [    0.0854],\n",
      "        [    0.0886],\n",
      "        [    0.0902],\n",
      "        [    0.0933],\n",
      "        [    0.0935],\n",
      "        [    0.0938],\n",
      "        [    0.0946],\n",
      "        [    0.0961],\n",
      "        [    0.0979],\n",
      "        [    0.0996],\n",
      "        [    0.1198],\n",
      "        [    0.1244],\n",
      "        [    0.1261],\n",
      "        [    0.1295],\n",
      "        [    0.1322],\n",
      "        [    0.1338],\n",
      "        [    0.1349],\n",
      "        [    0.1440],\n",
      "        [    0.1563],\n",
      "        [    0.1641],\n",
      "        [    0.2011],\n",
      "        [    0.2023],\n",
      "        [    0.2046],\n",
      "        [    0.2431]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0047],\n",
      "        [0.0046],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0010],\n",
      "        [0.0025],\n",
      "        [0.0067],\n",
      "        [0.0077],\n",
      "        [0.0084],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0124],\n",
      "        [0.0053],\n",
      "        [0.0023],\n",
      "        [0.0005],\n",
      "        [0.0104],\n",
      "        [0.0060],\n",
      "        [0.0117],\n",
      "        [0.0058],\n",
      "        [0.0069],\n",
      "        [0.0153],\n",
      "        [0.0166],\n",
      "        [0.0058],\n",
      "        [0.0152],\n",
      "        [0.0095],\n",
      "        [0.0191],\n",
      "        [0.0160],\n",
      "        [0.0182],\n",
      "        [0.0132],\n",
      "        [0.0116],\n",
      "        [0.0231],\n",
      "        [0.0151],\n",
      "        [0.0196],\n",
      "        [0.0227],\n",
      "        [0.0123],\n",
      "        [0.0162],\n",
      "        [0.0168],\n",
      "        [0.0143],\n",
      "        [0.0206],\n",
      "        [0.0128],\n",
      "        [0.0137],\n",
      "        [0.0245],\n",
      "        [0.0281],\n",
      "        [0.0229],\n",
      "        [0.0242],\n",
      "        [0.0310],\n",
      "        [0.0230],\n",
      "        [0.0239],\n",
      "        [0.0231],\n",
      "        [0.0184],\n",
      "        [0.0244],\n",
      "        [0.0338],\n",
      "        [0.0315],\n",
      "        [0.0254],\n",
      "        [0.0213],\n",
      "        [0.0224],\n",
      "        [0.0316],\n",
      "        [0.0273],\n",
      "        [0.0266],\n",
      "        [0.0311],\n",
      "        [0.0252],\n",
      "        [0.0288],\n",
      "        [0.0330],\n",
      "        [0.0266],\n",
      "        [0.0246],\n",
      "        [0.0301],\n",
      "        [0.0266],\n",
      "        [0.0273],\n",
      "        [0.0260],\n",
      "        [0.0318],\n",
      "        [0.0300],\n",
      "        [0.0391],\n",
      "        [0.0317],\n",
      "        [0.0381],\n",
      "        [0.0380],\n",
      "        [0.0450],\n",
      "        [0.0361],\n",
      "        [0.0323],\n",
      "        [0.0385],\n",
      "        [0.0352],\n",
      "        [0.0428],\n",
      "        [0.0365],\n",
      "        [0.0424],\n",
      "        [0.0378],\n",
      "        [0.0411],\n",
      "        [0.0347],\n",
      "        [0.0344],\n",
      "        [0.0362],\n",
      "        [0.0383],\n",
      "        [0.0350],\n",
      "        [0.0433],\n",
      "        [0.0405],\n",
      "        [0.0373],\n",
      "        [0.0425],\n",
      "        [0.0369],\n",
      "        [0.0484],\n",
      "        [0.0433],\n",
      "        [0.0516],\n",
      "        [0.0433],\n",
      "        [0.0340],\n",
      "        [0.0459],\n",
      "        [0.0448],\n",
      "        [0.0477],\n",
      "        [0.0411],\n",
      "        [0.0478],\n",
      "        [0.0492],\n",
      "        [0.0562],\n",
      "        [0.0398],\n",
      "        [0.0485],\n",
      "        [0.0604],\n",
      "        [0.0499],\n",
      "        [0.0569],\n",
      "        [0.0605],\n",
      "        [0.0556],\n",
      "        [0.0567],\n",
      "        [0.0587],\n",
      "        [0.0605],\n",
      "        [0.0596],\n",
      "        [0.0620],\n",
      "        [0.0610],\n",
      "        [0.0628],\n",
      "        [0.0721],\n",
      "        [0.0649],\n",
      "        [0.0663],\n",
      "        [0.0700],\n",
      "        [0.0714],\n",
      "        [0.0710],\n",
      "        [0.0783],\n",
      "        [0.0733],\n",
      "        [0.0811],\n",
      "        [0.0788],\n",
      "        [0.0733],\n",
      "        [0.0745],\n",
      "        [0.0841],\n",
      "        [0.0864],\n",
      "        [0.0789],\n",
      "        [0.0894],\n",
      "        [0.0858],\n",
      "        [0.0917],\n",
      "        [0.0869],\n",
      "        [0.0915],\n",
      "        [0.0944],\n",
      "        [0.1060],\n",
      "        [0.1227],\n",
      "        [0.1298],\n",
      "        [0.1237],\n",
      "        [0.1256],\n",
      "        [0.1311],\n",
      "        [0.1375],\n",
      "        [0.1396],\n",
      "        [0.1485],\n",
      "        [0.1530],\n",
      "        [0.1676],\n",
      "        [0.1897],\n",
      "        [0.2021],\n",
      "        [0.2051],\n",
      "        [0.2313]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 40.6237256526947\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.841998139047064e-07, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [4, 95, 92, 22, 81, 47, 93, 51, 116, 87, 12, 96, 46, 71, 70, 94, 45, 26, 64, 62, 52, 139, 86, 14, 11, 79, 16, 140, 118, 25, 73, 28, 32, 59, 125, 40, 0, 18, 29, 117, 143, 142, 13, 134, 106, 6, 58, 107, 110, 120, 23, 8, 60, 98, 144, 9, 61, 24, 15, 97, 48, 132, 141, 133, 7, 27, 49, 50, 53, 126, 39, 42, 17, 154, 149, 145, 2, 72, 31, 146, 127, 148, 147, 91, 138, 44, 85, 109, 63, 153, 88, 3, 108, 137, 100, 30, 101, 119, 135, 99, 5, 131, 111, 33, 80, 82, 105, 10, 19, 130, 69, 89, 136, 90, 57, 21, 124, 43, 83, 123, 54, 38, 84, 103, 128, 102, 78, 104, 152, 151, 74, 115, 155, 68, 122, 1, 121, 150, 56, 37, 129, 55, 20, 77, 34, 36, 65, 114, 76, 67, 66, 35, 41, 156, 112, 113, 157, 75] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9772],\n",
      "        [0.6607],\n",
      "        [0.6967],\n",
      "        [0.8332],\n",
      "        [0.6588],\n",
      "        [0.9004],\n",
      "        [0.6882],\n",
      "        [0.9143],\n",
      "        [0.5167],\n",
      "        [0.6856],\n",
      "        [0.9380],\n",
      "        [0.6294],\n",
      "        [0.8960],\n",
      "        [0.7394],\n",
      "        [0.7682],\n",
      "        [0.6856],\n",
      "        [0.8429],\n",
      "        [0.8600],\n",
      "        [0.8273],\n",
      "        [0.8468],\n",
      "        [0.8943],\n",
      "        [0.5790],\n",
      "        [0.6911],\n",
      "        [0.9265],\n",
      "        [0.9545],\n",
      "        [0.6422],\n",
      "        [0.9022],\n",
      "        [0.5732],\n",
      "        [0.5363],\n",
      "        [0.8448],\n",
      "        [0.7371],\n",
      "        [0.8254],\n",
      "        [0.8264],\n",
      "        [0.8783],\n",
      "        [0.5469],\n",
      "        [0.9200],\n",
      "        [0.9106],\n",
      "        [0.8738],\n",
      "        [0.8396],\n",
      "        [0.5060],\n",
      "        [0.5693],\n",
      "        [0.5782],\n",
      "        [0.9117],\n",
      "        [0.5841],\n",
      "        [0.6185],\n",
      "        [1.0158],\n",
      "        [0.8602],\n",
      "        [0.6215],\n",
      "        [0.6130],\n",
      "        [0.5607],\n",
      "        [0.8282],\n",
      "        [0.9727],\n",
      "        [0.8491],\n",
      "        [0.6158],\n",
      "        [0.5524],\n",
      "        [0.9568],\n",
      "        [0.8090],\n",
      "        [0.8226],\n",
      "        [0.9094],\n",
      "        [0.6201],\n",
      "        [0.9116],\n",
      "        [0.6045],\n",
      "        [0.5911],\n",
      "        [0.5985],\n",
      "        [1.0101],\n",
      "        [0.8354],\n",
      "        [0.8989],\n",
      "        [0.9242],\n",
      "        [0.8882],\n",
      "        [0.5489],\n",
      "        [0.9199],\n",
      "        [0.8763],\n",
      "        [0.8872],\n",
      "        [0.4986],\n",
      "        [0.5575],\n",
      "        [0.5753],\n",
      "        [0.9397],\n",
      "        [0.7234],\n",
      "        [0.8039],\n",
      "        [0.5729],\n",
      "        [0.5940],\n",
      "        [0.5698],\n",
      "        [0.5713],\n",
      "        [0.6672],\n",
      "        [0.5693],\n",
      "        [0.8844],\n",
      "        [0.6735],\n",
      "        [0.5975],\n",
      "        [0.8327],\n",
      "        [0.5117],\n",
      "        [0.6651],\n",
      "        [0.9583],\n",
      "        [0.5953],\n",
      "        [0.5853],\n",
      "        [0.6165],\n",
      "        [0.8572],\n",
      "        [0.6362],\n",
      "        [0.5672],\n",
      "        [0.5732],\n",
      "        [0.6144],\n",
      "        [1.0205],\n",
      "        [0.5756],\n",
      "        [0.6146],\n",
      "        [0.8157],\n",
      "        [0.6720],\n",
      "        [0.6683],\n",
      "        [0.6228],\n",
      "        [0.9301],\n",
      "        [0.8970],\n",
      "        [0.5873],\n",
      "        [0.8017],\n",
      "        [0.6594],\n",
      "        [0.5870],\n",
      "        [0.6491],\n",
      "        [0.8607],\n",
      "        [0.8669],\n",
      "        [0.5503],\n",
      "        [0.8902],\n",
      "        [0.6745],\n",
      "        [0.5567],\n",
      "        [0.8726],\n",
      "        [0.9020],\n",
      "        [0.6679],\n",
      "        [0.6208],\n",
      "        [0.6058],\n",
      "        [0.6252],\n",
      "        [0.7055],\n",
      "        [0.6160],\n",
      "        [0.5472],\n",
      "        [0.5617],\n",
      "        [0.7414],\n",
      "        [0.5741],\n",
      "        [0.5193],\n",
      "        [0.8272],\n",
      "        [0.5330],\n",
      "        [0.8963],\n",
      "        [0.5374],\n",
      "        [0.5578],\n",
      "        [0.8658],\n",
      "        [0.8767],\n",
      "        [0.5800],\n",
      "        [0.8650],\n",
      "        [0.8988],\n",
      "        [0.7370],\n",
      "        [0.8293],\n",
      "        [0.8428],\n",
      "        [0.8337],\n",
      "        [0.5879],\n",
      "        [0.7589],\n",
      "        [0.7981],\n",
      "        [0.7977],\n",
      "        [0.8470],\n",
      "        [0.8755],\n",
      "        [0.5160],\n",
      "        [0.6226],\n",
      "        [0.6288],\n",
      "        [0.5180],\n",
      "        [0.7471]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0046],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0084],\n",
      "        [0.0095],\n",
      "        [0.0104],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0117],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0128],\n",
      "        [0.0132],\n",
      "        [0.0137],\n",
      "        [0.0143],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0182],\n",
      "        [0.0184],\n",
      "        [0.0191],\n",
      "        [0.0196],\n",
      "        [0.0206],\n",
      "        [0.0213],\n",
      "        [0.0224],\n",
      "        [0.0227],\n",
      "        [0.0229],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0245],\n",
      "        [0.0246],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0260],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0273],\n",
      "        [0.0273],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0300],\n",
      "        [0.0301],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0315],\n",
      "        [0.0316],\n",
      "        [0.0317],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0344],\n",
      "        [0.0347],\n",
      "        [0.0350],\n",
      "        [0.0352],\n",
      "        [0.0361],\n",
      "        [0.0362],\n",
      "        [0.0365],\n",
      "        [0.0369],\n",
      "        [0.0373],\n",
      "        [0.0378],\n",
      "        [0.0380],\n",
      "        [0.0381],\n",
      "        [0.0383],\n",
      "        [0.0385],\n",
      "        [0.0391],\n",
      "        [0.0398],\n",
      "        [0.0405],\n",
      "        [0.0411],\n",
      "        [0.0411],\n",
      "        [0.0424],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0433],\n",
      "        [0.0433],\n",
      "        [0.0433],\n",
      "        [0.0448],\n",
      "        [0.0450],\n",
      "        [0.0459],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0484],\n",
      "        [0.0485],\n",
      "        [0.0492],\n",
      "        [0.0499],\n",
      "        [0.0516],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0567],\n",
      "        [0.0569],\n",
      "        [0.0587],\n",
      "        [0.0596],\n",
      "        [0.0604],\n",
      "        [0.0605],\n",
      "        [0.0605],\n",
      "        [0.0610],\n",
      "        [0.0620],\n",
      "        [0.0628],\n",
      "        [0.0649],\n",
      "        [0.0663],\n",
      "        [0.0700],\n",
      "        [0.0710],\n",
      "        [0.0714],\n",
      "        [0.0721],\n",
      "        [0.0733],\n",
      "        [0.0733],\n",
      "        [0.0745],\n",
      "        [0.0783],\n",
      "        [0.0788],\n",
      "        [0.0789],\n",
      "        [0.0811],\n",
      "        [0.0841],\n",
      "        [0.0858],\n",
      "        [0.0864],\n",
      "        [0.0869],\n",
      "        [0.0894],\n",
      "        [0.0915],\n",
      "        [0.0917],\n",
      "        [0.0944],\n",
      "        [0.1060],\n",
      "        [0.1227],\n",
      "        [0.1237],\n",
      "        [0.1256],\n",
      "        [0.1298],\n",
      "        [0.1311],\n",
      "        [0.1375],\n",
      "        [0.1396],\n",
      "        [0.1485],\n",
      "        [0.1530],\n",
      "        [0.1676],\n",
      "        [0.1897],\n",
      "        [0.2021],\n",
      "        [0.2051],\n",
      "        [0.2313],\n",
      "        [0.2859]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0018],\n",
      "        [0.0013],\n",
      "        [0.0010],\n",
      "        [0.0008],\n",
      "        [0.0039],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0073],\n",
      "        [0.0055],\n",
      "        [0.0085],\n",
      "        [0.0056],\n",
      "        [0.0096],\n",
      "        [0.0047],\n",
      "        [0.0062],\n",
      "        [0.0110],\n",
      "        [0.0082],\n",
      "        [0.0097],\n",
      "        [0.0105],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0130],\n",
      "        [0.0127],\n",
      "        [0.0170],\n",
      "        [0.0131],\n",
      "        [0.0112],\n",
      "        [0.0148],\n",
      "        [0.0135],\n",
      "        [0.0119],\n",
      "        [0.0143],\n",
      "        [0.0194],\n",
      "        [0.0143],\n",
      "        [0.0158],\n",
      "        [0.0170],\n",
      "        [0.0169],\n",
      "        [0.0197],\n",
      "        [0.0180],\n",
      "        [0.0222],\n",
      "        [0.0168],\n",
      "        [0.0185],\n",
      "        [0.0226],\n",
      "        [0.0219],\n",
      "        [0.0248],\n",
      "        [0.0233],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        [0.0218],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0245],\n",
      "        [0.0275],\n",
      "        [0.0280],\n",
      "        [0.0203],\n",
      "        [0.0269],\n",
      "        [0.0282],\n",
      "        [0.0282],\n",
      "        [0.0270],\n",
      "        [0.0302],\n",
      "        [0.0272],\n",
      "        [0.0277],\n",
      "        [0.0270],\n",
      "        [0.0287],\n",
      "        [0.0307],\n",
      "        [0.0286],\n",
      "        [0.0299],\n",
      "        [0.0293],\n",
      "        [0.0345],\n",
      "        [0.0299],\n",
      "        [0.0330],\n",
      "        [0.0308],\n",
      "        [0.0350],\n",
      "        [0.0251],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0337],\n",
      "        [0.0385],\n",
      "        [0.0391],\n",
      "        [0.0312],\n",
      "        [0.0341],\n",
      "        [0.0311],\n",
      "        [0.0323],\n",
      "        [0.0403],\n",
      "        [0.0395],\n",
      "        [0.0362],\n",
      "        [0.0399],\n",
      "        [0.0409],\n",
      "        [0.0379],\n",
      "        [0.0309],\n",
      "        [0.0432],\n",
      "        [0.0395],\n",
      "        [0.0430],\n",
      "        [0.0433],\n",
      "        [0.0451],\n",
      "        [0.0411],\n",
      "        [0.0460],\n",
      "        [0.0418],\n",
      "        [0.0428],\n",
      "        [0.0470],\n",
      "        [0.0461],\n",
      "        [0.0447],\n",
      "        [0.0453],\n",
      "        [0.0506],\n",
      "        [0.0462],\n",
      "        [0.0508],\n",
      "        [0.0509],\n",
      "        [0.0503],\n",
      "        [0.0524],\n",
      "        [0.0541],\n",
      "        [0.0534],\n",
      "        [0.0599],\n",
      "        [0.0571],\n",
      "        [0.0616],\n",
      "        [0.0628],\n",
      "        [0.0609],\n",
      "        [0.0594],\n",
      "        [0.0588],\n",
      "        [0.0636],\n",
      "        [0.0609],\n",
      "        [0.0656],\n",
      "        [0.0655],\n",
      "        [0.0687],\n",
      "        [0.0721],\n",
      "        [0.0688],\n",
      "        [0.0737],\n",
      "        [0.0675],\n",
      "        [0.0751],\n",
      "        [0.0658],\n",
      "        [0.0679],\n",
      "        [0.0755],\n",
      "        [0.0764],\n",
      "        [0.0701],\n",
      "        [0.0784],\n",
      "        [0.0831],\n",
      "        [0.0845],\n",
      "        [0.0849],\n",
      "        [0.0810],\n",
      "        [0.0915],\n",
      "        [0.0925],\n",
      "        [0.0895],\n",
      "        [0.0966],\n",
      "        [0.1066],\n",
      "        [0.1184],\n",
      "        [0.1262],\n",
      "        [0.1269],\n",
      "        [0.1284],\n",
      "        [0.1282],\n",
      "        [0.1337],\n",
      "        [0.1370],\n",
      "        [0.1460],\n",
      "        [0.1547],\n",
      "        [0.1657],\n",
      "        [0.1809],\n",
      "        [0.1999],\n",
      "        [0.2033],\n",
      "        [0.2220],\n",
      "        [0.2827]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 40.91339945793152\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.248535555641865e-07, 22)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [22, 92, 95, 47, 4, 93, 51, 116, 81, 70, 12, 46, 94, 87, 26, 96, 71, 64, 62, 45, 140, 73, 11, 14, 16, 25, 52, 86, 139, 28, 59, 118, 125, 143, 0, 79, 40, 29, 142, 32, 18, 144, 110, 134, 117, 13, 120, 6, 23, 8, 106, 154, 107, 58, 9, 141, 15, 48, 60, 132, 98, 61, 24, 149, 27, 133, 145, 50, 126, 49, 97, 7, 42, 153, 148, 146, 147, 39, 2, 127, 53, 17, 44, 63, 72, 31, 138, 3, 85, 91, 109, 30, 119, 135, 108, 88, 137, 131, 100, 111, 101, 5, 80, 99, 10, 33, 82, 105, 19, 69, 130, 136, 43, 124, 89, 21, 123, 90, 57, 83, 38, 54, 152, 78, 151, 84, 128, 155, 103, 102, 104, 74, 115, 68, 150, 122, 1, 121, 129, 56, 37, 55, 20, 77, 34, 36, 114, 65, 76, 67, 66, 35, 41, 156, 112, 113, 157, 75, 158] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8323],\n",
      "        [0.6941],\n",
      "        [0.6585],\n",
      "        [0.8996],\n",
      "        [0.9785],\n",
      "        [0.6858],\n",
      "        [0.9128],\n",
      "        [0.5146],\n",
      "        [0.6571],\n",
      "        [0.7653],\n",
      "        [0.9383],\n",
      "        [0.8948],\n",
      "        [0.6834],\n",
      "        [0.6841],\n",
      "        [0.8577],\n",
      "        [0.6268],\n",
      "        [0.7367],\n",
      "        [0.8263],\n",
      "        [0.8460],\n",
      "        [0.8415],\n",
      "        [0.5702],\n",
      "        [0.7337],\n",
      "        [0.9546],\n",
      "        [0.9271],\n",
      "        [0.9028],\n",
      "        [0.8431],\n",
      "        [0.8922],\n",
      "        [0.6896],\n",
      "        [0.5769],\n",
      "        [0.8237],\n",
      "        [0.8760],\n",
      "        [0.5366],\n",
      "        [0.5480],\n",
      "        [0.5648],\n",
      "        [0.9121],\n",
      "        [0.6383],\n",
      "        [0.9189],\n",
      "        [0.8380],\n",
      "        [0.5743],\n",
      "        [0.8232],\n",
      "        [0.8745],\n",
      "        [0.5468],\n",
      "        [0.6106],\n",
      "        [0.5851],\n",
      "        [0.5043],\n",
      "        [0.9116],\n",
      "        [0.5624],\n",
      "        [1.0161],\n",
      "        [0.8271],\n",
      "        [0.9728],\n",
      "        [0.6167],\n",
      "        [0.4896],\n",
      "        [0.6195],\n",
      "        [0.8574],\n",
      "        [0.9565],\n",
      "        [0.5881],\n",
      "        [0.9096],\n",
      "        [0.9107],\n",
      "        [0.8469],\n",
      "        [0.6056],\n",
      "        [0.6132],\n",
      "        [0.8075],\n",
      "        [0.8210],\n",
      "        [0.5516],\n",
      "        [0.8329],\n",
      "        [0.6000],\n",
      "        [0.5698],\n",
      "        [0.9219],\n",
      "        [0.5508],\n",
      "        [0.8973],\n",
      "        [0.6173],\n",
      "        [1.0097],\n",
      "        [0.8741],\n",
      "        [0.5028],\n",
      "        [0.5641],\n",
      "        [0.5678],\n",
      "        [0.5663],\n",
      "        [0.9191],\n",
      "        [0.9410],\n",
      "        [0.5965],\n",
      "        [0.8855],\n",
      "        [0.8883],\n",
      "        [0.8825],\n",
      "        [0.8315],\n",
      "        [0.7201],\n",
      "        [0.8009],\n",
      "        [0.5678],\n",
      "        [0.9599],\n",
      "        [0.6719],\n",
      "        [0.6647],\n",
      "        [0.5951],\n",
      "        [0.8555],\n",
      "        [0.5687],\n",
      "        [0.5737],\n",
      "        [0.5934],\n",
      "        [0.6625],\n",
      "        [0.5845],\n",
      "        [0.5769],\n",
      "        [0.6139],\n",
      "        [0.6122],\n",
      "        [0.6334],\n",
      "        [1.0216],\n",
      "        [0.6698],\n",
      "        [0.6122],\n",
      "        [0.9298],\n",
      "        [0.8129],\n",
      "        [0.6660],\n",
      "        [0.6211],\n",
      "        [0.8979],\n",
      "        [0.7990],\n",
      "        [0.5889],\n",
      "        [0.5868],\n",
      "        [0.8884],\n",
      "        [0.5513],\n",
      "        [0.6562],\n",
      "        [0.8674],\n",
      "        [0.5578],\n",
      "        [0.6462],\n",
      "        [0.8575],\n",
      "        [0.6719],\n",
      "        [0.9014],\n",
      "        [0.8698],\n",
      "        [0.5398],\n",
      "        [0.7010],\n",
      "        [0.5551],\n",
      "        [0.6656],\n",
      "        [0.6079],\n",
      "        [0.5105],\n",
      "        [0.6186],\n",
      "        [0.6229],\n",
      "        [0.6142],\n",
      "        [0.7386],\n",
      "        [0.5717],\n",
      "        [0.8246],\n",
      "        [0.5519],\n",
      "        [0.5340],\n",
      "        [0.8976],\n",
      "        [0.5390],\n",
      "        [0.5822],\n",
      "        [0.8637],\n",
      "        [0.8757],\n",
      "        [0.8628],\n",
      "        [0.8995],\n",
      "        [0.7327],\n",
      "        [0.8268],\n",
      "        [0.8415],\n",
      "        [0.5851],\n",
      "        [0.8322],\n",
      "        [0.7550],\n",
      "        [0.7955],\n",
      "        [0.7952],\n",
      "        [0.8453],\n",
      "        [0.8736],\n",
      "        [0.5073],\n",
      "        [0.6203],\n",
      "        [0.6270],\n",
      "        [0.5088],\n",
      "        [0.7438],\n",
      "        [0.5102]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0010],\n",
      "        [0.0013],\n",
      "        [0.0018],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0039],\n",
      "        [0.0047],\n",
      "        [0.0055],\n",
      "        [0.0056],\n",
      "        [0.0062],\n",
      "        [0.0073],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0105],\n",
      "        [0.0110],\n",
      "        [0.0112],\n",
      "        [0.0119],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0135],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0143],\n",
      "        [0.0143],\n",
      "        [0.0148],\n",
      "        [0.0158],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0170],\n",
      "        [0.0180],\n",
      "        [0.0185],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0203],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0222],\n",
      "        [0.0226],\n",
      "        [0.0227],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0245],\n",
      "        [0.0248],\n",
      "        [0.0251],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        [0.0269],\n",
      "        [0.0270],\n",
      "        [0.0270],\n",
      "        [0.0272],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0282],\n",
      "        [0.0286],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0292],\n",
      "        [0.0293],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0307],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0311],\n",
      "        [0.0312],\n",
      "        [0.0323],\n",
      "        [0.0330],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0362],\n",
      "        [0.0379],\n",
      "        [0.0385],\n",
      "        [0.0391],\n",
      "        [0.0395],\n",
      "        [0.0395],\n",
      "        [0.0399],\n",
      "        [0.0403],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0418],\n",
      "        [0.0428],\n",
      "        [0.0430],\n",
      "        [0.0432],\n",
      "        [0.0433],\n",
      "        [0.0447],\n",
      "        [0.0451],\n",
      "        [0.0453],\n",
      "        [0.0460],\n",
      "        [0.0461],\n",
      "        [0.0462],\n",
      "        [0.0470],\n",
      "        [0.0503],\n",
      "        [0.0506],\n",
      "        [0.0508],\n",
      "        [0.0509],\n",
      "        [0.0524],\n",
      "        [0.0534],\n",
      "        [0.0541],\n",
      "        [0.0571],\n",
      "        [0.0588],\n",
      "        [0.0594],\n",
      "        [0.0599],\n",
      "        [0.0609],\n",
      "        [0.0609],\n",
      "        [0.0616],\n",
      "        [0.0628],\n",
      "        [0.0636],\n",
      "        [0.0655],\n",
      "        [0.0656],\n",
      "        [0.0658],\n",
      "        [0.0675],\n",
      "        [0.0679],\n",
      "        [0.0687],\n",
      "        [0.0688],\n",
      "        [0.0701],\n",
      "        [0.0721],\n",
      "        [0.0737],\n",
      "        [0.0751],\n",
      "        [0.0755],\n",
      "        [0.0764],\n",
      "        [0.0784],\n",
      "        [0.0810],\n",
      "        [0.0831],\n",
      "        [0.0845],\n",
      "        [0.0849],\n",
      "        [0.0895],\n",
      "        [0.0915],\n",
      "        [0.0925],\n",
      "        [0.0966],\n",
      "        [0.1066],\n",
      "        [0.1184],\n",
      "        [0.1262],\n",
      "        [0.1269],\n",
      "        [0.1282],\n",
      "        [0.1284],\n",
      "        [0.1337],\n",
      "        [0.1370],\n",
      "        [0.1460],\n",
      "        [0.1547],\n",
      "        [0.1657],\n",
      "        [0.1809],\n",
      "        [0.1999],\n",
      "        [0.2033],\n",
      "        [0.2220],\n",
      "        [0.2827],\n",
      "        [0.2829]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0031],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0014],\n",
      "        [0.0009],\n",
      "        [0.0027],\n",
      "        [0.0008],\n",
      "        [0.0034],\n",
      "        [0.0081],\n",
      "        [0.0004],\n",
      "        [0.0077],\n",
      "        [0.0019],\n",
      "        [0.0014],\n",
      "        [0.0111],\n",
      "        [0.0027],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0063],\n",
      "        [0.0073],\n",
      "        [0.0154],\n",
      "        [0.0024],\n",
      "        [0.0062],\n",
      "        [0.0150],\n",
      "        [0.0111],\n",
      "        [0.0150],\n",
      "        [0.0086],\n",
      "        [0.0185],\n",
      "        [0.0175],\n",
      "        [0.0215],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0192],\n",
      "        [0.0198],\n",
      "        [0.0065],\n",
      "        [0.0180],\n",
      "        [0.0237],\n",
      "        [0.0135],\n",
      "        [0.0133],\n",
      "        [0.0089],\n",
      "        [0.0257],\n",
      "        [0.0177],\n",
      "        [0.0086],\n",
      "        [0.0158],\n",
      "        [0.0260],\n",
      "        [0.0288],\n",
      "        [0.0198],\n",
      "        [0.0256],\n",
      "        [0.0214],\n",
      "        [0.0192],\n",
      "        [0.0270],\n",
      "        [0.0302],\n",
      "        [0.0097],\n",
      "        [0.0314],\n",
      "        [0.0313],\n",
      "        [0.0298],\n",
      "        [0.0185],\n",
      "        [0.0294],\n",
      "        [0.0239],\n",
      "        [0.0322],\n",
      "        [0.0316],\n",
      "        [0.0339],\n",
      "        [0.0324],\n",
      "        [0.0331],\n",
      "        [0.0168],\n",
      "        [0.0228],\n",
      "        [0.0322],\n",
      "        [0.0178],\n",
      "        [0.0246],\n",
      "        [0.0329],\n",
      "        [0.0258],\n",
      "        [0.0363],\n",
      "        [0.0281],\n",
      "        [0.0256],\n",
      "        [0.0156],\n",
      "        [0.0197],\n",
      "        [0.0204],\n",
      "        [0.0217],\n",
      "        [0.0360],\n",
      "        [0.0349],\n",
      "        [0.0364],\n",
      "        [0.0401],\n",
      "        [0.0336],\n",
      "        [0.0313],\n",
      "        [0.0342],\n",
      "        [0.0440],\n",
      "        [0.0455],\n",
      "        [0.0465],\n",
      "        [0.0404],\n",
      "        [0.0437],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0364],\n",
      "        [0.0449],\n",
      "        [0.0475],\n",
      "        [0.0486],\n",
      "        [0.0484],\n",
      "        [0.0495],\n",
      "        [0.0486],\n",
      "        [0.0511],\n",
      "        [0.0391],\n",
      "        [0.0521],\n",
      "        [0.0452],\n",
      "        [0.0415],\n",
      "        [0.0526],\n",
      "        [0.0534],\n",
      "        [0.0565],\n",
      "        [0.0555],\n",
      "        [0.0560],\n",
      "        [0.0507],\n",
      "        [0.0488],\n",
      "        [0.0575],\n",
      "        [0.0626],\n",
      "        [0.0542],\n",
      "        [0.0632],\n",
      "        [0.0659],\n",
      "        [0.0586],\n",
      "        [0.0646],\n",
      "        [0.0674],\n",
      "        [0.0686],\n",
      "        [0.0685],\n",
      "        [0.0685],\n",
      "        [0.0713],\n",
      "        [0.0525],\n",
      "        [0.0605],\n",
      "        [0.0555],\n",
      "        [0.0733],\n",
      "        [0.0716],\n",
      "        [0.0548],\n",
      "        [0.0777],\n",
      "        [0.0795],\n",
      "        [0.0804],\n",
      "        [0.0706],\n",
      "        [0.0698],\n",
      "        [0.0740],\n",
      "        [0.0692],\n",
      "        [0.0869],\n",
      "        [0.0860],\n",
      "        [0.0881],\n",
      "        [0.0924],\n",
      "        [0.0962],\n",
      "        [0.0959],\n",
      "        [0.1016],\n",
      "        [0.1048],\n",
      "        [0.1120],\n",
      "        [0.1317],\n",
      "        [0.1310],\n",
      "        [0.1212],\n",
      "        [0.1248],\n",
      "        [0.1280],\n",
      "        [0.1324],\n",
      "        [0.1413],\n",
      "        [0.1593],\n",
      "        [0.1610],\n",
      "        [0.1656],\n",
      "        [0.1937],\n",
      "        [0.1978],\n",
      "        [0.2060],\n",
      "        [0.2773],\n",
      "        [0.2671]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 41.20315074920654\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 2 個區塊累積花費時間(s) 1.2196478843688965\n",
      "<<The performance of 2 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2196478843688965\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1262.82\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1763.71\n",
      "The accuracy(2000) for l = 1: 83.65%\n",
      "The accuracy(3000) for l = 1: 90.57%\n",
      "The maximum error: tensor(7080.7188)\n",
      "The minimum error: tensor(9.3125)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 3895.7\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 4136.6\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 25.0%\n",
      "The maximum error: 5870.953125\n",
      "The minimum error: 2236.16015625\n",
      "------------------------------------------------------------\n",
      "0.8364779874213837\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<3>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.3302127399583696e-07, 66)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [66, 47, 0, 90, 43, 42, 136, 22, 89, 18, 112, 88, 69, 60, 91, 139, 58, 8, 77, 21, 140, 138, 24, 150, 55, 10, 83, 25, 36, 92, 67, 12, 7, 41, 149, 106, 145, 82, 14, 141, 48, 137, 19, 114, 144, 121, 9, 142, 2, 135, 143, 23, 75, 44, 46, 38, 116, 28, 45, 130, 4, 3, 113, 11, 5, 102, 54, 40, 103, 128, 56, 129, 57, 122, 20, 13, 94, 59, 35, 93, 26, 123, 107, 49, 76, 81, 68, 115, 1, 27, 87, 134, 105, 131, 84, 127, 104, 65, 133, 15, 96, 97, 148, 95, 6, 39, 151, 147, 78, 101, 29, 126, 17, 74, 132, 120, 119, 85, 86, 34, 79, 53, 146, 111, 70, 50, 124, 80, 64, 99, 98, 100, 118, 158, 117, 125, 33, 52, 51, 16, 73, 157, 110, 61, 72, 32, 30, 63, 62, 31, 37, 152, 156, 108, 109]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0086],\n",
      "        [0.0086],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0111],\n",
      "        [0.0111],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0168],\n",
      "        [0.0175],\n",
      "        [0.0177],\n",
      "        [0.0178],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0192],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0204],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0217],\n",
      "        [0.0228],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0246],\n",
      "        [0.0256],\n",
      "        [0.0256],\n",
      "        [0.0257],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0270],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0294],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0313],\n",
      "        [0.0313],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0322],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0331],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0342],\n",
      "        [0.0360],\n",
      "        [0.0363],\n",
      "        [0.0364],\n",
      "        [0.0364],\n",
      "        [0.0391],\n",
      "        [0.0401],\n",
      "        [0.0415],\n",
      "        [0.0437],\n",
      "        [0.0440],\n",
      "        [0.0449],\n",
      "        [0.0452],\n",
      "        [0.0455],\n",
      "        [0.0455],\n",
      "        [0.0465],\n",
      "        [0.0469],\n",
      "        [0.0475],\n",
      "        [0.0484],\n",
      "        [0.0486],\n",
      "        [0.0486],\n",
      "        [0.0488],\n",
      "        [0.0495],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0521],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0534],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0555],\n",
      "        [0.0555],\n",
      "        [0.0560],\n",
      "        [0.0565],\n",
      "        [0.0575],\n",
      "        [0.0586],\n",
      "        [0.0605],\n",
      "        [0.0626],\n",
      "        [0.0632],\n",
      "        [0.0646],\n",
      "        [0.0659],\n",
      "        [0.0674],\n",
      "        [0.0685],\n",
      "        [0.0685],\n",
      "        [0.0686],\n",
      "        [0.0692],\n",
      "        [0.0698],\n",
      "        [0.0706],\n",
      "        [0.0713],\n",
      "        [0.0716],\n",
      "        [0.0733],\n",
      "        [0.0740],\n",
      "        [0.0777],\n",
      "        [0.0795],\n",
      "        [0.0804],\n",
      "        [0.0869],\n",
      "        [0.0876],\n",
      "        [0.0881],\n",
      "        [0.0924],\n",
      "        [0.0959],\n",
      "        [0.0962],\n",
      "        [0.1016],\n",
      "        [0.1048],\n",
      "        [0.1120],\n",
      "        [0.1182],\n",
      "        [0.1212],\n",
      "        [0.1248],\n",
      "        [0.1280],\n",
      "        [0.1310],\n",
      "        [0.1317],\n",
      "        [0.1324],\n",
      "        [0.1413],\n",
      "        [0.1593],\n",
      "        [0.1610],\n",
      "        [0.1656],\n",
      "        [0.1746],\n",
      "        [0.1937],\n",
      "        [0.1978]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.3302127399583696e-07, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [66, 47, 0, 90, 43, 42, 136, 22, 89, 18, 112, 88, 69, 60, 91, 139, 58, 8, 77, 21, 140, 138, 24, 150, 55, 10, 83, 25, 36, 92, 67, 12, 7, 41, 149, 106, 145, 82, 14, 141, 48, 137, 19, 114, 144, 121, 9, 142, 2, 135, 143, 23, 75, 44, 46, 38, 116, 28, 45, 130, 4, 3, 113, 11, 5, 102, 54, 40, 103, 128, 56, 129, 57, 122, 20, 13, 94, 59, 35, 93, 26, 123, 107, 49, 76, 81, 68, 115, 1, 27, 87, 134, 105, 131, 84, 127, 104, 65, 133, 15, 96, 97, 148, 95, 6, 39, 151, 147, 78, 101, 29, 126, 17, 74, 132, 120, 119, 85, 86, 34, 79, 53, 146, 111, 70, 50, 124, 80, 64, 99, 98, 100, 118, 158, 117, 125, 33, 52, 51, 16, 73, 157, 110, 61, 72, 32, 30, 63, 62, 31, 37, 152, 156, 108, 109, 153] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7602],\n",
      "        [0.9088],\n",
      "        [0.9776],\n",
      "        [0.6785],\n",
      "        [0.8964],\n",
      "        [0.8912],\n",
      "        [0.5614],\n",
      "        [0.8523],\n",
      "        [0.6808],\n",
      "        [0.8284],\n",
      "        [0.5079],\n",
      "        [0.6889],\n",
      "        [0.7280],\n",
      "        [0.8230],\n",
      "        [0.6534],\n",
      "        [0.5545],\n",
      "        [0.8428],\n",
      "        [0.9361],\n",
      "        [0.6530],\n",
      "        [0.8381],\n",
      "        [0.5351],\n",
      "        [0.5647],\n",
      "        [0.8186],\n",
      "        [0.4742],\n",
      "        [0.8713],\n",
      "        [0.9252],\n",
      "        [0.6803],\n",
      "        [0.8332],\n",
      "        [0.9153],\n",
      "        [0.6212],\n",
      "        [0.7316],\n",
      "        [0.9009],\n",
      "        [0.9523],\n",
      "        [0.8370],\n",
      "        [0.4875],\n",
      "        [0.6046],\n",
      "        [0.5399],\n",
      "        [0.6858],\n",
      "        [0.8725],\n",
      "        [0.5584],\n",
      "        [0.8875],\n",
      "        [0.5795],\n",
      "        [0.8228],\n",
      "        [0.5322],\n",
      "        [0.5526],\n",
      "        [0.5440],\n",
      "        [0.9088],\n",
      "        [0.5570],\n",
      "        [1.0141],\n",
      "        [0.5692],\n",
      "        [0.5557],\n",
      "        [0.8271],\n",
      "        [0.6316],\n",
      "        [0.9074],\n",
      "        [0.9172],\n",
      "        [0.8689],\n",
      "        [0.5595],\n",
      "        [0.8169],\n",
      "        [0.8932],\n",
      "        [0.5810],\n",
      "        [0.9703],\n",
      "        [1.0072],\n",
      "        [0.4978],\n",
      "        [0.9073],\n",
      "        [0.9536],\n",
      "        [0.6113],\n",
      "        [0.8520],\n",
      "        [0.8776],\n",
      "        [0.6140],\n",
      "        [0.6018],\n",
      "        [0.8422],\n",
      "        [0.5964],\n",
      "        [0.8033],\n",
      "        [0.5478],\n",
      "        [0.8161],\n",
      "        [0.8870],\n",
      "        [0.6073],\n",
      "        [0.8278],\n",
      "        [0.9162],\n",
      "        [0.6111],\n",
      "        [0.8508],\n",
      "        [0.5941],\n",
      "        [0.6060],\n",
      "        [0.8799],\n",
      "        [0.6651],\n",
      "        [0.6681],\n",
      "        [0.7146],\n",
      "        [0.5656],\n",
      "        [1.0208],\n",
      "        [0.7946],\n",
      "        [0.6595],\n",
      "        [0.5607],\n",
      "        [0.5891],\n",
      "        [0.5690],\n",
      "        [0.6572],\n",
      "        [0.5730],\n",
      "        [0.5877],\n",
      "        [0.7943],\n",
      "        [0.5782],\n",
      "        [0.8962],\n",
      "        [0.6079],\n",
      "        [0.6273],\n",
      "        [0.5264],\n",
      "        [0.6066],\n",
      "        [0.9266],\n",
      "        [0.8839],\n",
      "        [0.4952],\n",
      "        [0.5427],\n",
      "        [0.6612],\n",
      "        [0.6160],\n",
      "        [0.8070],\n",
      "        [0.5855],\n",
      "        [0.8652],\n",
      "        [0.6940],\n",
      "        [0.5813],\n",
      "        [0.5476],\n",
      "        [0.5541],\n",
      "        [0.6502],\n",
      "        [0.6404],\n",
      "        [0.8984],\n",
      "        [0.6670],\n",
      "        [0.8517],\n",
      "        [0.5401],\n",
      "        [0.5651],\n",
      "        [0.7337],\n",
      "        [0.8641],\n",
      "        [0.6051],\n",
      "        [0.6609],\n",
      "        [0.8201],\n",
      "        [0.6130],\n",
      "        [0.6171],\n",
      "        [0.6089],\n",
      "        [0.5302],\n",
      "        [0.3903],\n",
      "        [0.5357],\n",
      "        [0.5793],\n",
      "        [0.8723],\n",
      "        [0.8590],\n",
      "        [0.8578],\n",
      "        [0.8977],\n",
      "        [0.7264],\n",
      "        [0.4092],\n",
      "        [0.5781],\n",
      "        [0.8287],\n",
      "        [0.7494],\n",
      "        [0.8375],\n",
      "        [0.8213],\n",
      "        [0.7908],\n",
      "        [0.7905],\n",
      "        [0.8407],\n",
      "        [0.8689],\n",
      "        [0.4920],\n",
      "        [0.4420],\n",
      "        [0.6142],\n",
      "        [0.6215],\n",
      "        [0.4928]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0086],\n",
      "        [0.0086],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0111],\n",
      "        [0.0111],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0168],\n",
      "        [0.0175],\n",
      "        [0.0177],\n",
      "        [0.0178],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0192],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0204],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0217],\n",
      "        [0.0228],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0246],\n",
      "        [0.0256],\n",
      "        [0.0256],\n",
      "        [0.0257],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0270],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0294],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0313],\n",
      "        [0.0313],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0322],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0331],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0342],\n",
      "        [0.0360],\n",
      "        [0.0363],\n",
      "        [0.0364],\n",
      "        [0.0364],\n",
      "        [0.0391],\n",
      "        [0.0401],\n",
      "        [0.0415],\n",
      "        [0.0437],\n",
      "        [0.0440],\n",
      "        [0.0449],\n",
      "        [0.0452],\n",
      "        [0.0455],\n",
      "        [0.0455],\n",
      "        [0.0465],\n",
      "        [0.0469],\n",
      "        [0.0475],\n",
      "        [0.0484],\n",
      "        [0.0486],\n",
      "        [0.0486],\n",
      "        [0.0488],\n",
      "        [0.0495],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0521],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0534],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0555],\n",
      "        [0.0555],\n",
      "        [0.0560],\n",
      "        [0.0565],\n",
      "        [0.0575],\n",
      "        [0.0586],\n",
      "        [0.0605],\n",
      "        [0.0626],\n",
      "        [0.0632],\n",
      "        [0.0646],\n",
      "        [0.0659],\n",
      "        [0.0674],\n",
      "        [0.0685],\n",
      "        [0.0685],\n",
      "        [0.0686],\n",
      "        [0.0692],\n",
      "        [0.0698],\n",
      "        [0.0706],\n",
      "        [0.0713],\n",
      "        [0.0716],\n",
      "        [0.0733],\n",
      "        [0.0740],\n",
      "        [0.0777],\n",
      "        [0.0795],\n",
      "        [0.0804],\n",
      "        [0.0869],\n",
      "        [0.0876],\n",
      "        [0.0881],\n",
      "        [0.0924],\n",
      "        [0.0959],\n",
      "        [0.0962],\n",
      "        [0.1016],\n",
      "        [0.1048],\n",
      "        [0.1120],\n",
      "        [0.1182],\n",
      "        [0.1212],\n",
      "        [0.1248],\n",
      "        [0.1280],\n",
      "        [0.1310],\n",
      "        [0.1317],\n",
      "        [0.1324],\n",
      "        [0.1413],\n",
      "        [0.1593],\n",
      "        [0.1610],\n",
      "        [0.1656],\n",
      "        [0.1746],\n",
      "        [0.1937],\n",
      "        [0.1978],\n",
      "        [0.2060]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0004],\n",
      "        [0.0045],\n",
      "        [0.0040],\n",
      "        [0.0006],\n",
      "        [0.0022],\n",
      "        [0.0013],\n",
      "        [0.0013],\n",
      "        [0.0002],\n",
      "        [0.0026],\n",
      "        [0.0043],\n",
      "        [0.0038],\n",
      "        [0.0084],\n",
      "        [0.0088],\n",
      "        [0.0040],\n",
      "        [0.0011],\n",
      "        [0.0095],\n",
      "        [0.0057],\n",
      "        [0.0045],\n",
      "        [0.0078],\n",
      "        [0.0016],\n",
      "        [0.0043],\n",
      "        [0.0083],\n",
      "        [0.0011],\n",
      "        [0.0106],\n",
      "        [0.0135],\n",
      "        [0.0068],\n",
      "        [0.0129],\n",
      "        [0.0143],\n",
      "        [0.0124],\n",
      "        [0.0128],\n",
      "        [0.0125],\n",
      "        [0.0131],\n",
      "        [0.0161],\n",
      "        [0.0052],\n",
      "        [0.0163],\n",
      "        [0.0096],\n",
      "        [0.0133],\n",
      "        [0.0202],\n",
      "        [0.0111],\n",
      "        [0.0189],\n",
      "        [0.0150],\n",
      "        [0.0194],\n",
      "        [0.0181],\n",
      "        [0.0129],\n",
      "        [0.0178],\n",
      "        [0.0212],\n",
      "        [0.0144],\n",
      "        [0.0237],\n",
      "        [0.0243],\n",
      "        [0.0159],\n",
      "        [0.0212],\n",
      "        [0.0236],\n",
      "        [0.0250],\n",
      "        [0.0242],\n",
      "        [0.0245],\n",
      "        [0.0230],\n",
      "        [0.0274],\n",
      "        [0.0259],\n",
      "        [0.0250],\n",
      "        [0.0257],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0275],\n",
      "        [0.0288],\n",
      "        [0.0285],\n",
      "        [0.0311],\n",
      "        [0.0304],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0311],\n",
      "        [0.0304],\n",
      "        [0.0307],\n",
      "        [0.0301],\n",
      "        [0.0337],\n",
      "        [0.0366],\n",
      "        [0.0324],\n",
      "        [0.0360],\n",
      "        [0.0341],\n",
      "        [0.0351],\n",
      "        [0.0362],\n",
      "        [0.0333],\n",
      "        [0.0392],\n",
      "        [0.0415],\n",
      "        [0.0440],\n",
      "        [0.0394],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0489],\n",
      "        [0.0472],\n",
      "        [0.0433],\n",
      "        [0.0487],\n",
      "        [0.0460],\n",
      "        [0.0472],\n",
      "        [0.0456],\n",
      "        [0.0472],\n",
      "        [0.0472],\n",
      "        [0.0508],\n",
      "        [0.0508],\n",
      "        [0.0536],\n",
      "        [0.0502],\n",
      "        [0.0515],\n",
      "        [0.0437],\n",
      "        [0.0511],\n",
      "        [0.0524],\n",
      "        [0.0539],\n",
      "        [0.0441],\n",
      "        [0.0476],\n",
      "        [0.0522],\n",
      "        [0.0540],\n",
      "        [0.0577],\n",
      "        [0.0558],\n",
      "        [0.0608],\n",
      "        [0.0613],\n",
      "        [0.0631],\n",
      "        [0.0610],\n",
      "        [0.0623],\n",
      "        [0.0641],\n",
      "        [0.0657],\n",
      "        [0.0667],\n",
      "        [0.0655],\n",
      "        [0.0690],\n",
      "        [0.0620],\n",
      "        [0.0695],\n",
      "        [0.0735],\n",
      "        [0.0727],\n",
      "        [0.0691],\n",
      "        [0.0700],\n",
      "        [0.0764],\n",
      "        [0.0761],\n",
      "        [0.0783],\n",
      "        [0.0786],\n",
      "        [0.0849],\n",
      "        [0.0721],\n",
      "        [0.0855],\n",
      "        [0.0901],\n",
      "        [0.0946],\n",
      "        [0.0955],\n",
      "        [0.1014],\n",
      "        [0.1075],\n",
      "        [0.1136],\n",
      "        [0.1045],\n",
      "        [0.1207],\n",
      "        [0.1272],\n",
      "        [0.1303],\n",
      "        [0.1304],\n",
      "        [0.1325],\n",
      "        [0.1344],\n",
      "        [0.1432],\n",
      "        [0.1593],\n",
      "        [0.1606],\n",
      "        [0.1547],\n",
      "        [0.1623],\n",
      "        [0.1939],\n",
      "        [0.1986],\n",
      "        [0.1943]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 41.76138710975647\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.980029189027846e-08, 89)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [89, 47, 43, 139, 150, 22, 136, 66, 140, 42, 18, 88, 91, 90, 112, 138, 0, 77, 149, 8, 83, 21, 24, 69, 60, 58, 145, 55, 141, 92, 12, 67, 25, 144, 7, 82, 10, 36, 142, 137, 143, 41, 106, 121, 114, 48, 19, 14, 23, 9, 116, 75, 2, 46, 135, 38, 130, 44, 4, 45, 28, 11, 102, 5, 3, 113, 103, 128, 122, 129, 40, 57, 56, 54, 94, 123, 20, 35, 93, 59, 26, 13, 107, 81, 49, 68, 115, 87, 148, 76, 151, 84, 105, 104, 127, 131, 27, 147, 134, 1, 96, 65, 133, 95, 97, 78, 6, 15, 39, 101, 126, 29, 17, 120, 74, 146, 119, 132, 85, 79, 86, 34, 53, 124, 111, 80, 158, 50, 70, 99, 64, 98, 100, 118, 117, 125, 33, 52, 51, 157, 16, 73, 110, 61, 72, 32, 30, 63, 62, 152, 31, 37, 156, 108, 153, 109, 155] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6833],\n",
      "        [0.9092],\n",
      "        [0.8973],\n",
      "        [0.5490],\n",
      "        [0.4634],\n",
      "        [0.8509],\n",
      "        [0.5577],\n",
      "        [0.7619],\n",
      "        [0.5281],\n",
      "        [0.8914],\n",
      "        [0.8290],\n",
      "        [0.6912],\n",
      "        [0.6558],\n",
      "        [0.6812],\n",
      "        [0.5071],\n",
      "        [0.5601],\n",
      "        [0.9812],\n",
      "        [0.6565],\n",
      "        [0.4771],\n",
      "        [0.9381],\n",
      "        [0.6846],\n",
      "        [0.8373],\n",
      "        [0.8178],\n",
      "        [0.7302],\n",
      "        [0.8255],\n",
      "        [0.8450],\n",
      "        [0.5326],\n",
      "        [0.8723],\n",
      "        [0.5517],\n",
      "        [0.6230],\n",
      "        [0.9034],\n",
      "        [0.7335],\n",
      "        [0.8328],\n",
      "        [0.5458],\n",
      "        [0.9542],\n",
      "        [0.6900],\n",
      "        [0.9276],\n",
      "        [0.9161],\n",
      "        [0.5510],\n",
      "        [0.5761],\n",
      "        [0.5499],\n",
      "        [0.8363],\n",
      "        [0.6052],\n",
      "        [0.5460],\n",
      "        [0.5333],\n",
      "        [0.8871],\n",
      "        [0.8231],\n",
      "        [0.8750],\n",
      "        [0.8254],\n",
      "        [0.9102],\n",
      "        [0.5621],\n",
      "        [0.6317],\n",
      "        [1.0164],\n",
      "        [0.9168],\n",
      "        [0.5664],\n",
      "        [0.8678],\n",
      "        [0.5820],\n",
      "        [0.9084],\n",
      "        [0.9716],\n",
      "        [0.8934],\n",
      "        [0.8151],\n",
      "        [0.9092],\n",
      "        [0.6131],\n",
      "        [0.9546],\n",
      "        [1.0087],\n",
      "        [0.4968],\n",
      "        [0.6156],\n",
      "        [0.6033],\n",
      "        [0.5506],\n",
      "        [0.5983],\n",
      "        [0.8767],\n",
      "        [0.8050],\n",
      "        [0.8432],\n",
      "        [0.8522],\n",
      "        [0.6088],\n",
      "        [0.5972],\n",
      "        [0.8155],\n",
      "        [0.9180],\n",
      "        [0.6123],\n",
      "        [0.8296],\n",
      "        [0.8506],\n",
      "        [0.8900],\n",
      "        [0.6061],\n",
      "        [0.6723],\n",
      "        [0.8785],\n",
      "        [0.7170],\n",
      "        [0.5681],\n",
      "        [0.6617],\n",
      "        [0.5177],\n",
      "        [0.6676],\n",
      "        [0.4845],\n",
      "        [0.6601],\n",
      "        [0.5900],\n",
      "        [0.5892],\n",
      "        [0.5744],\n",
      "        [0.5694],\n",
      "        [0.7928],\n",
      "        [0.5348],\n",
      "        [0.5585],\n",
      "        [1.0244],\n",
      "        [0.6088],\n",
      "        [0.7963],\n",
      "        [0.5769],\n",
      "        [0.6081],\n",
      "        [0.6280],\n",
      "        [0.6645],\n",
      "        [0.9276],\n",
      "        [0.8990],\n",
      "        [0.8836],\n",
      "        [0.6181],\n",
      "        [0.5871],\n",
      "        [0.8058],\n",
      "        [0.8674],\n",
      "        [0.5497],\n",
      "        [0.6947],\n",
      "        [0.5329],\n",
      "        [0.5564],\n",
      "        [0.5809],\n",
      "        [0.6520],\n",
      "        [0.6700],\n",
      "        [0.6420],\n",
      "        [0.9002],\n",
      "        [0.8513],\n",
      "        [0.6076],\n",
      "        [0.5648],\n",
      "        [0.6642],\n",
      "        [0.3748],\n",
      "        [0.8628],\n",
      "        [0.7365],\n",
      "        [0.6146],\n",
      "        [0.8225],\n",
      "        [0.6183],\n",
      "        [0.6107],\n",
      "        [0.5322],\n",
      "        [0.5383],\n",
      "        [0.5816],\n",
      "        [0.8735],\n",
      "        [0.8597],\n",
      "        [0.8579],\n",
      "        [0.3954],\n",
      "        [0.9003],\n",
      "        [0.7280],\n",
      "        [0.5776],\n",
      "        [0.8311],\n",
      "        [0.7516],\n",
      "        [0.8381],\n",
      "        [0.8205],\n",
      "        [0.7929],\n",
      "        [0.7925],\n",
      "        [0.4811],\n",
      "        [0.8407],\n",
      "        [0.8685],\n",
      "        [0.4298],\n",
      "        [0.6144],\n",
      "        [0.4810],\n",
      "        [0.6223],\n",
      "        [0.4832]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0011],\n",
      "        [0.0011],\n",
      "        [0.0013],\n",
      "        [0.0013],\n",
      "        [0.0013],\n",
      "        [0.0016],\n",
      "        [0.0022],\n",
      "        [0.0026],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0057],\n",
      "        [0.0068],\n",
      "        [0.0078],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0088],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0106],\n",
      "        [0.0111],\n",
      "        [0.0124],\n",
      "        [0.0125],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0129],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0150],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0178],\n",
      "        [0.0181],\n",
      "        [0.0189],\n",
      "        [0.0194],\n",
      "        [0.0202],\n",
      "        [0.0212],\n",
      "        [0.0212],\n",
      "        [0.0230],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0250],\n",
      "        [0.0250],\n",
      "        [0.0257],\n",
      "        [0.0259],\n",
      "        [0.0274],\n",
      "        [0.0275],\n",
      "        [0.0285],\n",
      "        [0.0288],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0301],\n",
      "        [0.0304],\n",
      "        [0.0304],\n",
      "        [0.0307],\n",
      "        [0.0311],\n",
      "        [0.0311],\n",
      "        [0.0324],\n",
      "        [0.0333],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0351],\n",
      "        [0.0360],\n",
      "        [0.0362],\n",
      "        [0.0366],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0415],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0433],\n",
      "        [0.0437],\n",
      "        [0.0440],\n",
      "        [0.0441],\n",
      "        [0.0456],\n",
      "        [0.0460],\n",
      "        [0.0472],\n",
      "        [0.0472],\n",
      "        [0.0472],\n",
      "        [0.0472],\n",
      "        [0.0476],\n",
      "        [0.0487],\n",
      "        [0.0489],\n",
      "        [0.0502],\n",
      "        [0.0508],\n",
      "        [0.0508],\n",
      "        [0.0511],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0524],\n",
      "        [0.0536],\n",
      "        [0.0539],\n",
      "        [0.0540],\n",
      "        [0.0558],\n",
      "        [0.0577],\n",
      "        [0.0608],\n",
      "        [0.0610],\n",
      "        [0.0613],\n",
      "        [0.0620],\n",
      "        [0.0623],\n",
      "        [0.0631],\n",
      "        [0.0641],\n",
      "        [0.0655],\n",
      "        [0.0657],\n",
      "        [0.0667],\n",
      "        [0.0690],\n",
      "        [0.0691],\n",
      "        [0.0695],\n",
      "        [0.0700],\n",
      "        [0.0721],\n",
      "        [0.0727],\n",
      "        [0.0735],\n",
      "        [0.0761],\n",
      "        [0.0764],\n",
      "        [0.0783],\n",
      "        [0.0786],\n",
      "        [0.0849],\n",
      "        [0.0855],\n",
      "        [0.0901],\n",
      "        [0.0946],\n",
      "        [0.0955],\n",
      "        [0.1014],\n",
      "        [0.1045],\n",
      "        [0.1075],\n",
      "        [0.1136],\n",
      "        [0.1207],\n",
      "        [0.1272],\n",
      "        [0.1303],\n",
      "        [0.1304],\n",
      "        [0.1325],\n",
      "        [0.1344],\n",
      "        [0.1432],\n",
      "        [0.1547],\n",
      "        [0.1593],\n",
      "        [0.1606],\n",
      "        [0.1623],\n",
      "        [0.1939],\n",
      "        [0.1943],\n",
      "        [0.1986],\n",
      "        [0.2182]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0001],\n",
      "        [    0.0054],\n",
      "        [    0.0132],\n",
      "        [    0.0008],\n",
      "        [    0.0062],\n",
      "        [    0.0015],\n",
      "        [    0.0064],\n",
      "        [    0.0022],\n",
      "        [    0.0027],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0047],\n",
      "        [    0.0068],\n",
      "        [    0.0015],\n",
      "        [    0.0072],\n",
      "        [    0.0031],\n",
      "        [    0.0066],\n",
      "        [    0.0042],\n",
      "        [    0.0045],\n",
      "        [    0.0063],\n",
      "        [    0.0066],\n",
      "        [    0.0088],\n",
      "        [    0.0103],\n",
      "        [    0.0108],\n",
      "        [    0.0012],\n",
      "        [    0.0109],\n",
      "        [    0.0034],\n",
      "        [    0.0127],\n",
      "        [    0.0106],\n",
      "        [    0.0126],\n",
      "        [    0.0116],\n",
      "        [    0.0049],\n",
      "        [    0.0117],\n",
      "        [    0.0112],\n",
      "        [    0.0153],\n",
      "        [    0.0147],\n",
      "        [    0.0072],\n",
      "        [    0.0104],\n",
      "        [    0.0090],\n",
      "        [    0.0172],\n",
      "        [    0.0152],\n",
      "        [    0.0174],\n",
      "        [    0.0187],\n",
      "        [    0.0197],\n",
      "        [    0.0190],\n",
      "        [    0.0222],\n",
      "        [    0.0188],\n",
      "        [    0.0221],\n",
      "        [    0.0222],\n",
      "        [    0.0253],\n",
      "        [    0.0253],\n",
      "        [    0.0235],\n",
      "        [    0.0283],\n",
      "        [    0.0231],\n",
      "        [    0.0253],\n",
      "        [    0.0256],\n",
      "        [    0.0249],\n",
      "        [    0.0256],\n",
      "        [    0.0299],\n",
      "        [    0.0261],\n",
      "        [    0.0285],\n",
      "        [    0.0283],\n",
      "        [    0.0305],\n",
      "        [    0.0324],\n",
      "        [    0.0300],\n",
      "        [    0.0298],\n",
      "        [    0.0291],\n",
      "        [    0.0299],\n",
      "        [    0.0292],\n",
      "        [    0.0299],\n",
      "        [    0.0308],\n",
      "        [    0.0316],\n",
      "        [    0.0330],\n",
      "        [    0.0318],\n",
      "        [    0.0351],\n",
      "        [    0.0328],\n",
      "        [    0.0360],\n",
      "        [    0.0370],\n",
      "        [    0.0352],\n",
      "        [    0.0390],\n",
      "        [    0.0377],\n",
      "        [    0.0372],\n",
      "        [    0.0433],\n",
      "        [    0.0411],\n",
      "        [    0.0417],\n",
      "        [    0.0431],\n",
      "        [    0.0337],\n",
      "        [    0.0446],\n",
      "        [    0.0320],\n",
      "        [    0.0448],\n",
      "        [    0.0469],\n",
      "        [    0.0475],\n",
      "        [    0.0473],\n",
      "        [    0.0482],\n",
      "        [    0.0498],\n",
      "        [    0.0385],\n",
      "        [    0.0521],\n",
      "        [    0.0517],\n",
      "        [    0.0512],\n",
      "        [    0.0515],\n",
      "        [    0.0534],\n",
      "        [    0.0516],\n",
      "        [    0.0527],\n",
      "        [    0.0512],\n",
      "        [    0.0519],\n",
      "        [    0.0559],\n",
      "        [    0.0533],\n",
      "        [    0.0538],\n",
      "        [    0.0556],\n",
      "        [    0.0596],\n",
      "        [    0.0624],\n",
      "        [    0.0605],\n",
      "        [    0.0602],\n",
      "        [    0.0535],\n",
      "        [    0.0616],\n",
      "        [    0.0648],\n",
      "        [    0.0643],\n",
      "        [    0.0646],\n",
      "        [    0.0661],\n",
      "        [    0.0655],\n",
      "        [    0.0701],\n",
      "        [    0.0681],\n",
      "        [    0.0675],\n",
      "        [    0.0688],\n",
      "        [    0.0549],\n",
      "        [    0.0746],\n",
      "        [    0.0746],\n",
      "        [    0.0766],\n",
      "        [    0.0775],\n",
      "        [    0.0790],\n",
      "        [    0.0787],\n",
      "        [    0.0847],\n",
      "        [    0.0848],\n",
      "        [    0.0894],\n",
      "        [    0.0940],\n",
      "        [    0.0956],\n",
      "        [    0.1020],\n",
      "        [    0.0890],\n",
      "        [    0.1095],\n",
      "        [    0.1135],\n",
      "        [    0.1185],\n",
      "        [    0.1285],\n",
      "        [    0.1309],\n",
      "        [    0.1304],\n",
      "        [    0.1342],\n",
      "        [    0.1353],\n",
      "        [    0.1440],\n",
      "        [    0.1425],\n",
      "        [    0.1599],\n",
      "        [    0.1598],\n",
      "        [    0.1484],\n",
      "        [    0.1925],\n",
      "        [    0.1811],\n",
      "        [    0.1977],\n",
      "        [    0.2049]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.050755977630615\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5907119177427376e-08, 43)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [43, 89, 47, 22, 145, 138, 66, 42, 18, 77, 141, 88, 91, 8, 83, 90, 144, 139, 136, 21, 140, 149, 24, 112, 0, 142, 69, 143, 60, 137, 12, 58, 55, 82, 25, 7, 67, 92, 150, 36, 106, 10, 41, 121, 114, 23, 19, 48, 9, 14, 116, 38, 46, 4, 130, 75, 2, 44, 45, 11, 5, 135, 102, 122, 40, 128, 57, 28, 129, 103, 3, 56, 54, 123, 151, 113, 35, 94, 148, 20, 26, 93, 59, 81, 107, 147, 13, 68, 115, 87, 49, 76, 84, 105, 127, 104, 131, 27, 78, 96, 65, 95, 1, 6, 134, 97, 39, 133, 146, 101, 158, 126, 15, 29, 74, 120, 119, 17, 85, 79, 132, 34, 86, 111, 124, 80, 53, 50, 70, 99, 64, 100, 98, 118, 117, 157, 125, 33, 52, 51, 16, 73, 110, 61, 32, 72, 30, 63, 152, 62, 156, 37, 31, 153, 108, 109, 155, 154] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8977],\n",
      "        [0.6838],\n",
      "        [0.9091],\n",
      "        [0.8487],\n",
      "        [0.5242],\n",
      "        [0.5543],\n",
      "        [0.7621],\n",
      "        [0.8914],\n",
      "        [0.8288],\n",
      "        [0.6580],\n",
      "        [0.5439],\n",
      "        [0.6914],\n",
      "        [0.6561],\n",
      "        [0.9396],\n",
      "        [0.6869],\n",
      "        [0.6819],\n",
      "        [0.5378],\n",
      "        [0.5425],\n",
      "        [0.5528],\n",
      "        [0.8358],\n",
      "        [0.5200],\n",
      "        [0.4653],\n",
      "        [0.8161],\n",
      "        [0.5046],\n",
      "        [0.9839],\n",
      "        [0.5439],\n",
      "        [0.7305],\n",
      "        [0.5430],\n",
      "        [0.8270],\n",
      "        [0.5714],\n",
      "        [0.9053],\n",
      "        [0.8463],\n",
      "        [0.8725],\n",
      "        [0.6922],\n",
      "        [0.8316],\n",
      "        [0.9556],\n",
      "        [0.7337],\n",
      "        [0.6227],\n",
      "        [0.4513],\n",
      "        [0.9166],\n",
      "        [0.6041],\n",
      "        [0.9294],\n",
      "        [0.8352],\n",
      "        [0.5464],\n",
      "        [0.5327],\n",
      "        [0.8230],\n",
      "        [0.8226],\n",
      "        [0.8862],\n",
      "        [0.9111],\n",
      "        [0.8769],\n",
      "        [0.5629],\n",
      "        [0.8664],\n",
      "        [0.9160],\n",
      "        [0.9724],\n",
      "        [0.5817],\n",
      "        [0.6300],\n",
      "        [1.0181],\n",
      "        [0.9090],\n",
      "        [0.8930],\n",
      "        [0.9106],\n",
      "        [0.9551],\n",
      "        [0.5624],\n",
      "        [0.6130],\n",
      "        [0.5516],\n",
      "        [0.8755],\n",
      "        [0.6035],\n",
      "        [0.8058],\n",
      "        [0.8127],\n",
      "        [0.5987],\n",
      "        [0.6154],\n",
      "        [1.0096],\n",
      "        [0.8435],\n",
      "        [0.8517],\n",
      "        [0.5987],\n",
      "        [0.4724],\n",
      "        [0.4941],\n",
      "        [0.9193],\n",
      "        [0.6082],\n",
      "        [0.5077],\n",
      "        [0.8141],\n",
      "        [0.8496],\n",
      "        [0.6114],\n",
      "        [0.8306],\n",
      "        [0.6745],\n",
      "        [0.6046],\n",
      "        [0.5257],\n",
      "        [0.8924],\n",
      "        [0.7175],\n",
      "        [0.5688],\n",
      "        [0.6620],\n",
      "        [0.8767],\n",
      "        [0.6682],\n",
      "        [0.6609],\n",
      "        [0.5891],\n",
      "        [0.5743],\n",
      "        [0.5889],\n",
      "        [0.5684],\n",
      "        [0.7902],\n",
      "        [0.6656],\n",
      "        [0.6078],\n",
      "        [0.7971],\n",
      "        [0.6076],\n",
      "        [1.0272],\n",
      "        [0.9282],\n",
      "        [0.5551],\n",
      "        [0.6268],\n",
      "        [0.8829],\n",
      "        [0.5744],\n",
      "        [0.5245],\n",
      "        [0.6182],\n",
      "        [0.3576],\n",
      "        [0.5874],\n",
      "        [0.9013],\n",
      "        [0.8039],\n",
      "        [0.6937],\n",
      "        [0.5502],\n",
      "        [0.5571],\n",
      "        [0.8689],\n",
      "        [0.6518],\n",
      "        [0.6709],\n",
      "        [0.5791],\n",
      "        [0.9015],\n",
      "        [0.6417],\n",
      "        [0.5629],\n",
      "        [0.6086],\n",
      "        [0.6654],\n",
      "        [0.8502],\n",
      "        [0.8609],\n",
      "        [0.7376],\n",
      "        [0.6142],\n",
      "        [0.8236],\n",
      "        [0.6106],\n",
      "        [0.6175],\n",
      "        [0.5324],\n",
      "        [0.5390],\n",
      "        [0.3799],\n",
      "        [0.5823],\n",
      "        [0.8742],\n",
      "        [0.8597],\n",
      "        [0.8574],\n",
      "        [0.9024],\n",
      "        [0.7278],\n",
      "        [0.5754],\n",
      "        [0.8324],\n",
      "        [0.8380],\n",
      "        [0.7522],\n",
      "        [0.8188],\n",
      "        [0.7938],\n",
      "        [0.4689],\n",
      "        [0.7932],\n",
      "        [0.4158],\n",
      "        [0.8677],\n",
      "        [0.8401],\n",
      "        [0.4679],\n",
      "        [0.6130],\n",
      "        [0.6214],\n",
      "        [0.4699],\n",
      "        [0.4694]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0008],\n",
      "        [    0.0012],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0022],\n",
      "        [    0.0027],\n",
      "        [    0.0031],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0042],\n",
      "        [    0.0045],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0054],\n",
      "        [    0.0062],\n",
      "        [    0.0063],\n",
      "        [    0.0064],\n",
      "        [    0.0066],\n",
      "        [    0.0066],\n",
      "        [    0.0068],\n",
      "        [    0.0072],\n",
      "        [    0.0072],\n",
      "        [    0.0088],\n",
      "        [    0.0090],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0106],\n",
      "        [    0.0108],\n",
      "        [    0.0109],\n",
      "        [    0.0112],\n",
      "        [    0.0116],\n",
      "        [    0.0117],\n",
      "        [    0.0126],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0147],\n",
      "        [    0.0152],\n",
      "        [    0.0153],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0187],\n",
      "        [    0.0188],\n",
      "        [    0.0190],\n",
      "        [    0.0197],\n",
      "        [    0.0221],\n",
      "        [    0.0222],\n",
      "        [    0.0222],\n",
      "        [    0.0231],\n",
      "        [    0.0235],\n",
      "        [    0.0249],\n",
      "        [    0.0253],\n",
      "        [    0.0253],\n",
      "        [    0.0253],\n",
      "        [    0.0256],\n",
      "        [    0.0256],\n",
      "        [    0.0261],\n",
      "        [    0.0283],\n",
      "        [    0.0283],\n",
      "        [    0.0285],\n",
      "        [    0.0291],\n",
      "        [    0.0292],\n",
      "        [    0.0298],\n",
      "        [    0.0299],\n",
      "        [    0.0299],\n",
      "        [    0.0299],\n",
      "        [    0.0300],\n",
      "        [    0.0305],\n",
      "        [    0.0308],\n",
      "        [    0.0316],\n",
      "        [    0.0318],\n",
      "        [    0.0320],\n",
      "        [    0.0324],\n",
      "        [    0.0328],\n",
      "        [    0.0330],\n",
      "        [    0.0337],\n",
      "        [    0.0351],\n",
      "        [    0.0352],\n",
      "        [    0.0360],\n",
      "        [    0.0370],\n",
      "        [    0.0372],\n",
      "        [    0.0377],\n",
      "        [    0.0385],\n",
      "        [    0.0390],\n",
      "        [    0.0411],\n",
      "        [    0.0417],\n",
      "        [    0.0431],\n",
      "        [    0.0433],\n",
      "        [    0.0446],\n",
      "        [    0.0448],\n",
      "        [    0.0469],\n",
      "        [    0.0473],\n",
      "        [    0.0475],\n",
      "        [    0.0482],\n",
      "        [    0.0498],\n",
      "        [    0.0512],\n",
      "        [    0.0512],\n",
      "        [    0.0515],\n",
      "        [    0.0516],\n",
      "        [    0.0517],\n",
      "        [    0.0519],\n",
      "        [    0.0521],\n",
      "        [    0.0527],\n",
      "        [    0.0533],\n",
      "        [    0.0534],\n",
      "        [    0.0535],\n",
      "        [    0.0538],\n",
      "        [    0.0549],\n",
      "        [    0.0556],\n",
      "        [    0.0559],\n",
      "        [    0.0596],\n",
      "        [    0.0602],\n",
      "        [    0.0605],\n",
      "        [    0.0616],\n",
      "        [    0.0624],\n",
      "        [    0.0643],\n",
      "        [    0.0646],\n",
      "        [    0.0648],\n",
      "        [    0.0655],\n",
      "        [    0.0661],\n",
      "        [    0.0675],\n",
      "        [    0.0681],\n",
      "        [    0.0688],\n",
      "        [    0.0701],\n",
      "        [    0.0746],\n",
      "        [    0.0746],\n",
      "        [    0.0766],\n",
      "        [    0.0775],\n",
      "        [    0.0787],\n",
      "        [    0.0790],\n",
      "        [    0.0847],\n",
      "        [    0.0848],\n",
      "        [    0.0890],\n",
      "        [    0.0894],\n",
      "        [    0.0940],\n",
      "        [    0.0956],\n",
      "        [    0.1020],\n",
      "        [    0.1095],\n",
      "        [    0.1135],\n",
      "        [    0.1185],\n",
      "        [    0.1285],\n",
      "        [    0.1304],\n",
      "        [    0.1309],\n",
      "        [    0.1342],\n",
      "        [    0.1353],\n",
      "        [    0.1425],\n",
      "        [    0.1440],\n",
      "        [    0.1484],\n",
      "        [    0.1598],\n",
      "        [    0.1599],\n",
      "        [    0.1811],\n",
      "        [    0.1925],\n",
      "        [    0.1977],\n",
      "        [    0.2049],\n",
      "        [    0.2420]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0049],\n",
      "        [    0.0043],\n",
      "        [    0.0040],\n",
      "        [    0.0011],\n",
      "        [    0.0046],\n",
      "        [    0.0041],\n",
      "        [    0.0057],\n",
      "        [    0.0068],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0015],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0023],\n",
      "        [    0.0015],\n",
      "        [    0.0089],\n",
      "        [    0.0003],\n",
      "        [    0.0089],\n",
      "        [    0.0079],\n",
      "        [    0.0089],\n",
      "        [    0.0117],\n",
      "        [    0.0161],\n",
      "        [    0.0090],\n",
      "        [    0.0061],\n",
      "        [    0.0146],\n",
      "        [    0.0030],\n",
      "        [    0.0128],\n",
      "        [    0.0050],\n",
      "        [    0.0163],\n",
      "        [    0.0090],\n",
      "        [    0.0036],\n",
      "        [    0.0167],\n",
      "        [    0.0156],\n",
      "        [    0.0052],\n",
      "        [    0.0145],\n",
      "        [    0.0053],\n",
      "        [    0.0085],\n",
      "        [    0.0095],\n",
      "        [    0.0231],\n",
      "        [    0.0198],\n",
      "        [    0.0177],\n",
      "        [    0.0222],\n",
      "        [    0.0137],\n",
      "        [    0.0134],\n",
      "        [    0.0159],\n",
      "        [    0.0204],\n",
      "        [    0.0229],\n",
      "        [    0.0161],\n",
      "        [    0.0280],\n",
      "        [    0.0289],\n",
      "        [    0.0180],\n",
      "        [    0.0263],\n",
      "        [    0.0271],\n",
      "        [    0.0193],\n",
      "        [    0.0218],\n",
      "        [    0.0235],\n",
      "        [    0.0317],\n",
      "        [    0.0306],\n",
      "        [    0.0295],\n",
      "        [    0.0197],\n",
      "        [    0.0229],\n",
      "        [    0.0289],\n",
      "        [    0.0251],\n",
      "        [    0.0244],\n",
      "        [    0.0327],\n",
      "        [    0.0259],\n",
      "        [    0.0246],\n",
      "        [    0.0284],\n",
      "        [    0.0257],\n",
      "        [    0.0265],\n",
      "        [    0.0362],\n",
      "        [    0.0261],\n",
      "        [    0.0277],\n",
      "        [    0.0266],\n",
      "        [    0.0222],\n",
      "        [    0.0319],\n",
      "        [    0.0270],\n",
      "        [    0.0301],\n",
      "        [    0.0263],\n",
      "        [    0.0323],\n",
      "        [    0.0382],\n",
      "        [    0.0335],\n",
      "        [    0.0425],\n",
      "        [    0.0312],\n",
      "        [    0.0398],\n",
      "        [    0.0320],\n",
      "        [    0.0465],\n",
      "        [    0.0369],\n",
      "        [    0.0375],\n",
      "        [    0.0391],\n",
      "        [    0.0408],\n",
      "        [    0.0489],\n",
      "        [    0.0403],\n",
      "        [    0.0441],\n",
      "        [    0.0437],\n",
      "        [    0.0442],\n",
      "        [    0.0455],\n",
      "        [    0.0485],\n",
      "        [    0.0466],\n",
      "        [    0.0489],\n",
      "        [    0.0564],\n",
      "        [    0.0487],\n",
      "        [    0.0593],\n",
      "        [    0.0465],\n",
      "        [    0.0521],\n",
      "        [    0.0505],\n",
      "        [    0.0573],\n",
      "        [    0.0524],\n",
      "        [    0.0477],\n",
      "        [    0.0501],\n",
      "        [    0.0387],\n",
      "        [    0.0516],\n",
      "        [    0.0630],\n",
      "        [    0.0575],\n",
      "        [    0.0628],\n",
      "        [    0.0564],\n",
      "        [    0.0573],\n",
      "        [    0.0686],\n",
      "        [    0.0609],\n",
      "        [    0.0601],\n",
      "        [    0.0630],\n",
      "        [    0.0597],\n",
      "        [    0.0628],\n",
      "        [    0.0689],\n",
      "        [    0.0633],\n",
      "        [    0.0639],\n",
      "        [    0.0669],\n",
      "        [    0.0721],\n",
      "        [    0.0795],\n",
      "        [    0.0736],\n",
      "        [    0.0829],\n",
      "        [    0.0754],\n",
      "        [    0.0764],\n",
      "        [    0.0810],\n",
      "        [    0.0808],\n",
      "        [    0.0749],\n",
      "        [    0.0849],\n",
      "        [    0.0890],\n",
      "        [    0.0912],\n",
      "        [    0.0982],\n",
      "        [    0.1164],\n",
      "        [    0.1171],\n",
      "        [    0.1196],\n",
      "        [    0.1343],\n",
      "        [    0.1262],\n",
      "        [    0.1354],\n",
      "        [    0.1318],\n",
      "        [    0.1404],\n",
      "        [    0.1325],\n",
      "        [    0.1490],\n",
      "        [    0.1361],\n",
      "        [    0.1636],\n",
      "        [    0.1564],\n",
      "        [    0.1699],\n",
      "        [    0.1946],\n",
      "        [    0.2004],\n",
      "        [    0.1934],\n",
      "        [    0.2306]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.33908796310425\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.756472904432485e-08, 91)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [91, 88, 144, 22, 141, 83, 18, 77, 8, 142, 12, 47, 138, 89, 145, 43, 143, 82, 7, 66, 112, 42, 136, 67, 139, 21, 90, 24, 137, 92, 140, 69, 121, 41, 25, 0, 55, 114, 149, 48, 60, 58, 106, 116, 4, 11, 36, 23, 130, 151, 10, 19, 5, 150, 75, 122, 57, 102, 129, 128, 56, 38, 148, 103, 123, 35, 46, 54, 9, 28, 14, 135, 45, 94, 44, 81, 2, 113, 147, 20, 40, 93, 3, 68, 115, 26, 158, 87, 107, 84, 49, 59, 127, 105, 104, 131, 13, 6, 78, 146, 27, 95, 76, 96, 101, 97, 126, 134, 133, 120, 65, 39, 119, 29, 1, 34, 79, 85, 86, 74, 15, 132, 124, 80, 53, 17, 111, 50, 99, 157, 100, 98, 70, 117, 118, 64, 125, 33, 52, 51, 16, 73, 110, 32, 30, 152, 61, 72, 156, 63, 62, 31, 37, 153, 155, 108, 109, 154, 71] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6600],\n",
      "        [0.6953],\n",
      "        [0.5327],\n",
      "        [0.8507],\n",
      "        [0.5391],\n",
      "        [0.6930],\n",
      "        [0.8332],\n",
      "        [0.6630],\n",
      "        [0.9461],\n",
      "        [0.5396],\n",
      "        [0.9123],\n",
      "        [0.9135],\n",
      "        [0.5518],\n",
      "        [0.6878],\n",
      "        [0.5184],\n",
      "        [0.9027],\n",
      "        [0.5390],\n",
      "        [0.6982],\n",
      "        [0.9620],\n",
      "        [0.7663],\n",
      "        [0.5052],\n",
      "        [0.8960],\n",
      "        [0.5511],\n",
      "        [0.7378],\n",
      "        [0.5390],\n",
      "        [0.8385],\n",
      "        [0.6861],\n",
      "        [0.8184],\n",
      "        [0.5700],\n",
      "        [0.6258],\n",
      "        [0.5148],\n",
      "        [0.7346],\n",
      "        [0.5503],\n",
      "        [0.8387],\n",
      "        [0.8344],\n",
      "        [0.9913],\n",
      "        [0.8772],\n",
      "        [0.5355],\n",
      "        [0.4559],\n",
      "        [0.8899],\n",
      "        [0.8330],\n",
      "        [0.8522],\n",
      "        [0.6065],\n",
      "        [0.5671],\n",
      "        [0.9779],\n",
      "        [0.9170],\n",
      "        [0.9216],\n",
      "        [0.8246],\n",
      "        [0.5852],\n",
      "        [0.4626],\n",
      "        [0.9363],\n",
      "        [0.8265],\n",
      "        [0.9604],\n",
      "        [0.4414],\n",
      "        [0.6318],\n",
      "        [0.5563],\n",
      "        [0.8111],\n",
      "        [0.6165],\n",
      "        [0.6029],\n",
      "        [0.6074],\n",
      "        [0.8482],\n",
      "        [0.8696],\n",
      "        [0.5003],\n",
      "        [0.6189],\n",
      "        [0.6040],\n",
      "        [0.9252],\n",
      "        [0.9196],\n",
      "        [0.8556],\n",
      "        [0.9169],\n",
      "        [0.8142],\n",
      "        [0.8837],\n",
      "        [0.5617],\n",
      "        [0.8970],\n",
      "        [0.6111],\n",
      "        [0.9141],\n",
      "        [0.6806],\n",
      "        [1.0244],\n",
      "        [0.4946],\n",
      "        [0.5192],\n",
      "        [0.8169],\n",
      "        [0.8789],\n",
      "        [0.6139],\n",
      "        [1.0153],\n",
      "        [0.7217],\n",
      "        [0.5730],\n",
      "        [0.8527],\n",
      "        [0.3414],\n",
      "        [0.6659],\n",
      "        [0.6067],\n",
      "        [0.6653],\n",
      "        [0.8792],\n",
      "        [0.8361],\n",
      "        [0.5779],\n",
      "        [0.5919],\n",
      "        [0.5922],\n",
      "        [0.5711],\n",
      "        [0.8998],\n",
      "        [0.9335],\n",
      "        [0.6701],\n",
      "        [0.5187],\n",
      "        [0.7916],\n",
      "        [0.6105],\n",
      "        [0.6725],\n",
      "        [0.6101],\n",
      "        [0.6219],\n",
      "        [0.6289],\n",
      "        [0.5914],\n",
      "        [0.5552],\n",
      "        [0.5753],\n",
      "        [0.5544],\n",
      "        [0.8019],\n",
      "        [0.8869],\n",
      "        [0.5614],\n",
      "        [0.8061],\n",
      "        [1.0348],\n",
      "        [0.9072],\n",
      "        [0.6754],\n",
      "        [0.6552],\n",
      "        [0.6450],\n",
      "        [0.6963],\n",
      "        [0.9084],\n",
      "        [0.5810],\n",
      "        [0.6134],\n",
      "        [0.6703],\n",
      "        [0.8534],\n",
      "        [0.8752],\n",
      "        [0.5642],\n",
      "        [0.8633],\n",
      "        [0.6172],\n",
      "        [0.3659],\n",
      "        [0.6139],\n",
      "        [0.6202],\n",
      "        [0.7425],\n",
      "        [0.5431],\n",
      "        [0.5361],\n",
      "        [0.8290],\n",
      "        [0.5868],\n",
      "        [0.8792],\n",
      "        [0.8640],\n",
      "        [0.8612],\n",
      "        [0.9093],\n",
      "        [0.7314],\n",
      "        [0.5764],\n",
      "        [0.8423],\n",
      "        [0.8212],\n",
      "        [0.4588],\n",
      "        [0.8381],\n",
      "        [0.7567],\n",
      "        [0.4035],\n",
      "        [0.7989],\n",
      "        [0.7983],\n",
      "        [0.8436],\n",
      "        [0.8715],\n",
      "        [0.4566],\n",
      "        [0.4584],\n",
      "        [0.6151],\n",
      "        [0.6241],\n",
      "        [0.4580],\n",
      "        [0.7465]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0011],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0023],\n",
      "        [    0.0030],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0043],\n",
      "        [    0.0046],\n",
      "        [    0.0049],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0053],\n",
      "        [    0.0057],\n",
      "        [    0.0061],\n",
      "        [    0.0068],\n",
      "        [    0.0079],\n",
      "        [    0.0085],\n",
      "        [    0.0089],\n",
      "        [    0.0089],\n",
      "        [    0.0089],\n",
      "        [    0.0090],\n",
      "        [    0.0090],\n",
      "        [    0.0095],\n",
      "        [    0.0117],\n",
      "        [    0.0128],\n",
      "        [    0.0134],\n",
      "        [    0.0137],\n",
      "        [    0.0145],\n",
      "        [    0.0146],\n",
      "        [    0.0156],\n",
      "        [    0.0159],\n",
      "        [    0.0161],\n",
      "        [    0.0161],\n",
      "        [    0.0163],\n",
      "        [    0.0167],\n",
      "        [    0.0177],\n",
      "        [    0.0180],\n",
      "        [    0.0193],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0204],\n",
      "        [    0.0218],\n",
      "        [    0.0222],\n",
      "        [    0.0222],\n",
      "        [    0.0229],\n",
      "        [    0.0229],\n",
      "        [    0.0231],\n",
      "        [    0.0235],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0251],\n",
      "        [    0.0257],\n",
      "        [    0.0259],\n",
      "        [    0.0261],\n",
      "        [    0.0263],\n",
      "        [    0.0263],\n",
      "        [    0.0265],\n",
      "        [    0.0266],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0277],\n",
      "        [    0.0280],\n",
      "        [    0.0284],\n",
      "        [    0.0289],\n",
      "        [    0.0289],\n",
      "        [    0.0295],\n",
      "        [    0.0301],\n",
      "        [    0.0306],\n",
      "        [    0.0312],\n",
      "        [    0.0317],\n",
      "        [    0.0319],\n",
      "        [    0.0320],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0335],\n",
      "        [    0.0362],\n",
      "        [    0.0369],\n",
      "        [    0.0375],\n",
      "        [    0.0382],\n",
      "        [    0.0387],\n",
      "        [    0.0391],\n",
      "        [    0.0398],\n",
      "        [    0.0403],\n",
      "        [    0.0408],\n",
      "        [    0.0425],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0442],\n",
      "        [    0.0455],\n",
      "        [    0.0465],\n",
      "        [    0.0465],\n",
      "        [    0.0466],\n",
      "        [    0.0477],\n",
      "        [    0.0485],\n",
      "        [    0.0487],\n",
      "        [    0.0489],\n",
      "        [    0.0489],\n",
      "        [    0.0501],\n",
      "        [    0.0505],\n",
      "        [    0.0516],\n",
      "        [    0.0521],\n",
      "        [    0.0524],\n",
      "        [    0.0564],\n",
      "        [    0.0564],\n",
      "        [    0.0573],\n",
      "        [    0.0573],\n",
      "        [    0.0575],\n",
      "        [    0.0593],\n",
      "        [    0.0597],\n",
      "        [    0.0601],\n",
      "        [    0.0609],\n",
      "        [    0.0628],\n",
      "        [    0.0628],\n",
      "        [    0.0630],\n",
      "        [    0.0630],\n",
      "        [    0.0633],\n",
      "        [    0.0639],\n",
      "        [    0.0669],\n",
      "        [    0.0686],\n",
      "        [    0.0689],\n",
      "        [    0.0721],\n",
      "        [    0.0736],\n",
      "        [    0.0749],\n",
      "        [    0.0754],\n",
      "        [    0.0764],\n",
      "        [    0.0795],\n",
      "        [    0.0808],\n",
      "        [    0.0810],\n",
      "        [    0.0829],\n",
      "        [    0.0849],\n",
      "        [    0.0890],\n",
      "        [    0.0912],\n",
      "        [    0.0982],\n",
      "        [    0.1164],\n",
      "        [    0.1171],\n",
      "        [    0.1196],\n",
      "        [    0.1262],\n",
      "        [    0.1318],\n",
      "        [    0.1325],\n",
      "        [    0.1343],\n",
      "        [    0.1354],\n",
      "        [    0.1361],\n",
      "        [    0.1404],\n",
      "        [    0.1490],\n",
      "        [    0.1564],\n",
      "        [    0.1636],\n",
      "        [    0.1699],\n",
      "        [    0.1934],\n",
      "        [    0.1946],\n",
      "        [    0.2004],\n",
      "        [    0.2306],\n",
      "        [    0.2854]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0032],\n",
      "        [0.0040],\n",
      "        [0.0052],\n",
      "        [0.0027],\n",
      "        [0.0062],\n",
      "        [0.0006],\n",
      "        [0.0003],\n",
      "        [0.0010],\n",
      "        [0.0006],\n",
      "        [0.0012],\n",
      "        [0.0042],\n",
      "        [0.0008],\n",
      "        [0.0070],\n",
      "        [0.0005],\n",
      "        [0.0098],\n",
      "        [0.0028],\n",
      "        [0.0011],\n",
      "        [0.0075],\n",
      "        [0.0073],\n",
      "        [0.0008],\n",
      "        [0.0079],\n",
      "        [0.0043],\n",
      "        [0.0101],\n",
      "        [0.0132],\n",
      "        [0.0124],\n",
      "        [0.0058],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0069],\n",
      "        [0.0131],\n",
      "        [0.0164],\n",
      "        [0.0080],\n",
      "        [0.0117],\n",
      "        [0.0166],\n",
      "        [0.0111],\n",
      "        [0.0132],\n",
      "        [0.0121],\n",
      "        [0.0155],\n",
      "        [0.0241],\n",
      "        [0.0196],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0157],\n",
      "        [0.0166],\n",
      "        [0.0221],\n",
      "        [0.0208],\n",
      "        [0.0174],\n",
      "        [0.0163],\n",
      "        [0.0202],\n",
      "        [0.0139],\n",
      "        [0.0211],\n",
      "        [0.0207],\n",
      "        [0.0258],\n",
      "        [0.0314],\n",
      "        [0.0288],\n",
      "        [0.0221],\n",
      "        [0.0271],\n",
      "        [0.0267],\n",
      "        [0.0236],\n",
      "        [0.0241],\n",
      "        [0.0293],\n",
      "        [0.0228],\n",
      "        [0.0196],\n",
      "        [0.0281],\n",
      "        [0.0240],\n",
      "        [0.0288],\n",
      "        [0.0232],\n",
      "        [0.0315],\n",
      "        [0.0260],\n",
      "        [0.0328],\n",
      "        [0.0283],\n",
      "        [0.0303],\n",
      "        [0.0263],\n",
      "        [0.0335],\n",
      "        [0.0283],\n",
      "        [0.0334],\n",
      "        [0.0291],\n",
      "        [0.0335],\n",
      "        [0.0260],\n",
      "        [0.0352],\n",
      "        [0.0293],\n",
      "        [0.0374],\n",
      "        [0.0331],\n",
      "        [0.0413],\n",
      "        [0.0360],\n",
      "        [0.0349],\n",
      "        [0.0259],\n",
      "        [0.0427],\n",
      "        [0.0377],\n",
      "        [0.0436],\n",
      "        [0.0452],\n",
      "        [0.0399],\n",
      "        [0.0418],\n",
      "        [0.0459],\n",
      "        [0.0454],\n",
      "        [0.0444],\n",
      "        [0.0464],\n",
      "        [0.0490],\n",
      "        [0.0502],\n",
      "        [0.0423],\n",
      "        [0.0528],\n",
      "        [0.0519],\n",
      "        [0.0454],\n",
      "        [0.0523],\n",
      "        [0.0517],\n",
      "        [0.0542],\n",
      "        [0.0496],\n",
      "        [0.0529],\n",
      "        [0.0527],\n",
      "        [0.0547],\n",
      "        [0.0520],\n",
      "        [0.0543],\n",
      "        [0.0557],\n",
      "        [0.0613],\n",
      "        [0.0576],\n",
      "        [0.0614],\n",
      "        [0.0636],\n",
      "        [0.0649],\n",
      "        [0.0666],\n",
      "        [0.0572],\n",
      "        [0.0625],\n",
      "        [0.0626],\n",
      "        [0.0611],\n",
      "        [0.0670],\n",
      "        [0.0710],\n",
      "        [0.0678],\n",
      "        [0.0670],\n",
      "        [0.0765],\n",
      "        [0.0761],\n",
      "        [0.0637],\n",
      "        [0.0775],\n",
      "        [0.0794],\n",
      "        [0.0753],\n",
      "        [0.0793],\n",
      "        [0.0797],\n",
      "        [0.0787],\n",
      "        [0.0825],\n",
      "        [0.0910],\n",
      "        [0.0944],\n",
      "        [0.1017],\n",
      "        [0.1157],\n",
      "        [0.1119],\n",
      "        [0.1171],\n",
      "        [0.1286],\n",
      "        [0.1356],\n",
      "        [0.1241],\n",
      "        [0.1314],\n",
      "        [0.1306],\n",
      "        [0.1262],\n",
      "        [0.1365],\n",
      "        [0.1455],\n",
      "        [0.1595],\n",
      "        [0.1606],\n",
      "        [0.1604],\n",
      "        [0.1837],\n",
      "        [0.1927],\n",
      "        [0.1990],\n",
      "        [0.2210],\n",
      "        [0.2811]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.62881898880005\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 3 個區塊累積花費時間(s) 1.2235023975372314\n",
      "<<The performance of 3 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2235023975372314\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1235.45\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1739.62\n",
      "The accuracy(2000) for l = 1: 83.02%\n",
      "The accuracy(3000) for l = 1: 89.94%\n",
      "The maximum error: tensor(7175.7891)\n",
      "The minimum error: tensor(8.1914)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1450.7\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1528.1\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1899.9453125\n",
      "The minimum error: 694.4375\n",
      "------------------------------------------------------------\n",
      "0.8301886792452831\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<4>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0298413144482765e-07, 14)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [14, 85, 4, 79, 43, 62, 73, 139, 138, 18, 39, 87, 84, 8, 38, 140, 86, 20, 17, 137, 133, 134, 3, 78, 108, 65, 141, 132, 21, 117, 51, 135, 88, 63, 147, 56, 54, 110, 102, 19, 136, 112, 37, 32, 44, 144, 126, 15, 7, 6, 118, 0, 34, 42, 125, 119, 145, 124, 1, 154, 143, 5, 41, 98, 53, 158, 99, 40, 10, 31, 71, 36, 52, 131, 146, 50, 24, 77, 90, 109, 22, 16, 111, 89, 103, 55, 64, 123, 142, 83, 80, 127, 45, 72, 100, 101, 9, 2, 122, 74, 97, 91, 61, 92, 129, 23, 130, 155, 93, 35, 116, 115, 70, 120, 25, 30, 11, 128, 75, 153, 81, 82, 107, 76, 13, 49, 157, 156, 66, 95, 46, 96, 60, 113, 94, 114, 121, 29, 48, 47, 69, 12, 106, 148, 152, 28, 68, 57, 26, 59, 58, 27, 149, 33, 151]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0075],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0111],\n",
      "        [0.0117],\n",
      "        [0.0121],\n",
      "        [0.0124],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0139],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0174],\n",
      "        [0.0196],\n",
      "        [0.0196],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0228],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0241],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0267],\n",
      "        [0.0271],\n",
      "        [0.0272],\n",
      "        [0.0281],\n",
      "        [0.0283],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0288],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0303],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0328],\n",
      "        [0.0334],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0349],\n",
      "        [0.0352],\n",
      "        [0.0360],\n",
      "        [0.0374],\n",
      "        [0.0377],\n",
      "        [0.0399],\n",
      "        [0.0413],\n",
      "        [0.0418],\n",
      "        [0.0423],\n",
      "        [0.0427],\n",
      "        [0.0436],\n",
      "        [0.0444],\n",
      "        [0.0452],\n",
      "        [0.0454],\n",
      "        [0.0454],\n",
      "        [0.0459],\n",
      "        [0.0464],\n",
      "        [0.0490],\n",
      "        [0.0496],\n",
      "        [0.0502],\n",
      "        [0.0517],\n",
      "        [0.0519],\n",
      "        [0.0520],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0528],\n",
      "        [0.0529],\n",
      "        [0.0540],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0547],\n",
      "        [0.0557],\n",
      "        [0.0572],\n",
      "        [0.0611],\n",
      "        [0.0613],\n",
      "        [0.0614],\n",
      "        [0.0625],\n",
      "        [0.0626],\n",
      "        [0.0636],\n",
      "        [0.0637],\n",
      "        [0.0649],\n",
      "        [0.0666],\n",
      "        [0.0670],\n",
      "        [0.0670],\n",
      "        [0.0678],\n",
      "        [0.0710],\n",
      "        [0.0717],\n",
      "        [0.0744],\n",
      "        [0.0753],\n",
      "        [0.0761],\n",
      "        [0.0765],\n",
      "        [0.0775],\n",
      "        [0.0787],\n",
      "        [0.0793],\n",
      "        [0.0794],\n",
      "        [0.0797],\n",
      "        [0.0825],\n",
      "        [0.0910],\n",
      "        [0.0944],\n",
      "        [0.1017],\n",
      "        [0.1119],\n",
      "        [0.1157],\n",
      "        [0.1171],\n",
      "        [0.1241],\n",
      "        [0.1262],\n",
      "        [0.1286],\n",
      "        [0.1306],\n",
      "        [0.1314],\n",
      "        [0.1356],\n",
      "        [0.1365],\n",
      "        [0.1455],\n",
      "        [0.1595],\n",
      "        [0.1604],\n",
      "        [0.1606],\n",
      "        [0.1837]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0298413144482765e-07, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [14, 85, 4, 79, 43, 62, 73, 139, 138, 18, 39, 87, 84, 8, 38, 140, 86, 20, 17, 137, 133, 134, 3, 78, 108, 65, 141, 132, 21, 117, 51, 135, 88, 63, 147, 56, 54, 110, 102, 19, 136, 112, 37, 32, 44, 144, 126, 15, 7, 6, 118, 0, 34, 42, 125, 119, 145, 124, 1, 154, 143, 5, 41, 98, 53, 158, 99, 40, 10, 31, 71, 36, 52, 131, 146, 50, 24, 77, 90, 109, 22, 16, 111, 89, 103, 55, 64, 123, 142, 83, 80, 127, 45, 72, 100, 101, 9, 2, 122, 74, 97, 91, 61, 92, 129, 23, 130, 155, 93, 35, 116, 115, 70, 120, 25, 30, 11, 128, 75, 153, 81, 82, 107, 76, 13, 49, 157, 156, 66, 95, 46, 96, 60, 113, 94, 114, 121, 29, 48, 47, 69, 12, 106, 148, 152, 28, 68, 57, 26, 59, 58, 27, 149, 33, 151, 104] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8312],\n",
      "        [0.6840],\n",
      "        [0.9444],\n",
      "        [0.6908],\n",
      "        [0.9104],\n",
      "        [0.7614],\n",
      "        [0.6601],\n",
      "        [0.5351],\n",
      "        [0.5354],\n",
      "        [0.8469],\n",
      "        [0.9006],\n",
      "        [0.6566],\n",
      "        [0.6911],\n",
      "        [0.9117],\n",
      "        [0.8936],\n",
      "        [0.5278],\n",
      "        [0.6826],\n",
      "        [0.8149],\n",
      "        [0.8353],\n",
      "        [0.5344],\n",
      "        [0.5679],\n",
      "        [0.5488],\n",
      "        [0.9600],\n",
      "        [0.6959],\n",
      "        [0.5034],\n",
      "        [0.7298],\n",
      "        [0.5132],\n",
      "        [0.5489],\n",
      "        [0.8311],\n",
      "        [0.5520],\n",
      "        [0.8738],\n",
      "        [0.5355],\n",
      "        [0.6222],\n",
      "        [0.7331],\n",
      "        [0.4543],\n",
      "        [0.8308],\n",
      "        [0.8499],\n",
      "        [0.5359],\n",
      "        [0.6046],\n",
      "        [0.8206],\n",
      "        [0.5100],\n",
      "        [0.5685],\n",
      "        [0.8358],\n",
      "        [0.9192],\n",
      "        [0.8863],\n",
      "        [0.4936],\n",
      "        [0.5867],\n",
      "        [0.8243],\n",
      "        [0.9158],\n",
      "        [0.9352],\n",
      "        [0.5586],\n",
      "        [0.9751],\n",
      "        [0.8661],\n",
      "        [0.9158],\n",
      "        [0.6050],\n",
      "        [0.6065],\n",
      "        [0.4478],\n",
      "        [0.6092],\n",
      "        [0.9576],\n",
      "        [0.3286],\n",
      "        [0.5132],\n",
      "        [0.9150],\n",
      "        [0.8937],\n",
      "        [0.6149],\n",
      "        [0.8086],\n",
      "        [0.3851],\n",
      "        [0.6173],\n",
      "        [0.9117],\n",
      "        [0.8831],\n",
      "        [0.9234],\n",
      "        [0.6265],\n",
      "        [0.8755],\n",
      "        [0.8450],\n",
      "        [0.5603],\n",
      "        [0.4332],\n",
      "        [0.8518],\n",
      "        [0.8097],\n",
      "        [0.6783],\n",
      "        [0.6077],\n",
      "        [0.4930],\n",
      "        [0.8493],\n",
      "        [0.8140],\n",
      "        [0.5745],\n",
      "        [0.6100],\n",
      "        [0.6046],\n",
      "        [0.8335],\n",
      "        [0.7173],\n",
      "        [0.5797],\n",
      "        [0.5133],\n",
      "        [0.6623],\n",
      "        [0.6620],\n",
      "        [0.5722],\n",
      "        [0.8748],\n",
      "        [0.6690],\n",
      "        [0.5909],\n",
      "        [0.5901],\n",
      "        [0.8997],\n",
      "        [0.9310],\n",
      "        [0.5934],\n",
      "        [0.6666],\n",
      "        [0.6203],\n",
      "        [0.6073],\n",
      "        [0.7975],\n",
      "        [0.6067],\n",
      "        [0.5750],\n",
      "        [0.7872],\n",
      "        [0.5543],\n",
      "        [0.3732],\n",
      "        [0.6253],\n",
      "        [0.8839],\n",
      "        [0.5561],\n",
      "        [0.5630],\n",
      "        [0.6907],\n",
      "        [0.6157],\n",
      "        [0.8022],\n",
      "        [0.9055],\n",
      "        [0.9079],\n",
      "        [0.5813],\n",
      "        [0.6719],\n",
      "        [0.3547],\n",
      "        [0.6512],\n",
      "        [0.6412],\n",
      "        [0.5623],\n",
      "        [0.6672],\n",
      "        [0.8743],\n",
      "        [0.8492],\n",
      "        [0.3788],\n",
      "        [0.3822],\n",
      "        [0.7383],\n",
      "        [0.6146],\n",
      "        [0.8589],\n",
      "        [0.6118],\n",
      "        [0.8248],\n",
      "        [0.5445],\n",
      "        [0.6172],\n",
      "        [0.5374],\n",
      "        [0.5892],\n",
      "        [0.8772],\n",
      "        [0.8608],\n",
      "        [0.8576],\n",
      "        [0.7263],\n",
      "        [0.9085],\n",
      "        [0.5740],\n",
      "        [0.4505],\n",
      "        [0.3936],\n",
      "        [0.8398],\n",
      "        [0.7520],\n",
      "        [0.8353],\n",
      "        [0.8173],\n",
      "        [0.7950],\n",
      "        [0.7947],\n",
      "        [0.8405],\n",
      "        [0.4472],\n",
      "        [0.8685],\n",
      "        [0.4486],\n",
      "        [0.6132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0075],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0111],\n",
      "        [0.0117],\n",
      "        [0.0121],\n",
      "        [0.0124],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0139],\n",
      "        [0.0141],\n",
      "        [0.0144],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0174],\n",
      "        [0.0196],\n",
      "        [0.0196],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0228],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0241],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0267],\n",
      "        [0.0271],\n",
      "        [0.0272],\n",
      "        [0.0281],\n",
      "        [0.0283],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0288],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0303],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0328],\n",
      "        [0.0334],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0349],\n",
      "        [0.0352],\n",
      "        [0.0360],\n",
      "        [0.0374],\n",
      "        [0.0377],\n",
      "        [0.0399],\n",
      "        [0.0413],\n",
      "        [0.0418],\n",
      "        [0.0423],\n",
      "        [0.0427],\n",
      "        [0.0436],\n",
      "        [0.0444],\n",
      "        [0.0452],\n",
      "        [0.0454],\n",
      "        [0.0454],\n",
      "        [0.0459],\n",
      "        [0.0464],\n",
      "        [0.0490],\n",
      "        [0.0496],\n",
      "        [0.0502],\n",
      "        [0.0517],\n",
      "        [0.0519],\n",
      "        [0.0520],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0528],\n",
      "        [0.0529],\n",
      "        [0.0540],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0547],\n",
      "        [0.0557],\n",
      "        [0.0572],\n",
      "        [0.0611],\n",
      "        [0.0613],\n",
      "        [0.0614],\n",
      "        [0.0625],\n",
      "        [0.0626],\n",
      "        [0.0636],\n",
      "        [0.0637],\n",
      "        [0.0649],\n",
      "        [0.0666],\n",
      "        [0.0670],\n",
      "        [0.0670],\n",
      "        [0.0678],\n",
      "        [0.0710],\n",
      "        [0.0717],\n",
      "        [0.0744],\n",
      "        [0.0753],\n",
      "        [0.0761],\n",
      "        [0.0765],\n",
      "        [0.0775],\n",
      "        [0.0787],\n",
      "        [0.0793],\n",
      "        [0.0794],\n",
      "        [0.0797],\n",
      "        [0.0825],\n",
      "        [0.0910],\n",
      "        [0.0944],\n",
      "        [0.1017],\n",
      "        [0.1119],\n",
      "        [0.1157],\n",
      "        [0.1171],\n",
      "        [0.1241],\n",
      "        [0.1262],\n",
      "        [0.1286],\n",
      "        [0.1306],\n",
      "        [0.1314],\n",
      "        [0.1356],\n",
      "        [0.1365],\n",
      "        [0.1455],\n",
      "        [0.1595],\n",
      "        [0.1604],\n",
      "        [0.1606],\n",
      "        [0.1837],\n",
      "        [0.1927]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0059],\n",
      "        [0.0069],\n",
      "        [0.0085],\n",
      "        [0.0083],\n",
      "        [0.0065],\n",
      "        [0.0054],\n",
      "        [0.0065],\n",
      "        [0.0029],\n",
      "        [0.0003],\n",
      "        [0.0021],\n",
      "        [0.0094],\n",
      "        [0.0034],\n",
      "        [0.0019],\n",
      "        [0.0048],\n",
      "        [0.0103],\n",
      "        [0.0043],\n",
      "        [0.0123],\n",
      "        [0.0100],\n",
      "        [0.0112],\n",
      "        [0.0054],\n",
      "        [0.0107],\n",
      "        [0.0043],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0039],\n",
      "        [0.0134],\n",
      "        [0.0094],\n",
      "        [0.0066],\n",
      "        [0.0163],\n",
      "        [0.0037],\n",
      "        [0.0180],\n",
      "        [0.0105],\n",
      "        [0.0072],\n",
      "        [0.0087],\n",
      "        [0.0104],\n",
      "        [0.0217],\n",
      "        [0.0214],\n",
      "        [0.0091],\n",
      "        [0.0226],\n",
      "        [0.0203],\n",
      "        [0.0162],\n",
      "        [0.0086],\n",
      "        [0.0124],\n",
      "        [0.0238],\n",
      "        [0.0148],\n",
      "        [0.0185],\n",
      "        [0.0124],\n",
      "        [0.0268],\n",
      "        [0.0126],\n",
      "        [0.0297],\n",
      "        [0.0132],\n",
      "        [0.0141],\n",
      "        [0.0269],\n",
      "        [0.0284],\n",
      "        [0.0147],\n",
      "        [0.0142],\n",
      "        [0.0272],\n",
      "        [0.0154],\n",
      "        [0.0185],\n",
      "        [0.0154],\n",
      "        [0.0256],\n",
      "        [0.0331],\n",
      "        [0.0318],\n",
      "        [0.0191],\n",
      "        [0.0210],\n",
      "        [0.0169],\n",
      "        [0.0204],\n",
      "        [0.0351],\n",
      "        [0.0368],\n",
      "        [0.0214],\n",
      "        [0.0249],\n",
      "        [0.0335],\n",
      "        [0.0237],\n",
      "        [0.0259],\n",
      "        [0.0349],\n",
      "        [0.0262],\n",
      "        [0.0292],\n",
      "        [0.0248],\n",
      "        [0.0278],\n",
      "        [0.0297],\n",
      "        [0.0406],\n",
      "        [0.0298],\n",
      "        [0.0280],\n",
      "        [0.0322],\n",
      "        [0.0442],\n",
      "        [0.0466],\n",
      "        [0.0355],\n",
      "        [0.0334],\n",
      "        [0.0427],\n",
      "        [0.0369],\n",
      "        [0.0364],\n",
      "        [0.0373],\n",
      "        [0.0411],\n",
      "        [0.0519],\n",
      "        [0.0380],\n",
      "        [0.0389],\n",
      "        [0.0556],\n",
      "        [0.0419],\n",
      "        [0.0408],\n",
      "        [0.0427],\n",
      "        [0.0440],\n",
      "        [0.0463],\n",
      "        [0.0571],\n",
      "        [0.0471],\n",
      "        [0.0470],\n",
      "        [0.0495],\n",
      "        [0.0481],\n",
      "        [0.0438],\n",
      "        [0.0488],\n",
      "        [0.0592],\n",
      "        [0.0466],\n",
      "        [0.0475],\n",
      "        [0.0617],\n",
      "        [0.0516],\n",
      "        [0.0573],\n",
      "        [0.0540],\n",
      "        [0.0714],\n",
      "        [0.0563],\n",
      "        [0.0561],\n",
      "        [0.0557],\n",
      "        [0.0591],\n",
      "        [0.0611],\n",
      "        [0.0722],\n",
      "        [0.0593],\n",
      "        [0.0761],\n",
      "        [0.0663],\n",
      "        [0.0602],\n",
      "        [0.0632],\n",
      "        [0.0814],\n",
      "        [0.0694],\n",
      "        [0.0727],\n",
      "        [0.0704],\n",
      "        [0.0845],\n",
      "        [0.0714],\n",
      "        [0.0734],\n",
      "        [0.0721],\n",
      "        [0.0734],\n",
      "        [0.0840],\n",
      "        [0.0884],\n",
      "        [0.0963],\n",
      "        [0.1174],\n",
      "        [0.1245],\n",
      "        [0.1225],\n",
      "        [0.1204],\n",
      "        [0.1202],\n",
      "        [0.1226],\n",
      "        [0.1368],\n",
      "        [0.1386],\n",
      "        [0.1314],\n",
      "        [0.1421],\n",
      "        [0.1512],\n",
      "        [0.1540],\n",
      "        [0.1555],\n",
      "        [0.1654],\n",
      "        [0.1787],\n",
      "        [0.1993]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.15531945228577\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0506029468615452e-07, 138)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [138, 3, 78, 84, 18, 139, 87, 117, 108, 134, 140, 8, 62, 137, 14, 73, 43, 132, 85, 88, 79, 4, 112, 63, 110, 141, 39, 20, 38, 147, 135, 133, 17, 86, 37, 126, 7, 118, 65, 0, 119, 125, 44, 124, 154, 136, 21, 158, 51, 144, 1, 98, 19, 99, 53, 31, 54, 56, 102, 52, 32, 77, 71, 143, 131, 50, 15, 34, 145, 90, 111, 42, 24, 109, 6, 16, 41, 89, 5, 123, 36, 146, 40, 64, 80, 10, 83, 127, 100, 101, 22, 122, 45, 2, 142, 74, 155, 97, 103, 91, 116, 55, 129, 92, 115, 130, 93, 23, 120, 72, 30, 9, 153, 75, 128, 61, 25, 81, 35, 76, 157, 82, 70, 156, 49, 95, 96, 11, 113, 114, 107, 46, 94, 121, 13, 66, 29, 60, 48, 47, 69, 152, 148, 106, 28, 12, 26, 68, 57, 59, 58, 27, 149, 33, 151, 104, 105] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5369],\n",
      "        [0.9682],\n",
      "        [0.7045],\n",
      "        [0.6970],\n",
      "        [0.8517],\n",
      "        [0.5369],\n",
      "        [0.6632],\n",
      "        [0.5601],\n",
      "        [0.5075],\n",
      "        [0.5516],\n",
      "        [0.5287],\n",
      "        [0.9207],\n",
      "        [0.7660],\n",
      "        [0.5352],\n",
      "        [0.8375],\n",
      "        [0.6675],\n",
      "        [0.9161],\n",
      "        [0.5523],\n",
      "        [0.6905],\n",
      "        [0.6281],\n",
      "        [0.6997],\n",
      "        [0.9523],\n",
      "        [0.5765],\n",
      "        [0.7376],\n",
      "        [0.5424],\n",
      "        [0.5136],\n",
      "        [0.9073],\n",
      "        [0.8194],\n",
      "        [0.8996],\n",
      "        [0.4509],\n",
      "        [0.5375],\n",
      "        [0.5717],\n",
      "        [0.8407],\n",
      "        [0.6894],\n",
      "        [0.8400],\n",
      "        [0.5946],\n",
      "        [0.9241],\n",
      "        [0.5675],\n",
      "        [0.7352],\n",
      "        [0.9831],\n",
      "        [0.6164],\n",
      "        [0.6139],\n",
      "        [0.8911],\n",
      "        [0.6180],\n",
      "        [0.3181],\n",
      "        [0.5103],\n",
      "        [0.8363],\n",
      "        [0.3748],\n",
      "        [0.8797],\n",
      "        [0.4924],\n",
      "        [0.9648],\n",
      "        [0.6224],\n",
      "        [0.8246],\n",
      "        [0.6249],\n",
      "        [0.8147],\n",
      "        [0.9308],\n",
      "        [0.8569],\n",
      "        [0.8383],\n",
      "        [0.6114],\n",
      "        [0.8507],\n",
      "        [0.9257],\n",
      "        [0.6870],\n",
      "        [0.6304],\n",
      "        [0.5128],\n",
      "        [0.5647],\n",
      "        [0.8571],\n",
      "        [0.8304],\n",
      "        [0.8701],\n",
      "        [0.4447],\n",
      "        [0.6134],\n",
      "        [0.5825],\n",
      "        [0.9210],\n",
      "        [0.8133],\n",
      "        [0.4969],\n",
      "        [0.9438],\n",
      "        [0.8194],\n",
      "        [0.8993],\n",
      "        [0.6152],\n",
      "        [0.9221],\n",
      "        [0.5882],\n",
      "        [0.8798],\n",
      "        [0.4296],\n",
      "        [0.9185],\n",
      "        [0.7231],\n",
      "        [0.6693],\n",
      "        [0.8916],\n",
      "        [0.6682],\n",
      "        [0.5793],\n",
      "        [0.5984],\n",
      "        [0.5971],\n",
      "        [0.8551],\n",
      "        [0.6022],\n",
      "        [0.8789],\n",
      "        [0.9381],\n",
      "        [0.5136],\n",
      "        [0.6741],\n",
      "        [0.3630],\n",
      "        [0.6281],\n",
      "        [0.6111],\n",
      "        [0.6129],\n",
      "        [0.5642],\n",
      "        [0.8403],\n",
      "        [0.5807],\n",
      "        [0.6119],\n",
      "        [0.5712],\n",
      "        [0.5591],\n",
      "        [0.6307],\n",
      "        [0.7905],\n",
      "        [0.6251],\n",
      "        [0.6755],\n",
      "        [0.9129],\n",
      "        [0.9090],\n",
      "        [0.3467],\n",
      "        [0.6794],\n",
      "        [0.5877],\n",
      "        [0.8026],\n",
      "        [0.8062],\n",
      "        [0.6570],\n",
      "        [0.8889],\n",
      "        [0.6750],\n",
      "        [0.3672],\n",
      "        [0.6467],\n",
      "        [0.6952],\n",
      "        [0.3710],\n",
      "        [0.8540],\n",
      "        [0.6213],\n",
      "        [0.6188],\n",
      "        [0.9168],\n",
      "        [0.5525],\n",
      "        [0.5450],\n",
      "        [0.5676],\n",
      "        [0.8628],\n",
      "        [0.6232],\n",
      "        [0.5983],\n",
      "        [0.8826],\n",
      "        [0.7444],\n",
      "        [0.8842],\n",
      "        [0.8306],\n",
      "        [0.8668],\n",
      "        [0.8630],\n",
      "        [0.7318],\n",
      "        [0.3876],\n",
      "        [0.4468],\n",
      "        [0.5793],\n",
      "        [0.8459],\n",
      "        [0.9174],\n",
      "        [0.8216],\n",
      "        [0.7582],\n",
      "        [0.8424],\n",
      "        [0.8006],\n",
      "        [0.8005],\n",
      "        [0.8460],\n",
      "        [0.4423],\n",
      "        [0.8733],\n",
      "        [0.4436],\n",
      "        [0.6198],\n",
      "        [0.6300]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0029],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0048],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0112],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0141],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0191],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0210],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0226],\n",
      "        [0.0237],\n",
      "        [0.0238],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0262],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.0278],\n",
      "        [0.0280],\n",
      "        [0.0284],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0318],\n",
      "        [0.0322],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0335],\n",
      "        [0.0349],\n",
      "        [0.0351],\n",
      "        [0.0355],\n",
      "        [0.0364],\n",
      "        [0.0368],\n",
      "        [0.0369],\n",
      "        [0.0373],\n",
      "        [0.0380],\n",
      "        [0.0389],\n",
      "        [0.0406],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0419],\n",
      "        [0.0427],\n",
      "        [0.0427],\n",
      "        [0.0438],\n",
      "        [0.0440],\n",
      "        [0.0442],\n",
      "        [0.0463],\n",
      "        [0.0466],\n",
      "        [0.0466],\n",
      "        [0.0470],\n",
      "        [0.0471],\n",
      "        [0.0475],\n",
      "        [0.0481],\n",
      "        [0.0488],\n",
      "        [0.0495],\n",
      "        [0.0516],\n",
      "        [0.0519],\n",
      "        [0.0540],\n",
      "        [0.0556],\n",
      "        [0.0557],\n",
      "        [0.0561],\n",
      "        [0.0563],\n",
      "        [0.0571],\n",
      "        [0.0573],\n",
      "        [0.0591],\n",
      "        [0.0592],\n",
      "        [0.0593],\n",
      "        [0.0602],\n",
      "        [0.0611],\n",
      "        [0.0617],\n",
      "        [0.0632],\n",
      "        [0.0663],\n",
      "        [0.0694],\n",
      "        [0.0704],\n",
      "        [0.0714],\n",
      "        [0.0714],\n",
      "        [0.0721],\n",
      "        [0.0722],\n",
      "        [0.0727],\n",
      "        [0.0734],\n",
      "        [0.0734],\n",
      "        [0.0761],\n",
      "        [0.0814],\n",
      "        [0.0840],\n",
      "        [0.0845],\n",
      "        [0.0884],\n",
      "        [0.0963],\n",
      "        [0.1174],\n",
      "        [0.1202],\n",
      "        [0.1204],\n",
      "        [0.1225],\n",
      "        [0.1226],\n",
      "        [0.1245],\n",
      "        [0.1314],\n",
      "        [0.1368],\n",
      "        [0.1386],\n",
      "        [0.1421],\n",
      "        [0.1512],\n",
      "        [0.1540],\n",
      "        [0.1555],\n",
      "        [0.1654],\n",
      "        [0.1787],\n",
      "        [0.1993],\n",
      "        [0.2063]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0034],\n",
      "        [    0.0001],\n",
      "        [    0.0008],\n",
      "        [    0.0007],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0015],\n",
      "        [    0.0027],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0085],\n",
      "        [    0.0046],\n",
      "        [    0.0021],\n",
      "        [    0.0096],\n",
      "        [    0.0040],\n",
      "        [    0.0053],\n",
      "        [    0.0042],\n",
      "        [    0.0091],\n",
      "        [    0.0048],\n",
      "        [    0.0096],\n",
      "        [    0.0080],\n",
      "        [    0.0074],\n",
      "        [    0.0077],\n",
      "        [    0.0120],\n",
      "        [    0.0096],\n",
      "        [    0.0140],\n",
      "        [    0.0080],\n",
      "        [    0.0073],\n",
      "        [    0.0082],\n",
      "        [    0.0032],\n",
      "        [    0.0139],\n",
      "        [    0.0085],\n",
      "        [    0.0089],\n",
      "        [    0.0104],\n",
      "        [    0.0156],\n",
      "        [    0.0117],\n",
      "        [    0.0133],\n",
      "        [    0.0117],\n",
      "        [    0.0109],\n",
      "        [    0.0154],\n",
      "        [    0.0121],\n",
      "        [    0.0132],\n",
      "        [    0.0178],\n",
      "        [    0.0139],\n",
      "        [    0.0036],\n",
      "        [    0.0207],\n",
      "        [    0.0140],\n",
      "        [    0.0054],\n",
      "        [    0.0156],\n",
      "        [    0.0127],\n",
      "        [    0.0202],\n",
      "        [    0.0200],\n",
      "        [    0.0173],\n",
      "        [    0.0212],\n",
      "        [    0.0232],\n",
      "        [    0.0223],\n",
      "        [    0.0197],\n",
      "        [    0.0204],\n",
      "        [    0.0212],\n",
      "        [    0.0263],\n",
      "        [    0.0220],\n",
      "        [    0.0251],\n",
      "        [    0.0287],\n",
      "        [    0.0204],\n",
      "        [    0.0277],\n",
      "        [    0.0290],\n",
      "        [    0.0249],\n",
      "        [    0.0234],\n",
      "        [    0.0342],\n",
      "        [    0.0301],\n",
      "        [    0.0272],\n",
      "        [    0.0257],\n",
      "        [    0.0326],\n",
      "        [    0.0322],\n",
      "        [    0.0291],\n",
      "        [    0.0319],\n",
      "        [    0.0294],\n",
      "        [    0.0349],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0303],\n",
      "        [    0.0422],\n",
      "        [    0.0336],\n",
      "        [    0.0377],\n",
      "        [    0.0378],\n",
      "        [    0.0363],\n",
      "        [    0.0393],\n",
      "        [    0.0371],\n",
      "        [    0.0388],\n",
      "        [    0.0401],\n",
      "        [    0.0386],\n",
      "        [    0.0393],\n",
      "        [    0.0446],\n",
      "        [    0.0436],\n",
      "        [    0.0379],\n",
      "        [    0.0438],\n",
      "        [    0.0324],\n",
      "        [    0.0446],\n",
      "        [    0.0427],\n",
      "        [    0.0486],\n",
      "        [    0.0455],\n",
      "        [    0.0448],\n",
      "        [    0.0478],\n",
      "        [    0.0497],\n",
      "        [    0.0465],\n",
      "        [    0.0495],\n",
      "        [    0.0512],\n",
      "        [    0.0531],\n",
      "        [    0.0498],\n",
      "        [    0.0501],\n",
      "        [    0.0549],\n",
      "        [    0.0556],\n",
      "        [    0.0455],\n",
      "        [    0.0574],\n",
      "        [    0.0566],\n",
      "        [    0.0541],\n",
      "        [    0.0603],\n",
      "        [    0.0616],\n",
      "        [    0.0564],\n",
      "        [    0.0603],\n",
      "        [    0.0479],\n",
      "        [    0.0638],\n",
      "        [    0.0582],\n",
      "        [    0.0511],\n",
      "        [    0.0695],\n",
      "        [    0.0709],\n",
      "        [    0.0716],\n",
      "        [    0.0713],\n",
      "        [    0.0706],\n",
      "        [    0.0715],\n",
      "        [    0.0702],\n",
      "        [    0.0763],\n",
      "        [    0.0753],\n",
      "        [    0.0718],\n",
      "        [    0.0755],\n",
      "        [    0.0792],\n",
      "        [    0.0850],\n",
      "        [    0.0819],\n",
      "        [    0.0906],\n",
      "        [    0.0989],\n",
      "        [    0.1146],\n",
      "        [    0.1113],\n",
      "        [    0.1132],\n",
      "        [    0.1204],\n",
      "        [    0.1243],\n",
      "        [    0.1243],\n",
      "        [    0.1342],\n",
      "        [    0.1345],\n",
      "        [    0.1369],\n",
      "        [    0.1394],\n",
      "        [    0.1487],\n",
      "        [    0.1561],\n",
      "        [    0.1474],\n",
      "        [    0.1624],\n",
      "        [    0.1704],\n",
      "        [    0.1980],\n",
      "        [    0.2055]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.44465613365173\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.761978651870777e-08, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [3, 139, 18, 84, 78, 87, 62, 117, 147, 138, 154, 14, 43, 8, 85, 73, 158, 108, 134, 20, 4, 112, 39, 79, 38, 140, 133, 17, 132, 88, 110, 137, 86, 65, 126, 118, 63, 119, 144, 125, 7, 124, 135, 141, 21, 0, 51, 37, 19, 44, 54, 98, 1, 56, 143, 136, 99, 102, 32, 31, 53, 34, 15, 77, 42, 52, 111, 131, 71, 50, 6, 41, 90, 36, 5, 16, 123, 109, 155, 24, 40, 145, 89, 10, 127, 64, 80, 142, 22, 100, 122, 83, 101, 146, 103, 2, 74, 97, 45, 55, 153, 116, 115, 129, 157, 91, 130, 92, 120, 72, 156, 93, 23, 61, 30, 9, 35, 128, 75, 70, 76, 25, 81, 82, 49, 107, 113, 95, 11, 114, 96, 121, 94, 13, 46, 66, 60, 29, 48, 47, 152, 148, 69, 106, 28, 12, 26, 68, 57, 59, 149, 58, 27, 33, 151, 104, 105, 150] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9672],\n",
      "        [0.5334],\n",
      "        [0.8490],\n",
      "        [0.6944],\n",
      "        [0.7041],\n",
      "        [0.6612],\n",
      "        [0.7627],\n",
      "        [0.5610],\n",
      "        [0.4436],\n",
      "        [0.5332],\n",
      "        [0.3063],\n",
      "        [0.8355],\n",
      "        [0.9138],\n",
      "        [0.9205],\n",
      "        [0.6883],\n",
      "        [0.6664],\n",
      "        [0.3632],\n",
      "        [0.5048],\n",
      "        [0.5487],\n",
      "        [0.8167],\n",
      "        [0.9512],\n",
      "        [0.5774],\n",
      "        [0.9058],\n",
      "        [0.6994],\n",
      "        [0.8975],\n",
      "        [0.5244],\n",
      "        [0.5696],\n",
      "        [0.8384],\n",
      "        [0.5499],\n",
      "        [0.6258],\n",
      "        [0.5418],\n",
      "        [0.5309],\n",
      "        [0.6876],\n",
      "        [0.7327],\n",
      "        [0.5953],\n",
      "        [0.5690],\n",
      "        [0.7343],\n",
      "        [0.6185],\n",
      "        [0.4867],\n",
      "        [0.6154],\n",
      "        [0.9234],\n",
      "        [0.6194],\n",
      "        [0.5340],\n",
      "        [0.5090],\n",
      "        [0.8340],\n",
      "        [0.9819],\n",
      "        [0.8772],\n",
      "        [0.8368],\n",
      "        [0.8215],\n",
      "        [0.8881],\n",
      "        [0.8552],\n",
      "        [0.6216],\n",
      "        [0.9632],\n",
      "        [0.8370],\n",
      "        [0.5077],\n",
      "        [0.5057],\n",
      "        [0.6242],\n",
      "        [0.6101],\n",
      "        [0.9239],\n",
      "        [0.9298],\n",
      "        [0.8124],\n",
      "        [0.8667],\n",
      "        [0.8286],\n",
      "        [0.6866],\n",
      "        [0.9183],\n",
      "        [0.8481],\n",
      "        [0.5833],\n",
      "        [0.5630],\n",
      "        [0.6266],\n",
      "        [0.8543],\n",
      "        [0.9432],\n",
      "        [0.8969],\n",
      "        [0.6110],\n",
      "        [0.8765],\n",
      "        [0.9205],\n",
      "        [0.8173],\n",
      "        [0.5895],\n",
      "        [0.4943],\n",
      "        [0.3516],\n",
      "        [0.8099],\n",
      "        [0.9171],\n",
      "        [0.4377],\n",
      "        [0.6125],\n",
      "        [0.8911],\n",
      "        [0.5794],\n",
      "        [0.7209],\n",
      "        [0.6678],\n",
      "        [0.5089],\n",
      "        [0.8530],\n",
      "        [0.5975],\n",
      "        [0.6037],\n",
      "        [0.6657],\n",
      "        [0.5959],\n",
      "        [0.4223],\n",
      "        [0.6096],\n",
      "        [0.9365],\n",
      "        [0.6729],\n",
      "        [0.6274],\n",
      "        [0.8753],\n",
      "        [0.8384],\n",
      "        [0.3365],\n",
      "        [0.5652],\n",
      "        [0.5722],\n",
      "        [0.5799],\n",
      "        [0.3550],\n",
      "        [0.6105],\n",
      "        [0.5577],\n",
      "        [0.6093],\n",
      "        [0.6269],\n",
      "        [0.6736],\n",
      "        [0.3589],\n",
      "        [0.6282],\n",
      "        [0.7869],\n",
      "        [0.7997],\n",
      "        [0.9121],\n",
      "        [0.9090],\n",
      "        [0.8860],\n",
      "        [0.5873],\n",
      "        [0.6781],\n",
      "        [0.6917],\n",
      "        [0.6739],\n",
      "        [0.8032],\n",
      "        [0.6545],\n",
      "        [0.6440],\n",
      "        [0.8508],\n",
      "        [0.5656],\n",
      "        [0.5532],\n",
      "        [0.6198],\n",
      "        [0.9167],\n",
      "        [0.5456],\n",
      "        [0.6177],\n",
      "        [0.5999],\n",
      "        [0.6213],\n",
      "        [0.8820],\n",
      "        [0.8591],\n",
      "        [0.7423],\n",
      "        [0.8280],\n",
      "        [0.8831],\n",
      "        [0.8647],\n",
      "        [0.8605],\n",
      "        [0.3788],\n",
      "        [0.4395],\n",
      "        [0.7289],\n",
      "        [0.5773],\n",
      "        [0.8442],\n",
      "        [0.9171],\n",
      "        [0.8188],\n",
      "        [0.7558],\n",
      "        [0.8407],\n",
      "        [0.7979],\n",
      "        [0.4342],\n",
      "        [0.7979],\n",
      "        [0.8439],\n",
      "        [0.8703],\n",
      "        [0.4354],\n",
      "        [0.6185],\n",
      "        [0.6292],\n",
      "        [0.4349]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0015],\n",
      "        [    0.0021],\n",
      "        [    0.0027],\n",
      "        [    0.0032],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0042],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0053],\n",
      "        [    0.0054],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0074],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0082],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0089],\n",
      "        [    0.0091],\n",
      "        [    0.0096],\n",
      "        [    0.0096],\n",
      "        [    0.0096],\n",
      "        [    0.0104],\n",
      "        [    0.0109],\n",
      "        [    0.0117],\n",
      "        [    0.0117],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0133],\n",
      "        [    0.0139],\n",
      "        [    0.0139],\n",
      "        [    0.0140],\n",
      "        [    0.0140],\n",
      "        [    0.0154],\n",
      "        [    0.0156],\n",
      "        [    0.0156],\n",
      "        [    0.0173],\n",
      "        [    0.0178],\n",
      "        [    0.0197],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0204],\n",
      "        [    0.0204],\n",
      "        [    0.0207],\n",
      "        [    0.0212],\n",
      "        [    0.0212],\n",
      "        [    0.0220],\n",
      "        [    0.0223],\n",
      "        [    0.0232],\n",
      "        [    0.0234],\n",
      "        [    0.0249],\n",
      "        [    0.0251],\n",
      "        [    0.0257],\n",
      "        [    0.0263],\n",
      "        [    0.0272],\n",
      "        [    0.0277],\n",
      "        [    0.0287],\n",
      "        [    0.0290],\n",
      "        [    0.0291],\n",
      "        [    0.0294],\n",
      "        [    0.0301],\n",
      "        [    0.0303],\n",
      "        [    0.0315],\n",
      "        [    0.0319],\n",
      "        [    0.0321],\n",
      "        [    0.0322],\n",
      "        [    0.0324],\n",
      "        [    0.0326],\n",
      "        [    0.0336],\n",
      "        [    0.0342],\n",
      "        [    0.0349],\n",
      "        [    0.0363],\n",
      "        [    0.0371],\n",
      "        [    0.0377],\n",
      "        [    0.0378],\n",
      "        [    0.0379],\n",
      "        [    0.0386],\n",
      "        [    0.0388],\n",
      "        [    0.0393],\n",
      "        [    0.0393],\n",
      "        [    0.0401],\n",
      "        [    0.0422],\n",
      "        [    0.0427],\n",
      "        [    0.0436],\n",
      "        [    0.0438],\n",
      "        [    0.0446],\n",
      "        [    0.0446],\n",
      "        [    0.0448],\n",
      "        [    0.0455],\n",
      "        [    0.0455],\n",
      "        [    0.0465],\n",
      "        [    0.0478],\n",
      "        [    0.0479],\n",
      "        [    0.0486],\n",
      "        [    0.0495],\n",
      "        [    0.0497],\n",
      "        [    0.0498],\n",
      "        [    0.0501],\n",
      "        [    0.0511],\n",
      "        [    0.0512],\n",
      "        [    0.0531],\n",
      "        [    0.0541],\n",
      "        [    0.0549],\n",
      "        [    0.0556],\n",
      "        [    0.0564],\n",
      "        [    0.0566],\n",
      "        [    0.0574],\n",
      "        [    0.0582],\n",
      "        [    0.0603],\n",
      "        [    0.0603],\n",
      "        [    0.0616],\n",
      "        [    0.0638],\n",
      "        [    0.0695],\n",
      "        [    0.0702],\n",
      "        [    0.0706],\n",
      "        [    0.0709],\n",
      "        [    0.0713],\n",
      "        [    0.0715],\n",
      "        [    0.0716],\n",
      "        [    0.0718],\n",
      "        [    0.0753],\n",
      "        [    0.0755],\n",
      "        [    0.0763],\n",
      "        [    0.0792],\n",
      "        [    0.0819],\n",
      "        [    0.0850],\n",
      "        [    0.0906],\n",
      "        [    0.0989],\n",
      "        [    0.1113],\n",
      "        [    0.1132],\n",
      "        [    0.1146],\n",
      "        [    0.1204],\n",
      "        [    0.1243],\n",
      "        [    0.1243],\n",
      "        [    0.1342],\n",
      "        [    0.1345],\n",
      "        [    0.1369],\n",
      "        [    0.1394],\n",
      "        [    0.1474],\n",
      "        [    0.1487],\n",
      "        [    0.1561],\n",
      "        [    0.1624],\n",
      "        [    0.1704],\n",
      "        [    0.1980],\n",
      "        [    0.2055],\n",
      "        [    0.2075]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0013],\n",
      "        [    0.0059],\n",
      "        [    0.0036],\n",
      "        [    0.0035],\n",
      "        [    0.0001],\n",
      "        [    0.0010],\n",
      "        [    0.0013],\n",
      "        [    0.0033],\n",
      "        [    0.0062],\n",
      "        [    0.0091],\n",
      "        [    0.0105],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0040],\n",
      "        [    0.0023],\n",
      "        [    0.0039],\n",
      "        [    0.0083],\n",
      "        [    0.0105],\n",
      "        [    0.0120],\n",
      "        [    0.0041],\n",
      "        [    0.0060],\n",
      "        [    0.0081],\n",
      "        [    0.0064],\n",
      "        [    0.0075],\n",
      "        [    0.0060],\n",
      "        [    0.0146],\n",
      "        [    0.0045],\n",
      "        [    0.0062],\n",
      "        [    0.0134],\n",
      "        [    0.0126],\n",
      "        [    0.0114],\n",
      "        [    0.0158],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0126],\n",
      "        [    0.0118],\n",
      "        [    0.0154],\n",
      "        [    0.0115],\n",
      "        [    0.0050],\n",
      "        [    0.0133],\n",
      "        [    0.0145],\n",
      "        [    0.0139],\n",
      "        [    0.0194],\n",
      "        [    0.0205],\n",
      "        [    0.0114],\n",
      "        [    0.0170],\n",
      "        [    0.0129],\n",
      "        [    0.0192],\n",
      "        [    0.0137],\n",
      "        [    0.0209],\n",
      "        [    0.0176],\n",
      "        [    0.0217],\n",
      "        [    0.0222],\n",
      "        [    0.0188],\n",
      "        [    0.0133],\n",
      "        [    0.0274],\n",
      "        [    0.0229],\n",
      "        [    0.0189],\n",
      "        [    0.0202],\n",
      "        [    0.0232],\n",
      "        [    0.0259],\n",
      "        [    0.0195],\n",
      "        [    0.0225],\n",
      "        [    0.0257],\n",
      "        [    0.0230],\n",
      "        [    0.0291],\n",
      "        [    0.0276],\n",
      "        [    0.0313],\n",
      "        [    0.0331],\n",
      "        [    0.0321],\n",
      "        [    0.0282],\n",
      "        [    0.0270],\n",
      "        [    0.0333],\n",
      "        [    0.0266],\n",
      "        [    0.0294],\n",
      "        [    0.0346],\n",
      "        [    0.0324],\n",
      "        [    0.0361],\n",
      "        [    0.0188],\n",
      "        [    0.0363],\n",
      "        [    0.0322],\n",
      "        [    0.0434],\n",
      "        [    0.0384],\n",
      "        [    0.0355],\n",
      "        [    0.0387],\n",
      "        [    0.0402],\n",
      "        [    0.0397],\n",
      "        [    0.0313],\n",
      "        [    0.0362],\n",
      "        [    0.0408],\n",
      "        [    0.0393],\n",
      "        [    0.0423],\n",
      "        [    0.0424],\n",
      "        [    0.0518],\n",
      "        [    0.0400],\n",
      "        [    0.0457],\n",
      "        [    0.0452],\n",
      "        [    0.0461],\n",
      "        [    0.0485],\n",
      "        [    0.0425],\n",
      "        [    0.0330],\n",
      "        [    0.0460],\n",
      "        [    0.0469],\n",
      "        [    0.0504],\n",
      "        [    0.0336],\n",
      "        [    0.0518],\n",
      "        [    0.0528],\n",
      "        [    0.0531],\n",
      "        [    0.0495],\n",
      "        [    0.0479],\n",
      "        [    0.0369],\n",
      "        [    0.0545],\n",
      "        [    0.0572],\n",
      "        [    0.0512],\n",
      "        [    0.0557],\n",
      "        [    0.0552],\n",
      "        [    0.0532],\n",
      "        [    0.0587],\n",
      "        [    0.0589],\n",
      "        [    0.0544],\n",
      "        [    0.0616],\n",
      "        [    0.0637],\n",
      "        [    0.0647],\n",
      "        [    0.0671],\n",
      "        [    0.0730],\n",
      "        [    0.0671],\n",
      "        [    0.0712],\n",
      "        [    0.0732],\n",
      "        [    0.0709],\n",
      "        [    0.0723],\n",
      "        [    0.0736],\n",
      "        [    0.0717],\n",
      "        [    0.0780],\n",
      "        [    0.0745],\n",
      "        [    0.0804],\n",
      "        [    0.0770],\n",
      "        [    0.0794],\n",
      "        [    0.0861],\n",
      "        [    0.0930],\n",
      "        [    0.1018],\n",
      "        [    0.1002],\n",
      "        [    0.1037],\n",
      "        [    0.1116],\n",
      "        [    0.1171],\n",
      "        [    0.1261],\n",
      "        [    0.1238],\n",
      "        [    0.1373],\n",
      "        [    0.1321],\n",
      "        [    0.1351],\n",
      "        [    0.1367],\n",
      "        [    0.1371],\n",
      "        [    0.1461],\n",
      "        [    0.1584],\n",
      "        [    0.1591],\n",
      "        [    0.1600],\n",
      "        [    0.1956],\n",
      "        [    0.2037],\n",
      "        [    0.1969]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.734771728515625\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.2277755701006754e-08, 78)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [78, 87, 62, 3, 14, 43, 85, 117, 84, 18, 73, 8, 20, 133, 144, 139, 4, 38, 17, 147, 39, 79, 112, 65, 86, 158, 138, 154, 108, 21, 110, 119, 118, 134, 126, 88, 51, 125, 143, 132, 19, 124, 7, 140, 63, 137, 0, 54, 155, 56, 102, 37, 135, 34, 32, 141, 44, 98, 1, 15, 99, 42, 31, 77, 53, 36, 41, 136, 111, 6, 52, 5, 131, 142, 50, 40, 123, 153, 71, 90, 157, 16, 10, 109, 22, 24, 156, 89, 127, 122, 80, 103, 64, 100, 83, 101, 55, 145, 74, 2, 116, 97, 115, 72, 45, 120, 129, 61, 146, 91, 130, 92, 35, 70, 93, 9, 30, 23, 128, 75, 76, 25, 81, 107, 82, 11, 113, 121, 114, 49, 95, 96, 13, 66, 94, 60, 46, 29, 48, 152, 47, 148, 69, 106, 12, 28, 68, 57, 59, 149, 26, 58, 27, 33, 151, 104, 150, 105, 67] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7035],\n",
      "        [0.6587],\n",
      "        [0.7593],\n",
      "        [0.9660],\n",
      "        [0.8331],\n",
      "        [0.9114],\n",
      "        [0.6858],\n",
      "        [0.5604],\n",
      "        [0.6915],\n",
      "        [0.8460],\n",
      "        [0.6650],\n",
      "        [0.9199],\n",
      "        [0.8136],\n",
      "        [0.5656],\n",
      "        [0.4789],\n",
      "        [0.5281],\n",
      "        [0.9498],\n",
      "        [0.8953],\n",
      "        [0.8357],\n",
      "        [0.4342],\n",
      "        [0.9042],\n",
      "        [0.6989],\n",
      "        [0.5770],\n",
      "        [0.7300],\n",
      "        [0.6854],\n",
      "        [0.3496],\n",
      "        [0.5275],\n",
      "        [0.2922],\n",
      "        [0.5008],\n",
      "        [0.8313],\n",
      "        [0.5400],\n",
      "        [0.6191],\n",
      "        [0.5689],\n",
      "        [0.5438],\n",
      "        [0.5944],\n",
      "        [0.6227],\n",
      "        [0.8746],\n",
      "        [0.6154],\n",
      "        [0.5006],\n",
      "        [0.5456],\n",
      "        [0.8180],\n",
      "        [0.6195],\n",
      "        [0.9222],\n",
      "        [0.5184],\n",
      "        [0.7309],\n",
      "        [0.5247],\n",
      "        [0.9803],\n",
      "        [0.8531],\n",
      "        [0.3380],\n",
      "        [0.8355],\n",
      "        [0.6077],\n",
      "        [0.8332],\n",
      "        [0.5285],\n",
      "        [0.8627],\n",
      "        [0.9221],\n",
      "        [0.5025],\n",
      "        [0.8850],\n",
      "        [0.6198],\n",
      "        [0.9612],\n",
      "        [0.8262],\n",
      "        [0.6225],\n",
      "        [0.9156],\n",
      "        [0.9289],\n",
      "        [0.6861],\n",
      "        [0.8097],\n",
      "        [0.8728],\n",
      "        [0.8944],\n",
      "        [0.4990],\n",
      "        [0.5829],\n",
      "        [0.9423],\n",
      "        [0.8452],\n",
      "        [0.9184],\n",
      "        [0.5594],\n",
      "        [0.5023],\n",
      "        [0.8513],\n",
      "        [0.9156],\n",
      "        [0.5892],\n",
      "        [0.3239],\n",
      "        [0.6222],\n",
      "        [0.6079],\n",
      "        [0.3406],\n",
      "        [0.8146],\n",
      "        [0.8903],\n",
      "        [0.4904],\n",
      "        [0.8507],\n",
      "        [0.8062],\n",
      "        [0.3447],\n",
      "        [0.6090],\n",
      "        [0.5779],\n",
      "        [0.6036],\n",
      "        [0.6659],\n",
      "        [0.6069],\n",
      "        [0.7184],\n",
      "        [0.5956],\n",
      "        [0.6627],\n",
      "        [0.5936],\n",
      "        [0.8361],\n",
      "        [0.4285],\n",
      "        [0.6716],\n",
      "        [0.9343],\n",
      "        [0.5648],\n",
      "        [0.6259],\n",
      "        [0.5718],\n",
      "        [0.6715],\n",
      "        [0.8715],\n",
      "        [0.6273],\n",
      "        [0.5774],\n",
      "        [0.7968],\n",
      "        [0.4128],\n",
      "        [0.6073],\n",
      "        [0.5545],\n",
      "        [0.6059],\n",
      "        [0.8829],\n",
      "        [0.6879],\n",
      "        [0.6249],\n",
      "        [0.9086],\n",
      "        [0.9112],\n",
      "        [0.7828],\n",
      "        [0.5853],\n",
      "        [0.6766],\n",
      "        [0.6726],\n",
      "        [0.7998],\n",
      "        [0.6514],\n",
      "        [0.5624],\n",
      "        [0.6407],\n",
      "        [0.9164],\n",
      "        [0.5527],\n",
      "        [0.6000],\n",
      "        [0.5448],\n",
      "        [0.8473],\n",
      "        [0.6175],\n",
      "        [0.6157],\n",
      "        [0.8811],\n",
      "        [0.7400],\n",
      "        [0.6186],\n",
      "        [0.8255],\n",
      "        [0.8551],\n",
      "        [0.8821],\n",
      "        [0.8622],\n",
      "        [0.3676],\n",
      "        [0.8575],\n",
      "        [0.4300],\n",
      "        [0.7259],\n",
      "        [0.5740],\n",
      "        [0.9167],\n",
      "        [0.8424],\n",
      "        [0.7535],\n",
      "        [0.8390],\n",
      "        [0.7951],\n",
      "        [0.4238],\n",
      "        [0.8157],\n",
      "        [0.7953],\n",
      "        [0.8416],\n",
      "        [0.8670],\n",
      "        [0.4250],\n",
      "        [0.6161],\n",
      "        [0.4243],\n",
      "        [0.6274],\n",
      "        [0.7436]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0010],\n",
      "        [    0.0013],\n",
      "        [    0.0013],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0023],\n",
      "        [    0.0033],\n",
      "        [    0.0035],\n",
      "        [    0.0036],\n",
      "        [    0.0039],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0045],\n",
      "        [    0.0050],\n",
      "        [    0.0059],\n",
      "        [    0.0060],\n",
      "        [    0.0060],\n",
      "        [    0.0062],\n",
      "        [    0.0062],\n",
      "        [    0.0064],\n",
      "        [    0.0075],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0091],\n",
      "        [    0.0105],\n",
      "        [    0.0105],\n",
      "        [    0.0114],\n",
      "        [    0.0114],\n",
      "        [    0.0115],\n",
      "        [    0.0118],\n",
      "        [    0.0120],\n",
      "        [    0.0126],\n",
      "        [    0.0126],\n",
      "        [    0.0129],\n",
      "        [    0.0133],\n",
      "        [    0.0133],\n",
      "        [    0.0134],\n",
      "        [    0.0137],\n",
      "        [    0.0139],\n",
      "        [    0.0145],\n",
      "        [    0.0146],\n",
      "        [    0.0154],\n",
      "        [    0.0158],\n",
      "        [    0.0170],\n",
      "        [    0.0176],\n",
      "        [    0.0188],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0195],\n",
      "        [    0.0202],\n",
      "        [    0.0205],\n",
      "        [    0.0209],\n",
      "        [    0.0217],\n",
      "        [    0.0222],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0230],\n",
      "        [    0.0232],\n",
      "        [    0.0257],\n",
      "        [    0.0259],\n",
      "        [    0.0266],\n",
      "        [    0.0270],\n",
      "        [    0.0274],\n",
      "        [    0.0276],\n",
      "        [    0.0282],\n",
      "        [    0.0291],\n",
      "        [    0.0294],\n",
      "        [    0.0313],\n",
      "        [    0.0313],\n",
      "        [    0.0321],\n",
      "        [    0.0322],\n",
      "        [    0.0324],\n",
      "        [    0.0330],\n",
      "        [    0.0331],\n",
      "        [    0.0333],\n",
      "        [    0.0336],\n",
      "        [    0.0346],\n",
      "        [    0.0355],\n",
      "        [    0.0361],\n",
      "        [    0.0362],\n",
      "        [    0.0363],\n",
      "        [    0.0369],\n",
      "        [    0.0384],\n",
      "        [    0.0387],\n",
      "        [    0.0393],\n",
      "        [    0.0397],\n",
      "        [    0.0400],\n",
      "        [    0.0402],\n",
      "        [    0.0408],\n",
      "        [    0.0423],\n",
      "        [    0.0424],\n",
      "        [    0.0425],\n",
      "        [    0.0434],\n",
      "        [    0.0452],\n",
      "        [    0.0457],\n",
      "        [    0.0460],\n",
      "        [    0.0461],\n",
      "        [    0.0469],\n",
      "        [    0.0479],\n",
      "        [    0.0485],\n",
      "        [    0.0495],\n",
      "        [    0.0504],\n",
      "        [    0.0512],\n",
      "        [    0.0518],\n",
      "        [    0.0518],\n",
      "        [    0.0528],\n",
      "        [    0.0531],\n",
      "        [    0.0532],\n",
      "        [    0.0544],\n",
      "        [    0.0545],\n",
      "        [    0.0552],\n",
      "        [    0.0557],\n",
      "        [    0.0572],\n",
      "        [    0.0587],\n",
      "        [    0.0589],\n",
      "        [    0.0616],\n",
      "        [    0.0637],\n",
      "        [    0.0647],\n",
      "        [    0.0671],\n",
      "        [    0.0671],\n",
      "        [    0.0709],\n",
      "        [    0.0712],\n",
      "        [    0.0717],\n",
      "        [    0.0723],\n",
      "        [    0.0730],\n",
      "        [    0.0732],\n",
      "        [    0.0736],\n",
      "        [    0.0745],\n",
      "        [    0.0770],\n",
      "        [    0.0780],\n",
      "        [    0.0794],\n",
      "        [    0.0804],\n",
      "        [    0.0861],\n",
      "        [    0.0930],\n",
      "        [    0.1002],\n",
      "        [    0.1018],\n",
      "        [    0.1037],\n",
      "        [    0.1116],\n",
      "        [    0.1171],\n",
      "        [    0.1238],\n",
      "        [    0.1261],\n",
      "        [    0.1321],\n",
      "        [    0.1351],\n",
      "        [    0.1367],\n",
      "        [    0.1371],\n",
      "        [    0.1373],\n",
      "        [    0.1461],\n",
      "        [    0.1584],\n",
      "        [    0.1591],\n",
      "        [    0.1600],\n",
      "        [    0.1956],\n",
      "        [    0.1969],\n",
      "        [    0.2037],\n",
      "        [    0.2824]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0020],\n",
      "        [    0.0040],\n",
      "        [    0.0060],\n",
      "        [    0.0029],\n",
      "        [    0.0000],\n",
      "        [    0.0007],\n",
      "        [    0.0011],\n",
      "        [    0.0009],\n",
      "        [    0.0074],\n",
      "        [    0.0057],\n",
      "        [    0.0013],\n",
      "        [    0.0036],\n",
      "        [    0.0020],\n",
      "        [    0.0048],\n",
      "        [    0.0023],\n",
      "        [    0.0066],\n",
      "        [    0.0045],\n",
      "        [    0.0041],\n",
      "        [    0.0045],\n",
      "        [    0.0100],\n",
      "        [    0.0049],\n",
      "        [    0.0055],\n",
      "        [    0.0060],\n",
      "        [    0.0040],\n",
      "        [    0.0051],\n",
      "        [    0.0157],\n",
      "        [    0.0101],\n",
      "        [    0.0177],\n",
      "        [    0.0116],\n",
      "        [    0.0093],\n",
      "        [    0.0103],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0123],\n",
      "        [    0.0100],\n",
      "        [    0.0157],\n",
      "        [    0.0098],\n",
      "        [    0.0101],\n",
      "        [    0.0111],\n",
      "        [    0.0132],\n",
      "        [    0.0113],\n",
      "        [    0.0108],\n",
      "        [    0.0153],\n",
      "        [    0.0159],\n",
      "        [    0.0201],\n",
      "        [    0.0172],\n",
      "        [    0.0188],\n",
      "        [    0.0151],\n",
      "        [    0.0117],\n",
      "        [    0.0166],\n",
      "        [    0.0179],\n",
      "        [    0.0219],\n",
      "        [    0.0201],\n",
      "        [    0.0164],\n",
      "        [    0.0184],\n",
      "        [    0.0221],\n",
      "        [    0.0238],\n",
      "        [    0.0226],\n",
      "        [    0.0243],\n",
      "        [    0.0211],\n",
      "        [    0.0237],\n",
      "        [    0.0201],\n",
      "        [    0.0244],\n",
      "        [    0.0277],\n",
      "        [    0.0287],\n",
      "        [    0.0236],\n",
      "        [    0.0246],\n",
      "        [    0.0290],\n",
      "        [    0.0254],\n",
      "        [    0.0273],\n",
      "        [    0.0323],\n",
      "        [    0.0277],\n",
      "        [    0.0306],\n",
      "        [    0.0295],\n",
      "        [    0.0353],\n",
      "        [    0.0305],\n",
      "        [    0.0293],\n",
      "        [    0.0270],\n",
      "        [    0.0378],\n",
      "        [    0.0361],\n",
      "        [    0.0256],\n",
      "        [    0.0362],\n",
      "        [    0.0350],\n",
      "        [    0.0368],\n",
      "        [    0.0343],\n",
      "        [    0.0393],\n",
      "        [    0.0291],\n",
      "        [    0.0417],\n",
      "        [    0.0365],\n",
      "        [    0.0362],\n",
      "        [    0.0426],\n",
      "        [    0.0391],\n",
      "        [    0.0441],\n",
      "        [    0.0414],\n",
      "        [    0.0457],\n",
      "        [    0.0433],\n",
      "        [    0.0399],\n",
      "        [    0.0470],\n",
      "        [    0.0480],\n",
      "        [    0.0477],\n",
      "        [    0.0436],\n",
      "        [    0.0470],\n",
      "        [    0.0446],\n",
      "        [    0.0448],\n",
      "        [    0.0519],\n",
      "        [    0.0463],\n",
      "        [    0.0490],\n",
      "        [    0.0467],\n",
      "        [    0.0555],\n",
      "        [    0.0546],\n",
      "        [    0.0518],\n",
      "        [    0.0559],\n",
      "        [    0.0506],\n",
      "        [    0.0495],\n",
      "        [    0.0573],\n",
      "        [    0.0551],\n",
      "        [    0.0568],\n",
      "        [    0.0602],\n",
      "        [    0.0570],\n",
      "        [    0.0619],\n",
      "        [    0.0642],\n",
      "        [    0.0663],\n",
      "        [    0.0682],\n",
      "        [    0.0661],\n",
      "        [    0.0706],\n",
      "        [    0.0706],\n",
      "        [    0.0690],\n",
      "        [    0.0684],\n",
      "        [    0.0703],\n",
      "        [    0.0764],\n",
      "        [    0.0749],\n",
      "        [    0.0750],\n",
      "        [    0.0739],\n",
      "        [    0.0730],\n",
      "        [    0.0802],\n",
      "        [    0.0752],\n",
      "        [    0.0838],\n",
      "        [    0.0872],\n",
      "        [    0.0956],\n",
      "        [    0.0953],\n",
      "        [    0.1047],\n",
      "        [    0.0999],\n",
      "        [    0.1070],\n",
      "        [    0.1160],\n",
      "        [    0.1234],\n",
      "        [    0.1277],\n",
      "        [    0.1279],\n",
      "        [    0.1324],\n",
      "        [    0.1327],\n",
      "        [    0.1326],\n",
      "        [    0.1398],\n",
      "        [    0.1425],\n",
      "        [    0.1603],\n",
      "        [    0.1565],\n",
      "        [    0.1554],\n",
      "        [    0.1949],\n",
      "        [    0.1923],\n",
      "        [    0.2035],\n",
      "        [    0.2783]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.025522232055664\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 4 個區塊累積花費時間(s) 1.2295112609863281\n",
      "<<The performance of 4 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2295112609863281\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1182.82\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1671.63\n",
      "The accuracy(2000) for l = 1: 84.91%\n",
      "The accuracy(3000) for l = 1: 90.57%\n",
      "The maximum error: tensor(7105.2188)\n",
      "The minimum error: tensor(0.2695)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 383.9\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 472.5\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 712.046875\n",
      "The minimum error: 30.46875\n",
      "------------------------------------------------------------\n",
      "0.8490566037735849\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<5>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1004885891452432e-10, 10)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [10, 39, 113, 81, 158, 69, 16, 74, 140, 4, 61, 83, 34, 0, 13, 129, 35, 82, 75, 14, 108, 58, 135, 80, 155, 115, 114, 17, 47, 143, 122, 134, 121, 106, 120, 139, 15, 104, 151, 130, 128, 50, 3, 154, 84, 136, 30, 52, 133, 150, 98, 28, 131, 59, 38, 11, 33, 137, 94, 157, 32, 95, 40, 27, 37, 107, 153, 149, 2, 1, 73, 156, 49, 132, 152, 119, 138, 36, 127, 48, 18, 6, 46, 86, 12, 118, 123, 105, 67, 99, 20, 51, 96, 85, 76, 97, 112, 60, 111, 68, 79, 116, 57, 93, 141, 70, 125, 66, 31, 126, 41, 87, 5, 142, 88, 26, 124, 89, 19, 71, 72, 103, 21, 77, 117, 109, 110, 7, 78, 62, 9, 91, 92, 56, 45, 90, 42, 25, 148, 44, 144, 43, 65, 102, 8, 24, 64, 53, 145, 55, 22, 54, 147, 29, 23]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0011],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0020],\n",
      "        [    0.0020],\n",
      "        [    0.0023],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0045],\n",
      "        [    0.0045],\n",
      "        [    0.0048],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0060],\n",
      "        [    0.0060],\n",
      "        [    0.0066],\n",
      "        [    0.0074],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0093],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0101],\n",
      "        [    0.0103],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0116],\n",
      "        [    0.0117],\n",
      "        [    0.0123],\n",
      "        [    0.0132],\n",
      "        [    0.0151],\n",
      "        [    0.0153],\n",
      "        [    0.0157],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0164],\n",
      "        [    0.0166],\n",
      "        [    0.0172],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0211],\n",
      "        [    0.0219],\n",
      "        [    0.0221],\n",
      "        [    0.0226],\n",
      "        [    0.0229],\n",
      "        [    0.0236],\n",
      "        [    0.0237],\n",
      "        [    0.0238],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0254],\n",
      "        [    0.0256],\n",
      "        [    0.0270],\n",
      "        [    0.0273],\n",
      "        [    0.0277],\n",
      "        [    0.0277],\n",
      "        [    0.0279],\n",
      "        [    0.0287],\n",
      "        [    0.0290],\n",
      "        [    0.0291],\n",
      "        [    0.0293],\n",
      "        [    0.0295],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0323],\n",
      "        [    0.0343],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0361],\n",
      "        [    0.0362],\n",
      "        [    0.0362],\n",
      "        [    0.0365],\n",
      "        [    0.0368],\n",
      "        [    0.0378],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.0399],\n",
      "        [    0.0414],\n",
      "        [    0.0417],\n",
      "        [    0.0426],\n",
      "        [    0.0433],\n",
      "        [    0.0436],\n",
      "        [    0.0441],\n",
      "        [    0.0446],\n",
      "        [    0.0448],\n",
      "        [    0.0457],\n",
      "        [    0.0463],\n",
      "        [    0.0467],\n",
      "        [    0.0470],\n",
      "        [    0.0470],\n",
      "        [    0.0480],\n",
      "        [    0.0490],\n",
      "        [    0.0495],\n",
      "        [    0.0506],\n",
      "        [    0.0518],\n",
      "        [    0.0519],\n",
      "        [    0.0546],\n",
      "        [    0.0551],\n",
      "        [    0.0555],\n",
      "        [    0.0559],\n",
      "        [    0.0568],\n",
      "        [    0.0570],\n",
      "        [    0.0573],\n",
      "        [    0.0602],\n",
      "        [    0.0619],\n",
      "        [    0.0642],\n",
      "        [    0.0661],\n",
      "        [    0.0663],\n",
      "        [    0.0682],\n",
      "        [    0.0684],\n",
      "        [    0.0690],\n",
      "        [    0.0703],\n",
      "        [    0.0706],\n",
      "        [    0.0706],\n",
      "        [    0.0730],\n",
      "        [    0.0739],\n",
      "        [    0.0749],\n",
      "        [    0.0750],\n",
      "        [    0.0752],\n",
      "        [    0.0764],\n",
      "        [    0.0802],\n",
      "        [    0.0838],\n",
      "        [    0.0872],\n",
      "        [    0.0953],\n",
      "        [    0.0956],\n",
      "        [    0.0999],\n",
      "        [    0.1047],\n",
      "        [    0.1070],\n",
      "        [    0.1160],\n",
      "        [    0.1234],\n",
      "        [    0.1277],\n",
      "        [    0.1279],\n",
      "        [    0.1324],\n",
      "        [    0.1326],\n",
      "        [    0.1327],\n",
      "        [    0.1398],\n",
      "        [    0.1425],\n",
      "        [    0.1554],\n",
      "        [    0.1565],\n",
      "        [    0.1603]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1004885891452432e-10, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 39, 113, 81, 158, 69, 16, 74, 140, 4, 61, 83, 34, 0, 13, 129, 35, 82, 75, 14, 108, 58, 135, 80, 155, 115, 114, 17, 47, 143, 122, 134, 121, 106, 120, 139, 15, 104, 151, 130, 128, 50, 3, 154, 84, 136, 30, 52, 133, 150, 98, 28, 131, 59, 38, 11, 33, 137, 94, 157, 32, 95, 40, 27, 37, 107, 153, 149, 2, 1, 73, 156, 49, 132, 152, 119, 138, 36, 127, 48, 18, 6, 46, 86, 12, 118, 123, 105, 67, 99, 20, 51, 96, 85, 76, 97, 112, 60, 111, 68, 79, 116, 57, 93, 141, 70, 125, 66, 31, 126, 41, 87, 5, 142, 88, 26, 124, 89, 19, 71, 72, 103, 21, 77, 117, 109, 110, 7, 78, 62, 9, 91, 92, 56, 45, 90, 42, 25, 148, 44, 144, 43, 65, 102, 8, 24, 64, 53, 145, 55, 22, 54, 147, 29, 23, 146] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8315],\n",
      "        [0.9089],\n",
      "        [0.5629],\n",
      "        [0.6824],\n",
      "        [0.3623],\n",
      "        [0.6623],\n",
      "        [0.8114],\n",
      "        [0.7014],\n",
      "        [0.4762],\n",
      "        [0.9195],\n",
      "        [0.7257],\n",
      "        [0.6558],\n",
      "        [0.8933],\n",
      "        [0.9482],\n",
      "        [0.8340],\n",
      "        [0.5659],\n",
      "        [0.9027],\n",
      "        [0.6823],\n",
      "        [0.6970],\n",
      "        [0.8439],\n",
      "        [0.5791],\n",
      "        [0.7546],\n",
      "        [0.5274],\n",
      "        [0.6876],\n",
      "        [0.3498],\n",
      "        [0.6224],\n",
      "        [0.5718],\n",
      "        [0.8293],\n",
      "        [0.8714],\n",
      "        [0.4305],\n",
      "        [0.5970],\n",
      "        [0.5266],\n",
      "        [0.6185],\n",
      "        [0.5411],\n",
      "        [0.6226],\n",
      "        [0.4983],\n",
      "        [0.8155],\n",
      "        [0.4998],\n",
      "        [0.3309],\n",
      "        [0.5435],\n",
      "        [0.5458],\n",
      "        [0.8506],\n",
      "        [0.9213],\n",
      "        [0.3422],\n",
      "        [0.6196],\n",
      "        [0.5170],\n",
      "        [0.8597],\n",
      "        [0.8333],\n",
      "        [0.5234],\n",
      "        [0.2850],\n",
      "        [0.6067],\n",
      "        [0.9202],\n",
      "        [0.5279],\n",
      "        [0.7262],\n",
      "        [0.9127],\n",
      "        [0.8247],\n",
      "        [0.8305],\n",
      "        [0.5009],\n",
      "        [0.6189],\n",
      "        [0.3357],\n",
      "        [0.8699],\n",
      "        [0.6217],\n",
      "        [0.8821],\n",
      "        [0.9277],\n",
      "        [0.8920],\n",
      "        [0.5851],\n",
      "        [0.3327],\n",
      "        [0.3180],\n",
      "        [0.9414],\n",
      "        [0.9166],\n",
      "        [0.6840],\n",
      "        [0.3308],\n",
      "        [0.8069],\n",
      "        [0.4975],\n",
      "        [0.3369],\n",
      "        [0.5923],\n",
      "        [0.5005],\n",
      "        [0.9140],\n",
      "        [0.5601],\n",
      "        [0.8420],\n",
      "        [0.8488],\n",
      "        [0.8897],\n",
      "        [0.8480],\n",
      "        [0.6051],\n",
      "        [0.8130],\n",
      "        [0.6068],\n",
      "        [0.5800],\n",
      "        [0.4897],\n",
      "        [0.6175],\n",
      "        [0.6060],\n",
      "        [0.8032],\n",
      "        [0.8336],\n",
      "        [0.5950],\n",
      "        [0.6057],\n",
      "        [0.6630],\n",
      "        [0.5927],\n",
      "        [0.5672],\n",
      "        [0.7145],\n",
      "        [0.5741],\n",
      "        [0.6684],\n",
      "        [0.6593],\n",
      "        [0.6305],\n",
      "        [0.7922],\n",
      "        [0.6251],\n",
      "        [0.4249],\n",
      "        [0.6687],\n",
      "        [0.5787],\n",
      "        [0.6830],\n",
      "        [0.8803],\n",
      "        [0.5555],\n",
      "        [0.8681],\n",
      "        [0.6046],\n",
      "        [0.9084],\n",
      "        [0.4090],\n",
      "        [0.6031],\n",
      "        [0.9102],\n",
      "        [0.5870],\n",
      "        [0.6222],\n",
      "        [0.7798],\n",
      "        [0.6736],\n",
      "        [0.6700],\n",
      "        [0.5614],\n",
      "        [0.7972],\n",
      "        [0.6479],\n",
      "        [0.6033],\n",
      "        [0.5548],\n",
      "        [0.5468],\n",
      "        [0.9160],\n",
      "        [0.6372],\n",
      "        [0.7360],\n",
      "        [0.8804],\n",
      "        [0.6158],\n",
      "        [0.6143],\n",
      "        [0.8213],\n",
      "        [0.8439],\n",
      "        [0.6164],\n",
      "        [0.8516],\n",
      "        [0.8809],\n",
      "        [0.3627],\n",
      "        [0.8596],\n",
      "        [0.4263],\n",
      "        [0.8547],\n",
      "        [0.7213],\n",
      "        [0.5728],\n",
      "        [0.9162],\n",
      "        [0.8408],\n",
      "        [0.7492],\n",
      "        [0.8362],\n",
      "        [0.4194],\n",
      "        [0.7911],\n",
      "        [0.8132],\n",
      "        [0.7917],\n",
      "        [0.4203],\n",
      "        [0.8644],\n",
      "        [0.8397],\n",
      "        [0.4197]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0011],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0020],\n",
      "        [    0.0020],\n",
      "        [    0.0023],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0045],\n",
      "        [    0.0045],\n",
      "        [    0.0048],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0060],\n",
      "        [    0.0060],\n",
      "        [    0.0066],\n",
      "        [    0.0074],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0093],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0101],\n",
      "        [    0.0103],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0116],\n",
      "        [    0.0117],\n",
      "        [    0.0123],\n",
      "        [    0.0132],\n",
      "        [    0.0151],\n",
      "        [    0.0153],\n",
      "        [    0.0157],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0164],\n",
      "        [    0.0166],\n",
      "        [    0.0172],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0211],\n",
      "        [    0.0219],\n",
      "        [    0.0221],\n",
      "        [    0.0226],\n",
      "        [    0.0229],\n",
      "        [    0.0236],\n",
      "        [    0.0237],\n",
      "        [    0.0238],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0254],\n",
      "        [    0.0256],\n",
      "        [    0.0270],\n",
      "        [    0.0273],\n",
      "        [    0.0277],\n",
      "        [    0.0277],\n",
      "        [    0.0279],\n",
      "        [    0.0287],\n",
      "        [    0.0290],\n",
      "        [    0.0291],\n",
      "        [    0.0293],\n",
      "        [    0.0295],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0323],\n",
      "        [    0.0343],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0361],\n",
      "        [    0.0362],\n",
      "        [    0.0362],\n",
      "        [    0.0365],\n",
      "        [    0.0368],\n",
      "        [    0.0378],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.0399],\n",
      "        [    0.0414],\n",
      "        [    0.0417],\n",
      "        [    0.0426],\n",
      "        [    0.0433],\n",
      "        [    0.0436],\n",
      "        [    0.0441],\n",
      "        [    0.0446],\n",
      "        [    0.0448],\n",
      "        [    0.0457],\n",
      "        [    0.0463],\n",
      "        [    0.0467],\n",
      "        [    0.0470],\n",
      "        [    0.0470],\n",
      "        [    0.0480],\n",
      "        [    0.0490],\n",
      "        [    0.0495],\n",
      "        [    0.0506],\n",
      "        [    0.0518],\n",
      "        [    0.0519],\n",
      "        [    0.0546],\n",
      "        [    0.0551],\n",
      "        [    0.0555],\n",
      "        [    0.0559],\n",
      "        [    0.0568],\n",
      "        [    0.0570],\n",
      "        [    0.0573],\n",
      "        [    0.0602],\n",
      "        [    0.0619],\n",
      "        [    0.0642],\n",
      "        [    0.0661],\n",
      "        [    0.0663],\n",
      "        [    0.0682],\n",
      "        [    0.0684],\n",
      "        [    0.0690],\n",
      "        [    0.0703],\n",
      "        [    0.0706],\n",
      "        [    0.0706],\n",
      "        [    0.0730],\n",
      "        [    0.0739],\n",
      "        [    0.0749],\n",
      "        [    0.0750],\n",
      "        [    0.0752],\n",
      "        [    0.0764],\n",
      "        [    0.0802],\n",
      "        [    0.0838],\n",
      "        [    0.0872],\n",
      "        [    0.0953],\n",
      "        [    0.0956],\n",
      "        [    0.0999],\n",
      "        [    0.1047],\n",
      "        [    0.1070],\n",
      "        [    0.1160],\n",
      "        [    0.1234],\n",
      "        [    0.1277],\n",
      "        [    0.1279],\n",
      "        [    0.1324],\n",
      "        [    0.1326],\n",
      "        [    0.1327],\n",
      "        [    0.1398],\n",
      "        [    0.1425],\n",
      "        [    0.1554],\n",
      "        [    0.1565],\n",
      "        [    0.1603],\n",
      "        [    0.1923]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0015],\n",
      "        [    0.0001],\n",
      "        [    0.0073],\n",
      "        [    0.0053],\n",
      "        [    0.0008],\n",
      "        [    0.0088],\n",
      "        [    0.0034],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0050],\n",
      "        [    0.0088],\n",
      "        [    0.0028],\n",
      "        [    0.0047],\n",
      "        [    0.0042],\n",
      "        [    0.0063],\n",
      "        [    0.0098],\n",
      "        [    0.0063],\n",
      "        [    0.0118],\n",
      "        [    0.0137],\n",
      "        [    0.0042],\n",
      "        [    0.0032],\n",
      "        [    0.0029],\n",
      "        [    0.0016],\n",
      "        [    0.0014],\n",
      "        [    0.0070],\n",
      "        [    0.0006],\n",
      "        [    0.0001],\n",
      "        [    0.0112],\n",
      "        [    0.0107],\n",
      "        [    0.0072],\n",
      "        [    0.0029],\n",
      "        [    0.0056],\n",
      "        [    0.0022],\n",
      "        [    0.0019],\n",
      "        [    0.0028],\n",
      "        [    0.0150],\n",
      "        [    0.0125],\n",
      "        [    0.0045],\n",
      "        [    0.0130],\n",
      "        [    0.0078],\n",
      "        [    0.0083],\n",
      "        [    0.0166],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0090],\n",
      "        [    0.0113],\n",
      "        [    0.0152],\n",
      "        [    0.0193],\n",
      "        [    0.0136],\n",
      "        [    0.0165],\n",
      "        [    0.0255],\n",
      "        [    0.0191],\n",
      "        [    0.0160],\n",
      "        [    0.0166],\n",
      "        [    0.0206],\n",
      "        [    0.0230],\n",
      "        [    0.0223],\n",
      "        [    0.0177],\n",
      "        [    0.0146],\n",
      "        [    0.0211],\n",
      "        [    0.0224],\n",
      "        [    0.0157],\n",
      "        [    0.0239],\n",
      "        [    0.0225],\n",
      "        [    0.0255],\n",
      "        [    0.0162],\n",
      "        [    0.0259],\n",
      "        [    0.0291],\n",
      "        [    0.0279],\n",
      "        [    0.0271],\n",
      "        [    0.0197],\n",
      "        [    0.0261],\n",
      "        [    0.0273],\n",
      "        [    0.0257],\n",
      "        [    0.0295],\n",
      "        [    0.0213],\n",
      "        [    0.0339],\n",
      "        [    0.0322],\n",
      "        [    0.0254],\n",
      "        [    0.0313],\n",
      "        [    0.0366],\n",
      "        [    0.0369],\n",
      "        [    0.0345],\n",
      "        [    0.0294],\n",
      "        [    0.0343],\n",
      "        [    0.0281],\n",
      "        [    0.0299],\n",
      "        [    0.0295],\n",
      "        [    0.0332],\n",
      "        [    0.0462],\n",
      "        [    0.0380],\n",
      "        [    0.0415],\n",
      "        [    0.0333],\n",
      "        [    0.0353],\n",
      "        [    0.0353],\n",
      "        [    0.0355],\n",
      "        [    0.0352],\n",
      "        [    0.0389],\n",
      "        [    0.0361],\n",
      "        [    0.0512],\n",
      "        [    0.0399],\n",
      "        [    0.0381],\n",
      "        [    0.0493],\n",
      "        [    0.0386],\n",
      "        [    0.0439],\n",
      "        [    0.0401],\n",
      "        [    0.0432],\n",
      "        [    0.0541],\n",
      "        [    0.0499],\n",
      "        [    0.0464],\n",
      "        [    0.0524],\n",
      "        [    0.0480],\n",
      "        [    0.0570],\n",
      "        [    0.0525],\n",
      "        [    0.0496],\n",
      "        [    0.0544],\n",
      "        [    0.0509],\n",
      "        [    0.0508],\n",
      "        [    0.0591],\n",
      "        [    0.0543],\n",
      "        [    0.0565],\n",
      "        [    0.0735],\n",
      "        [    0.0647],\n",
      "        [    0.0623],\n",
      "        [    0.0602],\n",
      "        [    0.0598],\n",
      "        [    0.0615],\n",
      "        [    0.0726],\n",
      "        [    0.0648],\n",
      "        [    0.0780],\n",
      "        [    0.0760],\n",
      "        [    0.0672],\n",
      "        [    0.0669],\n",
      "        [    0.0777],\n",
      "        [    0.0760],\n",
      "        [    0.0730],\n",
      "        [    0.0844],\n",
      "        [    0.0845],\n",
      "        [    0.0979],\n",
      "        [    0.0943],\n",
      "        [    0.1026],\n",
      "        [    0.1039],\n",
      "        [    0.1118],\n",
      "        [    0.1234],\n",
      "        [    0.1255],\n",
      "        [    0.1253],\n",
      "        [    0.1328],\n",
      "        [    0.1351],\n",
      "        [    0.1349],\n",
      "        [    0.1352],\n",
      "        [    0.1380],\n",
      "        [    0.1451],\n",
      "        [    0.1579],\n",
      "        [    0.1560],\n",
      "        [    0.1581],\n",
      "        [    0.1944]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.55142068862915\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.7320417100563645e-08, 39)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [39, 114, 115, 158, 80, 10, 135, 106, 121, 120, 83, 122, 58, 108, 16, 0, 14, 104, 34, 4, 81, 134, 74, 140, 13, 35, 155, 143, 113, 130, 128, 61, 69, 84, 129, 47, 17, 136, 82, 15, 151, 133, 75, 94, 3, 154, 139, 30, 95, 131, 107, 150, 50, 59, 137, 28, 52, 73, 38, 157, 119, 33, 32, 27, 11, 40, 127, 98, 37, 132, 153, 156, 1, 49, 2, 118, 149, 86, 152, 105, 123, 48, 36, 67, 96, 138, 12, 46, 112, 85, 76, 97, 111, 18, 6, 20, 116, 93, 60, 79, 70, 51, 125, 141, 99, 126, 87, 57, 88, 31, 89, 124, 68, 41, 142, 66, 71, 26, 72, 5, 19, 109, 117, 110, 77, 21, 78, 92, 91, 7, 90, 103, 45, 9, 56, 62, 42, 25, 44, 148, 144, 43, 65, 102, 24, 8, 64, 145, 53, 55, 22, 54, 29, 147, 23, 146, 100] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9095],\n",
      "        [0.5806],\n",
      "        [0.6312],\n",
      "        [0.3643],\n",
      "        [0.6936],\n",
      "        [0.8330],\n",
      "        [0.5324],\n",
      "        [0.5495],\n",
      "        [0.6264],\n",
      "        [0.6306],\n",
      "        [0.6625],\n",
      "        [0.6041],\n",
      "        [0.7577],\n",
      "        [0.5883],\n",
      "        [0.8128],\n",
      "        [0.9480],\n",
      "        [0.8454],\n",
      "        [0.5068],\n",
      "        [0.8939],\n",
      "        [0.9209],\n",
      "        [0.6889],\n",
      "        [0.5310],\n",
      "        [0.7091],\n",
      "        [0.4798],\n",
      "        [0.8358],\n",
      "        [0.9041],\n",
      "        [0.3511],\n",
      "        [0.4332],\n",
      "        [0.5711],\n",
      "        [0.5480],\n",
      "        [0.5507],\n",
      "        [0.7305],\n",
      "        [0.6699],\n",
      "        [0.6263],\n",
      "        [0.5709],\n",
      "        [0.8724],\n",
      "        [0.8312],\n",
      "        [0.5217],\n",
      "        [0.6890],\n",
      "        [0.8168],\n",
      "        [0.3322],\n",
      "        [0.5270],\n",
      "        [0.7051],\n",
      "        [0.6269],\n",
      "        [0.9220],\n",
      "        [0.3431],\n",
      "        [0.5023],\n",
      "        [0.8584],\n",
      "        [0.6297],\n",
      "        [0.5320],\n",
      "        [0.5943],\n",
      "        [0.2862],\n",
      "        [0.8521],\n",
      "        [0.7297],\n",
      "        [0.5053],\n",
      "        [0.9209],\n",
      "        [0.8359],\n",
      "        [0.6921],\n",
      "        [0.9131],\n",
      "        [0.3374],\n",
      "        [0.6003],\n",
      "        [0.8301],\n",
      "        [0.8686],\n",
      "        [0.9296],\n",
      "        [0.8266],\n",
      "        [0.8821],\n",
      "        [0.5653],\n",
      "        [0.6143],\n",
      "        [0.8930],\n",
      "        [0.5008],\n",
      "        [0.3330],\n",
      "        [0.3326],\n",
      "        [0.9160],\n",
      "        [0.8084],\n",
      "        [0.9420],\n",
      "        [0.6148],\n",
      "        [0.3200],\n",
      "        [0.6118],\n",
      "        [0.3373],\n",
      "        [0.4970],\n",
      "        [0.5867],\n",
      "        [0.8430],\n",
      "        [0.9156],\n",
      "        [0.6221],\n",
      "        [0.6031],\n",
      "        [0.5048],\n",
      "        [0.8149],\n",
      "        [0.8488],\n",
      "        [0.5755],\n",
      "        [0.6121],\n",
      "        [0.6703],\n",
      "        [0.6005],\n",
      "        [0.5826],\n",
      "        [0.8510],\n",
      "        [0.8916],\n",
      "        [0.8046],\n",
      "        [0.6386],\n",
      "        [0.6334],\n",
      "        [0.7197],\n",
      "        [0.6651],\n",
      "        [0.6767],\n",
      "        [0.8351],\n",
      "        [0.5845],\n",
      "        [0.4280],\n",
      "        [0.6131],\n",
      "        [0.5609],\n",
      "        [0.6111],\n",
      "        [0.7948],\n",
      "        [0.6094],\n",
      "        [0.8796],\n",
      "        [0.6287],\n",
      "        [0.5931],\n",
      "        [0.6748],\n",
      "        [0.8676],\n",
      "        [0.4120],\n",
      "        [0.6876],\n",
      "        [0.6812],\n",
      "        [0.9125],\n",
      "        [0.6777],\n",
      "        [0.9103],\n",
      "        [0.7809],\n",
      "        [0.5641],\n",
      "        [0.6116],\n",
      "        [0.5555],\n",
      "        [0.6538],\n",
      "        [0.7989],\n",
      "        [0.6429],\n",
      "        [0.6224],\n",
      "        [0.6236],\n",
      "        [0.9181],\n",
      "        [0.6236],\n",
      "        [0.5689],\n",
      "        [0.8443],\n",
      "        [0.8825],\n",
      "        [0.8238],\n",
      "        [0.7410],\n",
      "        [0.8510],\n",
      "        [0.8837],\n",
      "        [0.8610],\n",
      "        [0.3653],\n",
      "        [0.4289],\n",
      "        [0.8555],\n",
      "        [0.7261],\n",
      "        [0.5803],\n",
      "        [0.8432],\n",
      "        [0.9184],\n",
      "        [0.7541],\n",
      "        [0.4217],\n",
      "        [0.8389],\n",
      "        [0.7937],\n",
      "        [0.8150],\n",
      "        [0.7943],\n",
      "        [0.8639],\n",
      "        [0.4228],\n",
      "        [0.8419],\n",
      "        [0.4218],\n",
      "        [0.6227]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0008],\n",
      "        [    0.0014],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0019],\n",
      "        [    0.0022],\n",
      "        [    0.0028],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0029],\n",
      "        [    0.0032],\n",
      "        [    0.0034],\n",
      "        [    0.0042],\n",
      "        [    0.0042],\n",
      "        [    0.0045],\n",
      "        [    0.0047],\n",
      "        [    0.0050],\n",
      "        [    0.0053],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0063],\n",
      "        [    0.0063],\n",
      "        [    0.0070],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0078],\n",
      "        [    0.0083],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0090],\n",
      "        [    0.0098],\n",
      "        [    0.0107],\n",
      "        [    0.0112],\n",
      "        [    0.0113],\n",
      "        [    0.0118],\n",
      "        [    0.0125],\n",
      "        [    0.0130],\n",
      "        [    0.0136],\n",
      "        [    0.0137],\n",
      "        [    0.0146],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0150],\n",
      "        [    0.0152],\n",
      "        [    0.0157],\n",
      "        [    0.0160],\n",
      "        [    0.0162],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0166],\n",
      "        [    0.0177],\n",
      "        [    0.0191],\n",
      "        [    0.0193],\n",
      "        [    0.0197],\n",
      "        [    0.0206],\n",
      "        [    0.0211],\n",
      "        [    0.0213],\n",
      "        [    0.0223],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0230],\n",
      "        [    0.0239],\n",
      "        [    0.0254],\n",
      "        [    0.0255],\n",
      "        [    0.0255],\n",
      "        [    0.0257],\n",
      "        [    0.0259],\n",
      "        [    0.0261],\n",
      "        [    0.0271],\n",
      "        [    0.0273],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0291],\n",
      "        [    0.0294],\n",
      "        [    0.0295],\n",
      "        [    0.0295],\n",
      "        [    0.0299],\n",
      "        [    0.0313],\n",
      "        [    0.0322],\n",
      "        [    0.0332],\n",
      "        [    0.0333],\n",
      "        [    0.0339],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0352],\n",
      "        [    0.0353],\n",
      "        [    0.0353],\n",
      "        [    0.0355],\n",
      "        [    0.0361],\n",
      "        [    0.0366],\n",
      "        [    0.0369],\n",
      "        [    0.0380],\n",
      "        [    0.0381],\n",
      "        [    0.0386],\n",
      "        [    0.0389],\n",
      "        [    0.0399],\n",
      "        [    0.0401],\n",
      "        [    0.0415],\n",
      "        [    0.0432],\n",
      "        [    0.0439],\n",
      "        [    0.0462],\n",
      "        [    0.0464],\n",
      "        [    0.0480],\n",
      "        [    0.0493],\n",
      "        [    0.0496],\n",
      "        [    0.0499],\n",
      "        [    0.0508],\n",
      "        [    0.0509],\n",
      "        [    0.0512],\n",
      "        [    0.0524],\n",
      "        [    0.0525],\n",
      "        [    0.0541],\n",
      "        [    0.0543],\n",
      "        [    0.0544],\n",
      "        [    0.0565],\n",
      "        [    0.0570],\n",
      "        [    0.0591],\n",
      "        [    0.0598],\n",
      "        [    0.0602],\n",
      "        [    0.0615],\n",
      "        [    0.0623],\n",
      "        [    0.0647],\n",
      "        [    0.0648],\n",
      "        [    0.0669],\n",
      "        [    0.0672],\n",
      "        [    0.0726],\n",
      "        [    0.0730],\n",
      "        [    0.0735],\n",
      "        [    0.0760],\n",
      "        [    0.0760],\n",
      "        [    0.0777],\n",
      "        [    0.0780],\n",
      "        [    0.0844],\n",
      "        [    0.0845],\n",
      "        [    0.0943],\n",
      "        [    0.0979],\n",
      "        [    0.1026],\n",
      "        [    0.1039],\n",
      "        [    0.1118],\n",
      "        [    0.1234],\n",
      "        [    0.1253],\n",
      "        [    0.1255],\n",
      "        [    0.1328],\n",
      "        [    0.1349],\n",
      "        [    0.1351],\n",
      "        [    0.1352],\n",
      "        [    0.1380],\n",
      "        [    0.1451],\n",
      "        [    0.1560],\n",
      "        [    0.1579],\n",
      "        [    0.1581],\n",
      "        [    0.1944],\n",
      "        [    0.2022]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0014],\n",
      "        [0.0010],\n",
      "        [0.0022],\n",
      "        [0.0035],\n",
      "        [0.0022],\n",
      "        [0.0004],\n",
      "        [0.0034],\n",
      "        [0.0013],\n",
      "        [0.0011],\n",
      "        [0.0015],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0045],\n",
      "        [0.0050],\n",
      "        [0.0021],\n",
      "        [0.0017],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0049],\n",
      "        [0.0078],\n",
      "        [0.0063],\n",
      "        [0.0026],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0119],\n",
      "        [0.0113],\n",
      "        [0.0081],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0080],\n",
      "        [0.0091],\n",
      "        [0.0098],\n",
      "        [0.0083],\n",
      "        [0.0088],\n",
      "        [0.0103],\n",
      "        [0.0133],\n",
      "        [0.0116],\n",
      "        [0.0110],\n",
      "        [0.0081],\n",
      "        [0.0163],\n",
      "        [0.0145],\n",
      "        [0.0144],\n",
      "        [0.0167],\n",
      "        [0.0199],\n",
      "        [0.0124],\n",
      "        [0.0120],\n",
      "        [0.0154],\n",
      "        [0.0186],\n",
      "        [0.0144],\n",
      "        [0.0221],\n",
      "        [0.0143],\n",
      "        [0.0183],\n",
      "        [0.0201],\n",
      "        [0.0182],\n",
      "        [0.0178],\n",
      "        [0.0189],\n",
      "        [0.0193],\n",
      "        [0.0259],\n",
      "        [0.0203],\n",
      "        [0.0250],\n",
      "        [0.0193],\n",
      "        [0.0224],\n",
      "        [0.0216],\n",
      "        [0.0259],\n",
      "        [0.0268],\n",
      "        [0.0254],\n",
      "        [0.0245],\n",
      "        [0.0290],\n",
      "        [0.0206],\n",
      "        [0.0309],\n",
      "        [0.0240],\n",
      "        [0.0297],\n",
      "        [0.0261],\n",
      "        [0.0270],\n",
      "        [0.0241],\n",
      "        [0.0302],\n",
      "        [0.0242],\n",
      "        [0.0302],\n",
      "        [0.0302],\n",
      "        [0.0335],\n",
      "        [0.0318],\n",
      "        [0.0356],\n",
      "        [0.0332],\n",
      "        [0.0315],\n",
      "        [0.0355],\n",
      "        [0.0366],\n",
      "        [0.0342],\n",
      "        [0.0363],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0349],\n",
      "        [0.0361],\n",
      "        [0.0356],\n",
      "        [0.0393],\n",
      "        [0.0369],\n",
      "        [0.0380],\n",
      "        [0.0394],\n",
      "        [0.0410],\n",
      "        [0.0395],\n",
      "        [0.0392],\n",
      "        [0.0441],\n",
      "        [0.0478],\n",
      "        [0.0459],\n",
      "        [0.0477],\n",
      "        [0.0490],\n",
      "        [0.0476],\n",
      "        [0.0507],\n",
      "        [0.0474],\n",
      "        [0.0515],\n",
      "        [0.0515],\n",
      "        [0.0507],\n",
      "        [0.0550],\n",
      "        [0.0567],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0541],\n",
      "        [0.0562],\n",
      "        [0.0558],\n",
      "        [0.0609],\n",
      "        [0.0581],\n",
      "        [0.0590],\n",
      "        [0.0604],\n",
      "        [0.0636],\n",
      "        [0.0658],\n",
      "        [0.0663],\n",
      "        [0.0665],\n",
      "        [0.0670],\n",
      "        [0.0719],\n",
      "        [0.0732],\n",
      "        [0.0733],\n",
      "        [0.0784],\n",
      "        [0.0749],\n",
      "        [0.0763],\n",
      "        [0.0775],\n",
      "        [0.0873],\n",
      "        [0.0840],\n",
      "        [0.0959],\n",
      "        [0.0935],\n",
      "        [0.0985],\n",
      "        [0.1060],\n",
      "        [0.1106],\n",
      "        [0.1233],\n",
      "        [0.1255],\n",
      "        [0.1248],\n",
      "        [0.1321],\n",
      "        [0.1306],\n",
      "        [0.1338],\n",
      "        [0.1334],\n",
      "        [0.1388],\n",
      "        [0.1435],\n",
      "        [0.1535],\n",
      "        [0.1539],\n",
      "        [0.1584],\n",
      "        [0.1901],\n",
      "        [0.2021]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.84259033203125\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.2759943501805537e-07, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 114, 121, 106, 39, 120, 0, 16, 80, 115, 83, 122, 140, 34, 135, 4, 158, 58, 81, 108, 13, 14, 104, 35, 74, 134, 61, 113, 151, 129, 47, 69, 84, 130, 128, 17, 15, 143, 82, 155, 30, 139, 136, 50, 94, 107, 75, 95, 133, 3, 52, 28, 59, 131, 73, 38, 32, 154, 137, 119, 153, 11, 150, 27, 1, 149, 152, 37, 33, 98, 157, 40, 2, 127, 118, 132, 49, 123, 86, 105, 156, 138, 36, 96, 48, 112, 111, 76, 12, 6, 67, 97, 18, 85, 46, 116, 93, 51, 20, 60, 70, 79, 125, 99, 31, 57, 126, 141, 87, 68, 88, 124, 89, 66, 26, 71, 41, 5, 72, 142, 109, 117, 110, 19, 77, 21, 78, 92, 91, 7, 90, 103, 9, 56, 62, 45, 25, 42, 148, 44, 144, 43, 65, 102, 8, 24, 145, 64, 55, 53, 22, 54, 29, 147, 23, 146, 100, 101] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8312],\n",
      "        [0.5817],\n",
      "        [0.6275],\n",
      "        [0.5501],\n",
      "        [0.9082],\n",
      "        [0.6319],\n",
      "        [0.9455],\n",
      "        [0.8115],\n",
      "        [0.6929],\n",
      "        [0.6327],\n",
      "        [0.6621],\n",
      "        [0.6044],\n",
      "        [0.4766],\n",
      "        [0.8925],\n",
      "        [0.5306],\n",
      "        [0.9194],\n",
      "        [0.3600],\n",
      "        [0.7561],\n",
      "        [0.6885],\n",
      "        [0.5901],\n",
      "        [0.8346],\n",
      "        [0.8442],\n",
      "        [0.5060],\n",
      "        [0.9035],\n",
      "        [0.7097],\n",
      "        [0.5288],\n",
      "        [0.7298],\n",
      "        [0.5719],\n",
      "        [0.3273],\n",
      "        [0.5694],\n",
      "        [0.8705],\n",
      "        [0.6701],\n",
      "        [0.6256],\n",
      "        [0.5459],\n",
      "        [0.5489],\n",
      "        [0.8303],\n",
      "        [0.8153],\n",
      "        [0.4291],\n",
      "        [0.6887],\n",
      "        [0.3461],\n",
      "        [0.8553],\n",
      "        [0.4996],\n",
      "        [0.5197],\n",
      "        [0.8498],\n",
      "        [0.6272],\n",
      "        [0.5961],\n",
      "        [0.7059],\n",
      "        [0.6300],\n",
      "        [0.5242],\n",
      "        [0.9200],\n",
      "        [0.8345],\n",
      "        [0.9201],\n",
      "        [0.7280],\n",
      "        [0.5294],\n",
      "        [0.6928],\n",
      "        [0.9119],\n",
      "        [0.8656],\n",
      "        [0.3380],\n",
      "        [0.5029],\n",
      "        [0.6013],\n",
      "        [0.3276],\n",
      "        [0.8252],\n",
      "        [0.2806],\n",
      "        [0.9298],\n",
      "        [0.9130],\n",
      "        [0.3150],\n",
      "        [0.3320],\n",
      "        [0.8920],\n",
      "        [0.8274],\n",
      "        [0.6142],\n",
      "        [0.3327],\n",
      "        [0.8800],\n",
      "        [0.9402],\n",
      "        [0.5639],\n",
      "        [0.6159],\n",
      "        [0.4974],\n",
      "        [0.8060],\n",
      "        [0.5864],\n",
      "        [0.6110],\n",
      "        [0.4963],\n",
      "        [0.3278],\n",
      "        [0.5024],\n",
      "        [0.9153],\n",
      "        [0.6032],\n",
      "        [0.8408],\n",
      "        [0.5765],\n",
      "        [0.5838],\n",
      "        [0.6702],\n",
      "        [0.8137],\n",
      "        [0.8904],\n",
      "        [0.6197],\n",
      "        [0.6003],\n",
      "        [0.8505],\n",
      "        [0.6111],\n",
      "        [0.8467],\n",
      "        [0.6399],\n",
      "        [0.6340],\n",
      "        [0.8328],\n",
      "        [0.8033],\n",
      "        [0.7192],\n",
      "        [0.6773],\n",
      "        [0.6640],\n",
      "        [0.5837],\n",
      "        [0.6128],\n",
      "        [0.8770],\n",
      "        [0.7931],\n",
      "        [0.5595],\n",
      "        [0.4241],\n",
      "        [0.6102],\n",
      "        [0.6743],\n",
      "        [0.6083],\n",
      "        [0.5925],\n",
      "        [0.6279],\n",
      "        [0.6860],\n",
      "        [0.9129],\n",
      "        [0.6813],\n",
      "        [0.8650],\n",
      "        [0.9091],\n",
      "        [0.6780],\n",
      "        [0.4079],\n",
      "        [0.5657],\n",
      "        [0.6127],\n",
      "        [0.5566],\n",
      "        [0.7791],\n",
      "        [0.6525],\n",
      "        [0.7977],\n",
      "        [0.6415],\n",
      "        [0.6228],\n",
      "        [0.6238],\n",
      "        [0.9174],\n",
      "        [0.6234],\n",
      "        [0.5687],\n",
      "        [0.8814],\n",
      "        [0.8224],\n",
      "        [0.7405],\n",
      "        [0.8419],\n",
      "        [0.8842],\n",
      "        [0.8481],\n",
      "        [0.3609],\n",
      "        [0.8593],\n",
      "        [0.4248],\n",
      "        [0.8534],\n",
      "        [0.7250],\n",
      "        [0.5802],\n",
      "        [0.9177],\n",
      "        [0.8430],\n",
      "        [0.4174],\n",
      "        [0.7535],\n",
      "        [0.7919],\n",
      "        [0.8376],\n",
      "        [0.8142],\n",
      "        [0.7927],\n",
      "        [0.8614],\n",
      "        [0.4188],\n",
      "        [0.8416],\n",
      "        [0.4174],\n",
      "        [0.6226],\n",
      "        [0.6356]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0103],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0119],\n",
      "        [0.0120],\n",
      "        [0.0124],\n",
      "        [0.0133],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0154],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0178],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0193],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0216],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0242],\n",
      "        [0.0245],\n",
      "        [0.0250],\n",
      "        [0.0254],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        [0.0261],\n",
      "        [0.0268],\n",
      "        [0.0270],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0302],\n",
      "        [0.0302],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0318],\n",
      "        [0.0332],\n",
      "        [0.0335],\n",
      "        [0.0342],\n",
      "        [0.0349],\n",
      "        [0.0354],\n",
      "        [0.0355],\n",
      "        [0.0356],\n",
      "        [0.0356],\n",
      "        [0.0357],\n",
      "        [0.0361],\n",
      "        [0.0363],\n",
      "        [0.0366],\n",
      "        [0.0369],\n",
      "        [0.0380],\n",
      "        [0.0392],\n",
      "        [0.0393],\n",
      "        [0.0394],\n",
      "        [0.0395],\n",
      "        [0.0410],\n",
      "        [0.0441],\n",
      "        [0.0459],\n",
      "        [0.0474],\n",
      "        [0.0476],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0490],\n",
      "        [0.0507],\n",
      "        [0.0507],\n",
      "        [0.0515],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0541],\n",
      "        [0.0542],\n",
      "        [0.0550],\n",
      "        [0.0558],\n",
      "        [0.0562],\n",
      "        [0.0567],\n",
      "        [0.0581],\n",
      "        [0.0590],\n",
      "        [0.0604],\n",
      "        [0.0609],\n",
      "        [0.0636],\n",
      "        [0.0658],\n",
      "        [0.0663],\n",
      "        [0.0665],\n",
      "        [0.0670],\n",
      "        [0.0719],\n",
      "        [0.0732],\n",
      "        [0.0733],\n",
      "        [0.0749],\n",
      "        [0.0763],\n",
      "        [0.0775],\n",
      "        [0.0784],\n",
      "        [0.0840],\n",
      "        [0.0873],\n",
      "        [0.0935],\n",
      "        [0.0959],\n",
      "        [0.0985],\n",
      "        [0.1060],\n",
      "        [0.1106],\n",
      "        [0.1233],\n",
      "        [0.1248],\n",
      "        [0.1255],\n",
      "        [0.1306],\n",
      "        [0.1321],\n",
      "        [0.1334],\n",
      "        [0.1338],\n",
      "        [0.1388],\n",
      "        [0.1435],\n",
      "        [0.1535],\n",
      "        [0.1539],\n",
      "        [0.1584],\n",
      "        [0.1901],\n",
      "        [0.2021],\n",
      "        [0.2119]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0029],\n",
      "        [    0.0010],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0007],\n",
      "        [    0.0021],\n",
      "        [    0.0031],\n",
      "        [    0.0022],\n",
      "        [    0.0043],\n",
      "        [    0.0024],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0041],\n",
      "        [    0.0037],\n",
      "        [    0.0044],\n",
      "        [    0.0058],\n",
      "        [    0.0044],\n",
      "        [    0.0050],\n",
      "        [    0.0072],\n",
      "        [    0.0061],\n",
      "        [    0.0043],\n",
      "        [    0.0056],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0098],\n",
      "        [    0.0053],\n",
      "        [    0.0084],\n",
      "        [    0.0093],\n",
      "        [    0.0096],\n",
      "        [    0.0100],\n",
      "        [    0.0104],\n",
      "        [    0.0102],\n",
      "        [    0.0115],\n",
      "        [    0.0119],\n",
      "        [    0.0136],\n",
      "        [    0.0118],\n",
      "        [    0.0147],\n",
      "        [    0.0119],\n",
      "        [    0.0111],\n",
      "        [    0.0139],\n",
      "        [    0.0143],\n",
      "        [    0.0140],\n",
      "        [    0.0123],\n",
      "        [    0.0155],\n",
      "        [    0.0149],\n",
      "        [    0.0173],\n",
      "        [    0.0160],\n",
      "        [    0.0183],\n",
      "        [    0.0198],\n",
      "        [    0.0183],\n",
      "        [    0.0194],\n",
      "        [    0.0180],\n",
      "        [    0.0204],\n",
      "        [    0.0194],\n",
      "        [    0.0227],\n",
      "        [    0.0211],\n",
      "        [    0.0184],\n",
      "        [    0.0177],\n",
      "        [    0.0224],\n",
      "        [    0.0255],\n",
      "        [    0.0200],\n",
      "        [    0.0240],\n",
      "        [    0.0209],\n",
      "        [    0.0212],\n",
      "        [    0.0257],\n",
      "        [    0.0249],\n",
      "        [    0.0255],\n",
      "        [    0.0286],\n",
      "        [    0.0253],\n",
      "        [    0.0269],\n",
      "        [    0.0268],\n",
      "        [    0.0251],\n",
      "        [    0.0303],\n",
      "        [    0.0297],\n",
      "        [    0.0292],\n",
      "        [    0.0304],\n",
      "        [    0.0302],\n",
      "        [    0.0337],\n",
      "        [    0.0304],\n",
      "        [    0.0335],\n",
      "        [    0.0329],\n",
      "        [    0.0334],\n",
      "        [    0.0324],\n",
      "        [    0.0330],\n",
      "        [    0.0352],\n",
      "        [    0.0346],\n",
      "        [    0.0367],\n",
      "        [    0.0369],\n",
      "        [    0.0357],\n",
      "        [    0.0375],\n",
      "        [    0.0367],\n",
      "        [    0.0364],\n",
      "        [    0.0348],\n",
      "        [    0.0373],\n",
      "        [    0.0392],\n",
      "        [    0.0383],\n",
      "        [    0.0389],\n",
      "        [    0.0388],\n",
      "        [    0.0412],\n",
      "        [    0.0435],\n",
      "        [    0.0459],\n",
      "        [    0.0478],\n",
      "        [    0.0478],\n",
      "        [    0.0475],\n",
      "        [    0.0500],\n",
      "        [    0.0493],\n",
      "        [    0.0508],\n",
      "        [    0.0511],\n",
      "        [    0.0507],\n",
      "        [    0.0517],\n",
      "        [    0.0520],\n",
      "        [    0.0517],\n",
      "        [    0.0539],\n",
      "        [    0.0550],\n",
      "        [    0.0568],\n",
      "        [    0.0557],\n",
      "        [    0.0591],\n",
      "        [    0.0561],\n",
      "        [    0.0571],\n",
      "        [    0.0588],\n",
      "        [    0.0603],\n",
      "        [    0.0640],\n",
      "        [    0.0647],\n",
      "        [    0.0668],\n",
      "        [    0.0659],\n",
      "        [    0.0665],\n",
      "        [    0.0734],\n",
      "        [    0.0730],\n",
      "        [    0.0736],\n",
      "        [    0.0759],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0783],\n",
      "        [    0.0816],\n",
      "        [    0.0875],\n",
      "        [    0.0908],\n",
      "        [    0.0954],\n",
      "        [    0.0962],\n",
      "        [    0.1056],\n",
      "        [    0.1105],\n",
      "        [    0.1235],\n",
      "        [    0.1263],\n",
      "        [    0.1236],\n",
      "        [    0.1282],\n",
      "        [    0.1325],\n",
      "        [    0.1336],\n",
      "        [    0.1343],\n",
      "        [    0.1373],\n",
      "        [    0.1439],\n",
      "        [    0.1539],\n",
      "        [    0.1516],\n",
      "        [    0.1567],\n",
      "        [    0.1876],\n",
      "        [    0.2023],\n",
      "        [    0.2125]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.13231682777405\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.032743395830039e-08, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 106, 39, 120, 140, 121, 122, 0, 80, 83, 114, 16, 135, 34, 115, 14, 58, 4, 81, 151, 104, 158, 13, 35, 74, 108, 129, 134, 61, 47, 69, 113, 84, 128, 130, 139, 17, 82, 15, 30, 107, 143, 136, 94, 50, 155, 95, 75, 3, 133, 153, 73, 59, 52, 119, 131, 32, 28, 27, 38, 149, 137, 152, 11, 154, 1, 33, 118, 40, 98, 150, 37, 127, 2, 157, 123, 49, 105, 132, 86, 138, 112, 96, 111, 48, 36, 156, 12, 116, 76, 97, 46, 6, 85, 67, 93, 18, 20, 70, 60, 51, 79, 125, 99, 126, 31, 57, 87, 141, 124, 68, 88, 26, 89, 66, 71, 41, 72, 109, 5, 117, 110, 142, 19, 77, 21, 92, 91, 78, 90, 7, 103, 9, 56, 62, 45, 25, 42, 148, 44, 144, 43, 65, 102, 24, 8, 145, 64, 55, 53, 22, 54, 147, 29, 23, 146, 100, 101, 63] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8317],\n",
      "        [0.5512],\n",
      "        [0.9093],\n",
      "        [0.6341],\n",
      "        [0.4749],\n",
      "        [0.6296],\n",
      "        [0.6059],\n",
      "        [0.9459],\n",
      "        [0.6928],\n",
      "        [0.6621],\n",
      "        [0.5836],\n",
      "        [0.8125],\n",
      "        [0.5303],\n",
      "        [0.8934],\n",
      "        [0.6349],\n",
      "        [0.8453],\n",
      "        [0.7562],\n",
      "        [0.9203],\n",
      "        [0.6886],\n",
      "        [0.3245],\n",
      "        [0.5057],\n",
      "        [0.3577],\n",
      "        [0.8356],\n",
      "        [0.9049],\n",
      "        [0.7105],\n",
      "        [0.5923],\n",
      "        [0.5695],\n",
      "        [0.5282],\n",
      "        [0.7303],\n",
      "        [0.8709],\n",
      "        [0.6707],\n",
      "        [0.5735],\n",
      "        [0.6253],\n",
      "        [0.5488],\n",
      "        [0.5455],\n",
      "        [0.4984],\n",
      "        [0.8315],\n",
      "        [0.6889],\n",
      "        [0.8161],\n",
      "        [0.8552],\n",
      "        [0.5982],\n",
      "        [0.4269],\n",
      "        [0.5190],\n",
      "        [0.6276],\n",
      "        [0.8498],\n",
      "        [0.3433],\n",
      "        [0.6305],\n",
      "        [0.7069],\n",
      "        [0.9206],\n",
      "        [0.5233],\n",
      "        [0.3247],\n",
      "        [0.6937],\n",
      "        [0.7280],\n",
      "        [0.8350],\n",
      "        [0.6032],\n",
      "        [0.5286],\n",
      "        [0.8656],\n",
      "        [0.9217],\n",
      "        [0.9321],\n",
      "        [0.9129],\n",
      "        [0.3118],\n",
      "        [0.5019],\n",
      "        [0.3290],\n",
      "        [0.8260],\n",
      "        [0.3352],\n",
      "        [0.9130],\n",
      "        [0.8275],\n",
      "        [0.6178],\n",
      "        [0.8806],\n",
      "        [0.6143],\n",
      "        [0.2772],\n",
      "        [0.8931],\n",
      "        [0.5639],\n",
      "        [0.9410],\n",
      "        [0.3300],\n",
      "        [0.5874],\n",
      "        [0.8060],\n",
      "        [0.4964],\n",
      "        [0.4961],\n",
      "        [0.6108],\n",
      "        [0.5014],\n",
      "        [0.5783],\n",
      "        [0.6035],\n",
      "        [0.5857],\n",
      "        [0.8410],\n",
      "        [0.9170],\n",
      "        [0.3250],\n",
      "        [0.8146],\n",
      "        [0.6419],\n",
      "        [0.6705],\n",
      "        [0.6003],\n",
      "        [0.8469],\n",
      "        [0.8914],\n",
      "        [0.6107],\n",
      "        [0.6184],\n",
      "        [0.6347],\n",
      "        [0.8520],\n",
      "        [0.8042],\n",
      "        [0.6780],\n",
      "        [0.7197],\n",
      "        [0.8328],\n",
      "        [0.6638],\n",
      "        [0.5843],\n",
      "        [0.6128],\n",
      "        [0.5597],\n",
      "        [0.8774],\n",
      "        [0.7933],\n",
      "        [0.6098],\n",
      "        [0.4219],\n",
      "        [0.5932],\n",
      "        [0.6744],\n",
      "        [0.6079],\n",
      "        [0.9153],\n",
      "        [0.6277],\n",
      "        [0.6854],\n",
      "        [0.6816],\n",
      "        [0.8650],\n",
      "        [0.6785],\n",
      "        [0.5677],\n",
      "        [0.9102],\n",
      "        [0.6146],\n",
      "        [0.5583],\n",
      "        [0.4055],\n",
      "        [0.7797],\n",
      "        [0.6521],\n",
      "        [0.7989],\n",
      "        [0.6233],\n",
      "        [0.6242],\n",
      "        [0.6410],\n",
      "        [0.6236],\n",
      "        [0.9189],\n",
      "        [0.5689],\n",
      "        [0.8824],\n",
      "        [0.8229],\n",
      "        [0.7411],\n",
      "        [0.8420],\n",
      "        [0.8866],\n",
      "        [0.8480],\n",
      "        [0.3583],\n",
      "        [0.8599],\n",
      "        [0.4226],\n",
      "        [0.8538],\n",
      "        [0.7249],\n",
      "        [0.5804],\n",
      "        [0.8448],\n",
      "        [0.9192],\n",
      "        [0.4150],\n",
      "        [0.7538],\n",
      "        [0.7921],\n",
      "        [0.8382],\n",
      "        [0.8157],\n",
      "        [0.7931],\n",
      "        [0.4165],\n",
      "        [0.8618],\n",
      "        [0.8433],\n",
      "        [0.4150],\n",
      "        [0.6227],\n",
      "        [0.6362],\n",
      "        [0.7435]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0007],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0021],\n",
      "        [    0.0022],\n",
      "        [    0.0024],\n",
      "        [    0.0029],\n",
      "        [    0.0031],\n",
      "        [    0.0037],\n",
      "        [    0.0041],\n",
      "        [    0.0043],\n",
      "        [    0.0043],\n",
      "        [    0.0044],\n",
      "        [    0.0044],\n",
      "        [    0.0050],\n",
      "        [    0.0053],\n",
      "        [    0.0056],\n",
      "        [    0.0058],\n",
      "        [    0.0061],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0072],\n",
      "        [    0.0084],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0093],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0104],\n",
      "        [    0.0111],\n",
      "        [    0.0115],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0119],\n",
      "        [    0.0123],\n",
      "        [    0.0136],\n",
      "        [    0.0139],\n",
      "        [    0.0140],\n",
      "        [    0.0143],\n",
      "        [    0.0147],\n",
      "        [    0.0149],\n",
      "        [    0.0155],\n",
      "        [    0.0160],\n",
      "        [    0.0173],\n",
      "        [    0.0177],\n",
      "        [    0.0180],\n",
      "        [    0.0183],\n",
      "        [    0.0183],\n",
      "        [    0.0184],\n",
      "        [    0.0194],\n",
      "        [    0.0194],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0204],\n",
      "        [    0.0209],\n",
      "        [    0.0211],\n",
      "        [    0.0212],\n",
      "        [    0.0224],\n",
      "        [    0.0227],\n",
      "        [    0.0240],\n",
      "        [    0.0249],\n",
      "        [    0.0251],\n",
      "        [    0.0253],\n",
      "        [    0.0255],\n",
      "        [    0.0255],\n",
      "        [    0.0257],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0286],\n",
      "        [    0.0292],\n",
      "        [    0.0297],\n",
      "        [    0.0302],\n",
      "        [    0.0303],\n",
      "        [    0.0304],\n",
      "        [    0.0304],\n",
      "        [    0.0324],\n",
      "        [    0.0329],\n",
      "        [    0.0330],\n",
      "        [    0.0334],\n",
      "        [    0.0335],\n",
      "        [    0.0337],\n",
      "        [    0.0346],\n",
      "        [    0.0348],\n",
      "        [    0.0352],\n",
      "        [    0.0357],\n",
      "        [    0.0364],\n",
      "        [    0.0367],\n",
      "        [    0.0367],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0375],\n",
      "        [    0.0383],\n",
      "        [    0.0388],\n",
      "        [    0.0389],\n",
      "        [    0.0392],\n",
      "        [    0.0412],\n",
      "        [    0.0435],\n",
      "        [    0.0459],\n",
      "        [    0.0475],\n",
      "        [    0.0478],\n",
      "        [    0.0478],\n",
      "        [    0.0493],\n",
      "        [    0.0500],\n",
      "        [    0.0507],\n",
      "        [    0.0508],\n",
      "        [    0.0511],\n",
      "        [    0.0517],\n",
      "        [    0.0517],\n",
      "        [    0.0520],\n",
      "        [    0.0539],\n",
      "        [    0.0550],\n",
      "        [    0.0557],\n",
      "        [    0.0561],\n",
      "        [    0.0568],\n",
      "        [    0.0571],\n",
      "        [    0.0588],\n",
      "        [    0.0591],\n",
      "        [    0.0603],\n",
      "        [    0.0640],\n",
      "        [    0.0647],\n",
      "        [    0.0659],\n",
      "        [    0.0665],\n",
      "        [    0.0668],\n",
      "        [    0.0730],\n",
      "        [    0.0734],\n",
      "        [    0.0736],\n",
      "        [    0.0759],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0783],\n",
      "        [    0.0816],\n",
      "        [    0.0875],\n",
      "        [    0.0908],\n",
      "        [    0.0954],\n",
      "        [    0.0962],\n",
      "        [    0.1056],\n",
      "        [    0.1105],\n",
      "        [    0.1235],\n",
      "        [    0.1236],\n",
      "        [    0.1263],\n",
      "        [    0.1282],\n",
      "        [    0.1325],\n",
      "        [    0.1336],\n",
      "        [    0.1343],\n",
      "        [    0.1373],\n",
      "        [    0.1439],\n",
      "        [    0.1516],\n",
      "        [    0.1539],\n",
      "        [    0.1567],\n",
      "        [    0.1876],\n",
      "        [    0.2023],\n",
      "        [    0.2125],\n",
      "        [    0.2824]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0014],\n",
      "        [    0.0002],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0002],\n",
      "        [    0.0027],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0059],\n",
      "        [    0.0008],\n",
      "        [    0.0042],\n",
      "        [    0.0020],\n",
      "        [    0.0035],\n",
      "        [    0.0025],\n",
      "        [    0.0057],\n",
      "        [    0.0054],\n",
      "        [    0.0082],\n",
      "        [    0.0029],\n",
      "        [    0.0016],\n",
      "        [    0.0041],\n",
      "        [    0.0064],\n",
      "        [    0.0072],\n",
      "        [    0.0051],\n",
      "        [    0.0058],\n",
      "        [    0.0041],\n",
      "        [    0.0084],\n",
      "        [    0.0088],\n",
      "        [    0.0085],\n",
      "        [    0.0049],\n",
      "        [    0.0065],\n",
      "        [    0.0064],\n",
      "        [    0.0109],\n",
      "        [    0.0131],\n",
      "        [    0.0098],\n",
      "        [    0.0103],\n",
      "        [    0.0106],\n",
      "        [    0.0104],\n",
      "        [    0.0085],\n",
      "        [    0.0107],\n",
      "        [    0.0098],\n",
      "        [    0.0110],\n",
      "        [    0.0146],\n",
      "        [    0.0140],\n",
      "        [    0.0157],\n",
      "        [    0.0114],\n",
      "        [    0.0163],\n",
      "        [    0.0166],\n",
      "        [    0.0126],\n",
      "        [    0.0178],\n",
      "        [    0.0176],\n",
      "        [    0.0163],\n",
      "        [    0.0211],\n",
      "        [    0.0222],\n",
      "        [    0.0156],\n",
      "        [    0.0167],\n",
      "        [    0.0194],\n",
      "        [    0.0173],\n",
      "        [    0.0186],\n",
      "        [    0.0207],\n",
      "        [    0.0185],\n",
      "        [    0.0196],\n",
      "        [    0.0213],\n",
      "        [    0.0198],\n",
      "        [    0.0212],\n",
      "        [    0.0241],\n",
      "        [    0.0216],\n",
      "        [    0.0268],\n",
      "        [    0.0236],\n",
      "        [    0.0274],\n",
      "        [    0.0238],\n",
      "        [    0.0269],\n",
      "        [    0.0241],\n",
      "        [    0.0263],\n",
      "        [    0.0250],\n",
      "        [    0.0301],\n",
      "        [    0.0281],\n",
      "        [    0.0325],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0332],\n",
      "        [    0.0301],\n",
      "        [    0.0313],\n",
      "        [    0.0344],\n",
      "        [    0.0319],\n",
      "        [    0.0361],\n",
      "        [    0.0322],\n",
      "        [    0.0353],\n",
      "        [    0.0356],\n",
      "        [    0.0334],\n",
      "        [    0.0385],\n",
      "        [    0.0374],\n",
      "        [    0.0391],\n",
      "        [    0.0353],\n",
      "        [    0.0397],\n",
      "        [    0.0412],\n",
      "        [    0.0389],\n",
      "        [    0.0364],\n",
      "        [    0.0396],\n",
      "        [    0.0421],\n",
      "        [    0.0424],\n",
      "        [    0.0363],\n",
      "        [    0.0446],\n",
      "        [    0.0427],\n",
      "        [    0.0443],\n",
      "        [    0.0469],\n",
      "        [    0.0460],\n",
      "        [    0.0440],\n",
      "        [    0.0521],\n",
      "        [    0.0510],\n",
      "        [    0.0499],\n",
      "        [    0.0474],\n",
      "        [    0.0538],\n",
      "        [    0.0523],\n",
      "        [    0.0544],\n",
      "        [    0.0477],\n",
      "        [    0.0575],\n",
      "        [    0.0573],\n",
      "        [    0.0591],\n",
      "        [    0.0549],\n",
      "        [    0.0554],\n",
      "        [    0.0555],\n",
      "        [    0.0578],\n",
      "        [    0.0601],\n",
      "        [    0.0618],\n",
      "        [    0.0676],\n",
      "        [    0.0658],\n",
      "        [    0.0677],\n",
      "        [    0.0685],\n",
      "        [    0.0703],\n",
      "        [    0.0753],\n",
      "        [    0.0722],\n",
      "        [    0.0727],\n",
      "        [    0.0745],\n",
      "        [    0.0730],\n",
      "        [    0.0743],\n",
      "        [    0.0810],\n",
      "        [    0.0821],\n",
      "        [    0.0899],\n",
      "        [    0.0898],\n",
      "        [    0.0977],\n",
      "        [    0.0953],\n",
      "        [    0.1080],\n",
      "        [    0.1063],\n",
      "        [    0.1224],\n",
      "        [    0.1244],\n",
      "        [    0.1250],\n",
      "        [    0.1272],\n",
      "        [    0.1284],\n",
      "        [    0.1301],\n",
      "        [    0.1314],\n",
      "        [    0.1383],\n",
      "        [    0.1407],\n",
      "        [    0.1505],\n",
      "        [    0.1521],\n",
      "        [    0.1575],\n",
      "        [    0.1865],\n",
      "        [    0.2009],\n",
      "        [    0.2115],\n",
      "        [    0.2785]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.424872398376465\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 5 個區塊累積花費時間(s) 1.231119155883789\n",
      "<<The performance of 5 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.231119155883789\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1135.79\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1643.59\n",
      "The accuracy(2000) for l = 1: 84.91%\n",
      "The accuracy(3000) for l = 1: 89.94%\n",
      "The maximum error: tensor(7109.8555)\n",
      "The minimum error: tensor(4.6445)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1141.3\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1242.0\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1585.1328125\n",
      "The minimum error: 351.79296875\n",
      "------------------------------------------------------------\n",
      "0.8490566037735849\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<6>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.3125012066648196e-08, 136)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [136, 102, 118, 79, 6, 77, 12, 35, 116, 30, 117, 0, 131, 70, 147, 110, 57, 9, 10, 111, 31, 76, 100, 65, 43, 154, 54, 104, 130, 78, 125, 26, 124, 126, 13, 135, 11, 109, 103, 46, 71, 80, 155, 132, 139, 48, 90, 149, 151, 91, 115, 28, 129, 34, 24, 127, 145, 148, 23, 69, 7, 133, 55, 114, 94, 33, 150, 123, 29, 146, 36, 119, 134, 153, 101, 128, 108, 107, 32, 45, 82, 112, 92, 152, 2, 8, 44, 47, 14, 93, 72, 89, 42, 16, 81, 63, 66, 56, 121, 156, 53, 95, 75, 27, 122, 64, 62, 120, 137, 83, 22, 84, 85, 105, 1, 113, 37, 67, 106, 68, 158, 138, 15, 157, 17, 73, 88, 87, 74, 3, 99, 52, 58, 5, 86, 41, 21, 144, 38, 140, 40, 61, 39, 98, 20, 4, 141, 60, 51, 49, 18, 50, 143, 25, 19]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0029],\n",
      "        [0.0035],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0064],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0072],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0103],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0126],\n",
      "        [0.0131],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0146],\n",
      "        [0.0156],\n",
      "        [0.0157],\n",
      "        [0.0163],\n",
      "        [0.0163],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0173],\n",
      "        [0.0176],\n",
      "        [0.0185],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0207],\n",
      "        [0.0211],\n",
      "        [0.0212],\n",
      "        [0.0213],\n",
      "        [0.0222],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0241],\n",
      "        [0.0241],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0274],\n",
      "        [0.0281],\n",
      "        [0.0301],\n",
      "        [0.0301],\n",
      "        [0.0305],\n",
      "        [0.0306],\n",
      "        [0.0313],\n",
      "        [0.0319],\n",
      "        [0.0322],\n",
      "        [0.0325],\n",
      "        [0.0332],\n",
      "        [0.0334],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0353],\n",
      "        [0.0356],\n",
      "        [0.0361],\n",
      "        [0.0363],\n",
      "        [0.0364],\n",
      "        [0.0374],\n",
      "        [0.0385],\n",
      "        [0.0389],\n",
      "        [0.0391],\n",
      "        [0.0396],\n",
      "        [0.0397],\n",
      "        [0.0412],\n",
      "        [0.0421],\n",
      "        [0.0424],\n",
      "        [0.0427],\n",
      "        [0.0437],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0446],\n",
      "        [0.0460],\n",
      "        [0.0469],\n",
      "        [0.0474],\n",
      "        [0.0477],\n",
      "        [0.0499],\n",
      "        [0.0510],\n",
      "        [0.0521],\n",
      "        [0.0523],\n",
      "        [0.0538],\n",
      "        [0.0544],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0555],\n",
      "        [0.0573],\n",
      "        [0.0575],\n",
      "        [0.0578],\n",
      "        [0.0591],\n",
      "        [0.0593],\n",
      "        [0.0601],\n",
      "        [0.0618],\n",
      "        [0.0621],\n",
      "        [0.0658],\n",
      "        [0.0676],\n",
      "        [0.0677],\n",
      "        [0.0685],\n",
      "        [0.0703],\n",
      "        [0.0722],\n",
      "        [0.0727],\n",
      "        [0.0730],\n",
      "        [0.0743],\n",
      "        [0.0745],\n",
      "        [0.0753],\n",
      "        [0.0810],\n",
      "        [0.0821],\n",
      "        [0.0898],\n",
      "        [0.0899],\n",
      "        [0.0953],\n",
      "        [0.0977],\n",
      "        [0.1063],\n",
      "        [0.1080],\n",
      "        [0.1224],\n",
      "        [0.1244],\n",
      "        [0.1250],\n",
      "        [0.1272],\n",
      "        [0.1284],\n",
      "        [0.1301],\n",
      "        [0.1314],\n",
      "        [0.1383],\n",
      "        [0.1407],\n",
      "        [0.1505],\n",
      "        [0.1521],\n",
      "        [0.1575]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.3125012066648196e-08, 136)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [136, 102, 118, 79, 6, 77, 12, 35, 116, 30, 117, 0, 131, 70, 147, 110, 57, 9, 10, 111, 31, 76, 100, 65, 43, 154, 54, 104, 130, 78, 125, 26, 124, 126, 13, 135, 11, 109, 103, 46, 71, 80, 155, 132, 139, 48, 90, 149, 151, 91, 115, 28, 129, 34, 24, 127, 145, 148, 23, 69, 7, 133, 55, 114, 94, 33, 150, 123, 29, 146, 36, 119, 134, 153, 101, 128, 108, 107, 32, 45, 82, 112, 92, 152, 2, 8, 44, 47, 14, 93, 72, 89, 42, 16, 81, 63, 66, 56, 121, 156, 53, 95, 75, 27, 122, 64, 62, 120, 137, 83, 22, 84, 85, 105, 1, 113, 37, 67, 106, 68, 158, 138, 15, 157, 17, 73, 88, 87, 74, 3, 99, 52, 58, 5, 86, 41, 21, 144, 38, 140, 40, 61, 39, 98, 20, 4, 141, 60, 51, 49, 18, 50, 143, 25, 19, 142] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4741],\n",
      "        [0.5517],\n",
      "        [0.6073],\n",
      "        [0.6590],\n",
      "        [0.8302],\n",
      "        [0.6851],\n",
      "        [0.8115],\n",
      "        [0.9075],\n",
      "        [0.6358],\n",
      "        [0.8917],\n",
      "        [0.6313],\n",
      "        [0.9188],\n",
      "        [0.5305],\n",
      "        [0.7075],\n",
      "        [0.3234],\n",
      "        [0.5849],\n",
      "        [0.7267],\n",
      "        [0.8346],\n",
      "        [0.8442],\n",
      "        [0.6362],\n",
      "        [0.9036],\n",
      "        [0.6892],\n",
      "        [0.5050],\n",
      "        [0.6675],\n",
      "        [0.8682],\n",
      "        [0.3562],\n",
      "        [0.7524],\n",
      "        [0.5935],\n",
      "        [0.5282],\n",
      "        [0.6856],\n",
      "        [0.5699],\n",
      "        [0.8531],\n",
      "        [0.5492],\n",
      "        [0.5456],\n",
      "        [0.8303],\n",
      "        [0.4978],\n",
      "        [0.8149],\n",
      "        [0.5747],\n",
      "        [0.5995],\n",
      "        [0.8469],\n",
      "        [0.7040],\n",
      "        [0.6222],\n",
      "        [0.4049],\n",
      "        [0.5189],\n",
      "        [0.4259],\n",
      "        [0.8323],\n",
      "        [0.6258],\n",
      "        [0.3234],\n",
      "        [0.3417],\n",
      "        [0.6288],\n",
      "        [0.6049],\n",
      "        [0.8636],\n",
      "        [0.5230],\n",
      "        [0.9111],\n",
      "        [0.9205],\n",
      "        [0.5286],\n",
      "        [0.3106],\n",
      "        [0.3276],\n",
      "        [0.9314],\n",
      "        [0.6907],\n",
      "        [0.8248],\n",
      "        [0.5017],\n",
      "        [0.7241],\n",
      "        [0.6194],\n",
      "        [0.6127],\n",
      "        [0.8915],\n",
      "        [0.3338],\n",
      "        [0.5644],\n",
      "        [0.8256],\n",
      "        [0.2758],\n",
      "        [0.8786],\n",
      "        [0.5885],\n",
      "        [0.5010],\n",
      "        [0.3284],\n",
      "        [0.4960],\n",
      "        [0.4958],\n",
      "        [0.5795],\n",
      "        [0.5868],\n",
      "        [0.9156],\n",
      "        [0.8032],\n",
      "        [0.6079],\n",
      "        [0.6433],\n",
      "        [0.6020],\n",
      "        [0.3235],\n",
      "        [0.8900],\n",
      "        [0.8136],\n",
      "        [0.8382],\n",
      "        [0.8299],\n",
      "        [0.8508],\n",
      "        [0.5986],\n",
      "        [0.6672],\n",
      "        [0.6331],\n",
      "        [0.8443],\n",
      "        [0.8029],\n",
      "        [0.6077],\n",
      "        [0.6141],\n",
      "        [0.6746],\n",
      "        [0.7162],\n",
      "        [0.5851],\n",
      "        [0.4074],\n",
      "        [0.7895],\n",
      "        [0.6112],\n",
      "        [0.6604],\n",
      "        [0.8756],\n",
      "        [0.5603],\n",
      "        [0.6710],\n",
      "        [0.6812],\n",
      "        [0.5941],\n",
      "        [0.4209],\n",
      "        [0.6071],\n",
      "        [0.9147],\n",
      "        [0.6052],\n",
      "        [0.6251],\n",
      "        [0.5689],\n",
      "        [0.9087],\n",
      "        [0.6162],\n",
      "        [0.8627],\n",
      "        [0.6780],\n",
      "        [0.5593],\n",
      "        [0.6752],\n",
      "        [0.3772],\n",
      "        [0.4045],\n",
      "        [0.7782],\n",
      "        [0.3901],\n",
      "        [0.7977],\n",
      "        [0.6485],\n",
      "        [0.6215],\n",
      "        [0.6222],\n",
      "        [0.6375],\n",
      "        [0.9176],\n",
      "        [0.5680],\n",
      "        [0.8192],\n",
      "        [0.7374],\n",
      "        [0.8810],\n",
      "        [0.6213],\n",
      "        [0.8393],\n",
      "        [0.8861],\n",
      "        [0.3572],\n",
      "        [0.8455],\n",
      "        [0.4217],\n",
      "        [0.8576],\n",
      "        [0.7206],\n",
      "        [0.8514],\n",
      "        [0.5793],\n",
      "        [0.8441],\n",
      "        [0.9179],\n",
      "        [0.4140],\n",
      "        [0.7498],\n",
      "        [0.7886],\n",
      "        [0.8352],\n",
      "        [0.8147],\n",
      "        [0.7899],\n",
      "        [0.4155],\n",
      "        [0.8601],\n",
      "        [0.8425],\n",
      "        [0.4139]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0008],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0025],\n",
      "        [    0.0027],\n",
      "        [    0.0029],\n",
      "        [    0.0035],\n",
      "        [    0.0041],\n",
      "        [    0.0041],\n",
      "        [    0.0042],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0054],\n",
      "        [    0.0057],\n",
      "        [    0.0058],\n",
      "        [    0.0059],\n",
      "        [    0.0064],\n",
      "        [    0.0064],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0082],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0088],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0106],\n",
      "        [    0.0107],\n",
      "        [    0.0109],\n",
      "        [    0.0110],\n",
      "        [    0.0114],\n",
      "        [    0.0126],\n",
      "        [    0.0131],\n",
      "        [    0.0138],\n",
      "        [    0.0140],\n",
      "        [    0.0146],\n",
      "        [    0.0156],\n",
      "        [    0.0157],\n",
      "        [    0.0163],\n",
      "        [    0.0163],\n",
      "        [    0.0166],\n",
      "        [    0.0167],\n",
      "        [    0.0173],\n",
      "        [    0.0176],\n",
      "        [    0.0185],\n",
      "        [    0.0186],\n",
      "        [    0.0194],\n",
      "        [    0.0196],\n",
      "        [    0.0198],\n",
      "        [    0.0207],\n",
      "        [    0.0211],\n",
      "        [    0.0212],\n",
      "        [    0.0213],\n",
      "        [    0.0222],\n",
      "        [    0.0236],\n",
      "        [    0.0238],\n",
      "        [    0.0241],\n",
      "        [    0.0241],\n",
      "        [    0.0263],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0274],\n",
      "        [    0.0281],\n",
      "        [    0.0301],\n",
      "        [    0.0301],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0313],\n",
      "        [    0.0319],\n",
      "        [    0.0322],\n",
      "        [    0.0325],\n",
      "        [    0.0332],\n",
      "        [    0.0334],\n",
      "        [    0.0344],\n",
      "        [    0.0353],\n",
      "        [    0.0353],\n",
      "        [    0.0356],\n",
      "        [    0.0361],\n",
      "        [    0.0363],\n",
      "        [    0.0364],\n",
      "        [    0.0374],\n",
      "        [    0.0385],\n",
      "        [    0.0389],\n",
      "        [    0.0391],\n",
      "        [    0.0396],\n",
      "        [    0.0397],\n",
      "        [    0.0412],\n",
      "        [    0.0421],\n",
      "        [    0.0424],\n",
      "        [    0.0427],\n",
      "        [    0.0437],\n",
      "        [    0.0440],\n",
      "        [    0.0443],\n",
      "        [    0.0446],\n",
      "        [    0.0460],\n",
      "        [    0.0469],\n",
      "        [    0.0474],\n",
      "        [    0.0477],\n",
      "        [    0.0499],\n",
      "        [    0.0510],\n",
      "        [    0.0521],\n",
      "        [    0.0523],\n",
      "        [    0.0538],\n",
      "        [    0.0544],\n",
      "        [    0.0549],\n",
      "        [    0.0554],\n",
      "        [    0.0555],\n",
      "        [    0.0573],\n",
      "        [    0.0575],\n",
      "        [    0.0578],\n",
      "        [    0.0591],\n",
      "        [    0.0593],\n",
      "        [    0.0601],\n",
      "        [    0.0618],\n",
      "        [    0.0621],\n",
      "        [    0.0658],\n",
      "        [    0.0676],\n",
      "        [    0.0677],\n",
      "        [    0.0685],\n",
      "        [    0.0703],\n",
      "        [    0.0722],\n",
      "        [    0.0727],\n",
      "        [    0.0730],\n",
      "        [    0.0743],\n",
      "        [    0.0745],\n",
      "        [    0.0753],\n",
      "        [    0.0810],\n",
      "        [    0.0821],\n",
      "        [    0.0898],\n",
      "        [    0.0899],\n",
      "        [    0.0953],\n",
      "        [    0.0977],\n",
      "        [    0.1063],\n",
      "        [    0.1080],\n",
      "        [    0.1224],\n",
      "        [    0.1244],\n",
      "        [    0.1250],\n",
      "        [    0.1272],\n",
      "        [    0.1284],\n",
      "        [    0.1301],\n",
      "        [    0.1314],\n",
      "        [    0.1383],\n",
      "        [    0.1407],\n",
      "        [    0.1505],\n",
      "        [    0.1521],\n",
      "        [    0.1575],\n",
      "        [    0.1865]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0040],\n",
      "        [    0.0065],\n",
      "        [    0.0051],\n",
      "        [    0.0047],\n",
      "        [    0.0000],\n",
      "        [    0.0071],\n",
      "        [    0.0044],\n",
      "        [    0.0007],\n",
      "        [    0.0081],\n",
      "        [    0.0035],\n",
      "        [    0.0081],\n",
      "        [    0.0036],\n",
      "        [    0.0009],\n",
      "        [    0.0101],\n",
      "        [    0.0089],\n",
      "        [    0.0100],\n",
      "        [    0.0095],\n",
      "        [    0.0075],\n",
      "        [    0.0029],\n",
      "        [    0.0115],\n",
      "        [    0.0075],\n",
      "        [    0.0006],\n",
      "        [    0.0009],\n",
      "        [    0.0124],\n",
      "        [    0.0077],\n",
      "        [    0.0014],\n",
      "        [    0.0049],\n",
      "        [    0.0154],\n",
      "        [    0.0045],\n",
      "        [    0.0140],\n",
      "        [    0.0126],\n",
      "        [    0.0094],\n",
      "        [    0.0061],\n",
      "        [    0.0068],\n",
      "        [    0.0130],\n",
      "        [    0.0147],\n",
      "        [    0.0130],\n",
      "        [    0.0165],\n",
      "        [    0.0041],\n",
      "        [    0.0123],\n",
      "        [    0.0190],\n",
      "        [    0.0077],\n",
      "        [    0.0195],\n",
      "        [    0.0095],\n",
      "        [    0.0110],\n",
      "        [    0.0176],\n",
      "        [    0.0099],\n",
      "        [    0.0208],\n",
      "        [    0.0113],\n",
      "        [    0.0107],\n",
      "        [    0.0112],\n",
      "        [    0.0168],\n",
      "        [    0.0142],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0161],\n",
      "        [    0.0239],\n",
      "        [    0.0243],\n",
      "        [    0.0180],\n",
      "        [    0.0147],\n",
      "        [    0.0232],\n",
      "        [    0.0171],\n",
      "        [    0.0187],\n",
      "        [    0.0181],\n",
      "        [    0.0295],\n",
      "        [    0.0259],\n",
      "        [    0.0193],\n",
      "        [    0.0225],\n",
      "        [    0.0267],\n",
      "        [    0.0226],\n",
      "        [    0.0266],\n",
      "        [    0.0238],\n",
      "        [    0.0343],\n",
      "        [    0.0248],\n",
      "        [    0.0248],\n",
      "        [    0.0277],\n",
      "        [    0.0255],\n",
      "        [    0.0259],\n",
      "        [    0.0343],\n",
      "        [    0.0315],\n",
      "        [    0.0278],\n",
      "        [    0.0280],\n",
      "        [    0.0285],\n",
      "        [    0.0299],\n",
      "        [    0.0366],\n",
      "        [    0.0332],\n",
      "        [    0.0351],\n",
      "        [    0.0373],\n",
      "        [    0.0394],\n",
      "        [    0.0318],\n",
      "        [    0.0326],\n",
      "        [    0.0326],\n",
      "        [    0.0380],\n",
      "        [    0.0369],\n",
      "        [    0.0344],\n",
      "        [    0.0374],\n",
      "        [    0.0355],\n",
      "        [    0.0377],\n",
      "        [    0.0387],\n",
      "        [    0.0383],\n",
      "        [    0.0467],\n",
      "        [    0.0496],\n",
      "        [    0.0398],\n",
      "        [    0.0460],\n",
      "        [    0.0432],\n",
      "        [    0.0525],\n",
      "        [    0.0519],\n",
      "        [    0.0458],\n",
      "        [    0.0472],\n",
      "        [    0.0469],\n",
      "        [    0.0492],\n",
      "        [    0.0488],\n",
      "        [    0.0490],\n",
      "        [    0.0480],\n",
      "        [    0.0565],\n",
      "        [    0.0501],\n",
      "        [    0.0569],\n",
      "        [    0.0514],\n",
      "        [    0.0515],\n",
      "        [    0.0529],\n",
      "        [    0.0541],\n",
      "        [    0.0564],\n",
      "        [    0.0596],\n",
      "        [    0.0570],\n",
      "        [    0.0630],\n",
      "        [    0.0628],\n",
      "        [    0.0614],\n",
      "        [    0.0623],\n",
      "        [    0.0657],\n",
      "        [    0.0739],\n",
      "        [    0.0785],\n",
      "        [    0.0757],\n",
      "        [    0.0789],\n",
      "        [    0.0760],\n",
      "        [    0.0694],\n",
      "        [    0.0801],\n",
      "        [    0.0786],\n",
      "        [    0.0941],\n",
      "        [    0.0896],\n",
      "        [    0.0988],\n",
      "        [    0.0962],\n",
      "        [    0.1107],\n",
      "        [    0.1069],\n",
      "        [    0.1284],\n",
      "        [    0.1213],\n",
      "        [    0.1268],\n",
      "        [    0.1310],\n",
      "        [    0.1329],\n",
      "        [    0.1325],\n",
      "        [    0.1337],\n",
      "        [    0.1352],\n",
      "        [    0.1433],\n",
      "        [    0.1547],\n",
      "        [    0.1524],\n",
      "        [    0.1543],\n",
      "        [    0.1903]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.95235276222229\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.3096723705530167e-10, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [6, 76, 35, 100, 131, 154, 10, 30, 0, 136, 103, 12, 130, 79, 54, 118, 124, 102, 126, 77, 31, 9, 80, 43, 116, 117, 147, 26, 132, 57, 90, 110, 70, 91, 139, 115, 151, 111, 46, 65, 125, 11, 13, 78, 129, 135, 69, 104, 127, 109, 28, 133, 48, 23, 114, 55, 71, 150, 155, 34, 24, 149, 123, 146, 7, 119, 145, 148, 101, 153, 108, 33, 107, 36, 29, 128, 82, 112, 92, 94, 152, 45, 93, 89, 72, 8, 134, 32, 81, 44, 66, 2, 16, 47, 63, 56, 42, 156, 121, 14, 75, 122, 120, 27, 53, 83, 137, 105, 84, 85, 22, 95, 113, 67, 106, 62, 64, 68, 158, 138, 1, 37, 157, 15, 88, 87, 73, 17, 74, 86, 3, 52, 5, 99, 21, 58, 41, 38, 144, 40, 140, 39, 61, 20, 4, 98, 141, 51, 60, 49, 18, 50, 25, 19, 143, 142, 96] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8315],\n",
      "        [0.6945],\n",
      "        [0.9089],\n",
      "        [0.5104],\n",
      "        [0.5349],\n",
      "        [0.3620],\n",
      "        [0.8467],\n",
      "        [0.8927],\n",
      "        [0.9195],\n",
      "        [0.4779],\n",
      "        [0.6064],\n",
      "        [0.8138],\n",
      "        [0.5321],\n",
      "        [0.6645],\n",
      "        [0.7557],\n",
      "        [0.6121],\n",
      "        [0.5528],\n",
      "        [0.5579],\n",
      "        [0.5491],\n",
      "        [0.6907],\n",
      "        [0.9053],\n",
      "        [0.8371],\n",
      "        [0.6277],\n",
      "        [0.8693],\n",
      "        [0.6414],\n",
      "        [0.6367],\n",
      "        [0.3281],\n",
      "        [0.8527],\n",
      "        [0.5234],\n",
      "        [0.7313],\n",
      "        [0.6317],\n",
      "        [0.5907],\n",
      "        [0.7135],\n",
      "        [0.6347],\n",
      "        [0.4295],\n",
      "        [0.6103],\n",
      "        [0.3467],\n",
      "        [0.6420],\n",
      "        [0.8478],\n",
      "        [0.6735],\n",
      "        [0.5737],\n",
      "        [0.8173],\n",
      "        [0.8330],\n",
      "        [0.6912],\n",
      "        [0.5263],\n",
      "        [0.5019],\n",
      "        [0.6970],\n",
      "        [0.6005],\n",
      "        [0.5318],\n",
      "        [0.5803],\n",
      "        [0.8631],\n",
      "        [0.5060],\n",
      "        [0.8343],\n",
      "        [0.9342],\n",
      "        [0.6248],\n",
      "        [0.7276],\n",
      "        [0.7104],\n",
      "        [0.3386],\n",
      "        [0.4106],\n",
      "        [0.9126],\n",
      "        [0.9220],\n",
      "        [0.3279],\n",
      "        [0.5682],\n",
      "        [0.2801],\n",
      "        [0.8268],\n",
      "        [0.5928],\n",
      "        [0.3148],\n",
      "        [0.3321],\n",
      "        [0.5018],\n",
      "        [0.3338],\n",
      "        [0.5852],\n",
      "        [0.8933],\n",
      "        [0.5928],\n",
      "        [0.8793],\n",
      "        [0.8257],\n",
      "        [0.4987],\n",
      "        [0.6133],\n",
      "        [0.6487],\n",
      "        [0.6079],\n",
      "        [0.6183],\n",
      "        [0.3288],\n",
      "        [0.8042],\n",
      "        [0.6042],\n",
      "        [0.6395],\n",
      "        [0.6730],\n",
      "        [0.8160],\n",
      "        [0.5052],\n",
      "        [0.9178],\n",
      "        [0.6130],\n",
      "        [0.8392],\n",
      "        [0.6813],\n",
      "        [0.8914],\n",
      "        [0.8056],\n",
      "        [0.8309],\n",
      "        [0.6179],\n",
      "        [0.7209],\n",
      "        [0.8454],\n",
      "        [0.4128],\n",
      "        [0.5891],\n",
      "        [0.8538],\n",
      "        [0.6652],\n",
      "        [0.5640],\n",
      "        [0.5982],\n",
      "        [0.8757],\n",
      "        [0.7922],\n",
      "        [0.6122],\n",
      "        [0.4247],\n",
      "        [0.5759],\n",
      "        [0.6102],\n",
      "        [0.6304],\n",
      "        [0.9177],\n",
      "        [0.6165],\n",
      "        [0.6216],\n",
      "        [0.6841],\n",
      "        [0.5655],\n",
      "        [0.6854],\n",
      "        [0.6761],\n",
      "        [0.6813],\n",
      "        [0.3824],\n",
      "        [0.4082],\n",
      "        [0.9098],\n",
      "        [0.8631],\n",
      "        [0.3952],\n",
      "        [0.7805],\n",
      "        [0.6279],\n",
      "        [0.6284],\n",
      "        [0.6533],\n",
      "        [0.8005],\n",
      "        [0.6421],\n",
      "        [0.6272],\n",
      "        [0.9193],\n",
      "        [0.8219],\n",
      "        [0.8826],\n",
      "        [0.5738],\n",
      "        [0.8896],\n",
      "        [0.7420],\n",
      "        [0.8402],\n",
      "        [0.8458],\n",
      "        [0.3615],\n",
      "        [0.8590],\n",
      "        [0.4252],\n",
      "        [0.8525],\n",
      "        [0.7250],\n",
      "        [0.8472],\n",
      "        [0.9197],\n",
      "        [0.5853],\n",
      "        [0.4178],\n",
      "        [0.7910],\n",
      "        [0.7543],\n",
      "        [0.8376],\n",
      "        [0.8178],\n",
      "        [0.7925],\n",
      "        [0.8603],\n",
      "        [0.8457],\n",
      "        [0.4196],\n",
      "        [0.4177],\n",
      "        [0.6269]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0009],\n",
      "        [    0.0014],\n",
      "        [    0.0029],\n",
      "        [    0.0035],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0044],\n",
      "        [    0.0045],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0061],\n",
      "        [    0.0065],\n",
      "        [    0.0068],\n",
      "        [    0.0071],\n",
      "        [    0.0075],\n",
      "        [    0.0075],\n",
      "        [    0.0077],\n",
      "        [    0.0077],\n",
      "        [    0.0081],\n",
      "        [    0.0081],\n",
      "        [    0.0089],\n",
      "        [    0.0094],\n",
      "        [    0.0095],\n",
      "        [    0.0095],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0107],\n",
      "        [    0.0110],\n",
      "        [    0.0112],\n",
      "        [    0.0113],\n",
      "        [    0.0115],\n",
      "        [    0.0123],\n",
      "        [    0.0124],\n",
      "        [    0.0126],\n",
      "        [    0.0130],\n",
      "        [    0.0130],\n",
      "        [    0.0140],\n",
      "        [    0.0142],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0154],\n",
      "        [    0.0161],\n",
      "        [    0.0165],\n",
      "        [    0.0168],\n",
      "        [    0.0171],\n",
      "        [    0.0176],\n",
      "        [    0.0180],\n",
      "        [    0.0181],\n",
      "        [    0.0187],\n",
      "        [    0.0190],\n",
      "        [    0.0193],\n",
      "        [    0.0195],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0208],\n",
      "        [    0.0225],\n",
      "        [    0.0226],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0239],\n",
      "        [    0.0243],\n",
      "        [    0.0248],\n",
      "        [    0.0248],\n",
      "        [    0.0255],\n",
      "        [    0.0259],\n",
      "        [    0.0259],\n",
      "        [    0.0266],\n",
      "        [    0.0267],\n",
      "        [    0.0277],\n",
      "        [    0.0278],\n",
      "        [    0.0280],\n",
      "        [    0.0285],\n",
      "        [    0.0295],\n",
      "        [    0.0299],\n",
      "        [    0.0315],\n",
      "        [    0.0318],\n",
      "        [    0.0326],\n",
      "        [    0.0326],\n",
      "        [    0.0332],\n",
      "        [    0.0343],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0351],\n",
      "        [    0.0355],\n",
      "        [    0.0366],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0374],\n",
      "        [    0.0377],\n",
      "        [    0.0380],\n",
      "        [    0.0383],\n",
      "        [    0.0387],\n",
      "        [    0.0394],\n",
      "        [    0.0398],\n",
      "        [    0.0432],\n",
      "        [    0.0458],\n",
      "        [    0.0460],\n",
      "        [    0.0467],\n",
      "        [    0.0469],\n",
      "        [    0.0472],\n",
      "        [    0.0480],\n",
      "        [    0.0488],\n",
      "        [    0.0490],\n",
      "        [    0.0492],\n",
      "        [    0.0496],\n",
      "        [    0.0501],\n",
      "        [    0.0514],\n",
      "        [    0.0515],\n",
      "        [    0.0519],\n",
      "        [    0.0525],\n",
      "        [    0.0529],\n",
      "        [    0.0541],\n",
      "        [    0.0564],\n",
      "        [    0.0565],\n",
      "        [    0.0569],\n",
      "        [    0.0570],\n",
      "        [    0.0596],\n",
      "        [    0.0614],\n",
      "        [    0.0623],\n",
      "        [    0.0628],\n",
      "        [    0.0630],\n",
      "        [    0.0657],\n",
      "        [    0.0694],\n",
      "        [    0.0739],\n",
      "        [    0.0757],\n",
      "        [    0.0760],\n",
      "        [    0.0785],\n",
      "        [    0.0786],\n",
      "        [    0.0789],\n",
      "        [    0.0801],\n",
      "        [    0.0896],\n",
      "        [    0.0941],\n",
      "        [    0.0962],\n",
      "        [    0.0988],\n",
      "        [    0.1069],\n",
      "        [    0.1107],\n",
      "        [    0.1213],\n",
      "        [    0.1268],\n",
      "        [    0.1284],\n",
      "        [    0.1310],\n",
      "        [    0.1325],\n",
      "        [    0.1329],\n",
      "        [    0.1337],\n",
      "        [    0.1352],\n",
      "        [    0.1433],\n",
      "        [    0.1524],\n",
      "        [    0.1543],\n",
      "        [    0.1547],\n",
      "        [    0.1903],\n",
      "        [    0.2064]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0014],\n",
      "        [    0.0002],\n",
      "        [    0.0014],\n",
      "        [    0.0018],\n",
      "        [    0.0036],\n",
      "        [    0.0032],\n",
      "        [    0.0024],\n",
      "        [    0.0026],\n",
      "        [    0.0052],\n",
      "        [    0.0057],\n",
      "        [    0.0050],\n",
      "        [    0.0046],\n",
      "        [    0.0052],\n",
      "        [    0.0074],\n",
      "        [    0.0070],\n",
      "        [    0.0082],\n",
      "        [    0.0077],\n",
      "        [    0.0083],\n",
      "        [    0.0083],\n",
      "        [    0.0077],\n",
      "        [    0.0074],\n",
      "        [    0.0090],\n",
      "        [    0.0088],\n",
      "        [    0.0079],\n",
      "        [    0.0084],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0097],\n",
      "        [    0.0104],\n",
      "        [    0.0111],\n",
      "        [    0.0104],\n",
      "        [    0.0130],\n",
      "        [    0.0108],\n",
      "        [    0.0120],\n",
      "        [    0.0122],\n",
      "        [    0.0112],\n",
      "        [    0.0132],\n",
      "        [    0.0116],\n",
      "        [    0.0138],\n",
      "        [    0.0141],\n",
      "        [    0.0146],\n",
      "        [    0.0159],\n",
      "        [    0.0135],\n",
      "        [    0.0136],\n",
      "        [    0.0169],\n",
      "        [    0.0179],\n",
      "        [    0.0168],\n",
      "        [    0.0159],\n",
      "        [    0.0182],\n",
      "        [    0.0173],\n",
      "        [    0.0157],\n",
      "        [    0.0176],\n",
      "        [    0.0187],\n",
      "        [    0.0202],\n",
      "        [    0.0202],\n",
      "        [    0.0199],\n",
      "        [    0.0209],\n",
      "        [    0.0214],\n",
      "        [    0.0200],\n",
      "        [    0.0236],\n",
      "        [    0.0245],\n",
      "        [    0.0233],\n",
      "        [    0.0243],\n",
      "        [    0.0219],\n",
      "        [    0.0233],\n",
      "        [    0.0250],\n",
      "        [    0.0253],\n",
      "        [    0.0251],\n",
      "        [    0.0269],\n",
      "        [    0.0252],\n",
      "        [    0.0266],\n",
      "        [    0.0276],\n",
      "        [    0.0299],\n",
      "        [    0.0279],\n",
      "        [    0.0275],\n",
      "        [    0.0285],\n",
      "        [    0.0295],\n",
      "        [    0.0306],\n",
      "        [    0.0326],\n",
      "        [    0.0320],\n",
      "        [    0.0318],\n",
      "        [    0.0321],\n",
      "        [    0.0326],\n",
      "        [    0.0331],\n",
      "        [    0.0356],\n",
      "        [    0.0345],\n",
      "        [    0.0357],\n",
      "        [    0.0343],\n",
      "        [    0.0366],\n",
      "        [    0.0357],\n",
      "        [    0.0362],\n",
      "        [    0.0387],\n",
      "        [    0.0369],\n",
      "        [    0.0384],\n",
      "        [    0.0381],\n",
      "        [    0.0395],\n",
      "        [    0.0409],\n",
      "        [    0.0399],\n",
      "        [    0.0444],\n",
      "        [    0.0465],\n",
      "        [    0.0455],\n",
      "        [    0.0469],\n",
      "        [    0.0473],\n",
      "        [    0.0492],\n",
      "        [    0.0467],\n",
      "        [    0.0492],\n",
      "        [    0.0490],\n",
      "        [    0.0469],\n",
      "        [    0.0494],\n",
      "        [    0.0496],\n",
      "        [    0.0506],\n",
      "        [    0.0509],\n",
      "        [    0.0518],\n",
      "        [    0.0527],\n",
      "        [    0.0520],\n",
      "        [    0.0544],\n",
      "        [    0.0585],\n",
      "        [    0.0562],\n",
      "        [    0.0574],\n",
      "        [    0.0573],\n",
      "        [    0.0591],\n",
      "        [    0.0606],\n",
      "        [    0.0616],\n",
      "        [    0.0632],\n",
      "        [    0.0618],\n",
      "        [    0.0662],\n",
      "        [    0.0690],\n",
      "        [    0.0744],\n",
      "        [    0.0761],\n",
      "        [    0.0761],\n",
      "        [    0.0787],\n",
      "        [    0.0761],\n",
      "        [    0.0798],\n",
      "        [    0.0806],\n",
      "        [    0.0905],\n",
      "        [    0.0923],\n",
      "        [    0.0963],\n",
      "        [    0.0968],\n",
      "        [    0.1072],\n",
      "        [    0.1109],\n",
      "        [    0.1195],\n",
      "        [    0.1274],\n",
      "        [    0.1287],\n",
      "        [    0.1292],\n",
      "        [    0.1324],\n",
      "        [    0.1336],\n",
      "        [    0.1338],\n",
      "        [    0.1335],\n",
      "        [    0.1433],\n",
      "        [    0.1521],\n",
      "        [    0.1524],\n",
      "        [    0.1532],\n",
      "        [    0.1885],\n",
      "        [    0.2063]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 46.243502140045166\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.8969147081170377e-09, 35)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [35, 76, 131, 6, 154, 100, 10, 136, 103, 0, 30, 54, 79, 12, 118, 130, 102, 124, 43, 80, 77, 147, 126, 9, 31, 26, 117, 116, 90, 132, 57, 110, 91, 115, 70, 46, 125, 151, 111, 139, 65, 135, 69, 11, 13, 78, 23, 28, 129, 109, 104, 48, 114, 127, 133, 55, 155, 149, 150, 71, 34, 24, 145, 148, 7, 123, 119, 146, 101, 108, 107, 153, 36, 33, 112, 29, 82, 92, 94, 128, 152, 89, 93, 72, 8, 45, 134, 66, 81, 32, 16, 44, 47, 2, 56, 156, 42, 63, 121, 75, 14, 122, 27, 120, 105, 53, 22, 83, 85, 84, 137, 95, 113, 67, 106, 62, 68, 64, 158, 1, 157, 37, 138, 15, 88, 87, 17, 73, 74, 86, 3, 5, 52, 21, 99, 58, 41, 38, 144, 40, 140, 39, 61, 20, 4, 98, 141, 51, 18, 60, 49, 50, 25, 19, 143, 142, 96, 97] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9097],\n",
      "        [0.6950],\n",
      "        [0.5342],\n",
      "        [0.8312],\n",
      "        [0.3621],\n",
      "        [0.5099],\n",
      "        [0.8478],\n",
      "        [0.4763],\n",
      "        [0.6078],\n",
      "        [0.9191],\n",
      "        [0.8928],\n",
      "        [0.7560],\n",
      "        [0.6648],\n",
      "        [0.8146],\n",
      "        [0.6122],\n",
      "        [0.5310],\n",
      "        [0.5584],\n",
      "        [0.5516],\n",
      "        [0.8691],\n",
      "        [0.6277],\n",
      "        [0.6913],\n",
      "        [0.3271],\n",
      "        [0.5476],\n",
      "        [0.8379],\n",
      "        [0.9062],\n",
      "        [0.8517],\n",
      "        [0.6374],\n",
      "        [0.6424],\n",
      "        [0.6319],\n",
      "        [0.5227],\n",
      "        [0.7321],\n",
      "        [0.5911],\n",
      "        [0.6349],\n",
      "        [0.6108],\n",
      "        [0.7145],\n",
      "        [0.8467],\n",
      "        [0.5727],\n",
      "        [0.3460],\n",
      "        [0.6427],\n",
      "        [0.4274],\n",
      "        [0.6742],\n",
      "        [0.5007],\n",
      "        [0.6981],\n",
      "        [0.8181],\n",
      "        [0.8340],\n",
      "        [0.6918],\n",
      "        [0.9364],\n",
      "        [0.8621],\n",
      "        [0.5247],\n",
      "        [0.5805],\n",
      "        [0.6020],\n",
      "        [0.8339],\n",
      "        [0.6253],\n",
      "        [0.5301],\n",
      "        [0.5048],\n",
      "        [0.7276],\n",
      "        [0.4110],\n",
      "        [0.3271],\n",
      "        [0.3377],\n",
      "        [0.7116],\n",
      "        [0.9135],\n",
      "        [0.9233],\n",
      "        [0.3128],\n",
      "        [0.3311],\n",
      "        [0.8270],\n",
      "        [0.5670],\n",
      "        [0.5923],\n",
      "        [0.2782],\n",
      "        [0.5015],\n",
      "        [0.5857],\n",
      "        [0.5935],\n",
      "        [0.3333],\n",
      "        [0.8793],\n",
      "        [0.8943],\n",
      "        [0.6493],\n",
      "        [0.8248],\n",
      "        [0.6133],\n",
      "        [0.6079],\n",
      "        [0.6183],\n",
      "        [0.4965],\n",
      "        [0.3281],\n",
      "        [0.6402],\n",
      "        [0.6040],\n",
      "        [0.6735],\n",
      "        [0.8166],\n",
      "        [0.8030],\n",
      "        [0.5040],\n",
      "        [0.6825],\n",
      "        [0.6129],\n",
      "        [0.9191],\n",
      "        [0.8068],\n",
      "        [0.8386],\n",
      "        [0.8299],\n",
      "        [0.8913],\n",
      "        [0.7217],\n",
      "        [0.4129],\n",
      "        [0.8450],\n",
      "        [0.6166],\n",
      "        [0.5883],\n",
      "        [0.6651],\n",
      "        [0.8553],\n",
      "        [0.5629],\n",
      "        [0.8752],\n",
      "        [0.5975],\n",
      "        [0.5772],\n",
      "        [0.7924],\n",
      "        [0.9200],\n",
      "        [0.6119],\n",
      "        [0.6304],\n",
      "        [0.6098],\n",
      "        [0.4227],\n",
      "        [0.6163],\n",
      "        [0.6221],\n",
      "        [0.6848],\n",
      "        [0.5662],\n",
      "        [0.6852],\n",
      "        [0.6822],\n",
      "        [0.6763],\n",
      "        [0.3822],\n",
      "        [0.9096],\n",
      "        [0.3948],\n",
      "        [0.8626],\n",
      "        [0.4060],\n",
      "        [0.7810],\n",
      "        [0.6286],\n",
      "        [0.6291],\n",
      "        [0.8017],\n",
      "        [0.6529],\n",
      "        [0.6416],\n",
      "        [0.6276],\n",
      "        [0.9198],\n",
      "        [0.8826],\n",
      "        [0.8222],\n",
      "        [0.8921],\n",
      "        [0.5740],\n",
      "        [0.7428],\n",
      "        [0.8397],\n",
      "        [0.8450],\n",
      "        [0.3598],\n",
      "        [0.8590],\n",
      "        [0.4231],\n",
      "        [0.8522],\n",
      "        [0.7252],\n",
      "        [0.8490],\n",
      "        [0.9202],\n",
      "        [0.5856],\n",
      "        [0.4159],\n",
      "        [0.7909],\n",
      "        [0.8195],\n",
      "        [0.7549],\n",
      "        [0.8376],\n",
      "        [0.7926],\n",
      "        [0.8600],\n",
      "        [0.8476],\n",
      "        [0.4182],\n",
      "        [0.4159],\n",
      "        [0.6268],\n",
      "        [0.6417]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0014],\n",
      "        [    0.0014],\n",
      "        [    0.0018],\n",
      "        [    0.0024],\n",
      "        [    0.0026],\n",
      "        [    0.0032],\n",
      "        [    0.0036],\n",
      "        [    0.0046],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0052],\n",
      "        [    0.0057],\n",
      "        [    0.0070],\n",
      "        [    0.0074],\n",
      "        [    0.0074],\n",
      "        [    0.0077],\n",
      "        [    0.0077],\n",
      "        [    0.0079],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0083],\n",
      "        [    0.0084],\n",
      "        [    0.0088],\n",
      "        [    0.0090],\n",
      "        [    0.0097],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0104],\n",
      "        [    0.0104],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0112],\n",
      "        [    0.0116],\n",
      "        [    0.0120],\n",
      "        [    0.0122],\n",
      "        [    0.0130],\n",
      "        [    0.0132],\n",
      "        [    0.0135],\n",
      "        [    0.0136],\n",
      "        [    0.0138],\n",
      "        [    0.0141],\n",
      "        [    0.0146],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0159],\n",
      "        [    0.0168],\n",
      "        [    0.0169],\n",
      "        [    0.0173],\n",
      "        [    0.0176],\n",
      "        [    0.0179],\n",
      "        [    0.0182],\n",
      "        [    0.0187],\n",
      "        [    0.0199],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0202],\n",
      "        [    0.0209],\n",
      "        [    0.0214],\n",
      "        [    0.0219],\n",
      "        [    0.0233],\n",
      "        [    0.0233],\n",
      "        [    0.0236],\n",
      "        [    0.0243],\n",
      "        [    0.0245],\n",
      "        [    0.0250],\n",
      "        [    0.0251],\n",
      "        [    0.0252],\n",
      "        [    0.0253],\n",
      "        [    0.0266],\n",
      "        [    0.0269],\n",
      "        [    0.0275],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0285],\n",
      "        [    0.0295],\n",
      "        [    0.0299],\n",
      "        [    0.0306],\n",
      "        [    0.0318],\n",
      "        [    0.0320],\n",
      "        [    0.0321],\n",
      "        [    0.0326],\n",
      "        [    0.0326],\n",
      "        [    0.0331],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0356],\n",
      "        [    0.0357],\n",
      "        [    0.0357],\n",
      "        [    0.0362],\n",
      "        [    0.0366],\n",
      "        [    0.0369],\n",
      "        [    0.0381],\n",
      "        [    0.0384],\n",
      "        [    0.0387],\n",
      "        [    0.0395],\n",
      "        [    0.0399],\n",
      "        [    0.0409],\n",
      "        [    0.0444],\n",
      "        [    0.0455],\n",
      "        [    0.0465],\n",
      "        [    0.0467],\n",
      "        [    0.0469],\n",
      "        [    0.0469],\n",
      "        [    0.0473],\n",
      "        [    0.0490],\n",
      "        [    0.0492],\n",
      "        [    0.0492],\n",
      "        [    0.0494],\n",
      "        [    0.0496],\n",
      "        [    0.0506],\n",
      "        [    0.0509],\n",
      "        [    0.0518],\n",
      "        [    0.0520],\n",
      "        [    0.0527],\n",
      "        [    0.0544],\n",
      "        [    0.0562],\n",
      "        [    0.0573],\n",
      "        [    0.0574],\n",
      "        [    0.0585],\n",
      "        [    0.0591],\n",
      "        [    0.0606],\n",
      "        [    0.0616],\n",
      "        [    0.0618],\n",
      "        [    0.0632],\n",
      "        [    0.0662],\n",
      "        [    0.0690],\n",
      "        [    0.0744],\n",
      "        [    0.0761],\n",
      "        [    0.0761],\n",
      "        [    0.0761],\n",
      "        [    0.0787],\n",
      "        [    0.0798],\n",
      "        [    0.0806],\n",
      "        [    0.0905],\n",
      "        [    0.0923],\n",
      "        [    0.0963],\n",
      "        [    0.0968],\n",
      "        [    0.1072],\n",
      "        [    0.1109],\n",
      "        [    0.1195],\n",
      "        [    0.1274],\n",
      "        [    0.1287],\n",
      "        [    0.1292],\n",
      "        [    0.1324],\n",
      "        [    0.1335],\n",
      "        [    0.1336],\n",
      "        [    0.1338],\n",
      "        [    0.1433],\n",
      "        [    0.1521],\n",
      "        [    0.1524],\n",
      "        [    0.1532],\n",
      "        [    0.1885],\n",
      "        [    0.2063],\n",
      "        [    0.2180]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0035],\n",
      "        [    0.0018],\n",
      "        [    0.0000],\n",
      "        [    0.0032],\n",
      "        [    0.0019],\n",
      "        [    0.0028],\n",
      "        [    0.0054],\n",
      "        [    0.0033],\n",
      "        [    0.0051],\n",
      "        [    0.0040],\n",
      "        [    0.0077],\n",
      "        [    0.0057],\n",
      "        [    0.0093],\n",
      "        [    0.0063],\n",
      "        [    0.0096],\n",
      "        [    0.0063],\n",
      "        [    0.0067],\n",
      "        [    0.0104],\n",
      "        [    0.0081],\n",
      "        [    0.0080],\n",
      "        [    0.0072],\n",
      "        [    0.0080],\n",
      "        [    0.0084],\n",
      "        [    0.0117],\n",
      "        [    0.0120],\n",
      "        [    0.0096],\n",
      "        [    0.0093],\n",
      "        [    0.0124],\n",
      "        [    0.0117],\n",
      "        [    0.0099],\n",
      "        [    0.0093],\n",
      "        [    0.0098],\n",
      "        [    0.0128],\n",
      "        [    0.0111],\n",
      "        [    0.0155],\n",
      "        [    0.0118],\n",
      "        [    0.0113],\n",
      "        [    0.0148],\n",
      "        [    0.0136],\n",
      "        [    0.0140],\n",
      "        [    0.0131],\n",
      "        [    0.0148],\n",
      "        [    0.0147],\n",
      "        [    0.0181],\n",
      "        [    0.0156],\n",
      "        [    0.0165],\n",
      "        [    0.0159],\n",
      "        [    0.0187],\n",
      "        [    0.0202],\n",
      "        [    0.0202],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0193],\n",
      "        [    0.0210],\n",
      "        [    0.0190],\n",
      "        [    0.0208],\n",
      "        [    0.0217],\n",
      "        [    0.0196],\n",
      "        [    0.0223],\n",
      "        [    0.0226],\n",
      "        [    0.0256],\n",
      "        [    0.0260],\n",
      "        [    0.0263],\n",
      "        [    0.0268],\n",
      "        [    0.0261],\n",
      "        [    0.0261],\n",
      "        [    0.0261],\n",
      "        [    0.0273],\n",
      "        [    0.0267],\n",
      "        [    0.0285],\n",
      "        [    0.0288],\n",
      "        [    0.0298],\n",
      "        [    0.0306],\n",
      "        [    0.0272],\n",
      "        [    0.0324],\n",
      "        [    0.0315],\n",
      "        [    0.0335],\n",
      "        [    0.0344],\n",
      "        [    0.0337],\n",
      "        [    0.0330],\n",
      "        [    0.0344],\n",
      "        [    0.0309],\n",
      "        [    0.0354],\n",
      "        [    0.0364],\n",
      "        [    0.0356],\n",
      "        [    0.0355],\n",
      "        [    0.0371],\n",
      "        [    0.0344],\n",
      "        [    0.0355],\n",
      "        [    0.0378],\n",
      "        [    0.0385],\n",
      "        [    0.0396],\n",
      "        [    0.0413],\n",
      "        [    0.0412],\n",
      "        [    0.0417],\n",
      "        [    0.0410],\n",
      "        [    0.0464],\n",
      "        [    0.0446],\n",
      "        [    0.0482],\n",
      "        [    0.0473],\n",
      "        [    0.0460],\n",
      "        [    0.0460],\n",
      "        [    0.0494],\n",
      "        [    0.0510],\n",
      "        [    0.0513],\n",
      "        [    0.0516],\n",
      "        [    0.0471],\n",
      "        [    0.0508],\n",
      "        [    0.0522],\n",
      "        [    0.0519],\n",
      "        [    0.0501],\n",
      "        [    0.0534],\n",
      "        [    0.0510],\n",
      "        [    0.0551],\n",
      "        [    0.0549],\n",
      "        [    0.0581],\n",
      "        [    0.0585],\n",
      "        [    0.0611],\n",
      "        [    0.0593],\n",
      "        [    0.0622],\n",
      "        [    0.0631],\n",
      "        [    0.0615],\n",
      "        [    0.0652],\n",
      "        [    0.0683],\n",
      "        [    0.0707],\n",
      "        [    0.0738],\n",
      "        [    0.0750],\n",
      "        [    0.0753],\n",
      "        [    0.0751],\n",
      "        [    0.0769],\n",
      "        [    0.0789],\n",
      "        [    0.0818],\n",
      "        [    0.0917],\n",
      "        [    0.0901],\n",
      "        [    0.0973],\n",
      "        [    0.0944],\n",
      "        [    0.1084],\n",
      "        [    0.1095],\n",
      "        [    0.1188],\n",
      "        [    0.1267],\n",
      "        [    0.1269],\n",
      "        [    0.1270],\n",
      "        [    0.1313],\n",
      "        [    0.1328],\n",
      "        [    0.1325],\n",
      "        [    0.1326],\n",
      "        [    0.1424],\n",
      "        [    0.1513],\n",
      "        [    0.1518],\n",
      "        [    0.1512],\n",
      "        [    0.1863],\n",
      "        [    0.2042],\n",
      "        [    0.2162]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 46.534470081329346\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.540590126111965e-10, 136)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [136, 35, 131, 6, 76, 154, 10, 0, 30, 103, 79, 100, 118, 12, 54, 102, 77, 43, 147, 26, 130, 117, 31, 9, 116, 110, 124, 46, 80, 57, 125, 70, 126, 111, 135, 90, 115, 65, 132, 91, 151, 78, 11, 13, 28, 23, 69, 139, 109, 48, 104, 129, 114, 71, 149, 145, 55, 155, 127, 133, 34, 150, 24, 148, 7, 123, 119, 107, 153, 108, 146, 33, 101, 94, 36, 112, 29, 82, 92, 134, 152, 128, 8, 89, 72, 93, 47, 45, 66, 16, 2, 32, 81, 44, 56, 156, 42, 14, 121, 63, 75, 27, 22, 53, 122, 95, 105, 120, 83, 62, 113, 85, 64, 84, 137, 106, 67, 68, 1, 158, 157, 37, 15, 138, 17, 88, 87, 73, 74, 86, 3, 5, 21, 52, 99, 58, 41, 144, 38, 140, 40, 39, 61, 20, 4, 98, 141, 51, 60, 49, 18, 50, 143, 25, 19, 142, 96, 97, 59] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4739],\n",
      "        [0.9093],\n",
      "        [0.5325],\n",
      "        [0.8300],\n",
      "        [0.6935],\n",
      "        [0.3618],\n",
      "        [0.8478],\n",
      "        [0.9178],\n",
      "        [0.8920],\n",
      "        [0.6073],\n",
      "        [0.6630],\n",
      "        [0.5078],\n",
      "        [0.6109],\n",
      "        [0.8145],\n",
      "        [0.7551],\n",
      "        [0.5571],\n",
      "        [0.6898],\n",
      "        [0.8680],\n",
      "        [0.3260],\n",
      "        [0.8504],\n",
      "        [0.5290],\n",
      "        [0.6366],\n",
      "        [0.9058],\n",
      "        [0.8376],\n",
      "        [0.6417],\n",
      "        [0.5899],\n",
      "        [0.5497],\n",
      "        [0.8448],\n",
      "        [0.6257],\n",
      "        [0.7314],\n",
      "        [0.5709],\n",
      "        [0.7132],\n",
      "        [0.5454],\n",
      "        [0.6416],\n",
      "        [0.4986],\n",
      "        [0.6299],\n",
      "        [0.6098],\n",
      "        [0.6728],\n",
      "        [0.5209],\n",
      "        [0.6330],\n",
      "        [0.3452],\n",
      "        [0.6903],\n",
      "        [0.8179],\n",
      "        [0.8340],\n",
      "        [0.8609],\n",
      "        [0.9374],\n",
      "        [0.6969],\n",
      "        [0.4249],\n",
      "        [0.5794],\n",
      "        [0.8325],\n",
      "        [0.6016],\n",
      "        [0.5225],\n",
      "        [0.6243],\n",
      "        [0.7104],\n",
      "        [0.3264],\n",
      "        [0.3105],\n",
      "        [0.7265],\n",
      "        [0.4110],\n",
      "        [0.5278],\n",
      "        [0.5028],\n",
      "        [0.9133],\n",
      "        [0.3369],\n",
      "        [0.9235],\n",
      "        [0.3301],\n",
      "        [0.8262],\n",
      "        [0.5650],\n",
      "        [0.5906],\n",
      "        [0.5926],\n",
      "        [0.3325],\n",
      "        [0.5847],\n",
      "        [0.2764],\n",
      "        [0.8941],\n",
      "        [0.4997],\n",
      "        [0.6161],\n",
      "        [0.8786],\n",
      "        [0.6482],\n",
      "        [0.8236],\n",
      "        [0.6113],\n",
      "        [0.6058],\n",
      "        [0.5019],\n",
      "        [0.3272],\n",
      "        [0.4940],\n",
      "        [0.8162],\n",
      "        [0.6385],\n",
      "        [0.6719],\n",
      "        [0.6016],\n",
      "        [0.8280],\n",
      "        [0.8012],\n",
      "        [0.6814],\n",
      "        [0.8071],\n",
      "        [0.8903],\n",
      "        [0.9191],\n",
      "        [0.6111],\n",
      "        [0.8372],\n",
      "        [0.7208],\n",
      "        [0.4126],\n",
      "        [0.8437],\n",
      "        [0.8555],\n",
      "        [0.5866],\n",
      "        [0.6140],\n",
      "        [0.6634],\n",
      "        [0.8743],\n",
      "        [0.9210],\n",
      "        [0.7915],\n",
      "        [0.5609],\n",
      "        [0.6140],\n",
      "        [0.5766],\n",
      "        [0.5958],\n",
      "        [0.6098],\n",
      "        [0.6836],\n",
      "        [0.6210],\n",
      "        [0.6285],\n",
      "        [0.6746],\n",
      "        [0.6077],\n",
      "        [0.4203],\n",
      "        [0.5652],\n",
      "        [0.6833],\n",
      "        [0.6808],\n",
      "        [0.9083],\n",
      "        [0.3815],\n",
      "        [0.3941],\n",
      "        [0.8615],\n",
      "        [0.7807],\n",
      "        [0.4034],\n",
      "        [0.8021],\n",
      "        [0.6270],\n",
      "        [0.6276],\n",
      "        [0.6509],\n",
      "        [0.6394],\n",
      "        [0.6259],\n",
      "        [0.9192],\n",
      "        [0.8815],\n",
      "        [0.8931],\n",
      "        [0.8214],\n",
      "        [0.5722],\n",
      "        [0.7419],\n",
      "        [0.8385],\n",
      "        [0.3576],\n",
      "        [0.8437],\n",
      "        [0.4207],\n",
      "        [0.8580],\n",
      "        [0.8510],\n",
      "        [0.7238],\n",
      "        [0.8496],\n",
      "        [0.9196],\n",
      "        [0.5838],\n",
      "        [0.4138],\n",
      "        [0.7898],\n",
      "        [0.7539],\n",
      "        [0.8365],\n",
      "        [0.8202],\n",
      "        [0.7916],\n",
      "        [0.4161],\n",
      "        [0.8592],\n",
      "        [0.8482],\n",
      "        [0.4137],\n",
      "        [0.6247],\n",
      "        [0.6399],\n",
      "        [0.7430]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0028],\n",
      "        [    0.0032],\n",
      "        [    0.0033],\n",
      "        [    0.0035],\n",
      "        [    0.0040],\n",
      "        [    0.0051],\n",
      "        [    0.0054],\n",
      "        [    0.0057],\n",
      "        [    0.0063],\n",
      "        [    0.0063],\n",
      "        [    0.0067],\n",
      "        [    0.0072],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0084],\n",
      "        [    0.0093],\n",
      "        [    0.0093],\n",
      "        [    0.0093],\n",
      "        [    0.0096],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0104],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0117],\n",
      "        [    0.0117],\n",
      "        [    0.0118],\n",
      "        [    0.0120],\n",
      "        [    0.0124],\n",
      "        [    0.0128],\n",
      "        [    0.0131],\n",
      "        [    0.0136],\n",
      "        [    0.0140],\n",
      "        [    0.0147],\n",
      "        [    0.0148],\n",
      "        [    0.0148],\n",
      "        [    0.0155],\n",
      "        [    0.0156],\n",
      "        [    0.0159],\n",
      "        [    0.0165],\n",
      "        [    0.0181],\n",
      "        [    0.0187],\n",
      "        [    0.0190],\n",
      "        [    0.0193],\n",
      "        [    0.0196],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0202],\n",
      "        [    0.0202],\n",
      "        [    0.0208],\n",
      "        [    0.0210],\n",
      "        [    0.0217],\n",
      "        [    0.0223],\n",
      "        [    0.0226],\n",
      "        [    0.0256],\n",
      "        [    0.0260],\n",
      "        [    0.0261],\n",
      "        [    0.0261],\n",
      "        [    0.0261],\n",
      "        [    0.0263],\n",
      "        [    0.0267],\n",
      "        [    0.0268],\n",
      "        [    0.0272],\n",
      "        [    0.0273],\n",
      "        [    0.0285],\n",
      "        [    0.0288],\n",
      "        [    0.0298],\n",
      "        [    0.0306],\n",
      "        [    0.0309],\n",
      "        [    0.0315],\n",
      "        [    0.0324],\n",
      "        [    0.0330],\n",
      "        [    0.0335],\n",
      "        [    0.0337],\n",
      "        [    0.0344],\n",
      "        [    0.0344],\n",
      "        [    0.0344],\n",
      "        [    0.0354],\n",
      "        [    0.0355],\n",
      "        [    0.0355],\n",
      "        [    0.0356],\n",
      "        [    0.0364],\n",
      "        [    0.0371],\n",
      "        [    0.0378],\n",
      "        [    0.0385],\n",
      "        [    0.0396],\n",
      "        [    0.0410],\n",
      "        [    0.0412],\n",
      "        [    0.0413],\n",
      "        [    0.0417],\n",
      "        [    0.0446],\n",
      "        [    0.0460],\n",
      "        [    0.0460],\n",
      "        [    0.0464],\n",
      "        [    0.0471],\n",
      "        [    0.0473],\n",
      "        [    0.0482],\n",
      "        [    0.0494],\n",
      "        [    0.0501],\n",
      "        [    0.0508],\n",
      "        [    0.0510],\n",
      "        [    0.0510],\n",
      "        [    0.0513],\n",
      "        [    0.0516],\n",
      "        [    0.0519],\n",
      "        [    0.0522],\n",
      "        [    0.0534],\n",
      "        [    0.0549],\n",
      "        [    0.0551],\n",
      "        [    0.0581],\n",
      "        [    0.0585],\n",
      "        [    0.0593],\n",
      "        [    0.0611],\n",
      "        [    0.0615],\n",
      "        [    0.0622],\n",
      "        [    0.0631],\n",
      "        [    0.0652],\n",
      "        [    0.0683],\n",
      "        [    0.0707],\n",
      "        [    0.0738],\n",
      "        [    0.0750],\n",
      "        [    0.0751],\n",
      "        [    0.0753],\n",
      "        [    0.0769],\n",
      "        [    0.0789],\n",
      "        [    0.0818],\n",
      "        [    0.0901],\n",
      "        [    0.0917],\n",
      "        [    0.0944],\n",
      "        [    0.0973],\n",
      "        [    0.1084],\n",
      "        [    0.1095],\n",
      "        [    0.1188],\n",
      "        [    0.1267],\n",
      "        [    0.1269],\n",
      "        [    0.1270],\n",
      "        [    0.1313],\n",
      "        [    0.1325],\n",
      "        [    0.1326],\n",
      "        [    0.1328],\n",
      "        [    0.1424],\n",
      "        [    0.1512],\n",
      "        [    0.1513],\n",
      "        [    0.1518],\n",
      "        [    0.1863],\n",
      "        [    0.2042],\n",
      "        [    0.2162],\n",
      "        [    0.2818]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0024],\n",
      "        [0.0018],\n",
      "        [0.0036],\n",
      "        [0.0057],\n",
      "        [0.0010],\n",
      "        [0.0029],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0036],\n",
      "        [0.0007],\n",
      "        [0.0052],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0094],\n",
      "        [0.0047],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0073],\n",
      "        [0.0049],\n",
      "        [0.0082],\n",
      "        [0.0080],\n",
      "        [0.0059],\n",
      "        [0.0069],\n",
      "        [0.0084],\n",
      "        [0.0087],\n",
      "        [0.0098],\n",
      "        [0.0056],\n",
      "        [0.0135],\n",
      "        [0.0056],\n",
      "        [0.0093],\n",
      "        [0.0057],\n",
      "        [0.0111],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0148],\n",
      "        [0.0118],\n",
      "        [0.0076],\n",
      "        [0.0124],\n",
      "        [0.0154],\n",
      "        [0.0124],\n",
      "        [0.0090],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0124],\n",
      "        [0.0158],\n",
      "        [0.0190],\n",
      "        [0.0162],\n",
      "        [0.0151],\n",
      "        [0.0124],\n",
      "        [0.0162],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0150],\n",
      "        [0.0201],\n",
      "        [0.0194],\n",
      "        [0.0239],\n",
      "        [0.0202],\n",
      "        [0.0208],\n",
      "        [0.0207],\n",
      "        [0.0187],\n",
      "        [0.0204],\n",
      "        [0.0202],\n",
      "        [0.0229],\n",
      "        [0.0209],\n",
      "        [0.0262],\n",
      "        [0.0264],\n",
      "        [0.0267],\n",
      "        [0.0257],\n",
      "        [0.0266],\n",
      "        [0.0260],\n",
      "        [0.0247],\n",
      "        [0.0281],\n",
      "        [0.0244],\n",
      "        [0.0296],\n",
      "        [0.0290],\n",
      "        [0.0310],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0302],\n",
      "        [0.0311],\n",
      "        [0.0331],\n",
      "        [0.0342],\n",
      "        [0.0364],\n",
      "        [0.0379],\n",
      "        [0.0374],\n",
      "        [0.0307],\n",
      "        [0.0379],\n",
      "        [0.0396],\n",
      "        [0.0366],\n",
      "        [0.0333],\n",
      "        [0.0337],\n",
      "        [0.0400],\n",
      "        [0.0405],\n",
      "        [0.0418],\n",
      "        [0.0385],\n",
      "        [0.0427],\n",
      "        [0.0396],\n",
      "        [0.0416],\n",
      "        [0.0460],\n",
      "        [0.0457],\n",
      "        [0.0426],\n",
      "        [0.0469],\n",
      "        [0.0419],\n",
      "        [0.0469],\n",
      "        [0.0443],\n",
      "        [0.0476],\n",
      "        [0.0487],\n",
      "        [0.0530],\n",
      "        [0.0455],\n",
      "        [0.0511],\n",
      "        [0.0544],\n",
      "        [0.0467],\n",
      "        [0.0548],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0567],\n",
      "        [0.0577],\n",
      "        [0.0525],\n",
      "        [0.0552],\n",
      "        [0.0583],\n",
      "        [0.0610],\n",
      "        [0.0607],\n",
      "        [0.0618],\n",
      "        [0.0625],\n",
      "        [0.0651],\n",
      "        [0.0661],\n",
      "        [0.0695],\n",
      "        [0.0725],\n",
      "        [0.0739],\n",
      "        [0.0717],\n",
      "        [0.0727],\n",
      "        [0.0759],\n",
      "        [0.0713],\n",
      "        [0.0749],\n",
      "        [0.0747],\n",
      "        [0.0848],\n",
      "        [0.0899],\n",
      "        [0.0943],\n",
      "        [0.0938],\n",
      "        [0.1001],\n",
      "        [0.1112],\n",
      "        [0.1048],\n",
      "        [0.1198],\n",
      "        [0.1247],\n",
      "        [0.1248],\n",
      "        [0.1266],\n",
      "        [0.1274],\n",
      "        [0.1280],\n",
      "        [0.1290],\n",
      "        [0.1336],\n",
      "        [0.1388],\n",
      "        [0.1508],\n",
      "        [0.1493],\n",
      "        [0.1527],\n",
      "        [0.1859],\n",
      "        [0.2015],\n",
      "        [0.2138],\n",
      "        [0.2773]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 46.82209587097168\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 6 個區塊累積花費時間(s) 1.2297968864440918\n",
      "<<The performance of 6 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2297968864440918\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1143.14\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1640.50\n",
      "The accuracy(2000) for l = 1: 85.53%\n",
      "The accuracy(3000) for l = 1: 89.94%\n",
      "The maximum error: tensor(7080.6289)\n",
      "The minimum error: tensor(13.5820)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 444.9\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 466.2\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 631.984375\n",
      "The minimum error: 240.2109375\n",
      "------------------------------------------------------------\n",
      "0.8553459119496856\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<7>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.319355528219603e-07, 26)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [26, 75, 132, 150, 127, 73, 31, 6, 39, 99, 2, 114, 8, 98, 22, 96, 42, 53, 66, 72, 27, 5, 143, 61, 113, 126, 112, 106, 74, 121, 158, 50, 120, 107, 131, 122, 111, 24, 147, 7, 44, 128, 9, 76, 86, 67, 105, 87, 19, 100, 135, 157, 156, 30, 125, 110, 65, 141, 145, 20, 151, 146, 129, 123, 3, 144, 51, 90, 29, 155, 149, 142, 119, 115, 104, 103, 97, 108, 32, 130, 43, 25, 148, 124, 78, 88, 28, 4, 85, 12, 89, 41, 68, 152, 62, 10, 77, 40, 117, 52, 49, 23, 38, 91, 58, 71, 59, 60, 118, 18, 101, 116, 109, 133, 102, 79, 81, 80, 154, 63, 64, 153, 11, 33, 134, 13, 84, 83, 69, 48, 70, 1, 82, 54, 95, 17, 37, 140, 136, 34, 36, 57, 35, 16, 0, 94, 137, 47, 56, 45, 14, 46, 21, 139, 15]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0036],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0047],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0111],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0135],\n",
      "        [0.0148],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0162],\n",
      "        [0.0171],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0229],\n",
      "        [0.0239],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0262],\n",
      "        [0.0264],\n",
      "        [0.0266],\n",
      "        [0.0267],\n",
      "        [0.0281],\n",
      "        [0.0290],\n",
      "        [0.0296],\n",
      "        [0.0302],\n",
      "        [0.0307],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0342],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0374],\n",
      "        [0.0379],\n",
      "        [0.0379],\n",
      "        [0.0385],\n",
      "        [0.0396],\n",
      "        [0.0396],\n",
      "        [0.0400],\n",
      "        [0.0405],\n",
      "        [0.0416],\n",
      "        [0.0418],\n",
      "        [0.0419],\n",
      "        [0.0426],\n",
      "        [0.0427],\n",
      "        [0.0443],\n",
      "        [0.0455],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0467],\n",
      "        [0.0469],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0487],\n",
      "        [0.0511],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0530],\n",
      "        [0.0544],\n",
      "        [0.0548],\n",
      "        [0.0552],\n",
      "        [0.0567],\n",
      "        [0.0577],\n",
      "        [0.0583],\n",
      "        [0.0607],\n",
      "        [0.0610],\n",
      "        [0.0618],\n",
      "        [0.0625],\n",
      "        [0.0651],\n",
      "        [0.0661],\n",
      "        [0.0695],\n",
      "        [0.0713],\n",
      "        [0.0725],\n",
      "        [0.0727],\n",
      "        [0.0739],\n",
      "        [0.0747],\n",
      "        [0.0749],\n",
      "        [0.0759],\n",
      "        [0.0848],\n",
      "        [0.0899],\n",
      "        [0.0938],\n",
      "        [0.0943],\n",
      "        [0.1001],\n",
      "        [0.1048],\n",
      "        [0.1112],\n",
      "        [0.1198],\n",
      "        [0.1247],\n",
      "        [0.1248],\n",
      "        [0.1266],\n",
      "        [0.1274],\n",
      "        [0.1280],\n",
      "        [0.1290],\n",
      "        [0.1336],\n",
      "        [0.1388],\n",
      "        [0.1493],\n",
      "        [0.1508],\n",
      "        [0.1527]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.319355528219603e-07, 26)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [26, 75, 132, 150, 127, 73, 31, 6, 39, 99, 2, 114, 8, 98, 22, 96, 42, 53, 66, 72, 27, 5, 143, 61, 113, 126, 112, 106, 74, 121, 158, 50, 120, 107, 131, 122, 111, 24, 147, 7, 44, 128, 9, 76, 86, 67, 105, 87, 19, 100, 135, 157, 156, 30, 125, 110, 65, 141, 145, 20, 151, 146, 129, 123, 3, 144, 51, 90, 29, 155, 149, 142, 119, 115, 104, 103, 97, 108, 32, 130, 43, 25, 148, 124, 78, 88, 28, 4, 85, 12, 89, 41, 68, 152, 62, 10, 77, 40, 117, 52, 49, 23, 38, 91, 58, 71, 59, 60, 118, 18, 101, 116, 109, 133, 102, 79, 81, 80, 154, 63, 64, 153, 11, 33, 134, 13, 84, 83, 69, 48, 70, 1, 82, 54, 95, 17, 37, 140, 136, 34, 36, 57, 35, 16, 0, 94, 137, 47, 56, 45, 14, 46, 21, 139, 15, 138] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8898],\n",
      "        [0.6590],\n",
      "        [0.4731],\n",
      "        [0.3624],\n",
      "        [0.5322],\n",
      "        [0.6857],\n",
      "        [0.9072],\n",
      "        [0.8467],\n",
      "        [0.8647],\n",
      "        [0.6069],\n",
      "        [0.8280],\n",
      "        [0.6108],\n",
      "        [0.8133],\n",
      "        [0.5562],\n",
      "        [0.8482],\n",
      "        [0.5061],\n",
      "        [0.8411],\n",
      "        [0.7274],\n",
      "        [0.7091],\n",
      "        [0.6893],\n",
      "        [0.9038],\n",
      "        [0.8364],\n",
      "        [0.3265],\n",
      "        [0.6686],\n",
      "        [0.6366],\n",
      "        [0.5284],\n",
      "        [0.6418],\n",
      "        [0.5894],\n",
      "        [0.6862],\n",
      "        [0.5704],\n",
      "        [0.3048],\n",
      "        [0.7512],\n",
      "        [0.5491],\n",
      "        [0.6410],\n",
      "        [0.4979],\n",
      "        [0.5448],\n",
      "        [0.6098],\n",
      "        [0.8586],\n",
      "        [0.3456],\n",
      "        [0.8167],\n",
      "        [0.8291],\n",
      "        [0.5205],\n",
      "        [0.8326],\n",
      "        [0.6218],\n",
      "        [0.6268],\n",
      "        [0.7064],\n",
      "        [0.5789],\n",
      "        [0.6300],\n",
      "        [0.9363],\n",
      "        [0.6013],\n",
      "        [0.4242],\n",
      "        [0.3713],\n",
      "        [0.3830],\n",
      "        [0.9113],\n",
      "        [0.5218],\n",
      "        [0.6241],\n",
      "        [0.6928],\n",
      "        [0.3104],\n",
      "        [0.3272],\n",
      "        [0.9220],\n",
      "        [0.4114],\n",
      "        [0.3375],\n",
      "        [0.5023],\n",
      "        [0.5271],\n",
      "        [0.8246],\n",
      "        [0.3307],\n",
      "        [0.7224],\n",
      "        [0.6132],\n",
      "        [0.8921],\n",
      "        [0.3742],\n",
      "        [0.3329],\n",
      "        [0.2767],\n",
      "        [0.5645],\n",
      "        [0.5901],\n",
      "        [0.5841],\n",
      "        [0.5921],\n",
      "        [0.4984],\n",
      "        [0.6477],\n",
      "        [0.8763],\n",
      "        [0.5012],\n",
      "        [0.8243],\n",
      "        [0.8214],\n",
      "        [0.3276],\n",
      "        [0.4933],\n",
      "        [0.6077],\n",
      "        [0.6029],\n",
      "        [0.9171],\n",
      "        [0.8150],\n",
      "        [0.6357],\n",
      "        [0.8060],\n",
      "        [0.5986],\n",
      "        [0.7977],\n",
      "        [0.6677],\n",
      "        [0.4126],\n",
      "        [0.6772],\n",
      "        [0.8541],\n",
      "        [0.6074],\n",
      "        [0.8339],\n",
      "        [0.5861],\n",
      "        [0.7168],\n",
      "        [0.7874],\n",
      "        [0.8722],\n",
      "        [0.8406],\n",
      "        [0.6112],\n",
      "        [0.6790],\n",
      "        [0.6593],\n",
      "        [0.6093],\n",
      "        [0.6703],\n",
      "        [0.5604],\n",
      "        [0.9200],\n",
      "        [0.5762],\n",
      "        [0.5952],\n",
      "        [0.6207],\n",
      "        [0.4196],\n",
      "        [0.5646],\n",
      "        [0.6061],\n",
      "        [0.6250],\n",
      "        [0.6042],\n",
      "        [0.3813],\n",
      "        [0.6788],\n",
      "        [0.6765],\n",
      "        [0.3939],\n",
      "        [0.7793],\n",
      "        [0.8590],\n",
      "        [0.4027],\n",
      "        [0.8010],\n",
      "        [0.6241],\n",
      "        [0.6246],\n",
      "        [0.6466],\n",
      "        [0.8174],\n",
      "        [0.6353],\n",
      "        [0.8793],\n",
      "        [0.6227],\n",
      "        [0.7377],\n",
      "        [0.5703],\n",
      "        [0.8923],\n",
      "        [0.8355],\n",
      "        [0.3573],\n",
      "        [0.4202],\n",
      "        [0.8411],\n",
      "        [0.8551],\n",
      "        [0.7191],\n",
      "        [0.8482],\n",
      "        [0.8487],\n",
      "        [0.9175],\n",
      "        [0.5817],\n",
      "        [0.4134],\n",
      "        [0.7859],\n",
      "        [0.7493],\n",
      "        [0.8329],\n",
      "        [0.8194],\n",
      "        [0.7880],\n",
      "        [0.8573],\n",
      "        [0.4158],\n",
      "        [0.8473],\n",
      "        [0.4133]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0036],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0047],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0111],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0135],\n",
      "        [0.0148],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0162],\n",
      "        [0.0171],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0229],\n",
      "        [0.0239],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0262],\n",
      "        [0.0264],\n",
      "        [0.0266],\n",
      "        [0.0267],\n",
      "        [0.0281],\n",
      "        [0.0290],\n",
      "        [0.0296],\n",
      "        [0.0302],\n",
      "        [0.0307],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0342],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0374],\n",
      "        [0.0379],\n",
      "        [0.0379],\n",
      "        [0.0385],\n",
      "        [0.0396],\n",
      "        [0.0396],\n",
      "        [0.0400],\n",
      "        [0.0405],\n",
      "        [0.0416],\n",
      "        [0.0418],\n",
      "        [0.0419],\n",
      "        [0.0426],\n",
      "        [0.0427],\n",
      "        [0.0443],\n",
      "        [0.0455],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0467],\n",
      "        [0.0469],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0487],\n",
      "        [0.0511],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0530],\n",
      "        [0.0544],\n",
      "        [0.0548],\n",
      "        [0.0552],\n",
      "        [0.0567],\n",
      "        [0.0577],\n",
      "        [0.0583],\n",
      "        [0.0607],\n",
      "        [0.0610],\n",
      "        [0.0618],\n",
      "        [0.0625],\n",
      "        [0.0651],\n",
      "        [0.0661],\n",
      "        [0.0695],\n",
      "        [0.0713],\n",
      "        [0.0725],\n",
      "        [0.0727],\n",
      "        [0.0739],\n",
      "        [0.0747],\n",
      "        [0.0749],\n",
      "        [0.0759],\n",
      "        [0.0848],\n",
      "        [0.0899],\n",
      "        [0.0938],\n",
      "        [0.0943],\n",
      "        [0.1001],\n",
      "        [0.1048],\n",
      "        [0.1112],\n",
      "        [0.1198],\n",
      "        [0.1247],\n",
      "        [0.1248],\n",
      "        [0.1266],\n",
      "        [0.1274],\n",
      "        [0.1280],\n",
      "        [0.1290],\n",
      "        [0.1336],\n",
      "        [0.1388],\n",
      "        [0.1493],\n",
      "        [0.1508],\n",
      "        [0.1527],\n",
      "        [0.1859]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0075],\n",
      "        [    0.0062],\n",
      "        [    0.0021],\n",
      "        [    0.0009],\n",
      "        [    0.0024],\n",
      "        [    0.0094],\n",
      "        [    0.0050],\n",
      "        [    0.0046],\n",
      "        [    0.0098],\n",
      "        [    0.0036],\n",
      "        [    0.0034],\n",
      "        [    0.0099],\n",
      "        [    0.0107],\n",
      "        [    0.0110],\n",
      "        [    0.0107],\n",
      "        [    0.0002],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0135],\n",
      "        [    0.0014],\n",
      "        [    0.0135],\n",
      "        [    0.0143],\n",
      "        [    0.0079],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0044],\n",
      "        [    0.0154],\n",
      "        [    0.0151],\n",
      "        [    0.0163],\n",
      "        [    0.0138],\n",
      "        [    0.0088],\n",
      "        [    0.0029],\n",
      "        [    0.0056],\n",
      "        [    0.0174],\n",
      "        [    0.0141],\n",
      "        [    0.0071],\n",
      "        [    0.0054],\n",
      "        [    0.0181],\n",
      "        [    0.0113],\n",
      "        [    0.0192],\n",
      "        [    0.0196],\n",
      "        [    0.0084],\n",
      "        [    0.0198],\n",
      "        [    0.0070],\n",
      "        [    0.0080],\n",
      "        [    0.0229],\n",
      "        [    0.0211],\n",
      "        [    0.0085],\n",
      "        [    0.0070],\n",
      "        [    0.0233],\n",
      "        [    0.0143],\n",
      "        [    0.0188],\n",
      "        [    0.0203],\n",
      "        [    0.0261],\n",
      "        [    0.0155],\n",
      "        [    0.0123],\n",
      "        [    0.0112],\n",
      "        [    0.0200],\n",
      "        [    0.0205],\n",
      "        [    0.0281],\n",
      "        [    0.0225],\n",
      "        [    0.0196],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0281],\n",
      "        [    0.0233],\n",
      "        [    0.0177],\n",
      "        [    0.0309],\n",
      "        [    0.0322],\n",
      "        [    0.0230],\n",
      "        [    0.0244],\n",
      "        [    0.0262],\n",
      "        [    0.0216],\n",
      "        [    0.0209],\n",
      "        [    0.0205],\n",
      "        [    0.0202],\n",
      "        [    0.0230],\n",
      "        [    0.0222],\n",
      "        [    0.0230],\n",
      "        [    0.0338],\n",
      "        [    0.0373],\n",
      "        [    0.0254],\n",
      "        [    0.0300],\n",
      "        [    0.0304],\n",
      "        [    0.0271],\n",
      "        [    0.0268],\n",
      "        [    0.0416],\n",
      "        [    0.0270],\n",
      "        [    0.0292],\n",
      "        [    0.0297],\n",
      "        [    0.0309],\n",
      "        [    0.0318],\n",
      "        [    0.0308],\n",
      "        [    0.0364],\n",
      "        [    0.0321],\n",
      "        [    0.0473],\n",
      "        [    0.0337],\n",
      "        [    0.0341],\n",
      "        [    0.0365],\n",
      "        [    0.0349],\n",
      "        [    0.0485],\n",
      "        [    0.0489],\n",
      "        [    0.0362],\n",
      "        [    0.0506],\n",
      "        [    0.0515],\n",
      "        [    0.0392],\n",
      "        [    0.0410],\n",
      "        [    0.0534],\n",
      "        [    0.0423],\n",
      "        [    0.0382],\n",
      "        [    0.0407],\n",
      "        [    0.0434],\n",
      "        [    0.0445],\n",
      "        [    0.0501],\n",
      "        [    0.0462],\n",
      "        [    0.0469],\n",
      "        [    0.0481],\n",
      "        [    0.0488],\n",
      "        [    0.0535],\n",
      "        [    0.0494],\n",
      "        [    0.0503],\n",
      "        [    0.0565],\n",
      "        [    0.0545],\n",
      "        [    0.0547],\n",
      "        [    0.0599],\n",
      "        [    0.0555],\n",
      "        [    0.0580],\n",
      "        [    0.0591],\n",
      "        [    0.0632],\n",
      "        [    0.0782],\n",
      "        [    0.0664],\n",
      "        [    0.0805],\n",
      "        [    0.0672],\n",
      "        [    0.0817],\n",
      "        [    0.0809],\n",
      "        [    0.0672],\n",
      "        [    0.0786],\n",
      "        [    0.0912],\n",
      "        [    0.0956],\n",
      "        [    0.0884],\n",
      "        [    0.0932],\n",
      "        [    0.1114],\n",
      "        [    0.1046],\n",
      "        [    0.1119],\n",
      "        [    0.1330],\n",
      "        [    0.1310],\n",
      "        [    0.1284],\n",
      "        [    0.1340],\n",
      "        [    0.1351],\n",
      "        [    0.1364],\n",
      "        [    0.1263],\n",
      "        [    0.1455],\n",
      "        [    0.1558],\n",
      "        [    0.1527],\n",
      "        [    0.1449],\n",
      "        [    0.1875]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 47.377872705459595\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.487233257715161e-08, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 150, 72, 132, 127, 50, 2, 99, 126, 6, 31, 111, 120, 75, 76, 19, 122, 26, 143, 86, 128, 87, 158, 73, 39, 114, 8, 22, 98, 65, 147, 42, 110, 53, 66, 27, 121, 131, 135, 5, 113, 61, 106, 112, 125, 74, 129, 123, 107, 51, 24, 157, 7, 146, 44, 9, 141, 103, 156, 145, 104, 115, 105, 119, 108, 151, 67, 32, 155, 97, 144, 100, 149, 25, 30, 142, 88, 4, 78, 20, 3, 85, 12, 148, 124, 68, 90, 89, 41, 62, 29, 77, 130, 40, 52, 38, 152, 117, 43, 18, 71, 101, 59, 28, 118, 116, 109, 102, 79, 10, 81, 49, 80, 23, 63, 133, 64, 91, 58, 60, 154, 11, 33, 13, 153, 84, 83, 134, 69, 70, 17, 82, 48, 37, 1, 95, 54, 34, 140, 36, 136, 35, 57, 16, 14, 137, 94, 0, 47, 56, 45, 15, 46, 139, 21, 138, 92] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5112],\n",
      "        [0.3644],\n",
      "        [0.6965],\n",
      "        [0.4761],\n",
      "        [0.5364],\n",
      "        [0.7577],\n",
      "        [0.8349],\n",
      "        [0.6141],\n",
      "        [0.5322],\n",
      "        [0.8542],\n",
      "        [0.9146],\n",
      "        [0.6162],\n",
      "        [0.5534],\n",
      "        [0.6660],\n",
      "        [0.6284],\n",
      "        [0.9451],\n",
      "        [0.5488],\n",
      "        [0.8968],\n",
      "        [0.3271],\n",
      "        [0.6336],\n",
      "        [0.5245],\n",
      "        [0.6369],\n",
      "        [0.3054],\n",
      "        [0.6929],\n",
      "        [0.8714],\n",
      "        [0.6169],\n",
      "        [0.8201],\n",
      "        [0.8540],\n",
      "        [0.5624],\n",
      "        [0.7006],\n",
      "        [0.3467],\n",
      "        [0.8478],\n",
      "        [0.6307],\n",
      "        [0.7343],\n",
      "        [0.7168],\n",
      "        [0.9113],\n",
      "        [0.5749],\n",
      "        [0.5013],\n",
      "        [0.4262],\n",
      "        [0.8438],\n",
      "        [0.6434],\n",
      "        [0.6758],\n",
      "        [0.5958],\n",
      "        [0.6487],\n",
      "        [0.5251],\n",
      "        [0.6934],\n",
      "        [0.5058],\n",
      "        [0.5307],\n",
      "        [0.6479],\n",
      "        [0.7286],\n",
      "        [0.8644],\n",
      "        [0.3731],\n",
      "        [0.8235],\n",
      "        [0.3383],\n",
      "        [0.8363],\n",
      "        [0.8397],\n",
      "        [0.3109],\n",
      "        [0.5985],\n",
      "        [0.3848],\n",
      "        [0.3276],\n",
      "        [0.5903],\n",
      "        [0.5957],\n",
      "        [0.5849],\n",
      "        [0.5691],\n",
      "        [0.6545],\n",
      "        [0.4136],\n",
      "        [0.7143],\n",
      "        [0.8830],\n",
      "        [0.3759],\n",
      "        [0.5035],\n",
      "        [0.3311],\n",
      "        [0.6084],\n",
      "        [0.3342],\n",
      "        [0.8271],\n",
      "        [0.9187],\n",
      "        [0.2765],\n",
      "        [0.6096],\n",
      "        [0.8222],\n",
      "        [0.6141],\n",
      "        [0.9299],\n",
      "        [0.8318],\n",
      "        [0.6429],\n",
      "        [0.8128],\n",
      "        [0.3287],\n",
      "        [0.4961],\n",
      "        [0.6749],\n",
      "        [0.6198],\n",
      "        [0.6051],\n",
      "        [0.8038],\n",
      "        [0.6847],\n",
      "        [0.8996],\n",
      "        [0.6137],\n",
      "        [0.5047],\n",
      "        [0.8403],\n",
      "        [0.7237],\n",
      "        [0.8471],\n",
      "        [0.4146],\n",
      "        [0.5913],\n",
      "        [0.8309],\n",
      "        [0.9288],\n",
      "        [0.6658],\n",
      "        [0.5831],\n",
      "        [0.6143],\n",
      "        [0.9250],\n",
      "        [0.5650],\n",
      "        [0.6005],\n",
      "        [0.6272],\n",
      "        [0.5708],\n",
      "        [0.6122],\n",
      "        [0.8617],\n",
      "        [0.6313],\n",
      "        [0.7940],\n",
      "        [0.6102],\n",
      "        [0.8786],\n",
      "        [0.6861],\n",
      "        [0.4218],\n",
      "        [0.6839],\n",
      "        [0.6175],\n",
      "        [0.6850],\n",
      "        [0.6769],\n",
      "        [0.3831],\n",
      "        [0.7856],\n",
      "        [0.8653],\n",
      "        [0.8080],\n",
      "        [0.3957],\n",
      "        [0.6313],\n",
      "        [0.6316],\n",
      "        [0.4046],\n",
      "        [0.6529],\n",
      "        [0.6413],\n",
      "        [0.9010],\n",
      "        [0.6294],\n",
      "        [0.8244],\n",
      "        [0.8417],\n",
      "        [0.8871],\n",
      "        [0.5763],\n",
      "        [0.7447],\n",
      "        [0.8471],\n",
      "        [0.3586],\n",
      "        [0.8620],\n",
      "        [0.4220],\n",
      "        [0.8548],\n",
      "        [0.7257],\n",
      "        [0.8566],\n",
      "        [0.8267],\n",
      "        [0.4151],\n",
      "        [0.5879],\n",
      "        [0.9259],\n",
      "        [0.7925],\n",
      "        [0.7564],\n",
      "        [0.8402],\n",
      "        [0.8551],\n",
      "        [0.7948],\n",
      "        [0.4177],\n",
      "        [0.8637],\n",
      "        [0.4149],\n",
      "        [0.6285]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0014],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0029],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0044],\n",
      "        [    0.0046],\n",
      "        [    0.0050],\n",
      "        [    0.0054],\n",
      "        [    0.0056],\n",
      "        [    0.0062],\n",
      "        [    0.0070],\n",
      "        [    0.0070],\n",
      "        [    0.0071],\n",
      "        [    0.0075],\n",
      "        [    0.0079],\n",
      "        [    0.0080],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0088],\n",
      "        [    0.0094],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0107],\n",
      "        [    0.0107],\n",
      "        [    0.0110],\n",
      "        [    0.0112],\n",
      "        [    0.0113],\n",
      "        [    0.0123],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0135],\n",
      "        [    0.0135],\n",
      "        [    0.0138],\n",
      "        [    0.0141],\n",
      "        [    0.0143],\n",
      "        [    0.0143],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0155],\n",
      "        [    0.0163],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0177],\n",
      "        [    0.0181],\n",
      "        [    0.0188],\n",
      "        [    0.0192],\n",
      "        [    0.0196],\n",
      "        [    0.0196],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0203],\n",
      "        [    0.0205],\n",
      "        [    0.0205],\n",
      "        [    0.0209],\n",
      "        [    0.0211],\n",
      "        [    0.0216],\n",
      "        [    0.0222],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0230],\n",
      "        [    0.0230],\n",
      "        [    0.0230],\n",
      "        [    0.0233],\n",
      "        [    0.0233],\n",
      "        [    0.0244],\n",
      "        [    0.0254],\n",
      "        [    0.0261],\n",
      "        [    0.0262],\n",
      "        [    0.0268],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0281],\n",
      "        [    0.0281],\n",
      "        [    0.0292],\n",
      "        [    0.0297],\n",
      "        [    0.0300],\n",
      "        [    0.0304],\n",
      "        [    0.0308],\n",
      "        [    0.0309],\n",
      "        [    0.0309],\n",
      "        [    0.0318],\n",
      "        [    0.0321],\n",
      "        [    0.0322],\n",
      "        [    0.0337],\n",
      "        [    0.0338],\n",
      "        [    0.0341],\n",
      "        [    0.0349],\n",
      "        [    0.0362],\n",
      "        [    0.0364],\n",
      "        [    0.0365],\n",
      "        [    0.0373],\n",
      "        [    0.0382],\n",
      "        [    0.0392],\n",
      "        [    0.0407],\n",
      "        [    0.0410],\n",
      "        [    0.0416],\n",
      "        [    0.0423],\n",
      "        [    0.0434],\n",
      "        [    0.0445],\n",
      "        [    0.0462],\n",
      "        [    0.0469],\n",
      "        [    0.0473],\n",
      "        [    0.0481],\n",
      "        [    0.0485],\n",
      "        [    0.0488],\n",
      "        [    0.0489],\n",
      "        [    0.0494],\n",
      "        [    0.0501],\n",
      "        [    0.0503],\n",
      "        [    0.0506],\n",
      "        [    0.0515],\n",
      "        [    0.0534],\n",
      "        [    0.0535],\n",
      "        [    0.0545],\n",
      "        [    0.0547],\n",
      "        [    0.0555],\n",
      "        [    0.0565],\n",
      "        [    0.0580],\n",
      "        [    0.0591],\n",
      "        [    0.0599],\n",
      "        [    0.0632],\n",
      "        [    0.0664],\n",
      "        [    0.0672],\n",
      "        [    0.0672],\n",
      "        [    0.0782],\n",
      "        [    0.0786],\n",
      "        [    0.0805],\n",
      "        [    0.0809],\n",
      "        [    0.0817],\n",
      "        [    0.0884],\n",
      "        [    0.0912],\n",
      "        [    0.0932],\n",
      "        [    0.0956],\n",
      "        [    0.1046],\n",
      "        [    0.1114],\n",
      "        [    0.1119],\n",
      "        [    0.1263],\n",
      "        [    0.1284],\n",
      "        [    0.1310],\n",
      "        [    0.1330],\n",
      "        [    0.1340],\n",
      "        [    0.1351],\n",
      "        [    0.1364],\n",
      "        [    0.1449],\n",
      "        [    0.1455],\n",
      "        [    0.1527],\n",
      "        [    0.1558],\n",
      "        [    0.1875],\n",
      "        [    0.2080]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0019],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0043],\n",
      "        [0.0021],\n",
      "        [0.0033],\n",
      "        [0.0065],\n",
      "        [0.0041],\n",
      "        [0.0040],\n",
      "        [0.0063],\n",
      "        [0.0076],\n",
      "        [0.0049],\n",
      "        [0.0085],\n",
      "        [0.0068],\n",
      "        [0.0093],\n",
      "        [0.0062],\n",
      "        [0.0057],\n",
      "        [0.0094],\n",
      "        [0.0102],\n",
      "        [0.0099],\n",
      "        [0.0110],\n",
      "        [0.0083],\n",
      "        [0.0080],\n",
      "        [0.0088],\n",
      "        [0.0099],\n",
      "        [0.0087],\n",
      "        [0.0100],\n",
      "        [0.0120],\n",
      "        [0.0131],\n",
      "        [0.0102],\n",
      "        [0.0132],\n",
      "        [0.0115],\n",
      "        [0.0126],\n",
      "        [0.0125],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0170],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0148],\n",
      "        [0.0179],\n",
      "        [0.0151],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0165],\n",
      "        [0.0192],\n",
      "        [0.0160],\n",
      "        [0.0173],\n",
      "        [0.0184],\n",
      "        [0.0215],\n",
      "        [0.0181],\n",
      "        [0.0191],\n",
      "        [0.0172],\n",
      "        [0.0211],\n",
      "        [0.0190],\n",
      "        [0.0185],\n",
      "        [0.0215],\n",
      "        [0.0225],\n",
      "        [0.0200],\n",
      "        [0.0235],\n",
      "        [0.0232],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0244],\n",
      "        [0.0245],\n",
      "        [0.0245],\n",
      "        [0.0212],\n",
      "        [0.0230],\n",
      "        [0.0260],\n",
      "        [0.0274],\n",
      "        [0.0253],\n",
      "        [0.0289],\n",
      "        [0.0283],\n",
      "        [0.0278],\n",
      "        [0.0286],\n",
      "        [0.0276],\n",
      "        [0.0272],\n",
      "        [0.0302],\n",
      "        [0.0302],\n",
      "        [0.0318],\n",
      "        [0.0331],\n",
      "        [0.0320],\n",
      "        [0.0294],\n",
      "        [0.0326],\n",
      "        [0.0341],\n",
      "        [0.0329],\n",
      "        [0.0313],\n",
      "        [0.0352],\n",
      "        [0.0316],\n",
      "        [0.0360],\n",
      "        [0.0360],\n",
      "        [0.0380],\n",
      "        [0.0377],\n",
      "        [0.0382],\n",
      "        [0.0352],\n",
      "        [0.0379],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0434],\n",
      "        [0.0409],\n",
      "        [0.0442],\n",
      "        [0.0451],\n",
      "        [0.0455],\n",
      "        [0.0471],\n",
      "        [0.0486],\n",
      "        [0.0468],\n",
      "        [0.0496],\n",
      "        [0.0470],\n",
      "        [0.0505],\n",
      "        [0.0472],\n",
      "        [0.0505],\n",
      "        [0.0527],\n",
      "        [0.0513],\n",
      "        [0.0489],\n",
      "        [0.0497],\n",
      "        [0.0519],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0564],\n",
      "        [0.0560],\n",
      "        [0.0581],\n",
      "        [0.0590],\n",
      "        [0.0601],\n",
      "        [0.0627],\n",
      "        [0.0650],\n",
      "        [0.0683],\n",
      "        [0.0668],\n",
      "        [0.0684],\n",
      "        [0.0768],\n",
      "        [0.0804],\n",
      "        [0.0794],\n",
      "        [0.0796],\n",
      "        [0.0805],\n",
      "        [0.0903],\n",
      "        [0.0886],\n",
      "        [0.0947],\n",
      "        [0.0929],\n",
      "        [0.1062],\n",
      "        [0.1098],\n",
      "        [0.1119],\n",
      "        [0.1265],\n",
      "        [0.1258],\n",
      "        [0.1296],\n",
      "        [0.1322],\n",
      "        [0.1324],\n",
      "        [0.1338],\n",
      "        [0.1350],\n",
      "        [0.1449],\n",
      "        [0.1441],\n",
      "        [0.1503],\n",
      "        [0.1542],\n",
      "        [0.1849],\n",
      "        [0.2064]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 47.66510343551636\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.671559222264477e-08, 150)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [150, 72, 132, 127, 96, 2, 99, 31, 6, 50, 75, 143, 26, 111, 126, 19, 120, 39, 73, 76, 22, 114, 122, 86, 87, 8, 98, 42, 128, 158, 53, 121, 131, 65, 27, 66, 147, 110, 61, 5, 113, 106, 112, 74, 24, 107, 135, 141, 157, 125, 44, 7, 145, 156, 9, 51, 129, 123, 105, 103, 144, 151, 146, 104, 67, 115, 100, 108, 119, 32, 155, 97, 30, 149, 3, 25, 20, 4, 88, 78, 142, 90, 85, 12, 29, 130, 148, 68, 89, 62, 124, 41, 43, 77, 52, 40, 152, 18, 38, 117, 71, 28, 101, 59, 118, 116, 109, 10, 49, 102, 23, 79, 91, 81, 58, 80, 63, 64, 60, 133, 154, 11, 13, 33, 153, 84, 83, 134, 69, 17, 70, 82, 48, 1, 95, 37, 54, 140, 34, 136, 36, 35, 57, 16, 137, 14, 94, 0, 47, 56, 45, 46, 15, 139, 21, 138, 92, 93] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3632],\n",
      "        [0.6953],\n",
      "        [0.4737],\n",
      "        [0.5346],\n",
      "        [0.5094],\n",
      "        [0.8336],\n",
      "        [0.6138],\n",
      "        [0.9136],\n",
      "        [0.8537],\n",
      "        [0.7563],\n",
      "        [0.6646],\n",
      "        [0.3250],\n",
      "        [0.8954],\n",
      "        [0.6153],\n",
      "        [0.5301],\n",
      "        [0.9453],\n",
      "        [0.5514],\n",
      "        [0.8697],\n",
      "        [0.6918],\n",
      "        [0.6268],\n",
      "        [0.8520],\n",
      "        [0.6158],\n",
      "        [0.5465],\n",
      "        [0.6322],\n",
      "        [0.6355],\n",
      "        [0.8193],\n",
      "        [0.5615],\n",
      "        [0.8457],\n",
      "        [0.5227],\n",
      "        [0.3032],\n",
      "        [0.7332],\n",
      "        [0.5730],\n",
      "        [0.4991],\n",
      "        [0.6998],\n",
      "        [0.9104],\n",
      "        [0.7159],\n",
      "        [0.3449],\n",
      "        [0.6298],\n",
      "        [0.6747],\n",
      "        [0.8432],\n",
      "        [0.6426],\n",
      "        [0.5947],\n",
      "        [0.6481],\n",
      "        [0.6923],\n",
      "        [0.8623],\n",
      "        [0.6470],\n",
      "        [0.4235],\n",
      "        [0.3082],\n",
      "        [0.3716],\n",
      "        [0.5226],\n",
      "        [0.8348],\n",
      "        [0.8227],\n",
      "        [0.3256],\n",
      "        [0.3834],\n",
      "        [0.8390],\n",
      "        [0.7271],\n",
      "        [0.5037],\n",
      "        [0.5284],\n",
      "        [0.5837],\n",
      "        [0.5977],\n",
      "        [0.3290],\n",
      "        [0.4126],\n",
      "        [0.3364],\n",
      "        [0.5893],\n",
      "        [0.7136],\n",
      "        [0.5941],\n",
      "        [0.6081],\n",
      "        [0.6535],\n",
      "        [0.5671],\n",
      "        [0.8815],\n",
      "        [0.3745],\n",
      "        [0.5020],\n",
      "        [0.9179],\n",
      "        [0.3325],\n",
      "        [0.8308],\n",
      "        [0.8251],\n",
      "        [0.9294],\n",
      "        [0.8214],\n",
      "        [0.6081],\n",
      "        [0.6126],\n",
      "        [0.2738],\n",
      "        [0.6182],\n",
      "        [0.6418],\n",
      "        [0.8123],\n",
      "        [0.8987],\n",
      "        [0.5025],\n",
      "        [0.3270],\n",
      "        [0.6736],\n",
      "        [0.6034],\n",
      "        [0.6839],\n",
      "        [0.4933],\n",
      "        [0.8015],\n",
      "        [0.8288],\n",
      "        [0.6122],\n",
      "        [0.7226],\n",
      "        [0.8383],\n",
      "        [0.4134],\n",
      "        [0.9290],\n",
      "        [0.8453],\n",
      "        [0.5896],\n",
      "        [0.6642],\n",
      "        [0.9244],\n",
      "        [0.5827],\n",
      "        [0.6119],\n",
      "        [0.5630],\n",
      "        [0.5989],\n",
      "        [0.6262],\n",
      "        [0.8613],\n",
      "        [0.7925],\n",
      "        [0.5700],\n",
      "        [0.8769],\n",
      "        [0.6105],\n",
      "        [0.6158],\n",
      "        [0.6298],\n",
      "        [0.6832],\n",
      "        [0.6085],\n",
      "        [0.6850],\n",
      "        [0.6829],\n",
      "        [0.6755],\n",
      "        [0.4192],\n",
      "        [0.3816],\n",
      "        [0.7845],\n",
      "        [0.8075],\n",
      "        [0.8636],\n",
      "        [0.3941],\n",
      "        [0.6303],\n",
      "        [0.6306],\n",
      "        [0.4019],\n",
      "        [0.6511],\n",
      "        [0.9014],\n",
      "        [0.6394],\n",
      "        [0.6282],\n",
      "        [0.8230],\n",
      "        [0.8860],\n",
      "        [0.5749],\n",
      "        [0.8399],\n",
      "        [0.7436],\n",
      "        [0.3560],\n",
      "        [0.8452],\n",
      "        [0.4193],\n",
      "        [0.8605],\n",
      "        [0.8531],\n",
      "        [0.7242],\n",
      "        [0.8565],\n",
      "        [0.4125],\n",
      "        [0.8265],\n",
      "        [0.5865],\n",
      "        [0.9251],\n",
      "        [0.7908],\n",
      "        [0.7552],\n",
      "        [0.8388],\n",
      "        [0.7933],\n",
      "        [0.8551],\n",
      "        [0.4152],\n",
      "        [0.8622],\n",
      "        [0.4123],\n",
      "        [0.6269],\n",
      "        [0.6433]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0049],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0120],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0136],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0140],\n",
      "        [0.0148],\n",
      "        [0.0151],\n",
      "        [0.0160],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0200],\n",
      "        [0.0211],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0215],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0232],\n",
      "        [0.0235],\n",
      "        [0.0244],\n",
      "        [0.0245],\n",
      "        [0.0245],\n",
      "        [0.0253],\n",
      "        [0.0260],\n",
      "        [0.0272],\n",
      "        [0.0274],\n",
      "        [0.0276],\n",
      "        [0.0278],\n",
      "        [0.0283],\n",
      "        [0.0286],\n",
      "        [0.0289],\n",
      "        [0.0294],\n",
      "        [0.0302],\n",
      "        [0.0302],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0320],\n",
      "        [0.0326],\n",
      "        [0.0329],\n",
      "        [0.0331],\n",
      "        [0.0341],\n",
      "        [0.0352],\n",
      "        [0.0352],\n",
      "        [0.0360],\n",
      "        [0.0360],\n",
      "        [0.0377],\n",
      "        [0.0379],\n",
      "        [0.0380],\n",
      "        [0.0382],\n",
      "        [0.0408],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0434],\n",
      "        [0.0442],\n",
      "        [0.0451],\n",
      "        [0.0455],\n",
      "        [0.0468],\n",
      "        [0.0470],\n",
      "        [0.0471],\n",
      "        [0.0472],\n",
      "        [0.0486],\n",
      "        [0.0489],\n",
      "        [0.0496],\n",
      "        [0.0497],\n",
      "        [0.0505],\n",
      "        [0.0505],\n",
      "        [0.0513],\n",
      "        [0.0519],\n",
      "        [0.0527],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0581],\n",
      "        [0.0590],\n",
      "        [0.0601],\n",
      "        [0.0627],\n",
      "        [0.0650],\n",
      "        [0.0668],\n",
      "        [0.0683],\n",
      "        [0.0684],\n",
      "        [0.0768],\n",
      "        [0.0794],\n",
      "        [0.0796],\n",
      "        [0.0804],\n",
      "        [0.0805],\n",
      "        [0.0886],\n",
      "        [0.0903],\n",
      "        [0.0929],\n",
      "        [0.0947],\n",
      "        [0.1062],\n",
      "        [0.1098],\n",
      "        [0.1119],\n",
      "        [0.1258],\n",
      "        [0.1265],\n",
      "        [0.1296],\n",
      "        [0.1322],\n",
      "        [0.1324],\n",
      "        [0.1338],\n",
      "        [0.1350],\n",
      "        [0.1441],\n",
      "        [0.1449],\n",
      "        [0.1503],\n",
      "        [0.1542],\n",
      "        [0.1849],\n",
      "        [0.2064],\n",
      "        [0.2196]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0011],\n",
      "        [    0.0024],\n",
      "        [    0.0031],\n",
      "        [    0.0018],\n",
      "        [    0.0048],\n",
      "        [    0.0000],\n",
      "        [    0.0013],\n",
      "        [    0.0021],\n",
      "        [    0.0029],\n",
      "        [    0.0065],\n",
      "        [    0.0020],\n",
      "        [    0.0040],\n",
      "        [    0.0040],\n",
      "        [    0.0083],\n",
      "        [    0.0092],\n",
      "        [    0.0077],\n",
      "        [    0.0102],\n",
      "        [    0.0055],\n",
      "        [    0.0055],\n",
      "        [    0.0115],\n",
      "        [    0.0066],\n",
      "        [    0.0066],\n",
      "        [    0.0120],\n",
      "        [    0.0126],\n",
      "        [    0.0131],\n",
      "        [    0.0085],\n",
      "        [    0.0077],\n",
      "        [    0.0075],\n",
      "        [    0.0128],\n",
      "        [    0.0126],\n",
      "        [    0.0094],\n",
      "        [    0.0094],\n",
      "        [    0.0092],\n",
      "        [    0.0145],\n",
      "        [    0.0105],\n",
      "        [    0.0099],\n",
      "        [    0.0144],\n",
      "        [    0.0153],\n",
      "        [    0.0109],\n",
      "        [    0.0123],\n",
      "        [    0.0120],\n",
      "        [    0.0117],\n",
      "        [    0.0129],\n",
      "        [    0.0123],\n",
      "        [    0.0138],\n",
      "        [    0.0140],\n",
      "        [    0.0198],\n",
      "        [    0.0147],\n",
      "        [    0.0162],\n",
      "        [    0.0207],\n",
      "        [    0.0157],\n",
      "        [    0.0169],\n",
      "        [    0.0173],\n",
      "        [    0.0179],\n",
      "        [    0.0175],\n",
      "        [    0.0215],\n",
      "        [    0.0220],\n",
      "        [    0.0223],\n",
      "        [    0.0178],\n",
      "        [    0.0232],\n",
      "        [    0.0197],\n",
      "        [    0.0206],\n",
      "        [    0.0227],\n",
      "        [    0.0237],\n",
      "        [    0.0195],\n",
      "        [    0.0249],\n",
      "        [    0.0212],\n",
      "        [    0.0255],\n",
      "        [    0.0261],\n",
      "        [    0.0265],\n",
      "        [    0.0257],\n",
      "        [    0.0272],\n",
      "        [    0.0235],\n",
      "        [    0.0273],\n",
      "        [    0.0254],\n",
      "        [    0.0296],\n",
      "        [    0.0262],\n",
      "        [    0.0292],\n",
      "        [    0.0315],\n",
      "        [    0.0315],\n",
      "        [    0.0310],\n",
      "        [    0.0260],\n",
      "        [    0.0332],\n",
      "        [    0.0314],\n",
      "        [    0.0295],\n",
      "        [    0.0288],\n",
      "        [    0.0331],\n",
      "        [    0.0349],\n",
      "        [    0.0360],\n",
      "        [    0.0355],\n",
      "        [    0.0359],\n",
      "        [    0.0368],\n",
      "        [    0.0326],\n",
      "        [    0.0381],\n",
      "        [    0.0382],\n",
      "        [    0.0386],\n",
      "        [    0.0387],\n",
      "        [    0.0388],\n",
      "        [    0.0404],\n",
      "        [    0.0406],\n",
      "        [    0.0436],\n",
      "        [    0.0391],\n",
      "        [    0.0430],\n",
      "        [    0.0467],\n",
      "        [    0.0467],\n",
      "        [    0.0476],\n",
      "        [    0.0478],\n",
      "        [    0.0453],\n",
      "        [    0.0448],\n",
      "        [    0.0493],\n",
      "        [    0.0453],\n",
      "        [    0.0517],\n",
      "        [    0.0456],\n",
      "        [    0.0526],\n",
      "        [    0.0468],\n",
      "        [    0.0535],\n",
      "        [    0.0534],\n",
      "        [    0.0540],\n",
      "        [    0.0491],\n",
      "        [    0.0554],\n",
      "        [    0.0562],\n",
      "        [    0.0570],\n",
      "        [    0.0571],\n",
      "        [    0.0586],\n",
      "        [    0.0594],\n",
      "        [    0.0618],\n",
      "        [    0.0628],\n",
      "        [    0.0655],\n",
      "        [    0.0680],\n",
      "        [    0.0676],\n",
      "        [    0.0714],\n",
      "        [    0.0713],\n",
      "        [    0.0746],\n",
      "        [    0.0773],\n",
      "        [    0.0767],\n",
      "        [    0.0828],\n",
      "        [    0.0782],\n",
      "        [    0.0860],\n",
      "        [    0.0925],\n",
      "        [    0.0902],\n",
      "        [    0.0970],\n",
      "        [    0.1085],\n",
      "        [    0.1070],\n",
      "        [    0.1129],\n",
      "        [    0.1232],\n",
      "        [    0.1274],\n",
      "        [    0.1266],\n",
      "        [    0.1304],\n",
      "        [    0.1301],\n",
      "        [    0.1313],\n",
      "        [    0.1326],\n",
      "        [    0.1419],\n",
      "        [    0.1459],\n",
      "        [    0.1477],\n",
      "        [    0.1524],\n",
      "        [    0.1823],\n",
      "        [    0.2031],\n",
      "        [    0.2165]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 47.95288419723511\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.846825328921113e-10, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [2, 150, 99, 127, 75, 31, 72, 6, 132, 26, 143, 96, 73, 39, 50, 22, 114, 42, 19, 98, 111, 8, 126, 131, 53, 121, 66, 120, 27, 61, 76, 106, 122, 113, 74, 5, 158, 86, 128, 112, 87, 24, 107, 147, 65, 141, 110, 44, 157, 7, 145, 9, 105, 156, 67, 144, 135, 151, 125, 100, 51, 129, 123, 146, 103, 30, 104, 115, 3, 108, 155, 90, 119, 20, 32, 97, 149, 130, 4, 29, 25, 142, 12, 88, 78, 43, 148, 85, 68, 62, 124, 89, 41, 77, 52, 40, 152, 18, 28, 38, 117, 101, 71, 49, 23, 10, 91, 59, 118, 58, 116, 109, 60, 102, 79, 81, 63, 80, 64, 133, 154, 11, 13, 33, 153, 84, 83, 134, 17, 69, 82, 70, 48, 95, 1, 54, 37, 140, 136, 34, 36, 57, 35, 16, 137, 94, 14, 47, 0, 56, 45, 46, 15, 139, 21, 138, 92, 93, 55] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8316],\n",
      "        [0.3623],\n",
      "        [0.6118],\n",
      "        [0.5322],\n",
      "        [0.6617],\n",
      "        [0.9117],\n",
      "        [0.6926],\n",
      "        [0.8525],\n",
      "        [0.4709],\n",
      "        [0.8932],\n",
      "        [0.3232],\n",
      "        [0.5065],\n",
      "        [0.6891],\n",
      "        [0.8672],\n",
      "        [0.7541],\n",
      "        [0.8498],\n",
      "        [0.6136],\n",
      "        [0.8429],\n",
      "        [0.9445],\n",
      "        [0.5591],\n",
      "        [0.6133],\n",
      "        [0.8179],\n",
      "        [0.5275],\n",
      "        [0.4964],\n",
      "        [0.7311],\n",
      "        [0.5704],\n",
      "        [0.7133],\n",
      "        [0.5488],\n",
      "        [0.9084],\n",
      "        [0.6720],\n",
      "        [0.6239],\n",
      "        [0.5924],\n",
      "        [0.5439],\n",
      "        [0.6406],\n",
      "        [0.6894],\n",
      "        [0.8418],\n",
      "        [0.3016],\n",
      "        [0.6289],\n",
      "        [0.5202],\n",
      "        [0.6463],\n",
      "        [0.6323],\n",
      "        [0.8601],\n",
      "        [0.6446],\n",
      "        [0.3436],\n",
      "        [0.6972],\n",
      "        [0.3056],\n",
      "        [0.6277],\n",
      "        [0.8324],\n",
      "        [0.3705],\n",
      "        [0.8212],\n",
      "        [0.3244],\n",
      "        [0.8375],\n",
      "        [0.5815],\n",
      "        [0.3824],\n",
      "        [0.7109],\n",
      "        [0.3275],\n",
      "        [0.4207],\n",
      "        [0.4118],\n",
      "        [0.5199],\n",
      "        [0.6063],\n",
      "        [0.7248],\n",
      "        [0.5010],\n",
      "        [0.5257],\n",
      "        [0.3351],\n",
      "        [0.5956],\n",
      "        [0.9161],\n",
      "        [0.5871],\n",
      "        [0.5916],\n",
      "        [0.8290],\n",
      "        [0.6512],\n",
      "        [0.3732],\n",
      "        [0.6149],\n",
      "        [0.5646],\n",
      "        [0.9280],\n",
      "        [0.8794],\n",
      "        [0.4993],\n",
      "        [0.3312],\n",
      "        [0.4997],\n",
      "        [0.8200],\n",
      "        [0.8969],\n",
      "        [0.8228],\n",
      "        [0.2716],\n",
      "        [0.8111],\n",
      "        [0.6049],\n",
      "        [0.6097],\n",
      "        [0.8262],\n",
      "        [0.3256],\n",
      "        [0.6388],\n",
      "        [0.6707],\n",
      "        [0.6813],\n",
      "        [0.4905],\n",
      "        [0.6000],\n",
      "        [0.7988],\n",
      "        [0.6094],\n",
      "        [0.7204],\n",
      "        [0.8357],\n",
      "        [0.4123],\n",
      "        [0.9281],\n",
      "        [0.9226],\n",
      "        [0.8429],\n",
      "        [0.5871],\n",
      "        [0.5808],\n",
      "        [0.6614],\n",
      "        [0.7903],\n",
      "        [0.8749],\n",
      "        [0.8598],\n",
      "        [0.6125],\n",
      "        [0.6087],\n",
      "        [0.5605],\n",
      "        [0.6803],\n",
      "        [0.5964],\n",
      "        [0.6240],\n",
      "        [0.6727],\n",
      "        [0.5678],\n",
      "        [0.6075],\n",
      "        [0.6268],\n",
      "        [0.6821],\n",
      "        [0.6055],\n",
      "        [0.6802],\n",
      "        [0.4165],\n",
      "        [0.3803],\n",
      "        [0.7830],\n",
      "        [0.8065],\n",
      "        [0.8614],\n",
      "        [0.3928],\n",
      "        [0.6274],\n",
      "        [0.6279],\n",
      "        [0.3991],\n",
      "        [0.9006],\n",
      "        [0.6481],\n",
      "        [0.6253],\n",
      "        [0.6364],\n",
      "        [0.8208],\n",
      "        [0.5720],\n",
      "        [0.8839],\n",
      "        [0.7412],\n",
      "        [0.8375],\n",
      "        [0.3534],\n",
      "        [0.4166],\n",
      "        [0.8429],\n",
      "        [0.8583],\n",
      "        [0.7214],\n",
      "        [0.8509],\n",
      "        [0.8556],\n",
      "        [0.4100],\n",
      "        [0.5835],\n",
      "        [0.8256],\n",
      "        [0.7886],\n",
      "        [0.9233],\n",
      "        [0.7526],\n",
      "        [0.8365],\n",
      "        [0.7911],\n",
      "        [0.8541],\n",
      "        [0.4126],\n",
      "        [0.8603],\n",
      "        [0.4097],\n",
      "        [0.6236],\n",
      "        [0.6402],\n",
      "        [0.7413]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0011],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0020],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0029],\n",
      "        [    0.0031],\n",
      "        [    0.0040],\n",
      "        [    0.0040],\n",
      "        [    0.0048],\n",
      "        [    0.0055],\n",
      "        [    0.0055],\n",
      "        [    0.0065],\n",
      "        [    0.0066],\n",
      "        [    0.0066],\n",
      "        [    0.0075],\n",
      "        [    0.0077],\n",
      "        [    0.0077],\n",
      "        [    0.0083],\n",
      "        [    0.0085],\n",
      "        [    0.0092],\n",
      "        [    0.0092],\n",
      "        [    0.0094],\n",
      "        [    0.0094],\n",
      "        [    0.0099],\n",
      "        [    0.0102],\n",
      "        [    0.0105],\n",
      "        [    0.0109],\n",
      "        [    0.0115],\n",
      "        [    0.0117],\n",
      "        [    0.0120],\n",
      "        [    0.0120],\n",
      "        [    0.0123],\n",
      "        [    0.0123],\n",
      "        [    0.0126],\n",
      "        [    0.0126],\n",
      "        [    0.0128],\n",
      "        [    0.0129],\n",
      "        [    0.0131],\n",
      "        [    0.0138],\n",
      "        [    0.0140],\n",
      "        [    0.0144],\n",
      "        [    0.0145],\n",
      "        [    0.0147],\n",
      "        [    0.0153],\n",
      "        [    0.0157],\n",
      "        [    0.0162],\n",
      "        [    0.0169],\n",
      "        [    0.0173],\n",
      "        [    0.0175],\n",
      "        [    0.0178],\n",
      "        [    0.0179],\n",
      "        [    0.0195],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0206],\n",
      "        [    0.0207],\n",
      "        [    0.0212],\n",
      "        [    0.0215],\n",
      "        [    0.0220],\n",
      "        [    0.0223],\n",
      "        [    0.0227],\n",
      "        [    0.0232],\n",
      "        [    0.0235],\n",
      "        [    0.0237],\n",
      "        [    0.0249],\n",
      "        [    0.0254],\n",
      "        [    0.0255],\n",
      "        [    0.0257],\n",
      "        [    0.0260],\n",
      "        [    0.0261],\n",
      "        [    0.0262],\n",
      "        [    0.0265],\n",
      "        [    0.0272],\n",
      "        [    0.0273],\n",
      "        [    0.0288],\n",
      "        [    0.0292],\n",
      "        [    0.0295],\n",
      "        [    0.0296],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0315],\n",
      "        [    0.0315],\n",
      "        [    0.0326],\n",
      "        [    0.0331],\n",
      "        [    0.0332],\n",
      "        [    0.0349],\n",
      "        [    0.0355],\n",
      "        [    0.0359],\n",
      "        [    0.0360],\n",
      "        [    0.0368],\n",
      "        [    0.0381],\n",
      "        [    0.0382],\n",
      "        [    0.0386],\n",
      "        [    0.0387],\n",
      "        [    0.0388],\n",
      "        [    0.0391],\n",
      "        [    0.0404],\n",
      "        [    0.0406],\n",
      "        [    0.0430],\n",
      "        [    0.0436],\n",
      "        [    0.0448],\n",
      "        [    0.0453],\n",
      "        [    0.0453],\n",
      "        [    0.0456],\n",
      "        [    0.0467],\n",
      "        [    0.0467],\n",
      "        [    0.0468],\n",
      "        [    0.0476],\n",
      "        [    0.0478],\n",
      "        [    0.0491],\n",
      "        [    0.0493],\n",
      "        [    0.0517],\n",
      "        [    0.0526],\n",
      "        [    0.0534],\n",
      "        [    0.0535],\n",
      "        [    0.0540],\n",
      "        [    0.0554],\n",
      "        [    0.0562],\n",
      "        [    0.0570],\n",
      "        [    0.0571],\n",
      "        [    0.0586],\n",
      "        [    0.0594],\n",
      "        [    0.0618],\n",
      "        [    0.0628],\n",
      "        [    0.0655],\n",
      "        [    0.0676],\n",
      "        [    0.0680],\n",
      "        [    0.0713],\n",
      "        [    0.0714],\n",
      "        [    0.0746],\n",
      "        [    0.0767],\n",
      "        [    0.0773],\n",
      "        [    0.0782],\n",
      "        [    0.0828],\n",
      "        [    0.0860],\n",
      "        [    0.0902],\n",
      "        [    0.0925],\n",
      "        [    0.0970],\n",
      "        [    0.1070],\n",
      "        [    0.1085],\n",
      "        [    0.1129],\n",
      "        [    0.1232],\n",
      "        [    0.1266],\n",
      "        [    0.1274],\n",
      "        [    0.1301],\n",
      "        [    0.1304],\n",
      "        [    0.1313],\n",
      "        [    0.1326],\n",
      "        [    0.1419],\n",
      "        [    0.1459],\n",
      "        [    0.1477],\n",
      "        [    0.1524],\n",
      "        [    0.1823],\n",
      "        [    0.2031],\n",
      "        [    0.2165],\n",
      "        [    0.2801]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0017],\n",
      "        [    0.0010],\n",
      "        [    0.0008],\n",
      "        [    0.0012],\n",
      "        [    0.0053],\n",
      "        [    0.0031],\n",
      "        [    0.0025],\n",
      "        [    0.0031],\n",
      "        [    0.0053],\n",
      "        [    0.0054],\n",
      "        [    0.0026],\n",
      "        [    0.0038],\n",
      "        [    0.0091],\n",
      "        [    0.0062],\n",
      "        [    0.0077],\n",
      "        [    0.0057],\n",
      "        [    0.0077],\n",
      "        [    0.0078],\n",
      "        [    0.0073],\n",
      "        [    0.0085],\n",
      "        [    0.0084],\n",
      "        [    0.0097],\n",
      "        [    0.0064],\n",
      "        [    0.0102],\n",
      "        [    0.0068],\n",
      "        [    0.0092],\n",
      "        [    0.0096],\n",
      "        [    0.0078],\n",
      "        [    0.0141],\n",
      "        [    0.0121],\n",
      "        [    0.0111],\n",
      "        [    0.0131],\n",
      "        [    0.0093],\n",
      "        [    0.0125],\n",
      "        [    0.0116],\n",
      "        [    0.0146],\n",
      "        [    0.0120],\n",
      "        [    0.0140],\n",
      "        [    0.0150],\n",
      "        [    0.0133],\n",
      "        [    0.0143],\n",
      "        [    0.0131],\n",
      "        [    0.0176],\n",
      "        [    0.0156],\n",
      "        [    0.0144],\n",
      "        [    0.0141],\n",
      "        [    0.0171],\n",
      "        [    0.0169],\n",
      "        [    0.0188],\n",
      "        [    0.0172],\n",
      "        [    0.0183],\n",
      "        [    0.0188],\n",
      "        [    0.0164],\n",
      "        [    0.0211],\n",
      "        [    0.0191],\n",
      "        [    0.0217],\n",
      "        [    0.0199],\n",
      "        [    0.0216],\n",
      "        [    0.0243],\n",
      "        [    0.0213],\n",
      "        [    0.0213],\n",
      "        [    0.0212],\n",
      "        [    0.0228],\n",
      "        [    0.0226],\n",
      "        [    0.0232],\n",
      "        [    0.0240],\n",
      "        [    0.0254],\n",
      "        [    0.0250],\n",
      "        [    0.0249],\n",
      "        [    0.0243],\n",
      "        [    0.0252],\n",
      "        [    0.0260],\n",
      "        [    0.0273],\n",
      "        [    0.0273],\n",
      "        [    0.0260],\n",
      "        [    0.0292],\n",
      "        [    0.0289],\n",
      "        [    0.0287],\n",
      "        [    0.0301],\n",
      "        [    0.0298],\n",
      "        [    0.0314],\n",
      "        [    0.0332],\n",
      "        [    0.0338],\n",
      "        [    0.0309],\n",
      "        [    0.0319],\n",
      "        [    0.0352],\n",
      "        [    0.0381],\n",
      "        [    0.0389],\n",
      "        [    0.0350],\n",
      "        [    0.0378],\n",
      "        [    0.0385],\n",
      "        [    0.0404],\n",
      "        [    0.0412],\n",
      "        [    0.0403],\n",
      "        [    0.0380],\n",
      "        [    0.0388],\n",
      "        [    0.0382],\n",
      "        [    0.0420],\n",
      "        [    0.0397],\n",
      "        [    0.0426],\n",
      "        [    0.0464],\n",
      "        [    0.0421],\n",
      "        [    0.0450],\n",
      "        [    0.0450],\n",
      "        [    0.0439],\n",
      "        [    0.0498],\n",
      "        [    0.0457],\n",
      "        [    0.0435],\n",
      "        [    0.0467],\n",
      "        [    0.0470],\n",
      "        [    0.0460],\n",
      "        [    0.0491],\n",
      "        [    0.0540],\n",
      "        [    0.0549],\n",
      "        [    0.0569],\n",
      "        [    0.0557],\n",
      "        [    0.0572],\n",
      "        [    0.0547],\n",
      "        [    0.0555],\n",
      "        [    0.0572],\n",
      "        [    0.0570],\n",
      "        [    0.0594],\n",
      "        [    0.0588],\n",
      "        [    0.0637],\n",
      "        [    0.0648],\n",
      "        [    0.0647],\n",
      "        [    0.0675],\n",
      "        [    0.0709],\n",
      "        [    0.0733],\n",
      "        [    0.0741],\n",
      "        [    0.0719],\n",
      "        [    0.0757],\n",
      "        [    0.0769],\n",
      "        [    0.0750],\n",
      "        [    0.0842],\n",
      "        [    0.0869],\n",
      "        [    0.0910],\n",
      "        [    0.0933],\n",
      "        [    0.0983],\n",
      "        [    0.1035],\n",
      "        [    0.1097],\n",
      "        [    0.1128],\n",
      "        [    0.1241],\n",
      "        [    0.1254],\n",
      "        [    0.1272],\n",
      "        [    0.1277],\n",
      "        [    0.1301],\n",
      "        [    0.1278],\n",
      "        [    0.1307],\n",
      "        [    0.1398],\n",
      "        [    0.1458],\n",
      "        [    0.1484],\n",
      "        [    0.1522],\n",
      "        [    0.1831],\n",
      "        [    0.2016],\n",
      "        [    0.2150],\n",
      "        [    0.2768]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 48.24128437042236\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 7 個區塊累積花費時間(s) 1.216172695159912\n",
      "<<The performance of 7 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.216172695159912\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1127.70\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1620.55\n",
      "The accuracy(2000) for l = 1: 85.53%\n",
      "The accuracy(3000) for l = 1: 90.57%\n",
      "The maximum error: tensor(7066.5469)\n",
      "The minimum error: tensor(4.4414)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1411.6\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1446.8\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1781.1875\n",
      "The minimum error: 940.9296875\n",
      "------------------------------------------------------------\n",
      "0.8553459119496856\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <<8>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.745593132493013e-08, 146)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [146, 71, 123, 27, 95, 128, 69, 22, 2, 35, 139, 68, 92, 38, 18, 49, 62, 107, 110, 15, 94, 57, 122, 4, 46, 116, 70, 23, 127, 117, 118, 154, 124, 102, 1, 109, 143, 20, 108, 40, 72, 103, 106, 82, 83, 137, 63, 3, 153, 5, 61, 101, 141, 152, 131, 121, 140, 142, 125, 119, 96, 147, 26, 99, 100, 111, 47, 86, 151, 104, 115, 16, 145, 28, 93, 25, 0, 126, 138, 21, 39, 8, 144, 84, 74, 120, 81, 157, 85, 148, 64, 24, 37, 14, 58, 113, 36, 73, 48, 34, 45, 97, 54, 87, 19, 6, 114, 56, 67, 112, 105, 98, 55, 158, 75, 129, 77, 150, 76, 59, 9, 7, 60, 149, 29, 155, 80, 130, 79, 13, 156, 65, 44, 78, 66, 50, 91, 33, 136, 132, 30, 32, 53, 31, 12, 133, 90, 10, 43, 52, 41, 42, 11, 135, 17]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0038],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0068],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0078],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0102],\n",
      "        [0.0111],\n",
      "        [0.0116],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0125],\n",
      "        [0.0131],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0140],\n",
      "        [0.0141],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0156],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0188],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0199],\n",
      "        [0.0211],\n",
      "        [0.0212],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0232],\n",
      "        [0.0240],\n",
      "        [0.0243],\n",
      "        [0.0243],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0273],\n",
      "        [0.0273],\n",
      "        [0.0287],\n",
      "        [0.0289],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.0301],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0350],\n",
      "        [0.0352],\n",
      "        [0.0369],\n",
      "        [0.0378],\n",
      "        [0.0380],\n",
      "        [0.0381],\n",
      "        [0.0382],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.0389],\n",
      "        [0.0397],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0412],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0439],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0464],\n",
      "        [0.0467],\n",
      "        [0.0470],\n",
      "        [0.0491],\n",
      "        [0.0498],\n",
      "        [0.0518],\n",
      "        [0.0540],\n",
      "        [0.0547],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0557],\n",
      "        [0.0569],\n",
      "        [0.0570],\n",
      "        [0.0572],\n",
      "        [0.0572],\n",
      "        [0.0588],\n",
      "        [0.0594],\n",
      "        [0.0628],\n",
      "        [0.0637],\n",
      "        [0.0647],\n",
      "        [0.0648],\n",
      "        [0.0675],\n",
      "        [0.0698],\n",
      "        [0.0709],\n",
      "        [0.0719],\n",
      "        [0.0733],\n",
      "        [0.0741],\n",
      "        [0.0750],\n",
      "        [0.0757],\n",
      "        [0.0842],\n",
      "        [0.0869],\n",
      "        [0.0910],\n",
      "        [0.0933],\n",
      "        [0.0983],\n",
      "        [0.1035],\n",
      "        [0.1097],\n",
      "        [0.1128],\n",
      "        [0.1241],\n",
      "        [0.1254],\n",
      "        [0.1272],\n",
      "        [0.1277],\n",
      "        [0.1278],\n",
      "        [0.1307],\n",
      "        [0.1398],\n",
      "        [0.1458],\n",
      "        [0.1484],\n",
      "        [0.1522]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.745593132493013e-08, 146)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [146, 71, 123, 27, 95, 128, 69, 22, 2, 35, 139, 68, 92, 38, 18, 49, 62, 107, 110, 15, 94, 57, 122, 4, 46, 116, 70, 23, 127, 117, 118, 154, 124, 102, 1, 109, 143, 20, 108, 40, 72, 103, 106, 82, 83, 137, 63, 3, 153, 5, 61, 101, 141, 152, 131, 121, 140, 142, 125, 119, 96, 147, 26, 99, 100, 111, 47, 86, 151, 104, 115, 16, 145, 28, 93, 25, 0, 126, 138, 21, 39, 8, 144, 84, 74, 120, 81, 157, 85, 148, 64, 24, 37, 14, 58, 113, 36, 73, 48, 34, 45, 97, 54, 87, 19, 6, 114, 56, 67, 112, 105, 98, 55, 158, 75, 129, 77, 150, 76, 59, 9, 7, 60, 149, 29, 155, 80, 130, 79, 13, 156, 65, 44, 78, 66, 50, 91, 33, 136, 132, 30, 32, 53, 31, 12, 133, 90, 10, 43, 52, 41, 42, 11, 135, 17, 134] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3638],\n",
      "        [0.6589],\n",
      "        [0.5330],\n",
      "        [0.9108],\n",
      "        [0.6122],\n",
      "        [0.4714],\n",
      "        [0.6862],\n",
      "        [0.8923],\n",
      "        [0.8527],\n",
      "        [0.8654],\n",
      "        [0.3245],\n",
      "        [0.6897],\n",
      "        [0.5060],\n",
      "        [0.8412],\n",
      "        [0.8495],\n",
      "        [0.7282],\n",
      "        [0.7102],\n",
      "        [0.6142],\n",
      "        [0.6146],\n",
      "        [0.9445],\n",
      "        [0.5592],\n",
      "        [0.6689],\n",
      "        [0.5282],\n",
      "        [0.8180],\n",
      "        [0.7515],\n",
      "        [0.5498],\n",
      "        [0.6865],\n",
      "        [0.9075],\n",
      "        [0.4969],\n",
      "        [0.5713],\n",
      "        [0.5447],\n",
      "        [0.3026],\n",
      "        [0.5209],\n",
      "        [0.5928],\n",
      "        [0.8421],\n",
      "        [0.6417],\n",
      "        [0.3449],\n",
      "        [0.8596],\n",
      "        [0.6473],\n",
      "        [0.8307],\n",
      "        [0.6213],\n",
      "        [0.6448],\n",
      "        [0.6286],\n",
      "        [0.6269],\n",
      "        [0.6304],\n",
      "        [0.3066],\n",
      "        [0.7078],\n",
      "        [0.8212],\n",
      "        [0.3714],\n",
      "        [0.8372],\n",
      "        [0.6941],\n",
      "        [0.5820],\n",
      "        [0.3259],\n",
      "        [0.3833],\n",
      "        [0.4214],\n",
      "        [0.5206],\n",
      "        [0.3289],\n",
      "        [0.3367],\n",
      "        [0.5017],\n",
      "        [0.5266],\n",
      "        [0.6067],\n",
      "        [0.4128],\n",
      "        [0.9152],\n",
      "        [0.5959],\n",
      "        [0.5875],\n",
      "        [0.5925],\n",
      "        [0.7220],\n",
      "        [0.6131],\n",
      "        [0.3741],\n",
      "        [0.6517],\n",
      "        [0.5655],\n",
      "        [0.9278],\n",
      "        [0.3325],\n",
      "        [0.8786],\n",
      "        [0.4992],\n",
      "        [0.8961],\n",
      "        [0.8203],\n",
      "        [0.5002],\n",
      "        [0.2729],\n",
      "        [0.8223],\n",
      "        [0.8245],\n",
      "        [0.8111],\n",
      "        [0.3268],\n",
      "        [0.6032],\n",
      "        [0.6074],\n",
      "        [0.4914],\n",
      "        [0.6369],\n",
      "        [0.3078],\n",
      "        [0.5982],\n",
      "        [0.4130],\n",
      "        [0.6676],\n",
      "        [0.9217],\n",
      "        [0.7971],\n",
      "        [0.9282],\n",
      "        [0.6779],\n",
      "        [0.5881],\n",
      "        [0.8340],\n",
      "        [0.6071],\n",
      "        [0.7174],\n",
      "        [0.8413],\n",
      "        [0.7876],\n",
      "        [0.5813],\n",
      "        [0.6770],\n",
      "        [0.6108],\n",
      "        [0.8746],\n",
      "        [0.8594],\n",
      "        [0.5615],\n",
      "        [0.6696],\n",
      "        [0.6587],\n",
      "        [0.5972],\n",
      "        [0.6247],\n",
      "        [0.5680],\n",
      "        [0.6055],\n",
      "        [0.3140],\n",
      "        [0.6052],\n",
      "        [0.4173],\n",
      "        [0.6245],\n",
      "        [0.3811],\n",
      "        [0.6033],\n",
      "        [0.6786],\n",
      "        [0.8066],\n",
      "        [0.7829],\n",
      "        [0.6770],\n",
      "        [0.3934],\n",
      "        [0.8606],\n",
      "        [0.2487],\n",
      "        [0.6255],\n",
      "        [0.3998],\n",
      "        [0.6260],\n",
      "        [0.9007],\n",
      "        [0.2555],\n",
      "        [0.6452],\n",
      "        [0.8181],\n",
      "        [0.6233],\n",
      "        [0.6337],\n",
      "        [0.7380],\n",
      "        [0.5710],\n",
      "        [0.8361],\n",
      "        [0.3543],\n",
      "        [0.4174],\n",
      "        [0.8421],\n",
      "        [0.8570],\n",
      "        [0.7178],\n",
      "        [0.8497],\n",
      "        [0.8557],\n",
      "        [0.4109],\n",
      "        [0.5823],\n",
      "        [0.8258],\n",
      "        [0.7861],\n",
      "        [0.7491],\n",
      "        [0.8345],\n",
      "        [0.7890],\n",
      "        [0.8542],\n",
      "        [0.4133],\n",
      "        [0.8602],\n",
      "        [0.4104]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0038],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0068],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0078],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0102],\n",
      "        [0.0111],\n",
      "        [0.0116],\n",
      "        [0.0120],\n",
      "        [0.0121],\n",
      "        [0.0125],\n",
      "        [0.0131],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0140],\n",
      "        [0.0141],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0156],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0188],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0199],\n",
      "        [0.0211],\n",
      "        [0.0212],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0217],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0232],\n",
      "        [0.0240],\n",
      "        [0.0243],\n",
      "        [0.0243],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0273],\n",
      "        [0.0273],\n",
      "        [0.0287],\n",
      "        [0.0289],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.0301],\n",
      "        [0.0309],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0350],\n",
      "        [0.0352],\n",
      "        [0.0369],\n",
      "        [0.0378],\n",
      "        [0.0380],\n",
      "        [0.0381],\n",
      "        [0.0382],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.0389],\n",
      "        [0.0397],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0412],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0439],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0464],\n",
      "        [0.0467],\n",
      "        [0.0470],\n",
      "        [0.0491],\n",
      "        [0.0498],\n",
      "        [0.0518],\n",
      "        [0.0540],\n",
      "        [0.0547],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0557],\n",
      "        [0.0569],\n",
      "        [0.0570],\n",
      "        [0.0572],\n",
      "        [0.0572],\n",
      "        [0.0588],\n",
      "        [0.0594],\n",
      "        [0.0628],\n",
      "        [0.0637],\n",
      "        [0.0647],\n",
      "        [0.0648],\n",
      "        [0.0675],\n",
      "        [0.0698],\n",
      "        [0.0709],\n",
      "        [0.0719],\n",
      "        [0.0733],\n",
      "        [0.0741],\n",
      "        [0.0750],\n",
      "        [0.0757],\n",
      "        [0.0842],\n",
      "        [0.0869],\n",
      "        [0.0910],\n",
      "        [0.0933],\n",
      "        [0.0983],\n",
      "        [0.1035],\n",
      "        [0.1097],\n",
      "        [0.1128],\n",
      "        [0.1241],\n",
      "        [0.1254],\n",
      "        [0.1272],\n",
      "        [0.1277],\n",
      "        [0.1278],\n",
      "        [0.1307],\n",
      "        [0.1398],\n",
      "        [0.1458],\n",
      "        [0.1484],\n",
      "        [0.1522],\n",
      "        [0.1831]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0027],\n",
      "        [0.0067],\n",
      "        [0.0040],\n",
      "        [0.0096],\n",
      "        [0.0085],\n",
      "        [0.0015],\n",
      "        [0.0103],\n",
      "        [0.0111],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0061],\n",
      "        [0.0024],\n",
      "        [0.0003],\n",
      "        [0.0143],\n",
      "        [0.0141],\n",
      "        [0.0137],\n",
      "        [0.0148],\n",
      "        [0.0005],\n",
      "        [0.0146],\n",
      "        [0.0018],\n",
      "        [0.0142],\n",
      "        [0.0150],\n",
      "        [0.0037],\n",
      "        [0.0163],\n",
      "        [0.0019],\n",
      "        [0.0037],\n",
      "        [0.0170],\n",
      "        [0.0178],\n",
      "        [0.0140],\n",
      "        [0.0158],\n",
      "        [0.0059],\n",
      "        [0.0096],\n",
      "        [0.0073],\n",
      "        [0.0187],\n",
      "        [0.0212],\n",
      "        [0.0203],\n",
      "        [0.0116],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0229],\n",
      "        [0.0068],\n",
      "        [0.0214],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0170],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0196],\n",
      "        [0.0251],\n",
      "        [0.0098],\n",
      "        [0.0247],\n",
      "        [0.0194],\n",
      "        [0.0214],\n",
      "        [0.0159],\n",
      "        [0.0155],\n",
      "        [0.0218],\n",
      "        [0.0199],\n",
      "        [0.0169],\n",
      "        [0.0162],\n",
      "        [0.0284],\n",
      "        [0.0242],\n",
      "        [0.0309],\n",
      "        [0.0163],\n",
      "        [0.0168],\n",
      "        [0.0175],\n",
      "        [0.0172],\n",
      "        [0.0313],\n",
      "        [0.0225],\n",
      "        [0.0179],\n",
      "        [0.0193],\n",
      "        [0.0350],\n",
      "        [0.0243],\n",
      "        [0.0193],\n",
      "        [0.0218],\n",
      "        [0.0369],\n",
      "        [0.0203],\n",
      "        [0.0336],\n",
      "        [0.0293],\n",
      "        [0.0227],\n",
      "        [0.0394],\n",
      "        [0.0237],\n",
      "        [0.0303],\n",
      "        [0.0260],\n",
      "        [0.0267],\n",
      "        [0.0308],\n",
      "        [0.0278],\n",
      "        [0.0388],\n",
      "        [0.0307],\n",
      "        [0.0357],\n",
      "        [0.0306],\n",
      "        [0.0466],\n",
      "        [0.0307],\n",
      "        [0.0295],\n",
      "        [0.0316],\n",
      "        [0.0335],\n",
      "        [0.0324],\n",
      "        [0.0333],\n",
      "        [0.0339],\n",
      "        [0.0341],\n",
      "        [0.0495],\n",
      "        [0.0358],\n",
      "        [0.0501],\n",
      "        [0.0508],\n",
      "        [0.0531],\n",
      "        [0.0532],\n",
      "        [0.0398],\n",
      "        [0.0531],\n",
      "        [0.0391],\n",
      "        [0.0403],\n",
      "        [0.0401],\n",
      "        [0.0427],\n",
      "        [0.0436],\n",
      "        [0.0537],\n",
      "        [0.0471],\n",
      "        [0.0512],\n",
      "        [0.0480],\n",
      "        [0.0532],\n",
      "        [0.0489],\n",
      "        [0.0495],\n",
      "        [0.0491],\n",
      "        [0.0498],\n",
      "        [0.0497],\n",
      "        [0.0565],\n",
      "        [0.0514],\n",
      "        [0.0615],\n",
      "        [0.0563],\n",
      "        [0.0616],\n",
      "        [0.0574],\n",
      "        [0.0584],\n",
      "        [0.0683],\n",
      "        [0.0638],\n",
      "        [0.0797],\n",
      "        [0.0662],\n",
      "        [0.0671],\n",
      "        [0.0823],\n",
      "        [0.0820],\n",
      "        [0.0764],\n",
      "        [0.0890],\n",
      "        [0.0940],\n",
      "        [0.0856],\n",
      "        [0.0900],\n",
      "        [0.1106],\n",
      "        [0.1016],\n",
      "        [0.1042],\n",
      "        [0.1269],\n",
      "        [0.1318],\n",
      "        [0.1192],\n",
      "        [0.1353],\n",
      "        [0.1352],\n",
      "        [0.1393],\n",
      "        [0.1476],\n",
      "        [0.1375],\n",
      "        [0.1510],\n",
      "        [0.1605],\n",
      "        [0.1857]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 48.80428671836853\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.617625096325355e-08, 92)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [92, 107, 128, 15, 46, 68, 146, 122, 116, 123, 118, 139, 71, 72, 124, 106, 82, 83, 95, 27, 154, 61, 69, 22, 143, 2, 35, 49, 127, 18, 94, 38, 110, 62, 57, 121, 117, 131, 119, 99, 4, 100, 125, 137, 70, 47, 111, 23, 104, 102, 28, 115, 141, 153, 142, 0, 109, 20, 108, 1, 152, 103, 93, 140, 151, 21, 40, 8, 147, 145, 63, 101, 3, 5, 84, 74, 81, 96, 138, 14, 144, 64, 37, 85, 120, 26, 86, 58, 36, 73, 113, 126, 48, 34, 16, 148, 97, 25, 157, 67, 39, 114, 105, 112, 98, 55, 24, 75, 77, 76, 9, 59, 45, 60, 7, 54, 87, 129, 29, 56, 19, 150, 6, 158, 80, 149, 79, 13, 155, 130, 65, 78, 66, 156, 33, 44, 91, 50, 30, 136, 32, 132, 31, 12, 53, 10, 133, 90, 52, 43, 11, 41, 42, 135, 17, 134, 88] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5117],\n",
      "        [0.6211],\n",
      "        [0.4755],\n",
      "        [0.9539],\n",
      "        [0.7587],\n",
      "        [0.6974],\n",
      "        [0.3661],\n",
      "        [0.5329],\n",
      "        [0.5553],\n",
      "        [0.5380],\n",
      "        [0.5500],\n",
      "        [0.3253],\n",
      "        [0.6665],\n",
      "        [0.6285],\n",
      "        [0.5257],\n",
      "        [0.6356],\n",
      "        [0.6341],\n",
      "        [0.6377],\n",
      "        [0.6190],\n",
      "        [0.9192],\n",
      "        [0.3045],\n",
      "        [0.7020],\n",
      "        [0.6939],\n",
      "        [0.9003],\n",
      "        [0.3464],\n",
      "        [0.8613],\n",
      "        [0.8734],\n",
      "        [0.7355],\n",
      "        [0.5012],\n",
      "        [0.8574],\n",
      "        [0.5657],\n",
      "        [0.8498],\n",
      "        [0.6216],\n",
      "        [0.7181],\n",
      "        [0.6761],\n",
      "        [0.5251],\n",
      "        [0.5769],\n",
      "        [0.4245],\n",
      "        [0.5317],\n",
      "        [0.6024],\n",
      "        [0.8258],\n",
      "        [0.5939],\n",
      "        [0.5061],\n",
      "        [0.3080],\n",
      "        [0.6942],\n",
      "        [0.7290],\n",
      "        [0.5991],\n",
      "        [0.9157],\n",
      "        [0.6588],\n",
      "        [0.5994],\n",
      "        [0.8866],\n",
      "        [0.5713],\n",
      "        [0.3265],\n",
      "        [0.3739],\n",
      "        [0.3380],\n",
      "        [0.8289],\n",
      "        [0.6489],\n",
      "        [0.8673],\n",
      "        [0.6545],\n",
      "        [0.8507],\n",
      "        [0.3858],\n",
      "        [0.6520],\n",
      "        [0.5047],\n",
      "        [0.3296],\n",
      "        [0.3764],\n",
      "        [0.8297],\n",
      "        [0.8395],\n",
      "        [0.8188],\n",
      "        [0.4153],\n",
      "        [0.3343],\n",
      "        [0.7158],\n",
      "        [0.5884],\n",
      "        [0.8290],\n",
      "        [0.8451],\n",
      "        [0.6104],\n",
      "        [0.6145],\n",
      "        [0.6442],\n",
      "        [0.6136],\n",
      "        [0.2734],\n",
      "        [0.9375],\n",
      "        [0.3284],\n",
      "        [0.6751],\n",
      "        [0.8050],\n",
      "        [0.6053],\n",
      "        [0.4956],\n",
      "        [0.9235],\n",
      "        [0.6202],\n",
      "        [0.6852],\n",
      "        [0.8419],\n",
      "        [0.6142],\n",
      "        [0.5943],\n",
      "        [0.5045],\n",
      "        [0.7247],\n",
      "        [0.8492],\n",
      "        [0.9368],\n",
      "        [0.4154],\n",
      "        [0.5881],\n",
      "        [0.9043],\n",
      "        [0.3098],\n",
      "        [0.6659],\n",
      "        [0.8330],\n",
      "        [0.5674],\n",
      "        [0.6317],\n",
      "        [0.6036],\n",
      "        [0.5744],\n",
      "        [0.6117],\n",
      "        [0.9301],\n",
      "        [0.6121],\n",
      "        [0.6314],\n",
      "        [0.6101],\n",
      "        [0.8144],\n",
      "        [0.6860],\n",
      "        [0.7950],\n",
      "        [0.6845],\n",
      "        [0.7902],\n",
      "        [0.6836],\n",
      "        [0.6177],\n",
      "        [0.4207],\n",
      "        [0.8686],\n",
      "        [0.6767],\n",
      "        [0.8828],\n",
      "        [0.3834],\n",
      "        [0.8676],\n",
      "        [0.3159],\n",
      "        [0.6329],\n",
      "        [0.3957],\n",
      "        [0.6333],\n",
      "        [0.9097],\n",
      "        [0.2500],\n",
      "        [0.4030],\n",
      "        [0.6523],\n",
      "        [0.6304],\n",
      "        [0.6407],\n",
      "        [0.2569],\n",
      "        [0.8439],\n",
      "        [0.8259],\n",
      "        [0.5773],\n",
      "        [0.7453],\n",
      "        [0.8499],\n",
      "        [0.3565],\n",
      "        [0.8652],\n",
      "        [0.4204],\n",
      "        [0.8578],\n",
      "        [0.8643],\n",
      "        [0.7249],\n",
      "        [0.8338],\n",
      "        [0.4136],\n",
      "        [0.5887],\n",
      "        [0.7566],\n",
      "        [0.7938],\n",
      "        [0.8625],\n",
      "        [0.8431],\n",
      "        [0.7969],\n",
      "        [0.4160],\n",
      "        [0.8685],\n",
      "        [0.4131],\n",
      "        [0.6290]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0005],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0067],\n",
      "        [0.0068],\n",
      "        [0.0073],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0085],\n",
      "        [0.0096],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0103],\n",
      "        [0.0111],\n",
      "        [0.0116],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0141],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0163],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0175],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0187],\n",
      "        [0.0193],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0212],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0218],\n",
      "        [0.0218],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0229],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0260],\n",
      "        [0.0267],\n",
      "        [0.0278],\n",
      "        [0.0284],\n",
      "        [0.0293],\n",
      "        [0.0295],\n",
      "        [0.0303],\n",
      "        [0.0306],\n",
      "        [0.0307],\n",
      "        [0.0307],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0333],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0341],\n",
      "        [0.0350],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0369],\n",
      "        [0.0388],\n",
      "        [0.0391],\n",
      "        [0.0394],\n",
      "        [0.0398],\n",
      "        [0.0401],\n",
      "        [0.0403],\n",
      "        [0.0427],\n",
      "        [0.0436],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0480],\n",
      "        [0.0489],\n",
      "        [0.0491],\n",
      "        [0.0495],\n",
      "        [0.0495],\n",
      "        [0.0497],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0508],\n",
      "        [0.0512],\n",
      "        [0.0514],\n",
      "        [0.0531],\n",
      "        [0.0531],\n",
      "        [0.0532],\n",
      "        [0.0532],\n",
      "        [0.0537],\n",
      "        [0.0563],\n",
      "        [0.0565],\n",
      "        [0.0574],\n",
      "        [0.0584],\n",
      "        [0.0615],\n",
      "        [0.0616],\n",
      "        [0.0638],\n",
      "        [0.0662],\n",
      "        [0.0671],\n",
      "        [0.0683],\n",
      "        [0.0764],\n",
      "        [0.0797],\n",
      "        [0.0820],\n",
      "        [0.0823],\n",
      "        [0.0856],\n",
      "        [0.0890],\n",
      "        [0.0900],\n",
      "        [0.0940],\n",
      "        [0.1016],\n",
      "        [0.1042],\n",
      "        [0.1106],\n",
      "        [0.1192],\n",
      "        [0.1269],\n",
      "        [0.1318],\n",
      "        [0.1352],\n",
      "        [0.1353],\n",
      "        [0.1375],\n",
      "        [0.1393],\n",
      "        [0.1476],\n",
      "        [0.1510],\n",
      "        [0.1605],\n",
      "        [0.1857],\n",
      "        [0.2085]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0019],\n",
      "        [0.0006],\n",
      "        [0.0012],\n",
      "        [0.0036],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0058],\n",
      "        [0.0056],\n",
      "        [0.0021],\n",
      "        [0.0078],\n",
      "        [0.0038],\n",
      "        [0.0051],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0088],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0072],\n",
      "        [0.0080],\n",
      "        [0.0109],\n",
      "        [0.0113],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0133],\n",
      "        [0.0110],\n",
      "        [0.0098],\n",
      "        [0.0121],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0127],\n",
      "        [0.0127],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0177],\n",
      "        [0.0139],\n",
      "        [0.0182],\n",
      "        [0.0182],\n",
      "        [0.0179],\n",
      "        [0.0152],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0146],\n",
      "        [0.0154],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0160],\n",
      "        [0.0196],\n",
      "        [0.0171],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0174],\n",
      "        [0.0185],\n",
      "        [0.0216],\n",
      "        [0.0210],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0200],\n",
      "        [0.0205],\n",
      "        [0.0203],\n",
      "        [0.0197],\n",
      "        [0.0235],\n",
      "        [0.0196],\n",
      "        [0.0238],\n",
      "        [0.0246],\n",
      "        [0.0216],\n",
      "        [0.0247],\n",
      "        [0.0230],\n",
      "        [0.0258],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0236],\n",
      "        [0.0239],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0295],\n",
      "        [0.0271],\n",
      "        [0.0317],\n",
      "        [0.0301],\n",
      "        [0.0319],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0331],\n",
      "        [0.0294],\n",
      "        [0.0294],\n",
      "        [0.0333],\n",
      "        [0.0344],\n",
      "        [0.0348],\n",
      "        [0.0351],\n",
      "        [0.0313],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0340],\n",
      "        [0.0370],\n",
      "        [0.0370],\n",
      "        [0.0353],\n",
      "        [0.0374],\n",
      "        [0.0408],\n",
      "        [0.0379],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0443],\n",
      "        [0.0458],\n",
      "        [0.0450],\n",
      "        [0.0488],\n",
      "        [0.0499],\n",
      "        [0.0507],\n",
      "        [0.0500],\n",
      "        [0.0513],\n",
      "        [0.0477],\n",
      "        [0.0513],\n",
      "        [0.0511],\n",
      "        [0.0481],\n",
      "        [0.0488],\n",
      "        [0.0533],\n",
      "        [0.0530],\n",
      "        [0.0511],\n",
      "        [0.0516],\n",
      "        [0.0545],\n",
      "        [0.0520],\n",
      "        [0.0523],\n",
      "        [0.0579],\n",
      "        [0.0579],\n",
      "        [0.0589],\n",
      "        [0.0590],\n",
      "        [0.0631],\n",
      "        [0.0637],\n",
      "        [0.0656],\n",
      "        [0.0679],\n",
      "        [0.0689],\n",
      "        [0.0700],\n",
      "        [0.0781],\n",
      "        [0.0779],\n",
      "        [0.0801],\n",
      "        [0.0805],\n",
      "        [0.0872],\n",
      "        [0.0867],\n",
      "        [0.0916],\n",
      "        [0.0918],\n",
      "        [0.1033],\n",
      "        [0.1049],\n",
      "        [0.1086],\n",
      "        [0.1200],\n",
      "        [0.1246],\n",
      "        [0.1299],\n",
      "        [0.1333],\n",
      "        [0.1336],\n",
      "        [0.1382],\n",
      "        [0.1379],\n",
      "        [0.1460],\n",
      "        [0.1487],\n",
      "        [0.1593],\n",
      "        [0.1833],\n",
      "        [0.2065]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 49.08843016624451\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.284079070908774e-07, 128)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [128, 68, 15, 146, 92, 107, 123, 46, 139, 71, 116, 122, 95, 118, 27, 72, 69, 106, 22, 124, 82, 83, 35, 154, 2, 61, 127, 49, 18, 38, 94, 62, 57, 110, 143, 117, 137, 4, 70, 23, 102, 141, 121, 99, 131, 119, 100, 153, 109, 125, 47, 111, 20, 104, 140, 103, 108, 152, 1, 0, 28, 115, 40, 142, 63, 147, 101, 93, 3, 151, 5, 21, 8, 145, 96, 84, 74, 26, 86, 81, 14, 126, 138, 144, 64, 37, 85, 120, 58, 16, 36, 73, 113, 25, 48, 34, 97, 148, 157, 39, 67, 114, 105, 112, 98, 24, 55, 45, 54, 87, 75, 77, 9, 76, 7, 56, 59, 60, 19, 6, 158, 29, 129, 150, 80, 149, 79, 13, 155, 130, 65, 78, 66, 156, 44, 33, 91, 50, 136, 30, 32, 132, 31, 12, 53, 10, 133, 90, 52, 43, 41, 11, 42, 135, 17, 134, 88, 89] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4734],\n",
      "        [0.6959],\n",
      "        [0.9533],\n",
      "        [0.3650],\n",
      "        [0.5098],\n",
      "        [0.6197],\n",
      "        [0.5361],\n",
      "        [0.7570],\n",
      "        [0.3230],\n",
      "        [0.6649],\n",
      "        [0.5534],\n",
      "        [0.5308],\n",
      "        [0.6177],\n",
      "        [0.5480],\n",
      "        [0.9176],\n",
      "        [0.6269],\n",
      "        [0.6923],\n",
      "        [0.6342],\n",
      "        [0.8984],\n",
      "        [0.5237],\n",
      "        [0.6322],\n",
      "        [0.6358],\n",
      "        [0.8715],\n",
      "        [0.3032],\n",
      "        [0.8606],\n",
      "        [0.7005],\n",
      "        [0.4992],\n",
      "        [0.7338],\n",
      "        [0.8558],\n",
      "        [0.8482],\n",
      "        [0.5642],\n",
      "        [0.7165],\n",
      "        [0.6743],\n",
      "        [0.6202],\n",
      "        [0.3447],\n",
      "        [0.5750],\n",
      "        [0.3055],\n",
      "        [0.8246],\n",
      "        [0.6926],\n",
      "        [0.9138],\n",
      "        [0.5977],\n",
      "        [0.3245],\n",
      "        [0.5229],\n",
      "        [0.6008],\n",
      "        [0.4223],\n",
      "        [0.5298],\n",
      "        [0.5923],\n",
      "        [0.3728],\n",
      "        [0.6477],\n",
      "        [0.5040],\n",
      "        [0.7272],\n",
      "        [0.5975],\n",
      "        [0.8654],\n",
      "        [0.6572],\n",
      "        [0.3274],\n",
      "        [0.6502],\n",
      "        [0.6533],\n",
      "        [0.3847],\n",
      "        [0.8500],\n",
      "        [0.8282],\n",
      "        [0.8849],\n",
      "        [0.5696],\n",
      "        [0.8382],\n",
      "        [0.3363],\n",
      "        [0.7142],\n",
      "        [0.4141],\n",
      "        [0.5868],\n",
      "        [0.5030],\n",
      "        [0.8278],\n",
      "        [0.3752],\n",
      "        [0.8439],\n",
      "        [0.8278],\n",
      "        [0.8179],\n",
      "        [0.3328],\n",
      "        [0.6122],\n",
      "        [0.6086],\n",
      "        [0.6129],\n",
      "        [0.9220],\n",
      "        [0.6183],\n",
      "        [0.6425],\n",
      "        [0.9368],\n",
      "        [0.5023],\n",
      "        [0.2710],\n",
      "        [0.3268],\n",
      "        [0.6734],\n",
      "        [0.8031],\n",
      "        [0.6034],\n",
      "        [0.4934],\n",
      "        [0.6835],\n",
      "        [0.9358],\n",
      "        [0.8400],\n",
      "        [0.6126],\n",
      "        [0.5926],\n",
      "        [0.9028],\n",
      "        [0.7230],\n",
      "        [0.8473],\n",
      "        [0.5868],\n",
      "        [0.4140],\n",
      "        [0.3083],\n",
      "        [0.8315],\n",
      "        [0.6642],\n",
      "        [0.5657],\n",
      "        [0.6301],\n",
      "        [0.6019],\n",
      "        [0.5728],\n",
      "        [0.9284],\n",
      "        [0.6095],\n",
      "        [0.7932],\n",
      "        [0.6816],\n",
      "        [0.6157],\n",
      "        [0.6103],\n",
      "        [0.6296],\n",
      "        [0.8136],\n",
      "        [0.6083],\n",
      "        [0.7890],\n",
      "        [0.6747],\n",
      "        [0.6842],\n",
      "        [0.6829],\n",
      "        [0.8813],\n",
      "        [0.8665],\n",
      "        [0.3145],\n",
      "        [0.8670],\n",
      "        [0.4186],\n",
      "        [0.3820],\n",
      "        [0.6314],\n",
      "        [0.3942],\n",
      "        [0.6318],\n",
      "        [0.9092],\n",
      "        [0.2484],\n",
      "        [0.4008],\n",
      "        [0.6505],\n",
      "        [0.6287],\n",
      "        [0.6388],\n",
      "        [0.2553],\n",
      "        [0.8241],\n",
      "        [0.8421],\n",
      "        [0.5755],\n",
      "        [0.7435],\n",
      "        [0.3542],\n",
      "        [0.8482],\n",
      "        [0.8636],\n",
      "        [0.4181],\n",
      "        [0.8561],\n",
      "        [0.8636],\n",
      "        [0.7229],\n",
      "        [0.8330],\n",
      "        [0.4114],\n",
      "        [0.5867],\n",
      "        [0.7546],\n",
      "        [0.7921],\n",
      "        [0.8417],\n",
      "        [0.8618],\n",
      "        [0.7953],\n",
      "        [0.4136],\n",
      "        [0.8672],\n",
      "        [0.4107],\n",
      "        [0.6270],\n",
      "        [0.6440]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0051],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0072],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0121],\n",
      "        [0.0125],\n",
      "        [0.0127],\n",
      "        [0.0127],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0146],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0171],\n",
      "        [0.0174],\n",
      "        [0.0177],\n",
      "        [0.0179],\n",
      "        [0.0182],\n",
      "        [0.0182],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0210],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0216],\n",
      "        [0.0216],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0230],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0246],\n",
      "        [0.0247],\n",
      "        [0.0258],\n",
      "        [0.0271],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0294],\n",
      "        [0.0294],\n",
      "        [0.0295],\n",
      "        [0.0301],\n",
      "        [0.0313],\n",
      "        [0.0317],\n",
      "        [0.0319],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0331],\n",
      "        [0.0333],\n",
      "        [0.0340],\n",
      "        [0.0344],\n",
      "        [0.0348],\n",
      "        [0.0351],\n",
      "        [0.0353],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0370],\n",
      "        [0.0370],\n",
      "        [0.0374],\n",
      "        [0.0379],\n",
      "        [0.0408],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0458],\n",
      "        [0.0477],\n",
      "        [0.0481],\n",
      "        [0.0488],\n",
      "        [0.0488],\n",
      "        [0.0499],\n",
      "        [0.0500],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0511],\n",
      "        [0.0513],\n",
      "        [0.0513],\n",
      "        [0.0516],\n",
      "        [0.0520],\n",
      "        [0.0523],\n",
      "        [0.0530],\n",
      "        [0.0533],\n",
      "        [0.0545],\n",
      "        [0.0579],\n",
      "        [0.0579],\n",
      "        [0.0589],\n",
      "        [0.0590],\n",
      "        [0.0631],\n",
      "        [0.0637],\n",
      "        [0.0656],\n",
      "        [0.0679],\n",
      "        [0.0689],\n",
      "        [0.0700],\n",
      "        [0.0779],\n",
      "        [0.0781],\n",
      "        [0.0801],\n",
      "        [0.0805],\n",
      "        [0.0867],\n",
      "        [0.0872],\n",
      "        [0.0916],\n",
      "        [0.0918],\n",
      "        [0.1033],\n",
      "        [0.1049],\n",
      "        [0.1086],\n",
      "        [0.1200],\n",
      "        [0.1246],\n",
      "        [0.1299],\n",
      "        [0.1333],\n",
      "        [0.1336],\n",
      "        [0.1379],\n",
      "        [0.1382],\n",
      "        [0.1460],\n",
      "        [0.1487],\n",
      "        [0.1593],\n",
      "        [0.1833],\n",
      "        [0.2065],\n",
      "        [0.2203]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0031],\n",
      "        [0.0018],\n",
      "        [0.0002],\n",
      "        [0.0007],\n",
      "        [0.0045],\n",
      "        [0.0044],\n",
      "        [0.0004],\n",
      "        [0.0058],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0079],\n",
      "        [0.0084],\n",
      "        [0.0043],\n",
      "        [0.0102],\n",
      "        [0.0058],\n",
      "        [0.0114],\n",
      "        [0.0059],\n",
      "        [0.0114],\n",
      "        [0.0067],\n",
      "        [0.0118],\n",
      "        [0.0129],\n",
      "        [0.0131],\n",
      "        [0.0073],\n",
      "        [0.0116],\n",
      "        [0.0096],\n",
      "        [0.0143],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0109],\n",
      "        [0.0106],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0101],\n",
      "        [0.0109],\n",
      "        [0.0145],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0136],\n",
      "        [0.0124],\n",
      "        [0.0134],\n",
      "        [0.0142],\n",
      "        [0.0161],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0206],\n",
      "        [0.0204],\n",
      "        [0.0212],\n",
      "        [0.0178],\n",
      "        [0.0166],\n",
      "        [0.0217],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0174],\n",
      "        [0.0224],\n",
      "        [0.0180],\n",
      "        [0.0166],\n",
      "        [0.0175],\n",
      "        [0.0195],\n",
      "        [0.0192],\n",
      "        [0.0222],\n",
      "        [0.0231],\n",
      "        [0.0234],\n",
      "        [0.0196],\n",
      "        [0.0228],\n",
      "        [0.0196],\n",
      "        [0.0220],\n",
      "        [0.0203],\n",
      "        [0.0264],\n",
      "        [0.0219],\n",
      "        [0.0247],\n",
      "        [0.0220],\n",
      "        [0.0265],\n",
      "        [0.0260],\n",
      "        [0.0269],\n",
      "        [0.0244],\n",
      "        [0.0311],\n",
      "        [0.0310],\n",
      "        [0.0272],\n",
      "        [0.0258],\n",
      "        [0.0330],\n",
      "        [0.0316],\n",
      "        [0.0285],\n",
      "        [0.0337],\n",
      "        [0.0332],\n",
      "        [0.0354],\n",
      "        [0.0348],\n",
      "        [0.0361],\n",
      "        [0.0354],\n",
      "        [0.0365],\n",
      "        [0.0324],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0375],\n",
      "        [0.0331],\n",
      "        [0.0382],\n",
      "        [0.0382],\n",
      "        [0.0396],\n",
      "        [0.0382],\n",
      "        [0.0363],\n",
      "        [0.0359],\n",
      "        [0.0436],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0445],\n",
      "        [0.0471],\n",
      "        [0.0425],\n",
      "        [0.0486],\n",
      "        [0.0455],\n",
      "        [0.0452],\n",
      "        [0.0452],\n",
      "        [0.0518],\n",
      "        [0.0530],\n",
      "        [0.0512],\n",
      "        [0.0536],\n",
      "        [0.0526],\n",
      "        [0.0481],\n",
      "        [0.0545],\n",
      "        [0.0544],\n",
      "        [0.0500],\n",
      "        [0.0500],\n",
      "        [0.0512],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0555],\n",
      "        [0.0611],\n",
      "        [0.0591],\n",
      "        [0.0620],\n",
      "        [0.0605],\n",
      "        [0.0639],\n",
      "        [0.0661],\n",
      "        [0.0684],\n",
      "        [0.0710],\n",
      "        [0.0718],\n",
      "        [0.0709],\n",
      "        [0.0756],\n",
      "        [0.0802],\n",
      "        [0.0768],\n",
      "        [0.0777],\n",
      "        [0.0843],\n",
      "        [0.0891],\n",
      "        [0.0938],\n",
      "        [0.0894],\n",
      "        [0.1054],\n",
      "        [0.1062],\n",
      "        [0.1056],\n",
      "        [0.1212],\n",
      "        [0.1224],\n",
      "        [0.1264],\n",
      "        [0.1304],\n",
      "        [0.1314],\n",
      "        [0.1358],\n",
      "        [0.1397],\n",
      "        [0.1439],\n",
      "        [0.1461],\n",
      "        [0.1579],\n",
      "        [0.1809],\n",
      "        [0.2029],\n",
      "        [0.2166]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 49.37365651130676\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.10392589805997e-08, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [15, 123, 146, 68, 139, 71, 128, 95, 107, 92, 46, 27, 69, 22, 35, 116, 122, 127, 2, 49, 94, 57, 62, 118, 38, 110, 18, 106, 72, 117, 154, 124, 137, 70, 82, 83, 23, 4, 102, 61, 143, 141, 103, 109, 20, 108, 153, 140, 1, 152, 40, 63, 121, 101, 119, 131, 99, 100, 47, 111, 125, 3, 5, 147, 0, 104, 142, 28, 115, 96, 151, 86, 8, 93, 21, 145, 26, 126, 74, 84, 14, 16, 81, 25, 144, 138, 37, 120, 64, 39, 85, 157, 58, 36, 73, 113, 48, 148, 34, 97, 24, 67, 114, 105, 112, 54, 87, 45, 98, 56, 55, 19, 6, 9, 158, 75, 7, 77, 76, 60, 59, 29, 150, 129, 149, 13, 80, 79, 155, 130, 65, 156, 78, 66, 44, 91, 50, 33, 136, 30, 132, 32, 31, 53, 12, 10, 133, 90, 52, 43, 41, 11, 42, 135, 17, 134, 88, 89, 51] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.9519],\n",
      "        [0.5336],\n",
      "        [0.3642],\n",
      "        [0.6932],\n",
      "        [0.3211],\n",
      "        [0.6619],\n",
      "        [0.4708],\n",
      "        [0.6148],\n",
      "        [0.6172],\n",
      "        [0.5068],\n",
      "        [0.7548],\n",
      "        [0.9154],\n",
      "        [0.6895],\n",
      "        [0.8959],\n",
      "        [0.8690],\n",
      "        [0.5511],\n",
      "        [0.5282],\n",
      "        [0.4965],\n",
      "        [0.8592],\n",
      "        [0.7314],\n",
      "        [0.5614],\n",
      "        [0.6711],\n",
      "        [0.7135],\n",
      "        [0.5456],\n",
      "        [0.8461],\n",
      "        [0.6179],\n",
      "        [0.8542],\n",
      "        [0.6316],\n",
      "        [0.6239],\n",
      "        [0.5725],\n",
      "        [0.3026],\n",
      "        [0.5211],\n",
      "        [0.3032],\n",
      "        [0.6895],\n",
      "        [0.6286],\n",
      "        [0.6323],\n",
      "        [0.9112],\n",
      "        [0.8230],\n",
      "        [0.5949],\n",
      "        [0.6975],\n",
      "        [0.3435],\n",
      "        [0.3232],\n",
      "        [0.6471],\n",
      "        [0.6452],\n",
      "        [0.8637],\n",
      "        [0.6509],\n",
      "        [0.3721],\n",
      "        [0.3258],\n",
      "        [0.8487],\n",
      "        [0.3840],\n",
      "        [0.8362],\n",
      "        [0.7110],\n",
      "        [0.5203],\n",
      "        [0.5841],\n",
      "        [0.5275],\n",
      "        [0.4198],\n",
      "        [0.5981],\n",
      "        [0.5896],\n",
      "        [0.7249],\n",
      "        [0.5950],\n",
      "        [0.5013],\n",
      "        [0.8262],\n",
      "        [0.8419],\n",
      "        [0.4131],\n",
      "        [0.8270],\n",
      "        [0.6543],\n",
      "        [0.3351],\n",
      "        [0.8828],\n",
      "        [0.5672],\n",
      "        [0.6095],\n",
      "        [0.3742],\n",
      "        [0.6147],\n",
      "        [0.8165],\n",
      "        [0.5002],\n",
      "        [0.8259],\n",
      "        [0.3317],\n",
      "        [0.9198],\n",
      "        [0.4994],\n",
      "        [0.6101],\n",
      "        [0.6053],\n",
      "        [0.9353],\n",
      "        [0.9342],\n",
      "        [0.6390],\n",
      "        [0.9005],\n",
      "        [0.3255],\n",
      "        [0.2690],\n",
      "        [0.8008],\n",
      "        [0.4911],\n",
      "        [0.6702],\n",
      "        [0.8295],\n",
      "        [0.5999],\n",
      "        [0.3073],\n",
      "        [0.6803],\n",
      "        [0.8376],\n",
      "        [0.6100],\n",
      "        [0.5903],\n",
      "        [0.7204],\n",
      "        [0.4128],\n",
      "        [0.8451],\n",
      "        [0.5842],\n",
      "        [0.9259],\n",
      "        [0.6615],\n",
      "        [0.5634],\n",
      "        [0.6274],\n",
      "        [0.5995],\n",
      "        [0.6787],\n",
      "        [0.6121],\n",
      "        [0.7910],\n",
      "        [0.5700],\n",
      "        [0.6716],\n",
      "        [0.6067],\n",
      "        [0.8797],\n",
      "        [0.8645],\n",
      "        [0.8123],\n",
      "        [0.3134],\n",
      "        [0.6073],\n",
      "        [0.7874],\n",
      "        [0.6264],\n",
      "        [0.6054],\n",
      "        [0.6798],\n",
      "        [0.6810],\n",
      "        [0.8651],\n",
      "        [0.3810],\n",
      "        [0.4164],\n",
      "        [0.3931],\n",
      "        [0.9077],\n",
      "        [0.6282],\n",
      "        [0.6287],\n",
      "        [0.2476],\n",
      "        [0.3985],\n",
      "        [0.6477],\n",
      "        [0.2544],\n",
      "        [0.6256],\n",
      "        [0.6360],\n",
      "        [0.8218],\n",
      "        [0.5722],\n",
      "        [0.7408],\n",
      "        [0.8400],\n",
      "        [0.3518],\n",
      "        [0.8464],\n",
      "        [0.4158],\n",
      "        [0.8615],\n",
      "        [0.8540],\n",
      "        [0.7199],\n",
      "        [0.8622],\n",
      "        [0.8318],\n",
      "        [0.4091],\n",
      "        [0.5833],\n",
      "        [0.7517],\n",
      "        [0.7899],\n",
      "        [0.8396],\n",
      "        [0.8603],\n",
      "        [0.7932],\n",
      "        [0.4111],\n",
      "        [0.8658],\n",
      "        [0.4083],\n",
      "        [0.6234],\n",
      "        [0.6403],\n",
      "        [0.7404]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0031],\n",
      "        [    0.0043],\n",
      "        [    0.0044],\n",
      "        [    0.0045],\n",
      "        [    0.0058],\n",
      "        [    0.0058],\n",
      "        [    0.0059],\n",
      "        [    0.0067],\n",
      "        [    0.0073],\n",
      "        [    0.0079],\n",
      "        [    0.0084],\n",
      "        [    0.0092],\n",
      "        [    0.0096],\n",
      "        [    0.0097],\n",
      "        [    0.0099],\n",
      "        [    0.0101],\n",
      "        [    0.0101],\n",
      "        [    0.0102],\n",
      "        [    0.0106],\n",
      "        [    0.0109],\n",
      "        [    0.0109],\n",
      "        [    0.0114],\n",
      "        [    0.0114],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0118],\n",
      "        [    0.0123],\n",
      "        [    0.0124],\n",
      "        [    0.0129],\n",
      "        [    0.0131],\n",
      "        [    0.0134],\n",
      "        [    0.0136],\n",
      "        [    0.0142],\n",
      "        [    0.0143],\n",
      "        [    0.0145],\n",
      "        [    0.0161],\n",
      "        [    0.0166],\n",
      "        [    0.0166],\n",
      "        [    0.0174],\n",
      "        [    0.0175],\n",
      "        [    0.0178],\n",
      "        [    0.0180],\n",
      "        [    0.0192],\n",
      "        [    0.0195],\n",
      "        [    0.0196],\n",
      "        [    0.0196],\n",
      "        [    0.0202],\n",
      "        [    0.0203],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0206],\n",
      "        [    0.0212],\n",
      "        [    0.0213],\n",
      "        [    0.0216],\n",
      "        [    0.0217],\n",
      "        [    0.0219],\n",
      "        [    0.0220],\n",
      "        [    0.0220],\n",
      "        [    0.0222],\n",
      "        [    0.0224],\n",
      "        [    0.0228],\n",
      "        [    0.0231],\n",
      "        [    0.0234],\n",
      "        [    0.0244],\n",
      "        [    0.0247],\n",
      "        [    0.0258],\n",
      "        [    0.0260],\n",
      "        [    0.0264],\n",
      "        [    0.0265],\n",
      "        [    0.0269],\n",
      "        [    0.0272],\n",
      "        [    0.0285],\n",
      "        [    0.0310],\n",
      "        [    0.0311],\n",
      "        [    0.0316],\n",
      "        [    0.0324],\n",
      "        [    0.0330],\n",
      "        [    0.0331],\n",
      "        [    0.0332],\n",
      "        [    0.0337],\n",
      "        [    0.0348],\n",
      "        [    0.0354],\n",
      "        [    0.0354],\n",
      "        [    0.0359],\n",
      "        [    0.0361],\n",
      "        [    0.0363],\n",
      "        [    0.0365],\n",
      "        [    0.0367],\n",
      "        [    0.0375],\n",
      "        [    0.0375],\n",
      "        [    0.0382],\n",
      "        [    0.0382],\n",
      "        [    0.0382],\n",
      "        [    0.0396],\n",
      "        [    0.0425],\n",
      "        [    0.0436],\n",
      "        [    0.0438],\n",
      "        [    0.0443],\n",
      "        [    0.0445],\n",
      "        [    0.0452],\n",
      "        [    0.0452],\n",
      "        [    0.0455],\n",
      "        [    0.0471],\n",
      "        [    0.0481],\n",
      "        [    0.0486],\n",
      "        [    0.0500],\n",
      "        [    0.0500],\n",
      "        [    0.0512],\n",
      "        [    0.0512],\n",
      "        [    0.0518],\n",
      "        [    0.0526],\n",
      "        [    0.0530],\n",
      "        [    0.0536],\n",
      "        [    0.0544],\n",
      "        [    0.0545],\n",
      "        [    0.0549],\n",
      "        [    0.0555],\n",
      "        [    0.0555],\n",
      "        [    0.0591],\n",
      "        [    0.0605],\n",
      "        [    0.0611],\n",
      "        [    0.0620],\n",
      "        [    0.0639],\n",
      "        [    0.0661],\n",
      "        [    0.0684],\n",
      "        [    0.0709],\n",
      "        [    0.0710],\n",
      "        [    0.0718],\n",
      "        [    0.0756],\n",
      "        [    0.0768],\n",
      "        [    0.0777],\n",
      "        [    0.0802],\n",
      "        [    0.0843],\n",
      "        [    0.0891],\n",
      "        [    0.0894],\n",
      "        [    0.0938],\n",
      "        [    0.1054],\n",
      "        [    0.1056],\n",
      "        [    0.1062],\n",
      "        [    0.1212],\n",
      "        [    0.1224],\n",
      "        [    0.1264],\n",
      "        [    0.1304],\n",
      "        [    0.1314],\n",
      "        [    0.1358],\n",
      "        [    0.1397],\n",
      "        [    0.1439],\n",
      "        [    0.1461],\n",
      "        [    0.1579],\n",
      "        [    0.1809],\n",
      "        [    0.2029],\n",
      "        [    0.2166],\n",
      "        [    0.2793]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0022],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0059],\n",
      "        [0.0021],\n",
      "        [0.0019],\n",
      "        [0.0034],\n",
      "        [0.0028],\n",
      "        [0.0050],\n",
      "        [0.0062],\n",
      "        [0.0097],\n",
      "        [0.0032],\n",
      "        [0.0017],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0080],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0084],\n",
      "        [0.0053],\n",
      "        [0.0085],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0103],\n",
      "        [0.0079],\n",
      "        [0.0106],\n",
      "        [0.0096],\n",
      "        [0.0121],\n",
      "        [0.0151],\n",
      "        [0.0112],\n",
      "        [0.0109],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0081],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0106],\n",
      "        [0.0121],\n",
      "        [0.0130],\n",
      "        [0.0189],\n",
      "        [0.0141],\n",
      "        [0.0166],\n",
      "        [0.0151],\n",
      "        [0.0161],\n",
      "        [0.0158],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0181],\n",
      "        [0.0197],\n",
      "        [0.0169],\n",
      "        [0.0150],\n",
      "        [0.0205],\n",
      "        [0.0193],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0254],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0204],\n",
      "        [0.0200],\n",
      "        [0.0220],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0221],\n",
      "        [0.0254],\n",
      "        [0.0235],\n",
      "        [0.0230],\n",
      "        [0.0247],\n",
      "        [0.0228],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0265],\n",
      "        [0.0246],\n",
      "        [0.0278],\n",
      "        [0.0344],\n",
      "        [0.0341],\n",
      "        [0.0336],\n",
      "        [0.0304],\n",
      "        [0.0365],\n",
      "        [0.0305],\n",
      "        [0.0328],\n",
      "        [0.0332],\n",
      "        [0.0375],\n",
      "        [0.0353],\n",
      "        [0.0398],\n",
      "        [0.0334],\n",
      "        [0.0391],\n",
      "        [0.0367],\n",
      "        [0.0414],\n",
      "        [0.0396],\n",
      "        [0.0408],\n",
      "        [0.0377],\n",
      "        [0.0426],\n",
      "        [0.0384],\n",
      "        [0.0411],\n",
      "        [0.0409],\n",
      "        [0.0396],\n",
      "        [0.0474],\n",
      "        [0.0438],\n",
      "        [0.0452],\n",
      "        [0.0448],\n",
      "        [0.0406],\n",
      "        [0.0423],\n",
      "        [0.0414],\n",
      "        [0.0484],\n",
      "        [0.0437],\n",
      "        [0.0525],\n",
      "        [0.0485],\n",
      "        [0.0478],\n",
      "        [0.0527],\n",
      "        [0.0515],\n",
      "        [0.0551],\n",
      "        [0.0542],\n",
      "        [0.0565],\n",
      "        [0.0568],\n",
      "        [0.0590],\n",
      "        [0.0594],\n",
      "        [0.0568],\n",
      "        [0.0556],\n",
      "        [0.0554],\n",
      "        [0.0593],\n",
      "        [0.0624],\n",
      "        [0.0644],\n",
      "        [0.0653],\n",
      "        [0.0630],\n",
      "        [0.0659],\n",
      "        [0.0723],\n",
      "        [0.0702],\n",
      "        [0.0743],\n",
      "        [0.0755],\n",
      "        [0.0715],\n",
      "        [0.0745],\n",
      "        [0.0730],\n",
      "        [0.0828],\n",
      "        [0.0845],\n",
      "        [0.0910],\n",
      "        [0.0895],\n",
      "        [0.0964],\n",
      "        [0.1078],\n",
      "        [0.1007],\n",
      "        [0.1079],\n",
      "        [0.1228],\n",
      "        [0.1225],\n",
      "        [0.1237],\n",
      "        [0.1254],\n",
      "        [0.1278],\n",
      "        [0.1326],\n",
      "        [0.1415],\n",
      "        [0.1406],\n",
      "        [0.1459],\n",
      "        [0.1565],\n",
      "        [0.1809],\n",
      "        [0.2001],\n",
      "        [0.2137],\n",
      "        [0.2745]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 49.660956144332886\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 8 個區塊累積花費時間(s) 1.208634614944458\n",
      "<<The performance of 8 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.208634614944458\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1123.71\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1595.35\n",
      "The accuracy(2000) for l = 1: 86.16%\n",
      "The accuracy(3000) for l = 1: 91.19%\n",
      "The maximum error: tensor(7008.8359)\n",
      "The minimum error: tensor(20.1992)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 3695.3\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 3726.1\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "The maximum error: 4445.53125\n",
      "The minimum error: 3116.3828125\n",
      "------------------------------------------------------------\n",
      "0.8616352201257862\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<9>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.258905500544643e-07, 119)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [119, 142, 65, 67, 135, 11, 91, 23, 124, 31, 18, 103, 45, 53, 58, 64, 88, 34, 112, 66, 90, 118, 123, 14, 42, 114, 19, 106, 150, 113, 102, 0, 120, 133, 98, 139, 59, 68, 99, 16, 105, 78, 79, 137, 36, 104, 149, 136, 57, 97, 148, 1, 115, 117, 127, 107, 95, 143, 138, 121, 96, 82, 92, 111, 100, 22, 147, 43, 24, 141, 4, 89, 122, 17, 12, 21, 140, 134, 35, 10, 80, 70, 116, 77, 153, 33, 109, 144, 81, 20, 32, 60, 50, 69, 93, 30, 54, 41, 83, 44, 52, 110, 108, 101, 63, 2, 94, 15, 154, 51, 5, 3, 71, 125, 146, 73, 72, 25, 56, 145, 55, 9, 151, 76, 75, 126, 152, 40, 61, 46, 74, 87, 62, 29, 132, 128, 26, 28, 49, 27, 8, 155, 129, 6, 86, 48, 39, 37, 38, 158, 7, 156, 131, 13, 157]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0059],\n",
      "        [0.0062],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0121],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0141],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0158],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0246],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0265],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0304],\n",
      "        [0.0305],\n",
      "        [0.0328],\n",
      "        [0.0332],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0341],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0365],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0384],\n",
      "        [0.0391],\n",
      "        [0.0396],\n",
      "        [0.0396],\n",
      "        [0.0398],\n",
      "        [0.0406],\n",
      "        [0.0408],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0414],\n",
      "        [0.0414],\n",
      "        [0.0423],\n",
      "        [0.0426],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0448],\n",
      "        [0.0452],\n",
      "        [0.0474],\n",
      "        [0.0478],\n",
      "        [0.0484],\n",
      "        [0.0485],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0542],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0565],\n",
      "        [0.0568],\n",
      "        [0.0568],\n",
      "        [0.0590],\n",
      "        [0.0593],\n",
      "        [0.0594],\n",
      "        [0.0624],\n",
      "        [0.0630],\n",
      "        [0.0644],\n",
      "        [0.0653],\n",
      "        [0.0659],\n",
      "        [0.0702],\n",
      "        [0.0715],\n",
      "        [0.0723],\n",
      "        [0.0730],\n",
      "        [0.0743],\n",
      "        [0.0745],\n",
      "        [0.0755],\n",
      "        [0.0828],\n",
      "        [0.0845],\n",
      "        [0.0895],\n",
      "        [0.0910],\n",
      "        [0.0964],\n",
      "        [0.1007],\n",
      "        [0.1078],\n",
      "        [0.1079],\n",
      "        [0.1221],\n",
      "        [0.1225],\n",
      "        [0.1228],\n",
      "        [0.1237],\n",
      "        [0.1254],\n",
      "        [0.1278],\n",
      "        [0.1326],\n",
      "        [0.1406],\n",
      "        [0.1413],\n",
      "        [0.1415],\n",
      "        [0.1415],\n",
      "        [0.1459],\n",
      "        [0.1565],\n",
      "        [0.1741]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.258905500544643e-07, 119)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [119, 142, 65, 67, 135, 11, 91, 23, 124, 31, 18, 103, 45, 53, 58, 64, 88, 34, 112, 66, 90, 118, 123, 14, 42, 114, 19, 106, 150, 113, 102, 0, 120, 133, 98, 139, 59, 68, 99, 16, 105, 78, 79, 137, 36, 104, 149, 136, 57, 97, 148, 1, 115, 117, 127, 107, 95, 143, 138, 121, 96, 82, 92, 111, 100, 22, 147, 43, 24, 141, 4, 89, 122, 17, 12, 21, 140, 134, 35, 10, 80, 70, 116, 77, 153, 33, 109, 144, 81, 20, 32, 60, 50, 69, 93, 30, 54, 41, 83, 44, 52, 110, 108, 101, 63, 2, 94, 15, 154, 51, 5, 3, 71, 125, 146, 73, 72, 25, 56, 145, 55, 9, 151, 76, 75, 126, 152, 40, 61, 46, 74, 87, 62, 29, 132, 128, 26, 28, 49, 27, 8, 155, 129, 6, 86, 48, 39, 37, 38, 158, 7, 156, 131, 13, 157, 130] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5332],\n",
      "        [0.3646],\n",
      "        [0.6853],\n",
      "        [0.6579],\n",
      "        [0.3213],\n",
      "        [0.9499],\n",
      "        [0.6133],\n",
      "        [0.9128],\n",
      "        [0.4706],\n",
      "        [0.8658],\n",
      "        [0.8934],\n",
      "        [0.6166],\n",
      "        [0.7270],\n",
      "        [0.6665],\n",
      "        [0.7089],\n",
      "        [0.6891],\n",
      "        [0.5051],\n",
      "        [0.8434],\n",
      "        [0.5510],\n",
      "        [0.6853],\n",
      "        [0.5600],\n",
      "        [0.5279],\n",
      "        [0.4960],\n",
      "        [0.8528],\n",
      "        [0.7508],\n",
      "        [0.5455],\n",
      "        [0.9084],\n",
      "        [0.6176],\n",
      "        [0.3033],\n",
      "        [0.5722],\n",
      "        [0.6309],\n",
      "        [0.8216],\n",
      "        [0.5207],\n",
      "        [0.3035],\n",
      "        [0.5937],\n",
      "        [0.3439],\n",
      "        [0.7064],\n",
      "        [0.6203],\n",
      "        [0.6456],\n",
      "        [0.8620],\n",
      "        [0.6447],\n",
      "        [0.6252],\n",
      "        [0.6290],\n",
      "        [0.3237],\n",
      "        [0.8335],\n",
      "        [0.6502],\n",
      "        [0.3723],\n",
      "        [0.3262],\n",
      "        [0.6929],\n",
      "        [0.5831],\n",
      "        [0.3842],\n",
      "        [0.8400],\n",
      "        [0.5277],\n",
      "        [0.5201],\n",
      "        [0.4198],\n",
      "        [0.5947],\n",
      "        [0.5967],\n",
      "        [0.4131],\n",
      "        [0.3358],\n",
      "        [0.5009],\n",
      "        [0.5884],\n",
      "        [0.6116],\n",
      "        [0.6081],\n",
      "        [0.5671],\n",
      "        [0.6531],\n",
      "        [0.9172],\n",
      "        [0.3743],\n",
      "        [0.7209],\n",
      "        [0.8805],\n",
      "        [0.3321],\n",
      "        [0.8149],\n",
      "        [0.4988],\n",
      "        [0.4988],\n",
      "        [0.8242],\n",
      "        [0.9323],\n",
      "        [0.8979],\n",
      "        [0.3259],\n",
      "        [0.2695],\n",
      "        [0.8270],\n",
      "        [0.9333],\n",
      "        [0.6023],\n",
      "        [0.6068],\n",
      "        [0.4912],\n",
      "        [0.6356],\n",
      "        [0.3077],\n",
      "        [0.7981],\n",
      "        [0.5901],\n",
      "        [0.4126],\n",
      "        [0.5969],\n",
      "        [0.9231],\n",
      "        [0.8347],\n",
      "        [0.6658],\n",
      "        [0.6741],\n",
      "        [0.6067],\n",
      "        [0.5829],\n",
      "        [0.8422],\n",
      "        [0.6754],\n",
      "        [0.7870],\n",
      "        [0.6092],\n",
      "        [0.7160],\n",
      "        [0.6673],\n",
      "        [0.5635],\n",
      "        [0.5991],\n",
      "        [0.6265],\n",
      "        [0.6576],\n",
      "        [0.8623],\n",
      "        [0.5686],\n",
      "        [0.8781],\n",
      "        [0.3137],\n",
      "        [0.6029],\n",
      "        [0.8108],\n",
      "        [0.7858],\n",
      "        [0.6040],\n",
      "        [0.4165],\n",
      "        [0.3809],\n",
      "        [0.6229],\n",
      "        [0.6022],\n",
      "        [0.8631],\n",
      "        [0.6752],\n",
      "        [0.3929],\n",
      "        [0.6761],\n",
      "        [0.9058],\n",
      "        [0.2484],\n",
      "        [0.6249],\n",
      "        [0.6254],\n",
      "        [0.3986],\n",
      "        [0.2551],\n",
      "        [0.8177],\n",
      "        [0.6438],\n",
      "        [0.7360],\n",
      "        [0.6223],\n",
      "        [0.5698],\n",
      "        [0.6323],\n",
      "        [0.8375],\n",
      "        [0.3519],\n",
      "        [0.4159],\n",
      "        [0.8445],\n",
      "        [0.8588],\n",
      "        [0.7150],\n",
      "        [0.8516],\n",
      "        [0.8605],\n",
      "        [0.3422],\n",
      "        [0.4092],\n",
      "        [0.8302],\n",
      "        [0.5806],\n",
      "        [0.7468],\n",
      "        [0.7863],\n",
      "        [0.8365],\n",
      "        [0.7898],\n",
      "        [0.3289],\n",
      "        [0.8585],\n",
      "        [0.3324],\n",
      "        [0.4108],\n",
      "        [0.8644],\n",
      "        [0.3369],\n",
      "        [0.4083]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0059],\n",
      "        [0.0062],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0121],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0141],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0158],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0246],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0265],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0304],\n",
      "        [0.0305],\n",
      "        [0.0328],\n",
      "        [0.0332],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0341],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0365],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0384],\n",
      "        [0.0391],\n",
      "        [0.0396],\n",
      "        [0.0396],\n",
      "        [0.0398],\n",
      "        [0.0406],\n",
      "        [0.0408],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0414],\n",
      "        [0.0414],\n",
      "        [0.0423],\n",
      "        [0.0426],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0448],\n",
      "        [0.0452],\n",
      "        [0.0474],\n",
      "        [0.0478],\n",
      "        [0.0484],\n",
      "        [0.0485],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0542],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0565],\n",
      "        [0.0568],\n",
      "        [0.0568],\n",
      "        [0.0590],\n",
      "        [0.0593],\n",
      "        [0.0594],\n",
      "        [0.0624],\n",
      "        [0.0630],\n",
      "        [0.0644],\n",
      "        [0.0653],\n",
      "        [0.0659],\n",
      "        [0.0702],\n",
      "        [0.0715],\n",
      "        [0.0723],\n",
      "        [0.0730],\n",
      "        [0.0743],\n",
      "        [0.0745],\n",
      "        [0.0755],\n",
      "        [0.0828],\n",
      "        [0.0845],\n",
      "        [0.0895],\n",
      "        [0.0910],\n",
      "        [0.0964],\n",
      "        [0.1007],\n",
      "        [0.1078],\n",
      "        [0.1079],\n",
      "        [0.1221],\n",
      "        [0.1225],\n",
      "        [0.1228],\n",
      "        [0.1237],\n",
      "        [0.1254],\n",
      "        [0.1278],\n",
      "        [0.1326],\n",
      "        [0.1406],\n",
      "        [0.1413],\n",
      "        [0.1415],\n",
      "        [0.1415],\n",
      "        [0.1459],\n",
      "        [0.1565],\n",
      "        [0.1741],\n",
      "        [0.1809]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0105],\n",
      "        [0.0074],\n",
      "        [0.0039],\n",
      "        [0.0104],\n",
      "        [0.0097],\n",
      "        [0.0062],\n",
      "        [0.0141],\n",
      "        [0.0057],\n",
      "        [0.0152],\n",
      "        [0.0150],\n",
      "        [0.0002],\n",
      "        [0.0115],\n",
      "        [0.0105],\n",
      "        [0.0120],\n",
      "        [0.0006],\n",
      "        [0.0057],\n",
      "        [0.0197],\n",
      "        [0.0060],\n",
      "        [0.0142],\n",
      "        [0.0107],\n",
      "        [0.0091],\n",
      "        [0.0071],\n",
      "        [0.0200],\n",
      "        [0.0034],\n",
      "        [0.0087],\n",
      "        [0.0217],\n",
      "        [0.0154],\n",
      "        [0.0235],\n",
      "        [0.0137],\n",
      "        [0.0069],\n",
      "        [0.0201],\n",
      "        [0.0135],\n",
      "        [0.0021],\n",
      "        [0.0171],\n",
      "        [0.0264],\n",
      "        [0.0215],\n",
      "        [0.0102],\n",
      "        [0.0207],\n",
      "        [0.0259],\n",
      "        [0.0215],\n",
      "        [0.0110],\n",
      "        [0.0109],\n",
      "        [0.0030],\n",
      "        [0.0283],\n",
      "        [0.0224],\n",
      "        [0.0065],\n",
      "        [0.0053],\n",
      "        [0.0128],\n",
      "        [0.0229],\n",
      "        [0.0081],\n",
      "        [0.0285],\n",
      "        [0.0194],\n",
      "        [0.0210],\n",
      "        [0.0252],\n",
      "        [0.0176],\n",
      "        [0.0181],\n",
      "        [0.0105],\n",
      "        [0.0346],\n",
      "        [0.0238],\n",
      "        [0.0187],\n",
      "        [0.0275],\n",
      "        [0.0262],\n",
      "        [0.0208],\n",
      "        [0.0179],\n",
      "        [0.0352],\n",
      "        [0.0365],\n",
      "        [0.0195],\n",
      "        [0.0150],\n",
      "        [0.0388],\n",
      "        [0.0205],\n",
      "        [0.0277],\n",
      "        [0.0263],\n",
      "        [0.0194],\n",
      "        [0.0421],\n",
      "        [0.0407],\n",
      "        [0.0453],\n",
      "        [0.0463],\n",
      "        [0.0447],\n",
      "        [0.0223],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0363],\n",
      "        [0.0313],\n",
      "        [0.0237],\n",
      "        [0.0273],\n",
      "        [0.0340],\n",
      "        [0.0498],\n",
      "        [0.0343],\n",
      "        [0.0507],\n",
      "        [0.0291],\n",
      "        [0.0344],\n",
      "        [0.0451],\n",
      "        [0.0364],\n",
      "        [0.0379],\n",
      "        [0.0306],\n",
      "        [0.0366],\n",
      "        [0.0487],\n",
      "        [0.0469],\n",
      "        [0.0362],\n",
      "        [0.0487],\n",
      "        [0.0408],\n",
      "        [0.0407],\n",
      "        [0.0402],\n",
      "        [0.0428],\n",
      "        [0.0568],\n",
      "        [0.0456],\n",
      "        [0.0592],\n",
      "        [0.0383],\n",
      "        [0.0494],\n",
      "        [0.0452],\n",
      "        [0.0474],\n",
      "        [0.0506],\n",
      "        [0.0596],\n",
      "        [0.0673],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0464],\n",
      "        [0.0538],\n",
      "        [0.0706],\n",
      "        [0.0542],\n",
      "        [0.0521],\n",
      "        [0.0769],\n",
      "        [0.0595],\n",
      "        [0.0607],\n",
      "        [0.0708],\n",
      "        [0.0839],\n",
      "        [0.0799],\n",
      "        [0.0676],\n",
      "        [0.0795],\n",
      "        [0.0700],\n",
      "        [0.0768],\n",
      "        [0.0711],\n",
      "        [0.0727],\n",
      "        [0.0761],\n",
      "        [0.0847],\n",
      "        [0.0808],\n",
      "        [0.0854],\n",
      "        [0.1066],\n",
      "        [0.0970],\n",
      "        [0.0986],\n",
      "        [0.1097],\n",
      "        [0.1165],\n",
      "        [0.1152],\n",
      "        [0.1262],\n",
      "        [0.1323],\n",
      "        [0.1359],\n",
      "        [0.1432],\n",
      "        [0.1489],\n",
      "        [0.1285],\n",
      "        [0.1327],\n",
      "        [0.1293],\n",
      "        [0.1394],\n",
      "        [0.1672],\n",
      "        [0.1618],\n",
      "        [0.1745]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 50.18230700492859\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.688172967031278e-08, 103)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [103, 64, 119, 133, 137, 42, 67, 136, 124, 88, 112, 91, 149, 102, 123, 65, 148, 114, 118, 11, 68, 135, 53, 142, 143, 90, 79, 78, 45, 58, 57, 120, 113, 23, 66, 24, 18, 31, 106, 98, 107, 100, 95, 96, 17, 115, 43, 34, 14, 0, 4, 99, 111, 117, 59, 105, 19, 10, 104, 97, 150, 153, 121, 127, 16, 92, 122, 139, 33, 82, 89, 36, 1, 32, 80, 70, 30, 77, 109, 81, 60, 138, 22, 44, 116, 69, 147, 54, 93, 154, 141, 101, 108, 21, 110, 12, 63, 35, 50, 5, 140, 94, 134, 25, 83, 3, 41, 52, 51, 144, 71, 20, 9, 73, 72, 56, 55, 2, 15, 76, 125, 75, 146, 61, 74, 145, 126, 62, 29, 132, 87, 151, 46, 40, 26, 152, 128, 28, 27, 8, 49, 155, 6, 129, 86, 158, 156, 48, 7, 39, 131, 37, 38, 157, 13, 130, 84] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6214],\n",
      "        [0.6944],\n",
      "        [0.5328],\n",
      "        [0.2930],\n",
      "        [0.3101],\n",
      "        [0.7572],\n",
      "        [0.6636],\n",
      "        [0.3131],\n",
      "        [0.4683],\n",
      "        [0.5056],\n",
      "        [0.5530],\n",
      "        [0.6167],\n",
      "        [0.3608],\n",
      "        [0.6361],\n",
      "        [0.4943],\n",
      "        [0.6910],\n",
      "        [0.3726],\n",
      "        [0.5471],\n",
      "        [0.5275],\n",
      "        [0.9619],\n",
      "        [0.6251],\n",
      "        [0.3088],\n",
      "        [0.6715],\n",
      "        [0.3530],\n",
      "        [0.4016],\n",
      "        [0.5621],\n",
      "        [0.6345],\n",
      "        [0.6306],\n",
      "        [0.7333],\n",
      "        [0.7154],\n",
      "        [0.6989],\n",
      "        [0.5195],\n",
      "        [0.5747],\n",
      "        [0.9237],\n",
      "        [0.6914],\n",
      "        [0.8909],\n",
      "        [0.9042],\n",
      "        [0.8769],\n",
      "        [0.6224],\n",
      "        [0.5978],\n",
      "        [0.5990],\n",
      "        [0.6588],\n",
      "        [0.6006],\n",
      "        [0.5921],\n",
      "        [0.8330],\n",
      "        [0.5285],\n",
      "        [0.7268],\n",
      "        [0.8552],\n",
      "        [0.8632],\n",
      "        [0.8295],\n",
      "        [0.8221],\n",
      "        [0.6513],\n",
      "        [0.5699],\n",
      "        [0.5196],\n",
      "        [0.7129],\n",
      "        [0.6502],\n",
      "        [0.9195],\n",
      "        [0.9447],\n",
      "        [0.6557],\n",
      "        [0.5867],\n",
      "        [0.2907],\n",
      "        [0.2947],\n",
      "        [0.4992],\n",
      "        [0.4152],\n",
      "        [0.8721],\n",
      "        [0.6113],\n",
      "        [0.4972],\n",
      "        [0.3316],\n",
      "        [0.8084],\n",
      "        [0.6163],\n",
      "        [0.4988],\n",
      "        [0.8449],\n",
      "        [0.8485],\n",
      "        [0.8453],\n",
      "        [0.6073],\n",
      "        [0.6114],\n",
      "        [0.8527],\n",
      "        [0.6407],\n",
      "        [0.5938],\n",
      "        [0.6017],\n",
      "        [0.6712],\n",
      "        [0.3233],\n",
      "        [0.9278],\n",
      "        [0.7224],\n",
      "        [0.4902],\n",
      "        [0.6110],\n",
      "        [0.3624],\n",
      "        [0.6801],\n",
      "        [0.5860],\n",
      "        [0.3005],\n",
      "        [0.3197],\n",
      "        [0.6316],\n",
      "        [0.6033],\n",
      "        [0.9081],\n",
      "        [0.5664],\n",
      "        [0.9440],\n",
      "        [0.6623],\n",
      "        [0.8383],\n",
      "        [0.6786],\n",
      "        [0.8183],\n",
      "        [0.3135],\n",
      "        [0.5715],\n",
      "        [0.2564],\n",
      "        [0.8735],\n",
      "        [0.6138],\n",
      "        [0.7926],\n",
      "        [0.7942],\n",
      "        [0.6723],\n",
      "        [0.6059],\n",
      "        [0.4012],\n",
      "        [0.6086],\n",
      "        [0.9342],\n",
      "        [0.9161],\n",
      "        [0.6271],\n",
      "        [0.6065],\n",
      "        [0.6805],\n",
      "        [0.6813],\n",
      "        [0.8713],\n",
      "        [0.8889],\n",
      "        [0.6297],\n",
      "        [0.4124],\n",
      "        [0.6300],\n",
      "        [0.3692],\n",
      "        [0.6485],\n",
      "        [0.6266],\n",
      "        [0.3815],\n",
      "        [0.3938],\n",
      "        [0.6367],\n",
      "        [0.8476],\n",
      "        [0.3436],\n",
      "        [0.5721],\n",
      "        [0.2345],\n",
      "        [0.7426],\n",
      "        [0.8261],\n",
      "        [0.8547],\n",
      "        [0.2414],\n",
      "        [0.4110],\n",
      "        [0.8699],\n",
      "        [0.8624],\n",
      "        [0.8699],\n",
      "        [0.7209],\n",
      "        [0.3299],\n",
      "        [0.8378],\n",
      "        [0.4033],\n",
      "        [0.5831],\n",
      "        [0.3161],\n",
      "        [0.3202],\n",
      "        [0.7536],\n",
      "        [0.8673],\n",
      "        [0.7943],\n",
      "        [0.4044],\n",
      "        [0.8470],\n",
      "        [0.7982],\n",
      "        [0.3245],\n",
      "        [0.8751],\n",
      "        [0.4019],\n",
      "        [0.6251]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0012],\n",
      "        [    0.0021],\n",
      "        [    0.0030],\n",
      "        [    0.0034],\n",
      "        [    0.0039],\n",
      "        [    0.0053],\n",
      "        [    0.0057],\n",
      "        [    0.0057],\n",
      "        [    0.0060],\n",
      "        [    0.0062],\n",
      "        [    0.0065],\n",
      "        [    0.0069],\n",
      "        [    0.0071],\n",
      "        [    0.0074],\n",
      "        [    0.0081],\n",
      "        [    0.0087],\n",
      "        [    0.0091],\n",
      "        [    0.0097],\n",
      "        [    0.0102],\n",
      "        [    0.0104],\n",
      "        [    0.0105],\n",
      "        [    0.0105],\n",
      "        [    0.0105],\n",
      "        [    0.0107],\n",
      "        [    0.0109],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0120],\n",
      "        [    0.0128],\n",
      "        [    0.0135],\n",
      "        [    0.0137],\n",
      "        [    0.0141],\n",
      "        [    0.0142],\n",
      "        [    0.0150],\n",
      "        [    0.0150],\n",
      "        [    0.0152],\n",
      "        [    0.0154],\n",
      "        [    0.0171],\n",
      "        [    0.0176],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0187],\n",
      "        [    0.0194],\n",
      "        [    0.0194],\n",
      "        [    0.0195],\n",
      "        [    0.0197],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0205],\n",
      "        [    0.0207],\n",
      "        [    0.0208],\n",
      "        [    0.0210],\n",
      "        [    0.0215],\n",
      "        [    0.0215],\n",
      "        [    0.0217],\n",
      "        [    0.0223],\n",
      "        [    0.0224],\n",
      "        [    0.0229],\n",
      "        [    0.0235],\n",
      "        [    0.0237],\n",
      "        [    0.0238],\n",
      "        [    0.0252],\n",
      "        [    0.0259],\n",
      "        [    0.0262],\n",
      "        [    0.0263],\n",
      "        [    0.0264],\n",
      "        [    0.0273],\n",
      "        [    0.0275],\n",
      "        [    0.0277],\n",
      "        [    0.0283],\n",
      "        [    0.0285],\n",
      "        [    0.0291],\n",
      "        [    0.0291],\n",
      "        [    0.0297],\n",
      "        [    0.0306],\n",
      "        [    0.0313],\n",
      "        [    0.0340],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0346],\n",
      "        [    0.0352],\n",
      "        [    0.0362],\n",
      "        [    0.0363],\n",
      "        [    0.0364],\n",
      "        [    0.0365],\n",
      "        [    0.0366],\n",
      "        [    0.0379],\n",
      "        [    0.0383],\n",
      "        [    0.0388],\n",
      "        [    0.0402],\n",
      "        [    0.0407],\n",
      "        [    0.0407],\n",
      "        [    0.0408],\n",
      "        [    0.0421],\n",
      "        [    0.0428],\n",
      "        [    0.0447],\n",
      "        [    0.0451],\n",
      "        [    0.0452],\n",
      "        [    0.0453],\n",
      "        [    0.0456],\n",
      "        [    0.0463],\n",
      "        [    0.0464],\n",
      "        [    0.0469],\n",
      "        [    0.0474],\n",
      "        [    0.0487],\n",
      "        [    0.0487],\n",
      "        [    0.0494],\n",
      "        [    0.0498],\n",
      "        [    0.0506],\n",
      "        [    0.0507],\n",
      "        [    0.0521],\n",
      "        [    0.0523],\n",
      "        [    0.0525],\n",
      "        [    0.0538],\n",
      "        [    0.0542],\n",
      "        [    0.0568],\n",
      "        [    0.0592],\n",
      "        [    0.0595],\n",
      "        [    0.0596],\n",
      "        [    0.0607],\n",
      "        [    0.0673],\n",
      "        [    0.0676],\n",
      "        [    0.0700],\n",
      "        [    0.0706],\n",
      "        [    0.0708],\n",
      "        [    0.0711],\n",
      "        [    0.0727],\n",
      "        [    0.0761],\n",
      "        [    0.0768],\n",
      "        [    0.0769],\n",
      "        [    0.0795],\n",
      "        [    0.0799],\n",
      "        [    0.0808],\n",
      "        [    0.0839],\n",
      "        [    0.0847],\n",
      "        [    0.0854],\n",
      "        [    0.0970],\n",
      "        [    0.0986],\n",
      "        [    0.1066],\n",
      "        [    0.1097],\n",
      "        [    0.1152],\n",
      "        [    0.1165],\n",
      "        [    0.1262],\n",
      "        [    0.1285],\n",
      "        [    0.1293],\n",
      "        [    0.1323],\n",
      "        [    0.1327],\n",
      "        [    0.1359],\n",
      "        [    0.1394],\n",
      "        [    0.1432],\n",
      "        [    0.1489],\n",
      "        [    0.1618],\n",
      "        [    0.1672],\n",
      "        [    0.1745],\n",
      "        [    0.2046]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0032],\n",
      "        [0.0032],\n",
      "        [0.0007],\n",
      "        [0.0010],\n",
      "        [0.0066],\n",
      "        [0.0020],\n",
      "        [0.0013],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0071],\n",
      "        [0.0041],\n",
      "        [0.0028],\n",
      "        [0.0081],\n",
      "        [0.0051],\n",
      "        [0.0050],\n",
      "        [0.0043],\n",
      "        [0.0099],\n",
      "        [0.0110],\n",
      "        [0.0061],\n",
      "        [0.0118],\n",
      "        [0.0142],\n",
      "        [0.0080],\n",
      "        [0.0141],\n",
      "        [0.0062],\n",
      "        [0.0088],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0124],\n",
      "        [0.0103],\n",
      "        [0.0120],\n",
      "        [0.0184],\n",
      "        [0.0112],\n",
      "        [0.0123],\n",
      "        [0.0142],\n",
      "        [0.0157],\n",
      "        [0.0187],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0229],\n",
      "        [0.0206],\n",
      "        [0.0224],\n",
      "        [0.0176],\n",
      "        [0.0171],\n",
      "        [0.0169],\n",
      "        [0.0239],\n",
      "        [0.0192],\n",
      "        [0.0219],\n",
      "        [0.0228],\n",
      "        [0.0191],\n",
      "        [0.0202],\n",
      "        [0.0178],\n",
      "        [0.0258],\n",
      "        [0.0211],\n",
      "        [0.0216],\n",
      "        [0.0266],\n",
      "        [0.0199],\n",
      "        [0.0259],\n",
      "        [0.0271],\n",
      "        [0.0225],\n",
      "        [0.0242],\n",
      "        [0.0242],\n",
      "        [0.0301],\n",
      "        [0.0295],\n",
      "        [0.0257],\n",
      "        [0.0298],\n",
      "        [0.0263],\n",
      "        [0.0252],\n",
      "        [0.0318],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0333],\n",
      "        [0.0332],\n",
      "        [0.0351],\n",
      "        [0.0358],\n",
      "        [0.0366],\n",
      "        [0.0381],\n",
      "        [0.0313],\n",
      "        [0.0384],\n",
      "        [0.0377],\n",
      "        [0.0380],\n",
      "        [0.0405],\n",
      "        [0.0392],\n",
      "        [0.0395],\n",
      "        [0.0343],\n",
      "        [0.0424],\n",
      "        [0.0415],\n",
      "        [0.0419],\n",
      "        [0.0368],\n",
      "        [0.0417],\n",
      "        [0.0385],\n",
      "        [0.0453],\n",
      "        [0.0427],\n",
      "        [0.0423],\n",
      "        [0.0484],\n",
      "        [0.0488],\n",
      "        [0.0471],\n",
      "        [0.0496],\n",
      "        [0.0493],\n",
      "        [0.0450],\n",
      "        [0.0506],\n",
      "        [0.0452],\n",
      "        [0.0463],\n",
      "        [0.0517],\n",
      "        [0.0541],\n",
      "        [0.0521],\n",
      "        [0.0468],\n",
      "        [0.0557],\n",
      "        [0.0543],\n",
      "        [0.0541],\n",
      "        [0.0562],\n",
      "        [0.0566],\n",
      "        [0.0534],\n",
      "        [0.0561],\n",
      "        [0.0613],\n",
      "        [0.0611],\n",
      "        [0.0625],\n",
      "        [0.0714],\n",
      "        [0.0697],\n",
      "        [0.0720],\n",
      "        [0.0747],\n",
      "        [0.0724],\n",
      "        [0.0733],\n",
      "        [0.0754],\n",
      "        [0.0736],\n",
      "        [0.0746],\n",
      "        [0.0799],\n",
      "        [0.0767],\n",
      "        [0.0765],\n",
      "        [0.0834],\n",
      "        [0.0870],\n",
      "        [0.0829],\n",
      "        [0.0879],\n",
      "        [0.0996],\n",
      "        [0.1019],\n",
      "        [0.1037],\n",
      "        [0.1057],\n",
      "        [0.1187],\n",
      "        [0.1144],\n",
      "        [0.1239],\n",
      "        [0.1245],\n",
      "        [0.1256],\n",
      "        [0.1294],\n",
      "        [0.1362],\n",
      "        [0.1329],\n",
      "        [0.1369],\n",
      "        [0.1407],\n",
      "        [0.1460],\n",
      "        [0.1578],\n",
      "        [0.1642],\n",
      "        [0.1721],\n",
      "        [0.2027]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 50.47033715248108\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.808596258953912e-07, 133)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [133, 137, 103, 136, 67, 149, 119, 64, 91, 148, 65, 123, 11, 143, 42, 112, 124, 88, 53, 102, 90, 45, 58, 114, 23, 118, 18, 68, 66, 31, 113, 79, 78, 142, 135, 106, 57, 120, 98, 0, 14, 34, 19, 24, 107, 59, 99, 100, 95, 153, 96, 105, 115, 104, 97, 111, 43, 16, 117, 17, 4, 122, 92, 1, 82, 10, 121, 36, 150, 127, 33, 89, 139, 80, 70, 22, 32, 77, 30, 154, 109, 81, 60, 21, 116, 69, 138, 44, 12, 54, 93, 147, 101, 110, 108, 50, 141, 35, 83, 41, 63, 52, 20, 94, 5, 140, 25, 134, 3, 51, 71, 2, 72, 144, 73, 9, 15, 56, 55, 125, 76, 75, 61, 146, 74, 126, 62, 132, 87, 145, 29, 40, 46, 151, 128, 26, 152, 28, 27, 8, 49, 155, 129, 6, 86, 158, 156, 48, 39, 7, 131, 37, 38, 157, 13, 130, 84, 85] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2903],\n",
      "        [0.3061],\n",
      "        [0.6203],\n",
      "        [0.3091],\n",
      "        [0.6617],\n",
      "        [0.3571],\n",
      "        [0.5308],\n",
      "        [0.6918],\n",
      "        [0.6146],\n",
      "        [0.3687],\n",
      "        [0.6886],\n",
      "        [0.4923],\n",
      "        [0.9583],\n",
      "        [0.3973],\n",
      "        [0.7540],\n",
      "        [0.5518],\n",
      "        [0.4665],\n",
      "        [0.5036],\n",
      "        [0.6691],\n",
      "        [0.6349],\n",
      "        [0.5602],\n",
      "        [0.7308],\n",
      "        [0.7130],\n",
      "        [0.5459],\n",
      "        [0.9199],\n",
      "        [0.5256],\n",
      "        [0.9004],\n",
      "        [0.6235],\n",
      "        [0.6892],\n",
      "        [0.8739],\n",
      "        [0.5735],\n",
      "        [0.6329],\n",
      "        [0.6290],\n",
      "        [0.3493],\n",
      "        [0.3051],\n",
      "        [0.6212],\n",
      "        [0.6966],\n",
      "        [0.5173],\n",
      "        [0.5964],\n",
      "        [0.8263],\n",
      "        [0.8603],\n",
      "        [0.8531],\n",
      "        [0.9157],\n",
      "        [0.8875],\n",
      "        [0.5979],\n",
      "        [0.7106],\n",
      "        [0.6498],\n",
      "        [0.6573],\n",
      "        [0.5991],\n",
      "        [0.2909],\n",
      "        [0.5907],\n",
      "        [0.6489],\n",
      "        [0.5273],\n",
      "        [0.6545],\n",
      "        [0.5853],\n",
      "        [0.5688],\n",
      "        [0.7239],\n",
      "        [0.8688],\n",
      "        [0.5178],\n",
      "        [0.8295],\n",
      "        [0.8187],\n",
      "        [0.4951],\n",
      "        [0.6093],\n",
      "        [0.8452],\n",
      "        [0.6145],\n",
      "        [0.9411],\n",
      "        [0.4971],\n",
      "        [0.8430],\n",
      "        [0.2876],\n",
      "        [0.4134],\n",
      "        [0.8061],\n",
      "        [0.4967],\n",
      "        [0.3279],\n",
      "        [0.6058],\n",
      "        [0.6100],\n",
      "        [0.9238],\n",
      "        [0.8426],\n",
      "        [0.6388],\n",
      "        [0.8500],\n",
      "        [0.2965],\n",
      "        [0.5927],\n",
      "        [0.6002],\n",
      "        [0.6691],\n",
      "        [0.9042],\n",
      "        [0.4887],\n",
      "        [0.6094],\n",
      "        [0.3198],\n",
      "        [0.7202],\n",
      "        [0.9404],\n",
      "        [0.6776],\n",
      "        [0.5844],\n",
      "        [0.3584],\n",
      "        [0.6302],\n",
      "        [0.5655],\n",
      "        [0.6020],\n",
      "        [0.6758],\n",
      "        [0.3162],\n",
      "        [0.8364],\n",
      "        [0.6119],\n",
      "        [0.7907],\n",
      "        [0.6597],\n",
      "        [0.6698],\n",
      "        [0.9303],\n",
      "        [0.5699],\n",
      "        [0.8151],\n",
      "        [0.3099],\n",
      "        [0.8706],\n",
      "        [0.2531],\n",
      "        [0.7894],\n",
      "        [0.6036],\n",
      "        [0.6070],\n",
      "        [0.8678],\n",
      "        [0.6049],\n",
      "        [0.3969],\n",
      "        [0.6251],\n",
      "        [0.9125],\n",
      "        [0.8857],\n",
      "        [0.6780],\n",
      "        [0.6789],\n",
      "        [0.4109],\n",
      "        [0.6280],\n",
      "        [0.6282],\n",
      "        [0.6464],\n",
      "        [0.3651],\n",
      "        [0.6246],\n",
      "        [0.3922],\n",
      "        [0.6345],\n",
      "        [0.3411],\n",
      "        [0.5699],\n",
      "        [0.3774],\n",
      "        [0.8449],\n",
      "        [0.8227],\n",
      "        [0.7397],\n",
      "        [0.2315],\n",
      "        [0.4093],\n",
      "        [0.8521],\n",
      "        [0.2382],\n",
      "        [0.8673],\n",
      "        [0.8598],\n",
      "        [0.8666],\n",
      "        [0.7181],\n",
      "        [0.3259],\n",
      "        [0.4012],\n",
      "        [0.8343],\n",
      "        [0.5807],\n",
      "        [0.3121],\n",
      "        [0.3164],\n",
      "        [0.7508],\n",
      "        [0.7913],\n",
      "        [0.8638],\n",
      "        [0.4019],\n",
      "        [0.8445],\n",
      "        [0.7953],\n",
      "        [0.3206],\n",
      "        [0.8722],\n",
      "        [0.3995],\n",
      "        [0.6232],\n",
      "        [0.6402]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0010],\n",
      "        [0.0013],\n",
      "        [0.0013],\n",
      "        [0.0020],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0032],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0103],\n",
      "        [0.0110],\n",
      "        [0.0112],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0141],\n",
      "        [0.0142],\n",
      "        [0.0142],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0157],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0176],\n",
      "        [0.0178],\n",
      "        [0.0184],\n",
      "        [0.0187],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0228],\n",
      "        [0.0229],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0242],\n",
      "        [0.0252],\n",
      "        [0.0257],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0271],\n",
      "        [0.0295],\n",
      "        [0.0298],\n",
      "        [0.0301],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0313],\n",
      "        [0.0318],\n",
      "        [0.0332],\n",
      "        [0.0333],\n",
      "        [0.0343],\n",
      "        [0.0351],\n",
      "        [0.0358],\n",
      "        [0.0366],\n",
      "        [0.0368],\n",
      "        [0.0377],\n",
      "        [0.0380],\n",
      "        [0.0381],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0392],\n",
      "        [0.0395],\n",
      "        [0.0405],\n",
      "        [0.0415],\n",
      "        [0.0417],\n",
      "        [0.0419],\n",
      "        [0.0423],\n",
      "        [0.0424],\n",
      "        [0.0427],\n",
      "        [0.0450],\n",
      "        [0.0452],\n",
      "        [0.0453],\n",
      "        [0.0463],\n",
      "        [0.0468],\n",
      "        [0.0471],\n",
      "        [0.0484],\n",
      "        [0.0488],\n",
      "        [0.0493],\n",
      "        [0.0496],\n",
      "        [0.0506],\n",
      "        [0.0517],\n",
      "        [0.0521],\n",
      "        [0.0534],\n",
      "        [0.0541],\n",
      "        [0.0541],\n",
      "        [0.0543],\n",
      "        [0.0557],\n",
      "        [0.0561],\n",
      "        [0.0562],\n",
      "        [0.0566],\n",
      "        [0.0611],\n",
      "        [0.0613],\n",
      "        [0.0625],\n",
      "        [0.0697],\n",
      "        [0.0714],\n",
      "        [0.0720],\n",
      "        [0.0724],\n",
      "        [0.0733],\n",
      "        [0.0736],\n",
      "        [0.0746],\n",
      "        [0.0747],\n",
      "        [0.0754],\n",
      "        [0.0765],\n",
      "        [0.0767],\n",
      "        [0.0799],\n",
      "        [0.0829],\n",
      "        [0.0834],\n",
      "        [0.0870],\n",
      "        [0.0879],\n",
      "        [0.0996],\n",
      "        [0.1019],\n",
      "        [0.1037],\n",
      "        [0.1057],\n",
      "        [0.1144],\n",
      "        [0.1187],\n",
      "        [0.1239],\n",
      "        [0.1245],\n",
      "        [0.1256],\n",
      "        [0.1294],\n",
      "        [0.1329],\n",
      "        [0.1362],\n",
      "        [0.1369],\n",
      "        [0.1407],\n",
      "        [0.1460],\n",
      "        [0.1578],\n",
      "        [0.1642],\n",
      "        [0.1721],\n",
      "        [0.2027],\n",
      "        [0.2165]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0017],\n",
      "        [0.0010],\n",
      "        [0.0005],\n",
      "        [0.0017],\n",
      "        [0.0023],\n",
      "        [0.0033],\n",
      "        [0.0039],\n",
      "        [0.0032],\n",
      "        [0.0036],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0060],\n",
      "        [0.0052],\n",
      "        [0.0067],\n",
      "        [0.0064],\n",
      "        [0.0073],\n",
      "        [0.0085],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0091],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0099],\n",
      "        [0.0110],\n",
      "        [0.0108],\n",
      "        [0.0119],\n",
      "        [0.0115],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0147],\n",
      "        [0.0151],\n",
      "        [0.0145],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0156],\n",
      "        [0.0171],\n",
      "        [0.0181],\n",
      "        [0.0187],\n",
      "        [0.0171],\n",
      "        [0.0183],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0189],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0191],\n",
      "        [0.0201],\n",
      "        [0.0204],\n",
      "        [0.0198],\n",
      "        [0.0213],\n",
      "        [0.0217],\n",
      "        [0.0213],\n",
      "        [0.0223],\n",
      "        [0.0231],\n",
      "        [0.0224],\n",
      "        [0.0228],\n",
      "        [0.0238],\n",
      "        [0.0238],\n",
      "        [0.0234],\n",
      "        [0.0251],\n",
      "        [0.0248],\n",
      "        [0.0261],\n",
      "        [0.0262],\n",
      "        [0.0273],\n",
      "        [0.0265],\n",
      "        [0.0268],\n",
      "        [0.0285],\n",
      "        [0.0305],\n",
      "        [0.0307],\n",
      "        [0.0311],\n",
      "        [0.0311],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0341],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0344],\n",
      "        [0.0363],\n",
      "        [0.0371],\n",
      "        [0.0363],\n",
      "        [0.0370],\n",
      "        [0.0379],\n",
      "        [0.0385],\n",
      "        [0.0383],\n",
      "        [0.0385],\n",
      "        [0.0401],\n",
      "        [0.0400],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0409],\n",
      "        [0.0414],\n",
      "        [0.0419],\n",
      "        [0.0430],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0451],\n",
      "        [0.0459],\n",
      "        [0.0456],\n",
      "        [0.0460],\n",
      "        [0.0476],\n",
      "        [0.0482],\n",
      "        [0.0494],\n",
      "        [0.0487],\n",
      "        [0.0503],\n",
      "        [0.0504],\n",
      "        [0.0518],\n",
      "        [0.0521],\n",
      "        [0.0530],\n",
      "        [0.0541],\n",
      "        [0.0552],\n",
      "        [0.0549],\n",
      "        [0.0561],\n",
      "        [0.0568],\n",
      "        [0.0570],\n",
      "        [0.0575],\n",
      "        [0.0606],\n",
      "        [0.0620],\n",
      "        [0.0631],\n",
      "        [0.0699],\n",
      "        [0.0724],\n",
      "        [0.0726],\n",
      "        [0.0720],\n",
      "        [0.0736],\n",
      "        [0.0732],\n",
      "        [0.0735],\n",
      "        [0.0757],\n",
      "        [0.0746],\n",
      "        [0.0764],\n",
      "        [0.0764],\n",
      "        [0.0797],\n",
      "        [0.0833],\n",
      "        [0.0824],\n",
      "        [0.0871],\n",
      "        [0.0873],\n",
      "        [0.0988],\n",
      "        [0.1019],\n",
      "        [0.1033],\n",
      "        [0.1045],\n",
      "        [0.1146],\n",
      "        [0.1187],\n",
      "        [0.1226],\n",
      "        [0.1230],\n",
      "        [0.1245],\n",
      "        [0.1290],\n",
      "        [0.1331],\n",
      "        [0.1364],\n",
      "        [0.1366],\n",
      "        [0.1412],\n",
      "        [0.1463],\n",
      "        [0.1564],\n",
      "        [0.1651],\n",
      "        [0.1720],\n",
      "        [0.2018],\n",
      "        [0.2152]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 50.75775957107544\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.650245107815863e-07, 136)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [136, 103, 133, 67, 137, 149, 91, 119, 148, 64, 65, 123, 143, 11, 112, 42, 53, 124, 102, 90, 88, 58, 45, 114, 23, 18, 118, 66, 68, 31, 113, 79, 78, 106, 142, 135, 98, 57, 120, 0, 19, 14, 107, 24, 59, 34, 99, 153, 100, 115, 95, 96, 105, 104, 111, 97, 43, 117, 17, 16, 92, 122, 4, 82, 1, 10, 121, 150, 127, 36, 33, 89, 139, 22, 80, 70, 32, 30, 154, 77, 109, 21, 81, 116, 60, 69, 44, 138, 12, 93, 54, 110, 108, 147, 101, 50, 141, 35, 83, 41, 52, 63, 20, 94, 5, 25, 140, 134, 3, 51, 71, 2, 72, 73, 144, 9, 15, 56, 55, 125, 76, 75, 61, 126, 146, 74, 132, 87, 62, 29, 145, 46, 40, 151, 26, 128, 152, 28, 27, 8, 49, 155, 129, 6, 86, 158, 156, 48, 39, 7, 131, 37, 38, 157, 13, 130, 84, 85, 47] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3083],\n",
      "        [0.6206],\n",
      "        [0.2898],\n",
      "        [0.6614],\n",
      "        [0.3054],\n",
      "        [0.3566],\n",
      "        [0.6137],\n",
      "        [0.5307],\n",
      "        [0.3681],\n",
      "        [0.6912],\n",
      "        [0.6880],\n",
      "        [0.4921],\n",
      "        [0.3963],\n",
      "        [0.9582],\n",
      "        [0.5526],\n",
      "        [0.7539],\n",
      "        [0.6683],\n",
      "        [0.4667],\n",
      "        [0.6350],\n",
      "        [0.5595],\n",
      "        [0.5028],\n",
      "        [0.7123],\n",
      "        [0.7309],\n",
      "        [0.5466],\n",
      "        [0.9195],\n",
      "        [0.9000],\n",
      "        [0.5257],\n",
      "        [0.6887],\n",
      "        [0.6234],\n",
      "        [0.8743],\n",
      "        [0.5741],\n",
      "        [0.6323],\n",
      "        [0.6283],\n",
      "        [0.6215],\n",
      "        [0.3487],\n",
      "        [0.3041],\n",
      "        [0.5962],\n",
      "        [0.6959],\n",
      "        [0.5169],\n",
      "        [0.8265],\n",
      "        [0.9150],\n",
      "        [0.8614],\n",
      "        [0.5983],\n",
      "        [0.8876],\n",
      "        [0.7097],\n",
      "        [0.8541],\n",
      "        [0.6494],\n",
      "        [0.2901],\n",
      "        [0.6571],\n",
      "        [0.5282],\n",
      "        [0.5989],\n",
      "        [0.5907],\n",
      "        [0.6490],\n",
      "        [0.6546],\n",
      "        [0.5694],\n",
      "        [0.5854],\n",
      "        [0.7240],\n",
      "        [0.5181],\n",
      "        [0.8296],\n",
      "        [0.8694],\n",
      "        [0.6085],\n",
      "        [0.4947],\n",
      "        [0.8187],\n",
      "        [0.6136],\n",
      "        [0.8451],\n",
      "        [0.9409],\n",
      "        [0.4968],\n",
      "        [0.2877],\n",
      "        [0.4136],\n",
      "        [0.8440],\n",
      "        [0.8071],\n",
      "        [0.4960],\n",
      "        [0.3273],\n",
      "        [0.9234],\n",
      "        [0.6053],\n",
      "        [0.6101],\n",
      "        [0.8432],\n",
      "        [0.8507],\n",
      "        [0.2955],\n",
      "        [0.6379],\n",
      "        [0.5933],\n",
      "        [0.9037],\n",
      "        [0.5997],\n",
      "        [0.4894],\n",
      "        [0.6685],\n",
      "        [0.6095],\n",
      "        [0.7203],\n",
      "        [0.3194],\n",
      "        [0.9404],\n",
      "        [0.5839],\n",
      "        [0.6767],\n",
      "        [0.5664],\n",
      "        [0.6026],\n",
      "        [0.3575],\n",
      "        [0.6301],\n",
      "        [0.6754],\n",
      "        [0.3155],\n",
      "        [0.8376],\n",
      "        [0.6112],\n",
      "        [0.7906],\n",
      "        [0.6692],\n",
      "        [0.6591],\n",
      "        [0.9295],\n",
      "        [0.5695],\n",
      "        [0.8154],\n",
      "        [0.8713],\n",
      "        [0.3093],\n",
      "        [0.2524],\n",
      "        [0.7896],\n",
      "        [0.6035],\n",
      "        [0.6070],\n",
      "        [0.8675],\n",
      "        [0.6049],\n",
      "        [0.6246],\n",
      "        [0.3958],\n",
      "        [0.9121],\n",
      "        [0.8864],\n",
      "        [0.6772],\n",
      "        [0.6780],\n",
      "        [0.4114],\n",
      "        [0.6273],\n",
      "        [0.6276],\n",
      "        [0.6462],\n",
      "        [0.3925],\n",
      "        [0.3641],\n",
      "        [0.6240],\n",
      "        [0.3407],\n",
      "        [0.5688],\n",
      "        [0.6342],\n",
      "        [0.8457],\n",
      "        [0.3765],\n",
      "        [0.7394],\n",
      "        [0.8226],\n",
      "        [0.2317],\n",
      "        [0.8531],\n",
      "        [0.4097],\n",
      "        [0.2381],\n",
      "        [0.8680],\n",
      "        [0.8606],\n",
      "        [0.8666],\n",
      "        [0.7176],\n",
      "        [0.3246],\n",
      "        [0.4013],\n",
      "        [0.8343],\n",
      "        [0.5795],\n",
      "        [0.3107],\n",
      "        [0.3154],\n",
      "        [0.7503],\n",
      "        [0.7916],\n",
      "        [0.8636],\n",
      "        [0.4015],\n",
      "        [0.8450],\n",
      "        [0.7955],\n",
      "        [0.3191],\n",
      "        [0.8730],\n",
      "        [0.3994],\n",
      "        [0.6223],\n",
      "        [0.6389],\n",
      "        [0.7396]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0023],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0060],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0099],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.0119],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0145],\n",
      "        [0.0147],\n",
      "        [0.0151],\n",
      "        [0.0156],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0171],\n",
      "        [0.0171],\n",
      "        [0.0181],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0201],\n",
      "        [0.0204],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0217],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0228],\n",
      "        [0.0231],\n",
      "        [0.0234],\n",
      "        [0.0238],\n",
      "        [0.0238],\n",
      "        [0.0248],\n",
      "        [0.0251],\n",
      "        [0.0261],\n",
      "        [0.0262],\n",
      "        [0.0265],\n",
      "        [0.0268],\n",
      "        [0.0273],\n",
      "        [0.0285],\n",
      "        [0.0305],\n",
      "        [0.0307],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0311],\n",
      "        [0.0311],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0341],\n",
      "        [0.0344],\n",
      "        [0.0363],\n",
      "        [0.0363],\n",
      "        [0.0370],\n",
      "        [0.0371],\n",
      "        [0.0379],\n",
      "        [0.0383],\n",
      "        [0.0385],\n",
      "        [0.0385],\n",
      "        [0.0400],\n",
      "        [0.0401],\n",
      "        [0.0409],\n",
      "        [0.0414],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0419],\n",
      "        [0.0430],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0451],\n",
      "        [0.0456],\n",
      "        [0.0459],\n",
      "        [0.0460],\n",
      "        [0.0476],\n",
      "        [0.0482],\n",
      "        [0.0487],\n",
      "        [0.0494],\n",
      "        [0.0503],\n",
      "        [0.0504],\n",
      "        [0.0518],\n",
      "        [0.0521],\n",
      "        [0.0530],\n",
      "        [0.0541],\n",
      "        [0.0549],\n",
      "        [0.0552],\n",
      "        [0.0561],\n",
      "        [0.0568],\n",
      "        [0.0570],\n",
      "        [0.0575],\n",
      "        [0.0606],\n",
      "        [0.0620],\n",
      "        [0.0631],\n",
      "        [0.0699],\n",
      "        [0.0720],\n",
      "        [0.0724],\n",
      "        [0.0726],\n",
      "        [0.0732],\n",
      "        [0.0735],\n",
      "        [0.0736],\n",
      "        [0.0746],\n",
      "        [0.0757],\n",
      "        [0.0764],\n",
      "        [0.0764],\n",
      "        [0.0797],\n",
      "        [0.0824],\n",
      "        [0.0833],\n",
      "        [0.0871],\n",
      "        [0.0873],\n",
      "        [0.0988],\n",
      "        [0.1019],\n",
      "        [0.1033],\n",
      "        [0.1045],\n",
      "        [0.1146],\n",
      "        [0.1187],\n",
      "        [0.1226],\n",
      "        [0.1230],\n",
      "        [0.1245],\n",
      "        [0.1290],\n",
      "        [0.1331],\n",
      "        [0.1364],\n",
      "        [0.1366],\n",
      "        [0.1412],\n",
      "        [0.1463],\n",
      "        [0.1564],\n",
      "        [0.1651],\n",
      "        [0.1720],\n",
      "        [0.2018],\n",
      "        [0.2152],\n",
      "        [0.2785]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0012],\n",
      "        [0.0005],\n",
      "        [0.0018],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0014],\n",
      "        [0.0034],\n",
      "        [0.0029],\n",
      "        [0.0080],\n",
      "        [0.0005],\n",
      "        [0.0049],\n",
      "        [0.0042],\n",
      "        [0.0032],\n",
      "        [0.0057],\n",
      "        [0.0107],\n",
      "        [0.0029],\n",
      "        [0.0069],\n",
      "        [0.0083],\n",
      "        [0.0066],\n",
      "        [0.0101],\n",
      "        [0.0045],\n",
      "        [0.0051],\n",
      "        [0.0085],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0108],\n",
      "        [0.0077],\n",
      "        [0.0150],\n",
      "        [0.0099],\n",
      "        [0.0134],\n",
      "        [0.0157],\n",
      "        [0.0160],\n",
      "        [0.0144],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0147],\n",
      "        [0.0204],\n",
      "        [0.0162],\n",
      "        [0.0152],\n",
      "        [0.0140],\n",
      "        [0.0167],\n",
      "        [0.0181],\n",
      "        [0.0207],\n",
      "        [0.0138],\n",
      "        [0.0165],\n",
      "        [0.0177],\n",
      "        [0.0187],\n",
      "        [0.0205],\n",
      "        [0.0189],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0200],\n",
      "        [0.0208],\n",
      "        [0.0207],\n",
      "        [0.0210],\n",
      "        [0.0262],\n",
      "        [0.0220],\n",
      "        [0.0249],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0237],\n",
      "        [0.0260],\n",
      "        [0.0222],\n",
      "        [0.0227],\n",
      "        [0.0290],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0260],\n",
      "        [0.0250],\n",
      "        [0.0306],\n",
      "        [0.0319],\n",
      "        [0.0308],\n",
      "        [0.0277],\n",
      "        [0.0334],\n",
      "        [0.0338],\n",
      "        [0.0336],\n",
      "        [0.0350],\n",
      "        [0.0326],\n",
      "        [0.0371],\n",
      "        [0.0341],\n",
      "        [0.0332],\n",
      "        [0.0386],\n",
      "        [0.0361],\n",
      "        [0.0411],\n",
      "        [0.0406],\n",
      "        [0.0421],\n",
      "        [0.0382],\n",
      "        [0.0359],\n",
      "        [0.0413],\n",
      "        [0.0448],\n",
      "        [0.0401],\n",
      "        [0.0412],\n",
      "        [0.0422],\n",
      "        [0.0422],\n",
      "        [0.0377],\n",
      "        [0.0431],\n",
      "        [0.0420],\n",
      "        [0.0420],\n",
      "        [0.0409],\n",
      "        [0.0416],\n",
      "        [0.0497],\n",
      "        [0.0427],\n",
      "        [0.0487],\n",
      "        [0.0502],\n",
      "        [0.0505],\n",
      "        [0.0495],\n",
      "        [0.0496],\n",
      "        [0.0523],\n",
      "        [0.0552],\n",
      "        [0.0547],\n",
      "        [0.0503],\n",
      "        [0.0566],\n",
      "        [0.0578],\n",
      "        [0.0563],\n",
      "        [0.0590],\n",
      "        [0.0550],\n",
      "        [0.0615],\n",
      "        [0.0620],\n",
      "        [0.0596],\n",
      "        [0.0649],\n",
      "        [0.0660],\n",
      "        [0.0733],\n",
      "        [0.0711],\n",
      "        [0.0733],\n",
      "        [0.0755],\n",
      "        [0.0738],\n",
      "        [0.0713],\n",
      "        [0.0769],\n",
      "        [0.0768],\n",
      "        [0.0766],\n",
      "        [0.0720],\n",
      "        [0.0722],\n",
      "        [0.0790],\n",
      "        [0.0839],\n",
      "        [0.0843],\n",
      "        [0.0868],\n",
      "        [0.0896],\n",
      "        [0.1008],\n",
      "        [0.1044],\n",
      "        [0.0988],\n",
      "        [0.1037],\n",
      "        [0.1154],\n",
      "        [0.1211],\n",
      "        [0.1201],\n",
      "        [0.1222],\n",
      "        [0.1239],\n",
      "        [0.1244],\n",
      "        [0.1295],\n",
      "        [0.1390],\n",
      "        [0.1369],\n",
      "        [0.1381],\n",
      "        [0.1429],\n",
      "        [0.1555],\n",
      "        [0.1635],\n",
      "        [0.1727],\n",
      "        [0.1994],\n",
      "        [0.2126],\n",
      "        [0.2742]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 51.04457759857178\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 9 個區塊累積花費時間(s) 1.2179787158966064\n",
      "<<The performance of 9 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2179787158966064\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1206.80\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1678.50\n",
      "The accuracy(2000) for l = 1: 83.65%\n",
      "The accuracy(3000) for l = 1: 89.94%\n",
      "The maximum error: tensor(6999.9141)\n",
      "The minimum error: tensor(13.0547)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 2961.5\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 3006.8\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 50.0%\n",
      "The maximum error: 3452.7265625\n",
      "The minimum error: 2213.91796875\n",
      "------------------------------------------------------------\n",
      "0.8364779874213837\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<10>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.61477026697321e-07, 61)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [61, 129, 132, 99, 87, 133, 145, 63, 49, 144, 7, 115, 139, 54, 119, 41, 108, 86, 120, 19, 62, 14, 60, 98, 110, 27, 84, 38, 114, 109, 55, 15, 102, 94, 64, 131, 138, 75, 74, 116, 30, 10, 95, 103, 149, 111, 101, 53, 96, 107, 20, 100, 91, 92, 93, 12, 88, 113, 78, 118, 13, 32, 123, 0, 39, 117, 146, 18, 6, 29, 135, 85, 150, 17, 76, 28, 66, 105, 26, 8, 112, 73, 46, 134, 77, 106, 65, 37, 56, 104, 89, 48, 79, 31, 40, 97, 143, 16, 137, 50, 90, 136, 130, 59, 1, 21, 67, 11, 47, 140, 68, 69, 5, 121, 52, 51, 72, 71, 122, 83, 42, 36, 142, 57, 128, 70, 141, 25, 58, 147, 22, 124, 158, 148, 24, 45, 23, 151, 4, 157, 125, 82, 2, 154, 152, 44, 35, 155, 156, 127, 33, 3, 34, 153, 9]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0018],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0144],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0177],\n",
      "        [0.0181],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0237],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0277],\n",
      "        [0.0290],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0319],\n",
      "        [0.0326],\n",
      "        [0.0332],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0338],\n",
      "        [0.0341],\n",
      "        [0.0350],\n",
      "        [0.0359],\n",
      "        [0.0361],\n",
      "        [0.0371],\n",
      "        [0.0377],\n",
      "        [0.0382],\n",
      "        [0.0386],\n",
      "        [0.0401],\n",
      "        [0.0406],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0412],\n",
      "        [0.0413],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0422],\n",
      "        [0.0422],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0448],\n",
      "        [0.0487],\n",
      "        [0.0495],\n",
      "        [0.0496],\n",
      "        [0.0497],\n",
      "        [0.0502],\n",
      "        [0.0505],\n",
      "        [0.0547],\n",
      "        [0.0550],\n",
      "        [0.0552],\n",
      "        [0.0563],\n",
      "        [0.0566],\n",
      "        [0.0578],\n",
      "        [0.0590],\n",
      "        [0.0596],\n",
      "        [0.0615],\n",
      "        [0.0620],\n",
      "        [0.0649],\n",
      "        [0.0660],\n",
      "        [0.0711],\n",
      "        [0.0713],\n",
      "        [0.0720],\n",
      "        [0.0722],\n",
      "        [0.0733],\n",
      "        [0.0733],\n",
      "        [0.0738],\n",
      "        [0.0755],\n",
      "        [0.0766],\n",
      "        [0.0768],\n",
      "        [0.0769],\n",
      "        [0.0790],\n",
      "        [0.0839],\n",
      "        [0.0843],\n",
      "        [0.0867],\n",
      "        [0.0868],\n",
      "        [0.0896],\n",
      "        [0.0988],\n",
      "        [0.1008],\n",
      "        [0.1037],\n",
      "        [0.1044],\n",
      "        [0.1072],\n",
      "        [0.1154],\n",
      "        [0.1201],\n",
      "        [0.1211],\n",
      "        [0.1222],\n",
      "        [0.1239],\n",
      "        [0.1244],\n",
      "        [0.1295],\n",
      "        [0.1348],\n",
      "        [0.1352],\n",
      "        [0.1369],\n",
      "        [0.1381],\n",
      "        [0.1390],\n",
      "        [0.1429],\n",
      "        [0.1555],\n",
      "        [0.1635]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.61477026697321e-07, 61)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [61, 129, 132, 99, 87, 133, 145, 63, 49, 144, 7, 115, 139, 54, 119, 41, 108, 86, 120, 19, 62, 14, 60, 98, 110, 27, 84, 38, 114, 109, 55, 15, 102, 94, 64, 131, 138, 75, 74, 116, 30, 10, 95, 103, 149, 111, 101, 53, 96, 107, 20, 100, 91, 92, 93, 12, 88, 113, 78, 118, 13, 32, 123, 0, 39, 117, 146, 18, 6, 29, 135, 85, 150, 17, 76, 28, 66, 105, 26, 8, 112, 73, 46, 134, 77, 106, 65, 37, 56, 104, 89, 48, 79, 31, 40, 97, 143, 16, 137, 50, 90, 136, 130, 59, 1, 21, 67, 11, 47, 140, 68, 69, 5, 121, 52, 51, 72, 71, 122, 83, 42, 36, 142, 57, 128, 70, 141, 25, 58, 147, 22, 124, 158, 148, 24, 45, 23, 151, 4, 157, 125, 82, 2, 154, 152, 44, 35, 155, 156, 127, 33, 3, 34, 153, 9, 126] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6841],\n",
      "        [0.2904],\n",
      "        [0.3084],\n",
      "        [0.6204],\n",
      "        [0.6119],\n",
      "        [0.3056],\n",
      "        [0.3561],\n",
      "        [0.6579],\n",
      "        [0.6640],\n",
      "        [0.3674],\n",
      "        [0.9553],\n",
      "        [0.5306],\n",
      "        [0.3953],\n",
      "        [0.7079],\n",
      "        [0.4922],\n",
      "        [0.7269],\n",
      "        [0.5533],\n",
      "        [0.5580],\n",
      "        [0.4671],\n",
      "        [0.9165],\n",
      "        [0.6848],\n",
      "        [0.8972],\n",
      "        [0.6871],\n",
      "        [0.6346],\n",
      "        [0.5473],\n",
      "        [0.8715],\n",
      "        [0.5013],\n",
      "        [0.7499],\n",
      "        [0.5258],\n",
      "        [0.5745],\n",
      "        [0.7053],\n",
      "        [0.9119],\n",
      "        [0.6214],\n",
      "        [0.5954],\n",
      "        [0.6203],\n",
      "        [0.3042],\n",
      "        [0.3484],\n",
      "        [0.6297],\n",
      "        [0.6256],\n",
      "        [0.5167],\n",
      "        [0.8520],\n",
      "        [0.8600],\n",
      "        [0.6483],\n",
      "        [0.5985],\n",
      "        [0.2897],\n",
      "        [0.5291],\n",
      "        [0.6486],\n",
      "        [0.6914],\n",
      "        [0.6562],\n",
      "        [0.5700],\n",
      "        [0.8852],\n",
      "        [0.6541],\n",
      "        [0.5978],\n",
      "        [0.5898],\n",
      "        [0.5848],\n",
      "        [0.8676],\n",
      "        [0.6068],\n",
      "        [0.5186],\n",
      "        [0.6110],\n",
      "        [0.4946],\n",
      "        [0.8275],\n",
      "        [0.8416],\n",
      "        [0.4144],\n",
      "        [0.8165],\n",
      "        [0.7201],\n",
      "        [0.4969],\n",
      "        [0.2880],\n",
      "        [0.9203],\n",
      "        [0.9379],\n",
      "        [0.8050],\n",
      "        [0.3273],\n",
      "        [0.4946],\n",
      "        [0.2948],\n",
      "        [0.9006],\n",
      "        [0.6030],\n",
      "        [0.8407],\n",
      "        [0.6074],\n",
      "        [0.5937],\n",
      "        [0.8483],\n",
      "        [0.9377],\n",
      "        [0.4904],\n",
      "        [0.6349],\n",
      "        [0.6712],\n",
      "        [0.3197],\n",
      "        [0.5974],\n",
      "        [0.5671],\n",
      "        [0.6068],\n",
      "        [0.7864],\n",
      "        [0.6645],\n",
      "        [0.6027],\n",
      "        [0.5825],\n",
      "        [0.6652],\n",
      "        [0.6089],\n",
      "        [0.8356],\n",
      "        [0.7165],\n",
      "        [0.6295],\n",
      "        [0.3567],\n",
      "        [0.9262],\n",
      "        [0.3154],\n",
      "        [0.6720],\n",
      "        [0.5683],\n",
      "        [0.3092],\n",
      "        [0.2531],\n",
      "        [0.6554],\n",
      "        [0.8133],\n",
      "        [0.8695],\n",
      "        [0.6044],\n",
      "        [0.8847],\n",
      "        [0.6001],\n",
      "        [0.3947],\n",
      "        [0.6024],\n",
      "        [0.6216],\n",
      "        [0.9091],\n",
      "        [0.4123],\n",
      "        [0.6728],\n",
      "        [0.6735],\n",
      "        [0.6243],\n",
      "        [0.6247],\n",
      "        [0.3935],\n",
      "        [0.5666],\n",
      "        [0.7350],\n",
      "        [0.8184],\n",
      "        [0.3632],\n",
      "        [0.6428],\n",
      "        [0.3412],\n",
      "        [0.6211],\n",
      "        [0.3755],\n",
      "        [0.8435],\n",
      "        [0.6309],\n",
      "        [0.2324],\n",
      "        [0.8515],\n",
      "        [0.4106],\n",
      "        [0.3203],\n",
      "        [0.2385],\n",
      "        [0.8657],\n",
      "        [0.7131],\n",
      "        [0.8585],\n",
      "        [0.3239],\n",
      "        [0.8641],\n",
      "        [0.2989],\n",
      "        [0.4022],\n",
      "        [0.5770],\n",
      "        [0.8319],\n",
      "        [0.3098],\n",
      "        [0.3148],\n",
      "        [0.7457],\n",
      "        [0.7880],\n",
      "        [0.3156],\n",
      "        [0.3193],\n",
      "        [0.4019],\n",
      "        [0.8420],\n",
      "        [0.8610],\n",
      "        [0.7922],\n",
      "        [0.3182],\n",
      "        [0.8714],\n",
      "        [0.4000]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0018],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0144],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0160],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0177],\n",
      "        [0.0181],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0237],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0277],\n",
      "        [0.0290],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0319],\n",
      "        [0.0326],\n",
      "        [0.0332],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0338],\n",
      "        [0.0341],\n",
      "        [0.0350],\n",
      "        [0.0359],\n",
      "        [0.0361],\n",
      "        [0.0371],\n",
      "        [0.0377],\n",
      "        [0.0382],\n",
      "        [0.0386],\n",
      "        [0.0401],\n",
      "        [0.0406],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0412],\n",
      "        [0.0413],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0422],\n",
      "        [0.0422],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0448],\n",
      "        [0.0487],\n",
      "        [0.0495],\n",
      "        [0.0496],\n",
      "        [0.0497],\n",
      "        [0.0502],\n",
      "        [0.0505],\n",
      "        [0.0547],\n",
      "        [0.0550],\n",
      "        [0.0552],\n",
      "        [0.0563],\n",
      "        [0.0566],\n",
      "        [0.0578],\n",
      "        [0.0590],\n",
      "        [0.0596],\n",
      "        [0.0615],\n",
      "        [0.0620],\n",
      "        [0.0649],\n",
      "        [0.0660],\n",
      "        [0.0711],\n",
      "        [0.0713],\n",
      "        [0.0720],\n",
      "        [0.0722],\n",
      "        [0.0733],\n",
      "        [0.0733],\n",
      "        [0.0738],\n",
      "        [0.0755],\n",
      "        [0.0766],\n",
      "        [0.0768],\n",
      "        [0.0769],\n",
      "        [0.0790],\n",
      "        [0.0839],\n",
      "        [0.0843],\n",
      "        [0.0867],\n",
      "        [0.0868],\n",
      "        [0.0896],\n",
      "        [0.0988],\n",
      "        [0.1008],\n",
      "        [0.1037],\n",
      "        [0.1044],\n",
      "        [0.1072],\n",
      "        [0.1154],\n",
      "        [0.1201],\n",
      "        [0.1211],\n",
      "        [0.1222],\n",
      "        [0.1239],\n",
      "        [0.1244],\n",
      "        [0.1295],\n",
      "        [0.1348],\n",
      "        [0.1352],\n",
      "        [0.1369],\n",
      "        [0.1381],\n",
      "        [0.1390],\n",
      "        [0.1429],\n",
      "        [0.1555],\n",
      "        [0.1635],\n",
      "        [0.1727]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0040],\n",
      "        [0.0091],\n",
      "        [0.0113],\n",
      "        [0.0023],\n",
      "        [0.0023],\n",
      "        [0.0139],\n",
      "        [0.0089],\n",
      "        [0.0022],\n",
      "        [0.0053],\n",
      "        [0.0081],\n",
      "        [0.0105],\n",
      "        [0.0042],\n",
      "        [0.0073],\n",
      "        [0.0081],\n",
      "        [0.0032],\n",
      "        [0.0092],\n",
      "        [0.0037],\n",
      "        [0.0068],\n",
      "        [0.0087],\n",
      "        [0.0137],\n",
      "        [0.0117],\n",
      "        [0.0146],\n",
      "        [0.0051],\n",
      "        [0.0047],\n",
      "        [0.0068],\n",
      "        [0.0182],\n",
      "        [0.0104],\n",
      "        [0.0071],\n",
      "        [0.0112],\n",
      "        [0.0157],\n",
      "        [0.0176],\n",
      "        [0.0206],\n",
      "        [0.0179],\n",
      "        [0.0176],\n",
      "        [0.0112],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0115],\n",
      "        [0.0120],\n",
      "        [0.0178],\n",
      "        [0.0256],\n",
      "        [0.0244],\n",
      "        [0.0216],\n",
      "        [0.0146],\n",
      "        [0.0068],\n",
      "        [0.0177],\n",
      "        [0.0238],\n",
      "        [0.0172],\n",
      "        [0.0165],\n",
      "        [0.0183],\n",
      "        [0.0139],\n",
      "        [0.0246],\n",
      "        [0.0186],\n",
      "        [0.0185],\n",
      "        [0.0235],\n",
      "        [0.0284],\n",
      "        [0.0226],\n",
      "        [0.0222],\n",
      "        [0.0255],\n",
      "        [0.0221],\n",
      "        [0.0194],\n",
      "        [0.0335],\n",
      "        [0.0295],\n",
      "        [0.0220],\n",
      "        [0.0226],\n",
      "        [0.0279],\n",
      "        [0.0369],\n",
      "        [0.0340],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0421],\n",
      "        [0.0328],\n",
      "        [0.0201],\n",
      "        [0.0392],\n",
      "        [0.0295],\n",
      "        [0.0254],\n",
      "        [0.0301],\n",
      "        [0.0310],\n",
      "        [0.0270],\n",
      "        [0.0433],\n",
      "        [0.0363],\n",
      "        [0.0337],\n",
      "        [0.0404],\n",
      "        [0.0495],\n",
      "        [0.0349],\n",
      "        [0.0375],\n",
      "        [0.0373],\n",
      "        [0.0451],\n",
      "        [0.0375],\n",
      "        [0.0379],\n",
      "        [0.0401],\n",
      "        [0.0441],\n",
      "        [0.0452],\n",
      "        [0.0507],\n",
      "        [0.0377],\n",
      "        [0.0387],\n",
      "        [0.0536],\n",
      "        [0.0492],\n",
      "        [0.0545],\n",
      "        [0.0427],\n",
      "        [0.0474],\n",
      "        [0.0608],\n",
      "        [0.0605],\n",
      "        [0.0472],\n",
      "        [0.0459],\n",
      "        [0.0431],\n",
      "        [0.0511],\n",
      "        [0.0626],\n",
      "        [0.0537],\n",
      "        [0.0676],\n",
      "        [0.0533],\n",
      "        [0.0550],\n",
      "        [0.0533],\n",
      "        [0.0623],\n",
      "        [0.0590],\n",
      "        [0.0594],\n",
      "        [0.0617],\n",
      "        [0.0630],\n",
      "        [0.0745],\n",
      "        [0.0724],\n",
      "        [0.0762],\n",
      "        [0.0775],\n",
      "        [0.0847],\n",
      "        [0.0701],\n",
      "        [0.0670],\n",
      "        [0.0728],\n",
      "        [0.0876],\n",
      "        [0.0691],\n",
      "        [0.0741],\n",
      "        [0.0904],\n",
      "        [0.0764],\n",
      "        [0.0808],\n",
      "        [0.0725],\n",
      "        [0.0983],\n",
      "        [0.0814],\n",
      "        [0.1026],\n",
      "        [0.0928],\n",
      "        [0.0917],\n",
      "        [0.0990],\n",
      "        [0.0929],\n",
      "        [0.1110],\n",
      "        [0.1214],\n",
      "        [0.1170],\n",
      "        [0.1095],\n",
      "        [0.1122],\n",
      "        [0.1288],\n",
      "        [0.1349],\n",
      "        [0.1218],\n",
      "        [0.1217],\n",
      "        [0.1317],\n",
      "        [0.1455],\n",
      "        [0.1341],\n",
      "        [0.1486],\n",
      "        [0.1432],\n",
      "        [0.1710],\n",
      "        [0.1677]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 51.56880593299866\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.950809397996636e-06, 63)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [63, 87, 99, 119, 108, 61, 115, 98, 60, 49, 110, 86, 149, 38, 139, 144, 54, 120, 145, 129, 41, 84, 7, 64, 114, 132, 75, 62, 74, 19, 20, 133, 14, 103, 109, 96, 53, 94, 55, 111, 116, 102, 27, 107, 92, 91, 13, 150, 15, 95, 0, 118, 113, 6, 29, 88, 39, 93, 101, 10, 100, 28, 78, 30, 131, 138, 26, 117, 12, 123, 76, 66, 105, 85, 32, 73, 18, 77, 112, 146, 65, 106, 56, 40, 104, 97, 17, 89, 46, 135, 50, 21, 8, 48, 37, 79, 1, 59, 90, 16, 134, 31, 67, 5, 68, 143, 47, 137, 69, 52, 51, 130, 136, 72, 121, 11, 71, 128, 140, 25, 57, 83, 158, 70, 58, 122, 42, 22, 36, 124, 24, 142, 141, 147, 151, 23, 157, 148, 4, 45, 154, 125, 152, 2, 82, 156, 155, 44, 127, 3, 35, 153, 33, 34, 126, 9, 80] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6620],\n",
      "        [0.6128],\n",
      "        [0.6239],\n",
      "        [0.4905],\n",
      "        [0.5553],\n",
      "        [0.6875],\n",
      "        [0.5298],\n",
      "        [0.6383],\n",
      "        [0.6900],\n",
      "        [0.6664],\n",
      "        [0.5491],\n",
      "        [0.5582],\n",
      "        [0.2778],\n",
      "        [0.7535],\n",
      "        [0.3838],\n",
      "        [0.3564],\n",
      "        [0.7115],\n",
      "        [0.4653],\n",
      "        [0.3454],\n",
      "        [0.2819],\n",
      "        [0.7310],\n",
      "        [0.5009],\n",
      "        [0.9626],\n",
      "        [0.6241],\n",
      "        [0.5254],\n",
      "        [0.2965],\n",
      "        [0.6339],\n",
      "        [0.6888],\n",
      "        [0.6296],\n",
      "        [0.9233],\n",
      "        [0.8921],\n",
      "        [0.2932],\n",
      "        [0.9038],\n",
      "        [0.6020],\n",
      "        [0.5768],\n",
      "        [0.6602],\n",
      "        [0.6945],\n",
      "        [0.5982],\n",
      "        [0.7091],\n",
      "        [0.5302],\n",
      "        [0.5151],\n",
      "        [0.6249],\n",
      "        [0.8799],\n",
      "        [0.5724],\n",
      "        [0.5922],\n",
      "        [0.6001],\n",
      "        [0.8330],\n",
      "        [0.2823],\n",
      "        [0.9185],\n",
      "        [0.6522],\n",
      "        [0.8205],\n",
      "        [0.4930],\n",
      "        [0.5183],\n",
      "        [0.9447],\n",
      "        [0.8133],\n",
      "        [0.6078],\n",
      "        [0.7237],\n",
      "        [0.5873],\n",
      "        [0.6525],\n",
      "        [0.8677],\n",
      "        [0.6579],\n",
      "        [0.8489],\n",
      "        [0.6143],\n",
      "        [0.8610],\n",
      "        [0.2930],\n",
      "        [0.3372],\n",
      "        [0.8563],\n",
      "        [0.4952],\n",
      "        [0.8746],\n",
      "        [0.4110],\n",
      "        [0.6068],\n",
      "        [0.6111],\n",
      "        [0.5967],\n",
      "        [0.4937],\n",
      "        [0.8501],\n",
      "        [0.6383],\n",
      "        [0.9266],\n",
      "        [0.6011],\n",
      "        [0.4901],\n",
      "        [0.2773],\n",
      "        [0.6101],\n",
      "        [0.5698],\n",
      "        [0.6681],\n",
      "        [0.7209],\n",
      "        [0.6061],\n",
      "        [0.6330],\n",
      "        [0.9066],\n",
      "        [0.5838],\n",
      "        [0.6739],\n",
      "        [0.3159],\n",
      "        [0.6741],\n",
      "        [0.8768],\n",
      "        [0.9451],\n",
      "        [0.6677],\n",
      "        [0.7906],\n",
      "        [0.6121],\n",
      "        [0.8177],\n",
      "        [0.6578],\n",
      "        [0.5697],\n",
      "        [0.9327],\n",
      "        [0.3084],\n",
      "        [0.8443],\n",
      "        [0.6081],\n",
      "        [0.9149],\n",
      "        [0.6057],\n",
      "        [0.3454],\n",
      "        [0.6016],\n",
      "        [0.3040],\n",
      "        [0.6244],\n",
      "        [0.6753],\n",
      "        [0.6761],\n",
      "        [0.2422],\n",
      "        [0.2979],\n",
      "        [0.6276],\n",
      "        [0.4096],\n",
      "        [0.8922],\n",
      "        [0.6277],\n",
      "        [0.3344],\n",
      "        [0.3834],\n",
      "        [0.8512],\n",
      "        [0.6460],\n",
      "        [0.5677],\n",
      "        [0.3061],\n",
      "        [0.6238],\n",
      "        [0.6337],\n",
      "        [0.3901],\n",
      "        [0.7392],\n",
      "        [0.8591],\n",
      "        [0.8236],\n",
      "        [0.4071],\n",
      "        [0.8739],\n",
      "        [0.3519],\n",
      "        [0.3646],\n",
      "        [0.2210],\n",
      "        [0.3118],\n",
      "        [0.8666],\n",
      "        [0.2846],\n",
      "        [0.2270],\n",
      "        [0.8694],\n",
      "        [0.7169],\n",
      "        [0.2971],\n",
      "        [0.3977],\n",
      "        [0.3031],\n",
      "        [0.8360],\n",
      "        [0.5782],\n",
      "        [0.3057],\n",
      "        [0.3026],\n",
      "        [0.7502],\n",
      "        [0.3966],\n",
      "        [0.8659],\n",
      "        [0.7934],\n",
      "        [0.3060],\n",
      "        [0.8494],\n",
      "        [0.7979],\n",
      "        [0.3951],\n",
      "        [0.8790],\n",
      "        [0.6229]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0022],\n",
      "        [0.0023],\n",
      "        [0.0023],\n",
      "        [0.0032],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0068],\n",
      "        [0.0068],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0073],\n",
      "        [0.0081],\n",
      "        [0.0081],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0113],\n",
      "        [0.0115],\n",
      "        [0.0117],\n",
      "        [0.0120],\n",
      "        [0.0137],\n",
      "        [0.0139],\n",
      "        [0.0139],\n",
      "        [0.0146],\n",
      "        [0.0146],\n",
      "        [0.0157],\n",
      "        [0.0165],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.0216],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0226],\n",
      "        [0.0226],\n",
      "        [0.0235],\n",
      "        [0.0238],\n",
      "        [0.0244],\n",
      "        [0.0246],\n",
      "        [0.0254],\n",
      "        [0.0255],\n",
      "        [0.0256],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0270],\n",
      "        [0.0279],\n",
      "        [0.0284],\n",
      "        [0.0295],\n",
      "        [0.0295],\n",
      "        [0.0301],\n",
      "        [0.0310],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0349],\n",
      "        [0.0363],\n",
      "        [0.0369],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0379],\n",
      "        [0.0387],\n",
      "        [0.0392],\n",
      "        [0.0401],\n",
      "        [0.0404],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0433],\n",
      "        [0.0441],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0459],\n",
      "        [0.0472],\n",
      "        [0.0474],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0507],\n",
      "        [0.0511],\n",
      "        [0.0533],\n",
      "        [0.0533],\n",
      "        [0.0536],\n",
      "        [0.0537],\n",
      "        [0.0545],\n",
      "        [0.0550],\n",
      "        [0.0590],\n",
      "        [0.0594],\n",
      "        [0.0605],\n",
      "        [0.0608],\n",
      "        [0.0617],\n",
      "        [0.0623],\n",
      "        [0.0626],\n",
      "        [0.0630],\n",
      "        [0.0670],\n",
      "        [0.0676],\n",
      "        [0.0691],\n",
      "        [0.0701],\n",
      "        [0.0724],\n",
      "        [0.0725],\n",
      "        [0.0728],\n",
      "        [0.0741],\n",
      "        [0.0745],\n",
      "        [0.0762],\n",
      "        [0.0764],\n",
      "        [0.0775],\n",
      "        [0.0808],\n",
      "        [0.0814],\n",
      "        [0.0847],\n",
      "        [0.0876],\n",
      "        [0.0904],\n",
      "        [0.0917],\n",
      "        [0.0928],\n",
      "        [0.0929],\n",
      "        [0.0983],\n",
      "        [0.0990],\n",
      "        [0.1026],\n",
      "        [0.1095],\n",
      "        [0.1110],\n",
      "        [0.1122],\n",
      "        [0.1170],\n",
      "        [0.1214],\n",
      "        [0.1217],\n",
      "        [0.1218],\n",
      "        [0.1288],\n",
      "        [0.1317],\n",
      "        [0.1341],\n",
      "        [0.1349],\n",
      "        [0.1432],\n",
      "        [0.1455],\n",
      "        [0.1486],\n",
      "        [0.1677],\n",
      "        [0.1710],\n",
      "        [0.2025]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0004],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0034],\n",
      "        [0.0027],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0067],\n",
      "        [0.0035],\n",
      "        [0.0065],\n",
      "        [0.0051],\n",
      "        [0.0045],\n",
      "        [0.0090],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0064],\n",
      "        [0.0090],\n",
      "        [0.0110],\n",
      "        [0.0101],\n",
      "        [0.0082],\n",
      "        [0.0116],\n",
      "        [0.0078],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0137],\n",
      "        [0.0119],\n",
      "        [0.0107],\n",
      "        [0.0124],\n",
      "        [0.0109],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0116],\n",
      "        [0.0146],\n",
      "        [0.0159],\n",
      "        [0.0172],\n",
      "        [0.0189],\n",
      "        [0.0170],\n",
      "        [0.0161],\n",
      "        [0.0174],\n",
      "        [0.0190],\n",
      "        [0.0175],\n",
      "        [0.0171],\n",
      "        [0.0182],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0219],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0209],\n",
      "        [0.0242],\n",
      "        [0.0212],\n",
      "        [0.0225],\n",
      "        [0.0251],\n",
      "        [0.0227],\n",
      "        [0.0209],\n",
      "        [0.0242],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0232],\n",
      "        [0.0240],\n",
      "        [0.0262],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0284],\n",
      "        [0.0286],\n",
      "        [0.0278],\n",
      "        [0.0287],\n",
      "        [0.0265],\n",
      "        [0.0296],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0310],\n",
      "        [0.0342],\n",
      "        [0.0329],\n",
      "        [0.0346],\n",
      "        [0.0310],\n",
      "        [0.0352],\n",
      "        [0.0362],\n",
      "        [0.0380],\n",
      "        [0.0374],\n",
      "        [0.0372],\n",
      "        [0.0385],\n",
      "        [0.0384],\n",
      "        [0.0380],\n",
      "        [0.0394],\n",
      "        [0.0362],\n",
      "        [0.0413],\n",
      "        [0.0390],\n",
      "        [0.0443],\n",
      "        [0.0444],\n",
      "        [0.0446],\n",
      "        [0.0407],\n",
      "        [0.0423],\n",
      "        [0.0428],\n",
      "        [0.0445],\n",
      "        [0.0479],\n",
      "        [0.0488],\n",
      "        [0.0484],\n",
      "        [0.0460],\n",
      "        [0.0516],\n",
      "        [0.0503],\n",
      "        [0.0511],\n",
      "        [0.0563],\n",
      "        [0.0535],\n",
      "        [0.0562],\n",
      "        [0.0549],\n",
      "        [0.0567],\n",
      "        [0.0558],\n",
      "        [0.0607],\n",
      "        [0.0611],\n",
      "        [0.0619],\n",
      "        [0.0629],\n",
      "        [0.0625],\n",
      "        [0.0621],\n",
      "        [0.0609],\n",
      "        [0.0638],\n",
      "        [0.0661],\n",
      "        [0.0706],\n",
      "        [0.0699],\n",
      "        [0.0708],\n",
      "        [0.0711],\n",
      "        [0.0683],\n",
      "        [0.0737],\n",
      "        [0.0750],\n",
      "        [0.0744],\n",
      "        [0.0748],\n",
      "        [0.0774],\n",
      "        [0.0754],\n",
      "        [0.0807],\n",
      "        [0.0824],\n",
      "        [0.0874],\n",
      "        [0.0903],\n",
      "        [0.0913],\n",
      "        [0.0887],\n",
      "        [0.0937],\n",
      "        [0.0890],\n",
      "        [0.0996],\n",
      "        [0.1015],\n",
      "        [0.1012],\n",
      "        [0.1063],\n",
      "        [0.1108],\n",
      "        [0.1095],\n",
      "        [0.1195],\n",
      "        [0.1200],\n",
      "        [0.1177],\n",
      "        [0.1182],\n",
      "        [0.1274],\n",
      "        [0.1310],\n",
      "        [0.1367],\n",
      "        [0.1333],\n",
      "        [0.1400],\n",
      "        [0.1442],\n",
      "        [0.1472],\n",
      "        [0.1672],\n",
      "        [0.1695],\n",
      "        [0.2015]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 51.85949110984802\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4379624246885214e-07, 87)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [87, 63, 99, 119, 61, 108, 49, 149, 115, 86, 98, 54, 110, 60, 7, 41, 38, 120, 129, 139, 144, 62, 19, 145, 64, 84, 14, 114, 75, 74, 132, 103, 109, 20, 55, 133, 94, 27, 96, 150, 111, 15, 102, 107, 53, 116, 92, 91, 95, 88, 118, 13, 113, 29, 93, 10, 101, 100, 39, 0, 78, 30, 6, 28, 12, 26, 131, 138, 117, 123, 76, 66, 18, 105, 32, 85, 73, 77, 112, 17, 106, 65, 104, 146, 40, 56, 46, 97, 8, 89, 48, 37, 135, 50, 79, 21, 16, 1, 90, 59, 31, 67, 134, 68, 47, 69, 143, 5, 137, 52, 11, 51, 130, 121, 72, 136, 71, 128, 158, 25, 140, 57, 83, 70, 122, 42, 58, 36, 22, 124, 24, 142, 151, 157, 141, 147, 23, 148, 45, 4, 154, 152, 125, 156, 155, 2, 82, 44, 127, 35, 3, 153, 33, 34, 126, 9, 80, 81] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6109],\n",
      "        [0.6614],\n",
      "        [0.6237],\n",
      "        [0.4897],\n",
      "        [0.6863],\n",
      "        [0.5556],\n",
      "        [0.6646],\n",
      "        [0.2754],\n",
      "        [0.5289],\n",
      "        [0.5566],\n",
      "        [0.6378],\n",
      "        [0.7098],\n",
      "        [0.5494],\n",
      "        [0.6883],\n",
      "        [0.9599],\n",
      "        [0.7300],\n",
      "        [0.7516],\n",
      "        [0.4649],\n",
      "        [0.2809],\n",
      "        [0.3807],\n",
      "        [0.3540],\n",
      "        [0.6879],\n",
      "        [0.9205],\n",
      "        [0.3433],\n",
      "        [0.6239],\n",
      "        [0.4998],\n",
      "        [0.9009],\n",
      "        [0.5248],\n",
      "        [0.6335],\n",
      "        [0.6291],\n",
      "        [0.2941],\n",
      "        [0.6019],\n",
      "        [0.5769],\n",
      "        [0.8899],\n",
      "        [0.7075],\n",
      "        [0.2908],\n",
      "        [0.5977],\n",
      "        [0.8787],\n",
      "        [0.6595],\n",
      "        [0.2796],\n",
      "        [0.5306],\n",
      "        [0.9153],\n",
      "        [0.6245],\n",
      "        [0.5724],\n",
      "        [0.6928],\n",
      "        [0.5140],\n",
      "        [0.5916],\n",
      "        [0.5993],\n",
      "        [0.6514],\n",
      "        [0.6060],\n",
      "        [0.4922],\n",
      "        [0.8305],\n",
      "        [0.5181],\n",
      "        [0.8130],\n",
      "        [0.5869],\n",
      "        [0.8664],\n",
      "        [0.6519],\n",
      "        [0.6574],\n",
      "        [0.7221],\n",
      "        [0.8183],\n",
      "        [0.6135],\n",
      "        [0.8605],\n",
      "        [0.9418],\n",
      "        [0.8481],\n",
      "        [0.8728],\n",
      "        [0.8555],\n",
      "        [0.2908],\n",
      "        [0.3348],\n",
      "        [0.4943],\n",
      "        [0.4108],\n",
      "        [0.6066],\n",
      "        [0.6111],\n",
      "        [0.9235],\n",
      "        [0.5968],\n",
      "        [0.8495],\n",
      "        [0.4923],\n",
      "        [0.6374],\n",
      "        [0.6008],\n",
      "        [0.4903],\n",
      "        [0.9036],\n",
      "        [0.5701],\n",
      "        [0.6100],\n",
      "        [0.6060],\n",
      "        [0.2762],\n",
      "        [0.7202],\n",
      "        [0.6672],\n",
      "        [0.6725],\n",
      "        [0.6323],\n",
      "        [0.9425],\n",
      "        [0.5826],\n",
      "        [0.6659],\n",
      "        [0.7884],\n",
      "        [0.3137],\n",
      "        [0.6723],\n",
      "        [0.6114],\n",
      "        [0.8754],\n",
      "        [0.9295],\n",
      "        [0.8156],\n",
      "        [0.5687],\n",
      "        [0.6562],\n",
      "        [0.8440],\n",
      "        [0.6080],\n",
      "        [0.3063],\n",
      "        [0.6055],\n",
      "        [0.6004],\n",
      "        [0.6236],\n",
      "        [0.3427],\n",
      "        [0.9119],\n",
      "        [0.3018],\n",
      "        [0.6735],\n",
      "        [0.8905],\n",
      "        [0.6744],\n",
      "        [0.2407],\n",
      "        [0.4099],\n",
      "        [0.6268],\n",
      "        [0.2958],\n",
      "        [0.6270],\n",
      "        [0.3336],\n",
      "        [0.3019],\n",
      "        [0.8504],\n",
      "        [0.3804],\n",
      "        [0.6452],\n",
      "        [0.5664],\n",
      "        [0.6229],\n",
      "        [0.3902],\n",
      "        [0.7378],\n",
      "        [0.6328],\n",
      "        [0.8216],\n",
      "        [0.8581],\n",
      "        [0.4071],\n",
      "        [0.8729],\n",
      "        [0.3491],\n",
      "        [0.3089],\n",
      "        [0.2807],\n",
      "        [0.3619],\n",
      "        [0.2201],\n",
      "        [0.8657],\n",
      "        [0.2257],\n",
      "        [0.7155],\n",
      "        [0.8670],\n",
      "        [0.2939],\n",
      "        [0.3004],\n",
      "        [0.3975],\n",
      "        [0.3018],\n",
      "        [0.2990],\n",
      "        [0.8335],\n",
      "        [0.5769],\n",
      "        [0.7487],\n",
      "        [0.3959],\n",
      "        [0.7918],\n",
      "        [0.8633],\n",
      "        [0.3028],\n",
      "        [0.8481],\n",
      "        [0.7964],\n",
      "        [0.3946],\n",
      "        [0.8774],\n",
      "        [0.6220],\n",
      "        [0.6380]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0045],\n",
      "        [0.0051],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0078],\n",
      "        [0.0082],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0119],\n",
      "        [0.0124],\n",
      "        [0.0137],\n",
      "        [0.0146],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0170],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0175],\n",
      "        [0.0182],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0219],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0240],\n",
      "        [0.0242],\n",
      "        [0.0242],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0251],\n",
      "        [0.0262],\n",
      "        [0.0265],\n",
      "        [0.0278],\n",
      "        [0.0284],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0296],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0310],\n",
      "        [0.0310],\n",
      "        [0.0329],\n",
      "        [0.0342],\n",
      "        [0.0346],\n",
      "        [0.0352],\n",
      "        [0.0362],\n",
      "        [0.0362],\n",
      "        [0.0372],\n",
      "        [0.0374],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0390],\n",
      "        [0.0394],\n",
      "        [0.0407],\n",
      "        [0.0413],\n",
      "        [0.0423],\n",
      "        [0.0428],\n",
      "        [0.0443],\n",
      "        [0.0444],\n",
      "        [0.0445],\n",
      "        [0.0446],\n",
      "        [0.0460],\n",
      "        [0.0479],\n",
      "        [0.0484],\n",
      "        [0.0488],\n",
      "        [0.0503],\n",
      "        [0.0511],\n",
      "        [0.0516],\n",
      "        [0.0535],\n",
      "        [0.0549],\n",
      "        [0.0558],\n",
      "        [0.0562],\n",
      "        [0.0563],\n",
      "        [0.0567],\n",
      "        [0.0607],\n",
      "        [0.0609],\n",
      "        [0.0611],\n",
      "        [0.0619],\n",
      "        [0.0621],\n",
      "        [0.0625],\n",
      "        [0.0629],\n",
      "        [0.0638],\n",
      "        [0.0661],\n",
      "        [0.0683],\n",
      "        [0.0699],\n",
      "        [0.0706],\n",
      "        [0.0708],\n",
      "        [0.0711],\n",
      "        [0.0737],\n",
      "        [0.0744],\n",
      "        [0.0748],\n",
      "        [0.0750],\n",
      "        [0.0754],\n",
      "        [0.0774],\n",
      "        [0.0807],\n",
      "        [0.0824],\n",
      "        [0.0874],\n",
      "        [0.0887],\n",
      "        [0.0890],\n",
      "        [0.0903],\n",
      "        [0.0913],\n",
      "        [0.0937],\n",
      "        [0.0996],\n",
      "        [0.1012],\n",
      "        [0.1015],\n",
      "        [0.1063],\n",
      "        [0.1095],\n",
      "        [0.1108],\n",
      "        [0.1177],\n",
      "        [0.1182],\n",
      "        [0.1195],\n",
      "        [0.1200],\n",
      "        [0.1274],\n",
      "        [0.1310],\n",
      "        [0.1333],\n",
      "        [0.1367],\n",
      "        [0.1400],\n",
      "        [0.1442],\n",
      "        [0.1472],\n",
      "        [0.1672],\n",
      "        [0.1695],\n",
      "        [0.2015],\n",
      "        [0.2143]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0025],\n",
      "        [0.0002],\n",
      "        [0.0009],\n",
      "        [0.0013],\n",
      "        [0.0009],\n",
      "        [0.0036],\n",
      "        [0.0012],\n",
      "        [0.0033],\n",
      "        [0.0064],\n",
      "        [0.0025],\n",
      "        [0.0067],\n",
      "        [0.0041],\n",
      "        [0.0067],\n",
      "        [0.0087],\n",
      "        [0.0057],\n",
      "        [0.0073],\n",
      "        [0.0104],\n",
      "        [0.0097],\n",
      "        [0.0107],\n",
      "        [0.0120],\n",
      "        [0.0115],\n",
      "        [0.0090],\n",
      "        [0.0087],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0137],\n",
      "        [0.0092],\n",
      "        [0.0128],\n",
      "        [0.0137],\n",
      "        [0.0143],\n",
      "        [0.0148],\n",
      "        [0.0156],\n",
      "        [0.0154],\n",
      "        [0.0176],\n",
      "        [0.0138],\n",
      "        [0.0172],\n",
      "        [0.0154],\n",
      "        [0.0163],\n",
      "        [0.0189],\n",
      "        [0.0158],\n",
      "        [0.0175],\n",
      "        [0.0148],\n",
      "        [0.0163],\n",
      "        [0.0188],\n",
      "        [0.0213],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0189],\n",
      "        [0.0183],\n",
      "        [0.0199],\n",
      "        [0.0236],\n",
      "        [0.0230],\n",
      "        [0.0228],\n",
      "        [0.0218],\n",
      "        [0.0227],\n",
      "        [0.0218],\n",
      "        [0.0226],\n",
      "        [0.0254],\n",
      "        [0.0257],\n",
      "        [0.0225],\n",
      "        [0.0246],\n",
      "        [0.0273],\n",
      "        [0.0267],\n",
      "        [0.0256],\n",
      "        [0.0283],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0299],\n",
      "        [0.0314],\n",
      "        [0.0309],\n",
      "        [0.0287],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0364],\n",
      "        [0.0368],\n",
      "        [0.0369],\n",
      "        [0.0362],\n",
      "        [0.0338],\n",
      "        [0.0375],\n",
      "        [0.0382],\n",
      "        [0.0388],\n",
      "        [0.0379],\n",
      "        [0.0392],\n",
      "        [0.0401],\n",
      "        [0.0376],\n",
      "        [0.0411],\n",
      "        [0.0388],\n",
      "        [0.0435],\n",
      "        [0.0402],\n",
      "        [0.0413],\n",
      "        [0.0452],\n",
      "        [0.0468],\n",
      "        [0.0425],\n",
      "        [0.0454],\n",
      "        [0.0432],\n",
      "        [0.0492],\n",
      "        [0.0504],\n",
      "        [0.0507],\n",
      "        [0.0501],\n",
      "        [0.0520],\n",
      "        [0.0523],\n",
      "        [0.0545],\n",
      "        [0.0562],\n",
      "        [0.0575],\n",
      "        [0.0575],\n",
      "        [0.0587],\n",
      "        [0.0577],\n",
      "        [0.0632],\n",
      "        [0.0600],\n",
      "        [0.0634],\n",
      "        [0.0625],\n",
      "        [0.0620],\n",
      "        [0.0644],\n",
      "        [0.0639],\n",
      "        [0.0656],\n",
      "        [0.0654],\n",
      "        [0.0650],\n",
      "        [0.0703],\n",
      "        [0.0722],\n",
      "        [0.0721],\n",
      "        [0.0688],\n",
      "        [0.0755],\n",
      "        [0.0745],\n",
      "        [0.0734],\n",
      "        [0.0764],\n",
      "        [0.0740],\n",
      "        [0.0777],\n",
      "        [0.0806],\n",
      "        [0.0831],\n",
      "        [0.0888],\n",
      "        [0.0868],\n",
      "        [0.0862],\n",
      "        [0.0917],\n",
      "        [0.0910],\n",
      "        [0.0942],\n",
      "        [0.0997],\n",
      "        [0.0997],\n",
      "        [0.1034],\n",
      "        [0.1040],\n",
      "        [0.1077],\n",
      "        [0.1107],\n",
      "        [0.1147],\n",
      "        [0.1156],\n",
      "        [0.1211],\n",
      "        [0.1176],\n",
      "        [0.1259],\n",
      "        [0.1303],\n",
      "        [0.1323],\n",
      "        [0.1387],\n",
      "        [0.1378],\n",
      "        [0.1431],\n",
      "        [0.1462],\n",
      "        [0.1668],\n",
      "        [0.1688],\n",
      "        [0.1993],\n",
      "        [0.2116]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 52.15105152130127\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.296144323096996e-08, 63)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [63, 99, 61, 49, 119, 86, 87, 149, 108, 54, 7, 115, 98, 110, 41, 60, 19, 62, 14, 120, 38, 129, 144, 145, 139, 64, 114, 84, 75, 55, 74, 15, 132, 94, 109, 103, 150, 27, 102, 133, 111, 20, 88, 107, 96, 95, 118, 116, 92, 91, 53, 93, 101, 78, 100, 10, 29, 113, 13, 30, 39, 12, 0, 28, 6, 26, 18, 131, 138, 123, 117, 66, 76, 105, 32, 17, 112, 85, 73, 77, 106, 46, 146, 65, 8, 104, 40, 56, 48, 97, 37, 79, 16, 89, 135, 21, 50, 1, 31, 90, 59, 67, 134, 68, 47, 69, 143, 137, 5, 11, 121, 130, 52, 51, 136, 72, 158, 128, 71, 83, 25, 57, 140, 42, 36, 122, 70, 58, 22, 124, 24, 157, 151, 142, 147, 141, 23, 148, 45, 4, 154, 152, 125, 156, 155, 82, 2, 44, 127, 35, 153, 3, 33, 34, 126, 9, 80, 81, 43] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6600],\n",
      "        [0.6224],\n",
      "        [0.6845],\n",
      "        [0.6623],\n",
      "        [0.4886],\n",
      "        [0.5540],\n",
      "        [0.6080],\n",
      "        [0.2743],\n",
      "        [0.5554],\n",
      "        [0.7075],\n",
      "        [0.9579],\n",
      "        [0.5276],\n",
      "        [0.6363],\n",
      "        [0.5491],\n",
      "        [0.7291],\n",
      "        [0.6864],\n",
      "        [0.9183],\n",
      "        [0.6862],\n",
      "        [0.8985],\n",
      "        [0.4642],\n",
      "        [0.7502],\n",
      "        [0.2802],\n",
      "        [0.3530],\n",
      "        [0.3425],\n",
      "        [0.3791],\n",
      "        [0.6229],\n",
      "        [0.5238],\n",
      "        [0.4977],\n",
      "        [0.6317],\n",
      "        [0.7052],\n",
      "        [0.6273],\n",
      "        [0.9126],\n",
      "        [0.2930],\n",
      "        [0.5961],\n",
      "        [0.5765],\n",
      "        [0.6010],\n",
      "        [0.2780],\n",
      "        [0.8779],\n",
      "        [0.6233],\n",
      "        [0.2899],\n",
      "        [0.5305],\n",
      "        [0.8884],\n",
      "        [0.6034],\n",
      "        [0.5719],\n",
      "        [0.6578],\n",
      "        [0.6495],\n",
      "        [0.4908],\n",
      "        [0.5124],\n",
      "        [0.5901],\n",
      "        [0.5975],\n",
      "        [0.6904],\n",
      "        [0.5856],\n",
      "        [0.6505],\n",
      "        [0.6113],\n",
      "        [0.6559],\n",
      "        [0.8660],\n",
      "        [0.8129],\n",
      "        [0.5176],\n",
      "        [0.8288],\n",
      "        [0.8601],\n",
      "        [0.7209],\n",
      "        [0.8718],\n",
      "        [0.8169],\n",
      "        [0.8476],\n",
      "        [0.9396],\n",
      "        [0.8550],\n",
      "        [0.9212],\n",
      "        [0.2897],\n",
      "        [0.3337],\n",
      "        [0.4106],\n",
      "        [0.4931],\n",
      "        [0.6103],\n",
      "        [0.6049],\n",
      "        [0.5961],\n",
      "        [0.8489],\n",
      "        [0.9012],\n",
      "        [0.4903],\n",
      "        [0.4902],\n",
      "        [0.6352],\n",
      "        [0.5991],\n",
      "        [0.5698],\n",
      "        [0.6711],\n",
      "        [0.2762],\n",
      "        [0.6092],\n",
      "        [0.9406],\n",
      "        [0.6051],\n",
      "        [0.7194],\n",
      "        [0.6655],\n",
      "        [0.6638],\n",
      "        [0.6307],\n",
      "        [0.7868],\n",
      "        [0.6094],\n",
      "        [0.9267],\n",
      "        [0.5804],\n",
      "        [0.3128],\n",
      "        [0.8746],\n",
      "        [0.6700],\n",
      "        [0.8143],\n",
      "        [0.8437],\n",
      "        [0.5667],\n",
      "        [0.6543],\n",
      "        [0.6071],\n",
      "        [0.3056],\n",
      "        [0.6045],\n",
      "        [0.5991],\n",
      "        [0.6220],\n",
      "        [0.3415],\n",
      "        [0.3008],\n",
      "        [0.9094],\n",
      "        [0.8896],\n",
      "        [0.4099],\n",
      "        [0.2402],\n",
      "        [0.6711],\n",
      "        [0.6721],\n",
      "        [0.2949],\n",
      "        [0.6249],\n",
      "        [0.2986],\n",
      "        [0.3329],\n",
      "        [0.6251],\n",
      "        [0.5641],\n",
      "        [0.8499],\n",
      "        [0.6440],\n",
      "        [0.3788],\n",
      "        [0.7364],\n",
      "        [0.8201],\n",
      "        [0.3901],\n",
      "        [0.6211],\n",
      "        [0.6314],\n",
      "        [0.8577],\n",
      "        [0.4070],\n",
      "        [0.8721],\n",
      "        [0.2779],\n",
      "        [0.3070],\n",
      "        [0.3477],\n",
      "        [0.2204],\n",
      "        [0.3605],\n",
      "        [0.8651],\n",
      "        [0.2256],\n",
      "        [0.7141],\n",
      "        [0.8651],\n",
      "        [0.2916],\n",
      "        [0.2986],\n",
      "        [0.3974],\n",
      "        [0.2987],\n",
      "        [0.2964],\n",
      "        [0.5745],\n",
      "        [0.8319],\n",
      "        [0.7472],\n",
      "        [0.3953],\n",
      "        [0.7908],\n",
      "        [0.3005],\n",
      "        [0.8613],\n",
      "        [0.8469],\n",
      "        [0.7954],\n",
      "        [0.3942],\n",
      "        [0.8767],\n",
      "        [0.6198],\n",
      "        [0.6353],\n",
      "        [0.7374]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0025],\n",
      "        [    0.0025],\n",
      "        [    0.0033],\n",
      "        [    0.0036],\n",
      "        [    0.0041],\n",
      "        [    0.0057],\n",
      "        [    0.0064],\n",
      "        [    0.0067],\n",
      "        [    0.0067],\n",
      "        [    0.0073],\n",
      "        [    0.0087],\n",
      "        [    0.0087],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0097],\n",
      "        [    0.0104],\n",
      "        [    0.0107],\n",
      "        [    0.0115],\n",
      "        [    0.0118],\n",
      "        [    0.0120],\n",
      "        [    0.0124],\n",
      "        [    0.0128],\n",
      "        [    0.0137],\n",
      "        [    0.0137],\n",
      "        [    0.0138],\n",
      "        [    0.0143],\n",
      "        [    0.0148],\n",
      "        [    0.0148],\n",
      "        [    0.0154],\n",
      "        [    0.0154],\n",
      "        [    0.0156],\n",
      "        [    0.0158],\n",
      "        [    0.0163],\n",
      "        [    0.0163],\n",
      "        [    0.0172],\n",
      "        [    0.0175],\n",
      "        [    0.0176],\n",
      "        [    0.0183],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0189],\n",
      "        [    0.0199],\n",
      "        [    0.0205],\n",
      "        [    0.0207],\n",
      "        [    0.0212],\n",
      "        [    0.0213],\n",
      "        [    0.0218],\n",
      "        [    0.0218],\n",
      "        [    0.0225],\n",
      "        [    0.0226],\n",
      "        [    0.0227],\n",
      "        [    0.0228],\n",
      "        [    0.0230],\n",
      "        [    0.0236],\n",
      "        [    0.0246],\n",
      "        [    0.0254],\n",
      "        [    0.0256],\n",
      "        [    0.0257],\n",
      "        [    0.0267],\n",
      "        [    0.0273],\n",
      "        [    0.0283],\n",
      "        [    0.0287],\n",
      "        [    0.0296],\n",
      "        [    0.0297],\n",
      "        [    0.0299],\n",
      "        [    0.0300],\n",
      "        [    0.0309],\n",
      "        [    0.0314],\n",
      "        [    0.0316],\n",
      "        [    0.0323],\n",
      "        [    0.0338],\n",
      "        [    0.0362],\n",
      "        [    0.0364],\n",
      "        [    0.0368],\n",
      "        [    0.0369],\n",
      "        [    0.0375],\n",
      "        [    0.0376],\n",
      "        [    0.0379],\n",
      "        [    0.0382],\n",
      "        [    0.0388],\n",
      "        [    0.0388],\n",
      "        [    0.0392],\n",
      "        [    0.0401],\n",
      "        [    0.0402],\n",
      "        [    0.0411],\n",
      "        [    0.0413],\n",
      "        [    0.0425],\n",
      "        [    0.0432],\n",
      "        [    0.0435],\n",
      "        [    0.0452],\n",
      "        [    0.0454],\n",
      "        [    0.0468],\n",
      "        [    0.0492],\n",
      "        [    0.0501],\n",
      "        [    0.0504],\n",
      "        [    0.0507],\n",
      "        [    0.0520],\n",
      "        [    0.0523],\n",
      "        [    0.0545],\n",
      "        [    0.0562],\n",
      "        [    0.0575],\n",
      "        [    0.0575],\n",
      "        [    0.0577],\n",
      "        [    0.0587],\n",
      "        [    0.0600],\n",
      "        [    0.0620],\n",
      "        [    0.0625],\n",
      "        [    0.0632],\n",
      "        [    0.0634],\n",
      "        [    0.0639],\n",
      "        [    0.0644],\n",
      "        [    0.0650],\n",
      "        [    0.0654],\n",
      "        [    0.0656],\n",
      "        [    0.0688],\n",
      "        [    0.0703],\n",
      "        [    0.0721],\n",
      "        [    0.0722],\n",
      "        [    0.0734],\n",
      "        [    0.0740],\n",
      "        [    0.0745],\n",
      "        [    0.0755],\n",
      "        [    0.0764],\n",
      "        [    0.0777],\n",
      "        [    0.0806],\n",
      "        [    0.0831],\n",
      "        [    0.0862],\n",
      "        [    0.0868],\n",
      "        [    0.0888],\n",
      "        [    0.0910],\n",
      "        [    0.0917],\n",
      "        [    0.0942],\n",
      "        [    0.0997],\n",
      "        [    0.0997],\n",
      "        [    0.1034],\n",
      "        [    0.1040],\n",
      "        [    0.1077],\n",
      "        [    0.1107],\n",
      "        [    0.1147],\n",
      "        [    0.1156],\n",
      "        [    0.1176],\n",
      "        [    0.1211],\n",
      "        [    0.1259],\n",
      "        [    0.1303],\n",
      "        [    0.1323],\n",
      "        [    0.1378],\n",
      "        [    0.1387],\n",
      "        [    0.1431],\n",
      "        [    0.1462],\n",
      "        [    0.1668],\n",
      "        [    0.1688],\n",
      "        [    0.1993],\n",
      "        [    0.2116],\n",
      "        [    0.2763]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0018],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0026],\n",
      "        [0.0016],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0015],\n",
      "        [0.0008],\n",
      "        [0.0042],\n",
      "        [0.0053],\n",
      "        [0.0061],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0114],\n",
      "        [0.0071],\n",
      "        [0.0066],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0129],\n",
      "        [0.0087],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0119],\n",
      "        [0.0139],\n",
      "        [0.0114],\n",
      "        [0.0143],\n",
      "        [0.0149],\n",
      "        [0.0105],\n",
      "        [0.0156],\n",
      "        [0.0128],\n",
      "        [0.0135],\n",
      "        [0.0156],\n",
      "        [0.0173],\n",
      "        [0.0142],\n",
      "        [0.0161],\n",
      "        [0.0154],\n",
      "        [0.0173],\n",
      "        [0.0158],\n",
      "        [0.0152],\n",
      "        [0.0185],\n",
      "        [0.0173],\n",
      "        [0.0170],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0210],\n",
      "        [0.0196],\n",
      "        [0.0204],\n",
      "        [0.0214],\n",
      "        [0.0247],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0211],\n",
      "        [0.0232],\n",
      "        [0.0231],\n",
      "        [0.0229],\n",
      "        [0.0211],\n",
      "        [0.0244],\n",
      "        [0.0242],\n",
      "        [0.0277],\n",
      "        [0.0254],\n",
      "        [0.0265],\n",
      "        [0.0272],\n",
      "        [0.0290],\n",
      "        [0.0288],\n",
      "        [0.0269],\n",
      "        [0.0282],\n",
      "        [0.0290],\n",
      "        [0.0276],\n",
      "        [0.0287],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0300],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0337],\n",
      "        [0.0369],\n",
      "        [0.0386],\n",
      "        [0.0379],\n",
      "        [0.0355],\n",
      "        [0.0350],\n",
      "        [0.0362],\n",
      "        [0.0394],\n",
      "        [0.0375],\n",
      "        [0.0374],\n",
      "        [0.0413],\n",
      "        [0.0427],\n",
      "        [0.0372],\n",
      "        [0.0406],\n",
      "        [0.0386],\n",
      "        [0.0414],\n",
      "        [0.0411],\n",
      "        [0.0440],\n",
      "        [0.0441],\n",
      "        [0.0456],\n",
      "        [0.0503],\n",
      "        [0.0499],\n",
      "        [0.0498],\n",
      "        [0.0507],\n",
      "        [0.0532],\n",
      "        [0.0530],\n",
      "        [0.0507],\n",
      "        [0.0554],\n",
      "        [0.0583],\n",
      "        [0.0590],\n",
      "        [0.0571],\n",
      "        [0.0567],\n",
      "        [0.0606],\n",
      "        [0.0599],\n",
      "        [0.0595],\n",
      "        [0.0605],\n",
      "        [0.0666],\n",
      "        [0.0668],\n",
      "        [0.0628],\n",
      "        [0.0660],\n",
      "        [0.0639],\n",
      "        [0.0673],\n",
      "        [0.0672],\n",
      "        [0.0677],\n",
      "        [0.0706],\n",
      "        [0.0740],\n",
      "        [0.0722],\n",
      "        [0.0707],\n",
      "        [0.0714],\n",
      "        [0.0721],\n",
      "        [0.0771],\n",
      "        [0.0783],\n",
      "        [0.0775],\n",
      "        [0.0831],\n",
      "        [0.0836],\n",
      "        [0.0855],\n",
      "        [0.0869],\n",
      "        [0.0886],\n",
      "        [0.0888],\n",
      "        [0.0915],\n",
      "        [0.0945],\n",
      "        [0.0979],\n",
      "        [0.0969],\n",
      "        [0.1047],\n",
      "        [0.1038],\n",
      "        [0.1080],\n",
      "        [0.1131],\n",
      "        [0.1137],\n",
      "        [0.1150],\n",
      "        [0.1163],\n",
      "        [0.1222],\n",
      "        [0.1229],\n",
      "        [0.1322],\n",
      "        [0.1303],\n",
      "        [0.1376],\n",
      "        [0.1400],\n",
      "        [0.1416],\n",
      "        [0.1445],\n",
      "        [0.1690],\n",
      "        [0.1688],\n",
      "        [0.1981],\n",
      "        [0.2100],\n",
      "        [0.2737]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 52.44053649902344\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 10 個區塊累積花費時間(s) 1.228367567062378\n",
      "<<The performance of 10 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.228367567062378\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1263.09\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1714.15\n",
      "The accuracy(2000) for l = 1: 81.13%\n",
      "The accuracy(3000) for l = 1: 91.82%\n",
      "The maximum error: tensor(6987.4688)\n",
      "The minimum error: tensor(21.3906)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 3096.1\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 3276.1\n",
      "The accuracy(2000) for l = 1: 25.0%\n",
      "The accuracy(3000) for l = 1: 25.0%\n",
      "The maximum error: 3970.75\n",
      "The minimum error: 1325.1171875\n",
      "------------------------------------------------------------\n",
      "0.8113207547169812\n",
      "<class 'float'>\n",
      "0.25\n",
      "<class 'float'>\n",
      "The <<11>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.019136774033541e-07, 50)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [50, 104, 57, 82, 59, 95, 45, 115, 83, 145, 3, 106, 37, 111, 94, 58, 15, 10, 116, 125, 51, 140, 141, 110, 56, 135, 11, 34, 128, 60, 99, 80, 71, 107, 23, 90, 70, 129, 146, 103, 105, 98, 84, 16, 92, 91, 112, 88, 114, 109, 74, 87, 89, 97, 25, 6, 96, 26, 9, 49, 8, 14, 24, 119, 35, 127, 113, 22, 134, 2, 101, 28, 13, 62, 72, 108, 42, 102, 142, 81, 44, 100, 4, 73, 33, 69, 61, 93, 12, 36, 75, 52, 85, 131, 17, 27, 46, 130, 86, 155, 63, 55, 64, 133, 139, 43, 65, 117, 7, 126, 1, 132, 154, 68, 48, 47, 67, 124, 79, 21, 38, 32, 118, 136, 53, 66, 18, 54, 120, 20, 153, 147, 138, 143, 137, 19, 41, 144, 150, 0, 148, 121, 152, 151, 78, 40, 158, 31, 123, 149, 29, 30, 157, 156, 5]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0026],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0071],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0087],\n",
      "        [0.0105],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0161],\n",
      "        [0.0170],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0196],\n",
      "        [0.0204],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0211],\n",
      "        [0.0214],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.0276],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0288],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0300],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0337],\n",
      "        [0.0350],\n",
      "        [0.0355],\n",
      "        [0.0362],\n",
      "        [0.0369],\n",
      "        [0.0372],\n",
      "        [0.0374],\n",
      "        [0.0375],\n",
      "        [0.0379],\n",
      "        [0.0386],\n",
      "        [0.0386],\n",
      "        [0.0394],\n",
      "        [0.0406],\n",
      "        [0.0411],\n",
      "        [0.0413],\n",
      "        [0.0414],\n",
      "        [0.0427],\n",
      "        [0.0440],\n",
      "        [0.0441],\n",
      "        [0.0456],\n",
      "        [0.0498],\n",
      "        [0.0503],\n",
      "        [0.0507],\n",
      "        [0.0507],\n",
      "        [0.0519],\n",
      "        [0.0530],\n",
      "        [0.0532],\n",
      "        [0.0554],\n",
      "        [0.0567],\n",
      "        [0.0571],\n",
      "        [0.0583],\n",
      "        [0.0590],\n",
      "        [0.0595],\n",
      "        [0.0599],\n",
      "        [0.0605],\n",
      "        [0.0606],\n",
      "        [0.0628],\n",
      "        [0.0639],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0668],\n",
      "        [0.0672],\n",
      "        [0.0673],\n",
      "        [0.0677],\n",
      "        [0.0706],\n",
      "        [0.0707],\n",
      "        [0.0714],\n",
      "        [0.0721],\n",
      "        [0.0722],\n",
      "        [0.0740],\n",
      "        [0.0771],\n",
      "        [0.0775],\n",
      "        [0.0783],\n",
      "        [0.0831],\n",
      "        [0.0836],\n",
      "        [0.0855],\n",
      "        [0.0869],\n",
      "        [0.0886],\n",
      "        [0.0888],\n",
      "        [0.0915],\n",
      "        [0.0945],\n",
      "        [0.0969],\n",
      "        [0.0979],\n",
      "        [0.1038],\n",
      "        [0.1047],\n",
      "        [0.1080],\n",
      "        [0.1131],\n",
      "        [0.1137],\n",
      "        [0.1150],\n",
      "        [0.1163],\n",
      "        [0.1229],\n",
      "        [0.1241],\n",
      "        [0.1303],\n",
      "        [0.1322],\n",
      "        [0.1376],\n",
      "        [0.1416],\n",
      "        [0.1445],\n",
      "        [0.1536],\n",
      "        [0.1555],\n",
      "        [0.1688]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.019136774033541e-07, 50)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [50, 104, 57, 82, 59, 95, 45, 115, 83, 145, 3, 106, 37, 111, 94, 58, 15, 10, 116, 125, 51, 140, 141, 110, 56, 135, 11, 34, 128, 60, 99, 80, 71, 107, 23, 90, 70, 129, 146, 103, 105, 98, 84, 16, 92, 91, 112, 88, 114, 109, 74, 87, 89, 97, 25, 6, 96, 26, 9, 49, 8, 14, 24, 119, 35, 127, 113, 22, 134, 2, 101, 28, 13, 62, 72, 108, 42, 102, 142, 81, 44, 100, 4, 73, 33, 69, 61, 93, 12, 36, 75, 52, 85, 131, 17, 27, 46, 130, 86, 155, 63, 55, 64, 133, 139, 43, 65, 117, 7, 126, 1, 132, 154, 68, 48, 47, 67, 124, 79, 21, 38, 32, 118, 136, 53, 66, 18, 54, 120, 20, 153, 147, 138, 143, 137, 19, 41, 144, 150, 0, 148, 121, 152, 151, 78, 40, 158, 31, 123, 149, 29, 30, 157, 156, 5, 122] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7042],\n",
      "        [0.5575],\n",
      "        [0.6819],\n",
      "        [0.5531],\n",
      "        [0.6580],\n",
      "        [0.6234],\n",
      "        [0.6590],\n",
      "        [0.4898],\n",
      "        [0.6068],\n",
      "        [0.2749],\n",
      "        [0.9563],\n",
      "        [0.5513],\n",
      "        [0.7269],\n",
      "        [0.5287],\n",
      "        [0.6369],\n",
      "        [0.6838],\n",
      "        [0.9167],\n",
      "        [0.8968],\n",
      "        [0.4659],\n",
      "        [0.2823],\n",
      "        [0.7019],\n",
      "        [0.3535],\n",
      "        [0.3433],\n",
      "        [0.5252],\n",
      "        [0.6836],\n",
      "        [0.3792],\n",
      "        [0.9106],\n",
      "        [0.7477],\n",
      "        [0.2943],\n",
      "        [0.6214],\n",
      "        [0.6024],\n",
      "        [0.4971],\n",
      "        [0.6304],\n",
      "        [0.5328],\n",
      "        [0.8771],\n",
      "        [0.5963],\n",
      "        [0.6259],\n",
      "        [0.2913],\n",
      "        [0.2783],\n",
      "        [0.5737],\n",
      "        [0.5783],\n",
      "        [0.6243],\n",
      "        [0.6024],\n",
      "        [0.8874],\n",
      "        [0.6580],\n",
      "        [0.6494],\n",
      "        [0.5134],\n",
      "        [0.5903],\n",
      "        [0.4920],\n",
      "        [0.5195],\n",
      "        [0.6100],\n",
      "        [0.5973],\n",
      "        [0.5860],\n",
      "        [0.6511],\n",
      "        [0.8127],\n",
      "        [0.8664],\n",
      "        [0.6565],\n",
      "        [0.8597],\n",
      "        [0.8280],\n",
      "        [0.6870],\n",
      "        [0.8717],\n",
      "        [0.9194],\n",
      "        [0.8471],\n",
      "        [0.4128],\n",
      "        [0.7186],\n",
      "        [0.2910],\n",
      "        [0.4943],\n",
      "        [0.8545],\n",
      "        [0.3345],\n",
      "        [0.9379],\n",
      "        [0.5978],\n",
      "        [0.8482],\n",
      "        [0.8994],\n",
      "        [0.6092],\n",
      "        [0.6040],\n",
      "        [0.4927],\n",
      "        [0.6685],\n",
      "        [0.5718],\n",
      "        [0.2780],\n",
      "        [0.4896],\n",
      "        [0.6608],\n",
      "        [0.6066],\n",
      "        [0.9394],\n",
      "        [0.5981],\n",
      "        [0.7841],\n",
      "        [0.6334],\n",
      "        [0.6081],\n",
      "        [0.6311],\n",
      "        [0.9246],\n",
      "        [0.7173],\n",
      "        [0.6083],\n",
      "        [0.6630],\n",
      "        [0.5798],\n",
      "        [0.3139],\n",
      "        [0.8744],\n",
      "        [0.8434],\n",
      "        [0.6665],\n",
      "        [0.3072],\n",
      "        [0.5663],\n",
      "        [0.3065],\n",
      "        [0.6061],\n",
      "        [0.6518],\n",
      "        [0.6036],\n",
      "        [0.3018],\n",
      "        [0.3418],\n",
      "        [0.5970],\n",
      "        [0.6204],\n",
      "        [0.4124],\n",
      "        [0.8895],\n",
      "        [0.2422],\n",
      "        [0.9076],\n",
      "        [0.2959],\n",
      "        [0.2975],\n",
      "        [0.6232],\n",
      "        [0.6677],\n",
      "        [0.6687],\n",
      "        [0.6235],\n",
      "        [0.3348],\n",
      "        [0.5631],\n",
      "        [0.8497],\n",
      "        [0.7337],\n",
      "        [0.8176],\n",
      "        [0.3925],\n",
      "        [0.3788],\n",
      "        [0.6421],\n",
      "        [0.6194],\n",
      "        [0.8579],\n",
      "        [0.6295],\n",
      "        [0.4094],\n",
      "        [0.8716],\n",
      "        [0.2772],\n",
      "        [0.3070],\n",
      "        [0.3479],\n",
      "        [0.2226],\n",
      "        [0.3607],\n",
      "        [0.8649],\n",
      "        [0.7113],\n",
      "        [0.2273],\n",
      "        [0.2914],\n",
      "        [0.8638],\n",
      "        [0.2989],\n",
      "        [0.3999],\n",
      "        [0.2977],\n",
      "        [0.2958],\n",
      "        [0.5732],\n",
      "        [0.7443],\n",
      "        [0.3132],\n",
      "        [0.7888],\n",
      "        [0.3971],\n",
      "        [0.3003],\n",
      "        [0.8454],\n",
      "        [0.7937],\n",
      "        [0.3127],\n",
      "        [0.3531],\n",
      "        [0.8767],\n",
      "        [0.3964]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0026],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0061],\n",
      "        [0.0066],\n",
      "        [0.0071],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0087],\n",
      "        [0.0105],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0161],\n",
      "        [0.0170],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0189],\n",
      "        [0.0196],\n",
      "        [0.0204],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0211],\n",
      "        [0.0214],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0229],\n",
      "        [0.0231],\n",
      "        [0.0232],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.0276],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0288],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0300],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0337],\n",
      "        [0.0350],\n",
      "        [0.0355],\n",
      "        [0.0362],\n",
      "        [0.0369],\n",
      "        [0.0372],\n",
      "        [0.0374],\n",
      "        [0.0375],\n",
      "        [0.0379],\n",
      "        [0.0386],\n",
      "        [0.0386],\n",
      "        [0.0394],\n",
      "        [0.0406],\n",
      "        [0.0411],\n",
      "        [0.0413],\n",
      "        [0.0414],\n",
      "        [0.0427],\n",
      "        [0.0440],\n",
      "        [0.0441],\n",
      "        [0.0456],\n",
      "        [0.0498],\n",
      "        [0.0503],\n",
      "        [0.0507],\n",
      "        [0.0507],\n",
      "        [0.0519],\n",
      "        [0.0530],\n",
      "        [0.0532],\n",
      "        [0.0554],\n",
      "        [0.0567],\n",
      "        [0.0571],\n",
      "        [0.0583],\n",
      "        [0.0590],\n",
      "        [0.0595],\n",
      "        [0.0599],\n",
      "        [0.0605],\n",
      "        [0.0606],\n",
      "        [0.0628],\n",
      "        [0.0639],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0668],\n",
      "        [0.0672],\n",
      "        [0.0673],\n",
      "        [0.0677],\n",
      "        [0.0706],\n",
      "        [0.0707],\n",
      "        [0.0714],\n",
      "        [0.0721],\n",
      "        [0.0722],\n",
      "        [0.0740],\n",
      "        [0.0771],\n",
      "        [0.0775],\n",
      "        [0.0783],\n",
      "        [0.0831],\n",
      "        [0.0836],\n",
      "        [0.0855],\n",
      "        [0.0869],\n",
      "        [0.0886],\n",
      "        [0.0888],\n",
      "        [0.0915],\n",
      "        [0.0945],\n",
      "        [0.0969],\n",
      "        [0.0979],\n",
      "        [0.1038],\n",
      "        [0.1047],\n",
      "        [0.1080],\n",
      "        [0.1131],\n",
      "        [0.1137],\n",
      "        [0.1150],\n",
      "        [0.1163],\n",
      "        [0.1229],\n",
      "        [0.1241],\n",
      "        [0.1303],\n",
      "        [0.1322],\n",
      "        [0.1376],\n",
      "        [0.1416],\n",
      "        [0.1445],\n",
      "        [0.1536],\n",
      "        [0.1555],\n",
      "        [0.1688],\n",
      "        [0.1690]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0020],\n",
      "        [    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0020],\n",
      "        [    0.0030],\n",
      "        [    0.0001],\n",
      "        [    0.0068],\n",
      "        [    0.0057],\n",
      "        [    0.0011],\n",
      "        [    0.0032],\n",
      "        [    0.0048],\n",
      "        [    0.0074],\n",
      "        [    0.0060],\n",
      "        [    0.0069],\n",
      "        [    0.0034],\n",
      "        [    0.0046],\n",
      "        [    0.0096],\n",
      "        [    0.0145],\n",
      "        [    0.0099],\n",
      "        [    0.0224],\n",
      "        [    0.0216],\n",
      "        [    0.0126],\n",
      "        [    0.0124],\n",
      "        [    0.0256],\n",
      "        [    0.0086],\n",
      "        [    0.0141],\n",
      "        [    0.0245],\n",
      "        [    0.0120],\n",
      "        [    0.0125],\n",
      "        [    0.0143],\n",
      "        [    0.0132],\n",
      "        [    0.0138],\n",
      "        [    0.0162],\n",
      "        [    0.0159],\n",
      "        [    0.0140],\n",
      "        [    0.0275],\n",
      "        [    0.0052],\n",
      "        [    0.0157],\n",
      "        [    0.0182],\n",
      "        [    0.0178],\n",
      "        [    0.0142],\n",
      "        [    0.0207],\n",
      "        [    0.0187],\n",
      "        [    0.0188],\n",
      "        [    0.0227],\n",
      "        [    0.0203],\n",
      "        [    0.0187],\n",
      "        [    0.0215],\n",
      "        [    0.0221],\n",
      "        [    0.0218],\n",
      "        [    0.0229],\n",
      "        [    0.0223],\n",
      "        [    0.0193],\n",
      "        [    0.0233],\n",
      "        [    0.0225],\n",
      "        [    0.0272],\n",
      "        [    0.0263],\n",
      "        [    0.0256],\n",
      "        [    0.0245],\n",
      "        [    0.0222],\n",
      "        [    0.0253],\n",
      "        [    0.0296],\n",
      "        [    0.0279],\n",
      "        [    0.0383],\n",
      "        [    0.0312],\n",
      "        [    0.0276],\n",
      "        [    0.0405],\n",
      "        [    0.0346],\n",
      "        [    0.0287],\n",
      "        [    0.0336],\n",
      "        [    0.0273],\n",
      "        [    0.0297],\n",
      "        [    0.0302],\n",
      "        [    0.0329],\n",
      "        [    0.0351],\n",
      "        [    0.0336],\n",
      "        [    0.0439],\n",
      "        [    0.0379],\n",
      "        [    0.0368],\n",
      "        [    0.0359],\n",
      "        [    0.0332],\n",
      "        [    0.0355],\n",
      "        [    0.0373],\n",
      "        [    0.0385],\n",
      "        [    0.0376],\n",
      "        [    0.0405],\n",
      "        [    0.0362],\n",
      "        [    0.0409],\n",
      "        [    0.0426],\n",
      "        [    0.0420],\n",
      "        [    0.0459],\n",
      "        [    0.0550],\n",
      "        [    0.0465],\n",
      "        [    0.0530],\n",
      "        [    0.0522],\n",
      "        [    0.0613],\n",
      "        [    0.0517],\n",
      "        [    0.0359],\n",
      "        [    0.0505],\n",
      "        [    0.0533],\n",
      "        [    0.0533],\n",
      "        [    0.0675],\n",
      "        [    0.0688],\n",
      "        [    0.0573],\n",
      "        [    0.0586],\n",
      "        [    0.0606],\n",
      "        [    0.0590],\n",
      "        [    0.0682],\n",
      "        [    0.0670],\n",
      "        [    0.0732],\n",
      "        [    0.0484],\n",
      "        [    0.0658],\n",
      "        [    0.0677],\n",
      "        [    0.0677],\n",
      "        [    0.0671],\n",
      "        [    0.0620],\n",
      "        [    0.0672],\n",
      "        [    0.0697],\n",
      "        [    0.0702],\n",
      "        [    0.0704],\n",
      "        [    0.0734],\n",
      "        [    0.0852],\n",
      "        [    0.0725],\n",
      "        [    0.0771],\n",
      "        [    0.0774],\n",
      "        [    0.0770],\n",
      "        [    0.0811],\n",
      "        [    0.0832],\n",
      "        [    0.0710],\n",
      "        [    0.0754],\n",
      "        [    0.1006],\n",
      "        [    0.0952],\n",
      "        [    0.1030],\n",
      "        [    0.0939],\n",
      "        [    0.0970],\n",
      "        [    0.1048],\n",
      "        [    0.0914],\n",
      "        [    0.1098],\n",
      "        [    0.0972],\n",
      "        [    0.1098],\n",
      "        [    0.0993],\n",
      "        [    0.1019],\n",
      "        [    0.1155],\n",
      "        [    0.1227],\n",
      "        [    0.1062],\n",
      "        [    0.1307],\n",
      "        [    0.1272],\n",
      "        [    0.1256],\n",
      "        [    0.1421],\n",
      "        [    0.1447],\n",
      "        [    0.1363],\n",
      "        [    0.1380],\n",
      "        [    0.1678],\n",
      "        [    0.1649]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 52.96510648727417\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.090022377387868e-09, 82)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [82, 104, 115, 50, 59, 3, 57, 95, 45, 106, 15, 10, 37, 146, 145, 94, 83, 58, 111, 11, 116, 51, 60, 56, 99, 110, 71, 107, 70, 34, 84, 80, 125, 103, 90, 23, 98, 105, 92, 114, 91, 25, 88, 16, 109, 141, 87, 74, 14, 97, 140, 96, 112, 89, 6, 8, 128, 24, 135, 49, 9, 26, 13, 129, 22, 35, 101, 119, 62, 72, 113, 108, 4, 28, 102, 2, 42, 73, 100, 155, 12, 44, 33, 61, 81, 127, 69, 134, 93, 36, 52, 75, 142, 85, 17, 154, 63, 86, 46, 27, 55, 64, 131, 43, 65, 7, 117, 130, 124, 68, 1, 67, 79, 133, 48, 47, 126, 139, 21, 38, 32, 153, 53, 132, 118, 147, 54, 66, 18, 120, 20, 136, 150, 19, 143, 41, 148, 152, 138, 151, 137, 144, 158, 0, 121, 78, 40, 149, 123, 31, 157, 156, 29, 30, 122, 5, 76] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5515],\n",
      "        [0.5589],\n",
      "        [0.4873],\n",
      "        [0.7035],\n",
      "        [0.6591],\n",
      "        [0.9510],\n",
      "        [0.6816],\n",
      "        [0.6236],\n",
      "        [0.6581],\n",
      "        [0.5526],\n",
      "        [0.9130],\n",
      "        [0.8938],\n",
      "        [0.7266],\n",
      "        [0.2674],\n",
      "        [0.2653],\n",
      "        [0.6370],\n",
      "        [0.6037],\n",
      "        [0.6841],\n",
      "        [0.5266],\n",
      "        [0.9064],\n",
      "        [0.4643],\n",
      "        [0.7013],\n",
      "        [0.6233],\n",
      "        [0.6826],\n",
      "        [0.6041],\n",
      "        [0.5240],\n",
      "        [0.6322],\n",
      "        [0.5341],\n",
      "        [0.6276],\n",
      "        [0.7465],\n",
      "        [0.5993],\n",
      "        [0.4970],\n",
      "        [0.2764],\n",
      "        [0.5750],\n",
      "        [0.5966],\n",
      "        [0.8779],\n",
      "        [0.6248],\n",
      "        [0.5793],\n",
      "        [0.6581],\n",
      "        [0.4896],\n",
      "        [0.6493],\n",
      "        [0.8163],\n",
      "        [0.5905],\n",
      "        [0.8853],\n",
      "        [0.5191],\n",
      "        [0.3327],\n",
      "        [0.5969],\n",
      "        [0.6110],\n",
      "        [0.9148],\n",
      "        [0.6509],\n",
      "        [0.3420],\n",
      "        [0.6558],\n",
      "        [0.5103],\n",
      "        [0.5867],\n",
      "        [0.8666],\n",
      "        [0.8708],\n",
      "        [0.2833],\n",
      "        [0.8490],\n",
      "        [0.3656],\n",
      "        [0.6862],\n",
      "        [0.8261],\n",
      "        [0.8627],\n",
      "        [0.8947],\n",
      "        [0.2796],\n",
      "        [0.8557],\n",
      "        [0.7184],\n",
      "        [0.5990],\n",
      "        [0.4109],\n",
      "        [0.6115],\n",
      "        [0.6062],\n",
      "        [0.4918],\n",
      "        [0.4935],\n",
      "        [0.9350],\n",
      "        [0.8502],\n",
      "        [0.5736],\n",
      "        [0.9323],\n",
      "        [0.6686],\n",
      "        [0.6005],\n",
      "        [0.6081],\n",
      "        [0.2905],\n",
      "        [0.9196],\n",
      "        [0.6603],\n",
      "        [0.7828],\n",
      "        [0.6098],\n",
      "        [0.4886],\n",
      "        [0.2809],\n",
      "        [0.6336],\n",
      "        [0.3229],\n",
      "        [0.6312],\n",
      "        [0.7177],\n",
      "        [0.6636],\n",
      "        [0.6095],\n",
      "        [0.2703],\n",
      "        [0.5779],\n",
      "        [0.8735],\n",
      "        [0.2820],\n",
      "        [0.6086],\n",
      "        [0.5654],\n",
      "        [0.6645],\n",
      "        [0.8466],\n",
      "        [0.6517],\n",
      "        [0.6056],\n",
      "        [0.3030],\n",
      "        [0.5980],\n",
      "        [0.6208],\n",
      "        [0.8887],\n",
      "        [0.4113],\n",
      "        [0.2966],\n",
      "        [0.3294],\n",
      "        [0.6235],\n",
      "        [0.9011],\n",
      "        [0.6236],\n",
      "        [0.5626],\n",
      "        [0.2911],\n",
      "        [0.6665],\n",
      "        [0.6678],\n",
      "        [0.2345],\n",
      "        [0.3301],\n",
      "        [0.8506],\n",
      "        [0.7333],\n",
      "        [0.8166],\n",
      "        [0.2627],\n",
      "        [0.6435],\n",
      "        [0.2855],\n",
      "        [0.3911],\n",
      "        [0.2956],\n",
      "        [0.6307],\n",
      "        [0.6195],\n",
      "        [0.8581],\n",
      "        [0.4074],\n",
      "        [0.8721],\n",
      "        [0.3658],\n",
      "        [0.2790],\n",
      "        [0.8654],\n",
      "        [0.2162],\n",
      "        [0.7114],\n",
      "        [0.2881],\n",
      "        [0.2834],\n",
      "        [0.3359],\n",
      "        [0.2827],\n",
      "        [0.3491],\n",
      "        [0.2205],\n",
      "        [0.2953],\n",
      "        [0.8586],\n",
      "        [0.3966],\n",
      "        [0.5723],\n",
      "        [0.7440],\n",
      "        [0.2883],\n",
      "        [0.3921],\n",
      "        [0.7892],\n",
      "        [0.2954],\n",
      "        [0.3355],\n",
      "        [0.8459],\n",
      "        [0.7939],\n",
      "        [0.3923],\n",
      "        [0.8758],\n",
      "        [0.6191]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0011],\n",
      "        [    0.0020],\n",
      "        [    0.0020],\n",
      "        [    0.0030],\n",
      "        [    0.0032],\n",
      "        [    0.0034],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0052],\n",
      "        [    0.0057],\n",
      "        [    0.0060],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0074],\n",
      "        [    0.0086],\n",
      "        [    0.0096],\n",
      "        [    0.0099],\n",
      "        [    0.0120],\n",
      "        [    0.0124],\n",
      "        [    0.0125],\n",
      "        [    0.0126],\n",
      "        [    0.0132],\n",
      "        [    0.0138],\n",
      "        [    0.0140],\n",
      "        [    0.0141],\n",
      "        [    0.0142],\n",
      "        [    0.0143],\n",
      "        [    0.0145],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0178],\n",
      "        [    0.0182],\n",
      "        [    0.0187],\n",
      "        [    0.0187],\n",
      "        [    0.0188],\n",
      "        [    0.0193],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0215],\n",
      "        [    0.0216],\n",
      "        [    0.0218],\n",
      "        [    0.0221],\n",
      "        [    0.0222],\n",
      "        [    0.0223],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0227],\n",
      "        [    0.0229],\n",
      "        [    0.0233],\n",
      "        [    0.0245],\n",
      "        [    0.0245],\n",
      "        [    0.0253],\n",
      "        [    0.0256],\n",
      "        [    0.0256],\n",
      "        [    0.0263],\n",
      "        [    0.0272],\n",
      "        [    0.0273],\n",
      "        [    0.0275],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0287],\n",
      "        [    0.0296],\n",
      "        [    0.0297],\n",
      "        [    0.0302],\n",
      "        [    0.0312],\n",
      "        [    0.0329],\n",
      "        [    0.0332],\n",
      "        [    0.0336],\n",
      "        [    0.0336],\n",
      "        [    0.0346],\n",
      "        [    0.0351],\n",
      "        [    0.0355],\n",
      "        [    0.0359],\n",
      "        [    0.0359],\n",
      "        [    0.0362],\n",
      "        [    0.0368],\n",
      "        [    0.0373],\n",
      "        [    0.0376],\n",
      "        [    0.0379],\n",
      "        [    0.0383],\n",
      "        [    0.0385],\n",
      "        [    0.0405],\n",
      "        [    0.0405],\n",
      "        [    0.0409],\n",
      "        [    0.0420],\n",
      "        [    0.0426],\n",
      "        [    0.0439],\n",
      "        [    0.0459],\n",
      "        [    0.0465],\n",
      "        [    0.0484],\n",
      "        [    0.0505],\n",
      "        [    0.0517],\n",
      "        [    0.0522],\n",
      "        [    0.0530],\n",
      "        [    0.0533],\n",
      "        [    0.0533],\n",
      "        [    0.0550],\n",
      "        [    0.0573],\n",
      "        [    0.0586],\n",
      "        [    0.0590],\n",
      "        [    0.0606],\n",
      "        [    0.0613],\n",
      "        [    0.0620],\n",
      "        [    0.0658],\n",
      "        [    0.0670],\n",
      "        [    0.0671],\n",
      "        [    0.0672],\n",
      "        [    0.0675],\n",
      "        [    0.0677],\n",
      "        [    0.0677],\n",
      "        [    0.0682],\n",
      "        [    0.0688],\n",
      "        [    0.0697],\n",
      "        [    0.0702],\n",
      "        [    0.0704],\n",
      "        [    0.0710],\n",
      "        [    0.0725],\n",
      "        [    0.0732],\n",
      "        [    0.0734],\n",
      "        [    0.0754],\n",
      "        [    0.0770],\n",
      "        [    0.0771],\n",
      "        [    0.0774],\n",
      "        [    0.0811],\n",
      "        [    0.0832],\n",
      "        [    0.0852],\n",
      "        [    0.0914],\n",
      "        [    0.0939],\n",
      "        [    0.0952],\n",
      "        [    0.0970],\n",
      "        [    0.0972],\n",
      "        [    0.0993],\n",
      "        [    0.1006],\n",
      "        [    0.1019],\n",
      "        [    0.1030],\n",
      "        [    0.1048],\n",
      "        [    0.1062],\n",
      "        [    0.1098],\n",
      "        [    0.1098],\n",
      "        [    0.1155],\n",
      "        [    0.1227],\n",
      "        [    0.1256],\n",
      "        [    0.1272],\n",
      "        [    0.1307],\n",
      "        [    0.1363],\n",
      "        [    0.1380],\n",
      "        [    0.1421],\n",
      "        [    0.1447],\n",
      "        [    0.1649],\n",
      "        [    0.1678],\n",
      "        [    0.1986]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0012],\n",
      "        [    0.0016],\n",
      "        [    0.0002],\n",
      "        [    0.0016],\n",
      "        [    0.0004],\n",
      "        [    0.0077],\n",
      "        [    0.0029],\n",
      "        [    0.0015],\n",
      "        [    0.0043],\n",
      "        [    0.0015],\n",
      "        [    0.0019],\n",
      "        [    0.0003],\n",
      "        [    0.0042],\n",
      "        [    0.0029],\n",
      "        [    0.0070],\n",
      "        [    0.0069],\n",
      "        [    0.0095],\n",
      "        [    0.0063],\n",
      "        [    0.0080],\n",
      "        [    0.0026],\n",
      "        [    0.0089],\n",
      "        [    0.0082],\n",
      "        [    0.0108],\n",
      "        [    0.0137],\n",
      "        [    0.0117],\n",
      "        [    0.0124],\n",
      "        [    0.0129],\n",
      "        [    0.0118],\n",
      "        [    0.0137],\n",
      "        [    0.0155],\n",
      "        [    0.0115],\n",
      "        [    0.0138],\n",
      "        [    0.0141],\n",
      "        [    0.0145],\n",
      "        [    0.0156],\n",
      "        [    0.0147],\n",
      "        [    0.0176],\n",
      "        [    0.0193],\n",
      "        [    0.0199],\n",
      "        [    0.0184],\n",
      "        [    0.0175],\n",
      "        [    0.0184],\n",
      "        [    0.0205],\n",
      "        [    0.0244],\n",
      "        [    0.0204],\n",
      "        [    0.0239],\n",
      "        [    0.0225],\n",
      "        [    0.0219],\n",
      "        [    0.0163],\n",
      "        [    0.0212],\n",
      "        [    0.0254],\n",
      "        [    0.0211],\n",
      "        [    0.0237],\n",
      "        [    0.0231],\n",
      "        [    0.0216],\n",
      "        [    0.0220],\n",
      "        [    0.0269],\n",
      "        [    0.0259],\n",
      "        [    0.0304],\n",
      "        [    0.0273],\n",
      "        [    0.0295],\n",
      "        [    0.0272],\n",
      "        [    0.0214],\n",
      "        [    0.0300],\n",
      "        [    0.0287],\n",
      "        [    0.0286],\n",
      "        [    0.0279],\n",
      "        [    0.0282],\n",
      "        [    0.0281],\n",
      "        [    0.0293],\n",
      "        [    0.0316],\n",
      "        [    0.0306],\n",
      "        [    0.0274],\n",
      "        [    0.0329],\n",
      "        [    0.0320],\n",
      "        [    0.0414],\n",
      "        [    0.0349],\n",
      "        [    0.0346],\n",
      "        [    0.0351],\n",
      "        [    0.0293],\n",
      "        [    0.0297],\n",
      "        [    0.0358],\n",
      "        [    0.0353],\n",
      "        [    0.0363],\n",
      "        [    0.0378],\n",
      "        [    0.0403],\n",
      "        [    0.0393],\n",
      "        [    0.0436],\n",
      "        [    0.0415],\n",
      "        [    0.0411],\n",
      "        [    0.0422],\n",
      "        [    0.0427],\n",
      "        [    0.0434],\n",
      "        [    0.0476],\n",
      "        [    0.0491],\n",
      "        [    0.0423],\n",
      "        [    0.0488],\n",
      "        [    0.0526],\n",
      "        [    0.0543],\n",
      "        [    0.0533],\n",
      "        [    0.0537],\n",
      "        [    0.0519],\n",
      "        [    0.0574],\n",
      "        [    0.0563],\n",
      "        [    0.0586],\n",
      "        [    0.0563],\n",
      "        [    0.0587],\n",
      "        [    0.0634],\n",
      "        [    0.0620],\n",
      "        [    0.0662],\n",
      "        [    0.0742],\n",
      "        [    0.0676],\n",
      "        [    0.0668],\n",
      "        [    0.0698],\n",
      "        [    0.0693],\n",
      "        [    0.0692],\n",
      "        [    0.0681],\n",
      "        [    0.0720],\n",
      "        [    0.0708],\n",
      "        [    0.0692],\n",
      "        [    0.0684],\n",
      "        [    0.0660],\n",
      "        [    0.0717],\n",
      "        [    0.0753],\n",
      "        [    0.0715],\n",
      "        [    0.0723],\n",
      "        [    0.0762],\n",
      "        [    0.0774],\n",
      "        [    0.0789],\n",
      "        [    0.0826],\n",
      "        [    0.0850],\n",
      "        [    0.0897],\n",
      "        [    0.0877],\n",
      "        [    0.0955],\n",
      "        [    0.0934],\n",
      "        [    0.0964],\n",
      "        [    0.0945],\n",
      "        [    0.0940],\n",
      "        [    0.1040],\n",
      "        [    0.0975],\n",
      "        [    0.1064],\n",
      "        [    0.1035],\n",
      "        [    0.0982],\n",
      "        [    0.1155],\n",
      "        [    0.1108],\n",
      "        [    0.1146],\n",
      "        [    0.1215],\n",
      "        [    0.1220],\n",
      "        [    0.1268],\n",
      "        [    0.1298],\n",
      "        [    0.1288],\n",
      "        [    0.1298],\n",
      "        [    0.1403],\n",
      "        [    0.1436],\n",
      "        [    0.1653],\n",
      "        [    0.1652],\n",
      "        [    0.1982]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 53.25250840187073\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.547597771103028e-08, 115)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [115, 10, 59, 82, 106, 95, 50, 104, 15, 11, 146, 57, 37, 45, 58, 94, 145, 3, 111, 51, 116, 83, 60, 84, 99, 107, 110, 71, 56, 70, 80, 125, 103, 23, 34, 90, 14, 91, 98, 25, 114, 105, 92, 109, 88, 96, 97, 13, 6, 74, 8, 87, 89, 112, 141, 16, 140, 24, 128, 26, 49, 4, 101, 62, 119, 35, 22, 72, 155, 9, 12, 129, 135, 108, 113, 102, 28, 73, 42, 100, 33, 44, 61, 81, 69, 127, 36, 2, 93, 52, 154, 75, 142, 134, 85, 63, 17, 64, 86, 27, 55, 46, 43, 7, 131, 65, 117, 124, 130, 153, 68, 79, 67, 126, 32, 38, 47, 48, 133, 21, 118, 53, 139, 147, 1, 132, 54, 66, 18, 120, 20, 150, 136, 143, 152, 148, 19, 41, 151, 158, 144, 138, 137, 121, 78, 0, 40, 149, 123, 157, 156, 31, 29, 30, 5, 122, 76, 77] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4870],\n",
      "        [0.8889],\n",
      "        [0.6593],\n",
      "        [0.5503],\n",
      "        [0.5544],\n",
      "        [0.6231],\n",
      "        [0.7018],\n",
      "        [0.5606],\n",
      "        [0.9077],\n",
      "        [0.9005],\n",
      "        [0.2651],\n",
      "        [0.6806],\n",
      "        [0.7259],\n",
      "        [0.6567],\n",
      "        [0.6835],\n",
      "        [0.6361],\n",
      "        [0.2640],\n",
      "        [0.9444],\n",
      "        [0.5260],\n",
      "        [0.6996],\n",
      "        [0.4651],\n",
      "        [0.6010],\n",
      "        [0.6245],\n",
      "        [0.5966],\n",
      "        [0.6049],\n",
      "        [0.5362],\n",
      "        [0.5242],\n",
      "        [0.6324],\n",
      "        [0.6814],\n",
      "        [0.6279],\n",
      "        [0.4976],\n",
      "        [0.2768],\n",
      "        [0.5762],\n",
      "        [0.8763],\n",
      "        [0.7451],\n",
      "        [0.5963],\n",
      "        [0.9089],\n",
      "        [0.6480],\n",
      "        [0.6246],\n",
      "        [0.8173],\n",
      "        [0.4893],\n",
      "        [0.5804],\n",
      "        [0.6569],\n",
      "        [0.5202],\n",
      "        [0.5903],\n",
      "        [0.6544],\n",
      "        [0.6498],\n",
      "        [0.8889],\n",
      "        [0.8649],\n",
      "        [0.6108],\n",
      "        [0.8682],\n",
      "        [0.5962],\n",
      "        [0.5868],\n",
      "        [0.5092],\n",
      "        [0.3304],\n",
      "        [0.8815],\n",
      "        [0.3391],\n",
      "        [0.8485],\n",
      "        [0.2809],\n",
      "        [0.8627],\n",
      "        [0.6845],\n",
      "        [0.9292],\n",
      "        [0.5999],\n",
      "        [0.6131],\n",
      "        [0.4123],\n",
      "        [0.7177],\n",
      "        [0.8546],\n",
      "        [0.6071],\n",
      "        [0.2838],\n",
      "        [0.8229],\n",
      "        [0.9132],\n",
      "        [0.2771],\n",
      "        [0.3607],\n",
      "        [0.4958],\n",
      "        [0.4914],\n",
      "        [0.5752],\n",
      "        [0.8496],\n",
      "        [0.6014],\n",
      "        [0.6684],\n",
      "        [0.6089],\n",
      "        [0.7809],\n",
      "        [0.6594],\n",
      "        [0.6111],\n",
      "        [0.4887],\n",
      "        [0.6328],\n",
      "        [0.2789],\n",
      "        [0.7175],\n",
      "        [0.9255],\n",
      "        [0.6303],\n",
      "        [0.6634],\n",
      "        [0.2759],\n",
      "        [0.6096],\n",
      "        [0.2708],\n",
      "        [0.3199],\n",
      "        [0.5763],\n",
      "        [0.6103],\n",
      "        [0.8709],\n",
      "        [0.6071],\n",
      "        [0.5645],\n",
      "        [0.8469],\n",
      "        [0.6513],\n",
      "        [0.6625],\n",
      "        [0.5990],\n",
      "        [0.8860],\n",
      "        [0.3007],\n",
      "        [0.6208],\n",
      "        [0.4133],\n",
      "        [0.3294],\n",
      "        [0.2945],\n",
      "        [0.2576],\n",
      "        [0.6230],\n",
      "        [0.5621],\n",
      "        [0.6232],\n",
      "        [0.2345],\n",
      "        [0.8145],\n",
      "        [0.7322],\n",
      "        [0.6662],\n",
      "        [0.6649],\n",
      "        [0.2888],\n",
      "        [0.8495],\n",
      "        [0.3931],\n",
      "        [0.6444],\n",
      "        [0.3269],\n",
      "        [0.2924],\n",
      "        [0.8940],\n",
      "        [0.2835],\n",
      "        [0.6315],\n",
      "        [0.6192],\n",
      "        [0.8565],\n",
      "        [0.4089],\n",
      "        [0.8703],\n",
      "        [0.2753],\n",
      "        [0.3614],\n",
      "        [0.2181],\n",
      "        [0.2780],\n",
      "        [0.2854],\n",
      "        [0.8639],\n",
      "        [0.7107],\n",
      "        [0.2783],\n",
      "        [0.2873],\n",
      "        [0.2217],\n",
      "        [0.3325],\n",
      "        [0.3458],\n",
      "        [0.3976],\n",
      "        [0.5715],\n",
      "        [0.8530],\n",
      "        [0.7429],\n",
      "        [0.2847],\n",
      "        [0.3918],\n",
      "        [0.2878],\n",
      "        [0.3273],\n",
      "        [0.7883],\n",
      "        [0.8442],\n",
      "        [0.7928],\n",
      "        [0.8731],\n",
      "        [0.3927],\n",
      "        [0.6187],\n",
      "        [0.6316]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0012],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0016],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0029],\n",
      "        [    0.0029],\n",
      "        [    0.0042],\n",
      "        [    0.0043],\n",
      "        [    0.0063],\n",
      "        [    0.0069],\n",
      "        [    0.0070],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0095],\n",
      "        [    0.0108],\n",
      "        [    0.0115],\n",
      "        [    0.0117],\n",
      "        [    0.0118],\n",
      "        [    0.0124],\n",
      "        [    0.0129],\n",
      "        [    0.0137],\n",
      "        [    0.0137],\n",
      "        [    0.0138],\n",
      "        [    0.0141],\n",
      "        [    0.0145],\n",
      "        [    0.0147],\n",
      "        [    0.0155],\n",
      "        [    0.0156],\n",
      "        [    0.0163],\n",
      "        [    0.0175],\n",
      "        [    0.0176],\n",
      "        [    0.0184],\n",
      "        [    0.0184],\n",
      "        [    0.0193],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0205],\n",
      "        [    0.0211],\n",
      "        [    0.0212],\n",
      "        [    0.0214],\n",
      "        [    0.0216],\n",
      "        [    0.0219],\n",
      "        [    0.0220],\n",
      "        [    0.0225],\n",
      "        [    0.0231],\n",
      "        [    0.0237],\n",
      "        [    0.0239],\n",
      "        [    0.0244],\n",
      "        [    0.0254],\n",
      "        [    0.0259],\n",
      "        [    0.0269],\n",
      "        [    0.0272],\n",
      "        [    0.0273],\n",
      "        [    0.0274],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0282],\n",
      "        [    0.0286],\n",
      "        [    0.0287],\n",
      "        [    0.0293],\n",
      "        [    0.0293],\n",
      "        [    0.0295],\n",
      "        [    0.0297],\n",
      "        [    0.0300],\n",
      "        [    0.0304],\n",
      "        [    0.0306],\n",
      "        [    0.0316],\n",
      "        [    0.0320],\n",
      "        [    0.0329],\n",
      "        [    0.0346],\n",
      "        [    0.0349],\n",
      "        [    0.0351],\n",
      "        [    0.0353],\n",
      "        [    0.0358],\n",
      "        [    0.0363],\n",
      "        [    0.0378],\n",
      "        [    0.0393],\n",
      "        [    0.0403],\n",
      "        [    0.0411],\n",
      "        [    0.0414],\n",
      "        [    0.0415],\n",
      "        [    0.0422],\n",
      "        [    0.0423],\n",
      "        [    0.0427],\n",
      "        [    0.0434],\n",
      "        [    0.0436],\n",
      "        [    0.0476],\n",
      "        [    0.0488],\n",
      "        [    0.0491],\n",
      "        [    0.0519],\n",
      "        [    0.0526],\n",
      "        [    0.0533],\n",
      "        [    0.0537],\n",
      "        [    0.0543],\n",
      "        [    0.0563],\n",
      "        [    0.0563],\n",
      "        [    0.0574],\n",
      "        [    0.0586],\n",
      "        [    0.0587],\n",
      "        [    0.0620],\n",
      "        [    0.0634],\n",
      "        [    0.0660],\n",
      "        [    0.0662],\n",
      "        [    0.0668],\n",
      "        [    0.0676],\n",
      "        [    0.0681],\n",
      "        [    0.0684],\n",
      "        [    0.0692],\n",
      "        [    0.0692],\n",
      "        [    0.0693],\n",
      "        [    0.0698],\n",
      "        [    0.0708],\n",
      "        [    0.0715],\n",
      "        [    0.0717],\n",
      "        [    0.0720],\n",
      "        [    0.0723],\n",
      "        [    0.0742],\n",
      "        [    0.0753],\n",
      "        [    0.0762],\n",
      "        [    0.0774],\n",
      "        [    0.0789],\n",
      "        [    0.0826],\n",
      "        [    0.0850],\n",
      "        [    0.0877],\n",
      "        [    0.0897],\n",
      "        [    0.0934],\n",
      "        [    0.0940],\n",
      "        [    0.0945],\n",
      "        [    0.0955],\n",
      "        [    0.0964],\n",
      "        [    0.0975],\n",
      "        [    0.0982],\n",
      "        [    0.1035],\n",
      "        [    0.1040],\n",
      "        [    0.1064],\n",
      "        [    0.1108],\n",
      "        [    0.1146],\n",
      "        [    0.1155],\n",
      "        [    0.1215],\n",
      "        [    0.1220],\n",
      "        [    0.1268],\n",
      "        [    0.1288],\n",
      "        [    0.1298],\n",
      "        [    0.1298],\n",
      "        [    0.1403],\n",
      "        [    0.1436],\n",
      "        [    0.1652],\n",
      "        [    0.1653],\n",
      "        [    0.1982],\n",
      "        [    0.2079]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0022],\n",
      "        [0.0013],\n",
      "        [0.0042],\n",
      "        [0.0025],\n",
      "        [0.0012],\n",
      "        [0.0034],\n",
      "        [0.0019],\n",
      "        [0.0061],\n",
      "        [0.0038],\n",
      "        [0.0074],\n",
      "        [0.0065],\n",
      "        [0.0057],\n",
      "        [0.0092],\n",
      "        [0.0071],\n",
      "        [0.0084],\n",
      "        [0.0070],\n",
      "        [0.0106],\n",
      "        [0.0087],\n",
      "        [0.0106],\n",
      "        [0.0098],\n",
      "        [0.0088],\n",
      "        [0.0109],\n",
      "        [0.0120],\n",
      "        [0.0128],\n",
      "        [0.0127],\n",
      "        [0.0125],\n",
      "        [0.0123],\n",
      "        [0.0122],\n",
      "        [0.0165],\n",
      "        [0.0139],\n",
      "        [0.0163],\n",
      "        [0.0153],\n",
      "        [0.0175],\n",
      "        [0.0187],\n",
      "        [0.0149],\n",
      "        [0.0194],\n",
      "        [0.0216],\n",
      "        [0.0196],\n",
      "        [0.0181],\n",
      "        [0.0195],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0203],\n",
      "        [0.0238],\n",
      "        [0.0225],\n",
      "        [0.0236],\n",
      "        [0.0220],\n",
      "        [0.0243],\n",
      "        [0.0232],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0250],\n",
      "        [0.0233],\n",
      "        [0.0262],\n",
      "        [0.0300],\n",
      "        [0.0271],\n",
      "        [0.0265],\n",
      "        [0.0259],\n",
      "        [0.0257],\n",
      "        [0.0257],\n",
      "        [0.0264],\n",
      "        [0.0264],\n",
      "        [0.0279],\n",
      "        [0.0262],\n",
      "        [0.0287],\n",
      "        [0.0280],\n",
      "        [0.0293],\n",
      "        [0.0312],\n",
      "        [0.0274],\n",
      "        [0.0306],\n",
      "        [0.0295],\n",
      "        [0.0351],\n",
      "        [0.0332],\n",
      "        [0.0368],\n",
      "        [0.0332],\n",
      "        [0.0368],\n",
      "        [0.0367],\n",
      "        [0.0339],\n",
      "        [0.0369],\n",
      "        [0.0390],\n",
      "        [0.0397],\n",
      "        [0.0390],\n",
      "        [0.0431],\n",
      "        [0.0411],\n",
      "        [0.0409],\n",
      "        [0.0397],\n",
      "        [0.0436],\n",
      "        [0.0407],\n",
      "        [0.0434],\n",
      "        [0.0478],\n",
      "        [0.0463],\n",
      "        [0.0477],\n",
      "        [0.0495],\n",
      "        [0.0523],\n",
      "        [0.0563],\n",
      "        [0.0523],\n",
      "        [0.0543],\n",
      "        [0.0538],\n",
      "        [0.0578],\n",
      "        [0.0566],\n",
      "        [0.0573],\n",
      "        [0.0558],\n",
      "        [0.0634],\n",
      "        [0.0624],\n",
      "        [0.0642],\n",
      "        [0.0656],\n",
      "        [0.0673],\n",
      "        [0.0668],\n",
      "        [0.0663],\n",
      "        [0.0698],\n",
      "        [0.0707],\n",
      "        [0.0689],\n",
      "        [0.0691],\n",
      "        [0.0691],\n",
      "        [0.0685],\n",
      "        [0.0687],\n",
      "        [0.0695],\n",
      "        [0.0718],\n",
      "        [0.0721],\n",
      "        [0.0763],\n",
      "        [0.0745],\n",
      "        [0.0741],\n",
      "        [0.0765],\n",
      "        [0.0768],\n",
      "        [0.0852],\n",
      "        [0.0834],\n",
      "        [0.0869],\n",
      "        [0.0903],\n",
      "        [0.0898],\n",
      "        [0.0919],\n",
      "        [0.0945],\n",
      "        [0.0936],\n",
      "        [0.0981],\n",
      "        [0.0962],\n",
      "        [0.0942],\n",
      "        [0.1004],\n",
      "        [0.1040],\n",
      "        [0.1062],\n",
      "        [0.1132],\n",
      "        [0.1148],\n",
      "        [0.1166],\n",
      "        [0.1229],\n",
      "        [0.1213],\n",
      "        [0.1282],\n",
      "        [0.1251],\n",
      "        [0.1256],\n",
      "        [0.1320],\n",
      "        [0.1418],\n",
      "        [0.1456],\n",
      "        [0.1666],\n",
      "        [0.1673],\n",
      "        [0.1986],\n",
      "        [0.2075]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 53.53953981399536\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.671397040416196e-07, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 115, 59, 82, 11, 106, 50, 57, 95, 15, 146, 45, 104, 145, 37, 94, 116, 111, 58, 51, 60, 107, 3, 99, 83, 84, 110, 71, 103, 125, 80, 70, 56, 34, 25, 14, 90, 23, 91, 109, 98, 114, 88, 92, 13, 96, 105, 97, 87, 74, 141, 112, 24, 8, 16, 6, 89, 140, 62, 119, 101, 128, 155, 35, 22, 4, 49, 108, 72, 12, 9, 129, 102, 26, 113, 135, 100, 73, 61, 28, 44, 42, 33, 81, 36, 69, 154, 127, 142, 52, 93, 2, 134, 75, 63, 17, 85, 64, 55, 86, 43, 46, 117, 27, 131, 65, 7, 130, 124, 153, 68, 126, 67, 79, 21, 118, 47, 48, 133, 53, 32, 38, 139, 147, 54, 132, 1, 66, 18, 20, 120, 150, 143, 136, 152, 19, 158, 148, 151, 41, 144, 138, 137, 121, 78, 0, 149, 40, 157, 156, 123, 31, 29, 30, 5, 122, 76, 77, 39] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8883],\n",
      "        [0.4882],\n",
      "        [0.6608],\n",
      "        [0.5502],\n",
      "        [0.8990],\n",
      "        [0.5571],\n",
      "        [0.7021],\n",
      "        [0.6816],\n",
      "        [0.6238],\n",
      "        [0.9071],\n",
      "        [0.2656],\n",
      "        [0.6573],\n",
      "        [0.5632],\n",
      "        [0.2653],\n",
      "        [0.7279],\n",
      "        [0.6365],\n",
      "        [0.4669],\n",
      "        [0.5269],\n",
      "        [0.6845],\n",
      "        [0.6998],\n",
      "        [0.6266],\n",
      "        [0.5392],\n",
      "        [0.9429],\n",
      "        [0.6067],\n",
      "        [0.5999],\n",
      "        [0.5957],\n",
      "        [0.5257],\n",
      "        [0.6334],\n",
      "        [0.5785],\n",
      "        [0.2786],\n",
      "        [0.4989],\n",
      "        [0.6288],\n",
      "        [0.6822],\n",
      "        [0.7467],\n",
      "        [0.8208],\n",
      "        [0.9079],\n",
      "        [0.5970],\n",
      "        [0.8782],\n",
      "        [0.6480],\n",
      "        [0.5225],\n",
      "        [0.6256],\n",
      "        [0.4903],\n",
      "        [0.5912],\n",
      "        [0.6571],\n",
      "        [0.8877],\n",
      "        [0.6546],\n",
      "        [0.5827],\n",
      "        [0.6503],\n",
      "        [0.5967],\n",
      "        [0.6114],\n",
      "        [0.3312],\n",
      "        [0.5097],\n",
      "        [0.8511],\n",
      "        [0.8698],\n",
      "        [0.8821],\n",
      "        [0.8671],\n",
      "        [0.5881],\n",
      "        [0.3395],\n",
      "        [0.6155],\n",
      "        [0.4148],\n",
      "        [0.6019],\n",
      "        [0.2816],\n",
      "        [0.2808],\n",
      "        [0.7199],\n",
      "        [0.8569],\n",
      "        [0.9283],\n",
      "        [0.6847],\n",
      "        [0.4990],\n",
      "        [0.6085],\n",
      "        [0.9115],\n",
      "        [0.8237],\n",
      "        [0.2778],\n",
      "        [0.5778],\n",
      "        [0.8655],\n",
      "        [0.4925],\n",
      "        [0.3599],\n",
      "        [0.6108],\n",
      "        [0.6028],\n",
      "        [0.6135],\n",
      "        [0.8518],\n",
      "        [0.6602],\n",
      "        [0.6702],\n",
      "        [0.7824],\n",
      "        [0.4897],\n",
      "        [0.7196],\n",
      "        [0.6330],\n",
      "        [0.2733],\n",
      "        [0.2796],\n",
      "        [0.2735],\n",
      "        [0.6647],\n",
      "        [0.6306],\n",
      "        [0.9238],\n",
      "        [0.3201],\n",
      "        [0.6105],\n",
      "        [0.6128],\n",
      "        [0.8723],\n",
      "        [0.5760],\n",
      "        [0.6095],\n",
      "        [0.6527],\n",
      "        [0.5648],\n",
      "        [0.6015],\n",
      "        [0.6625],\n",
      "        [0.4161],\n",
      "        [0.8499],\n",
      "        [0.3014],\n",
      "        [0.6221],\n",
      "        [0.8875],\n",
      "        [0.2955],\n",
      "        [0.3309],\n",
      "        [0.2559],\n",
      "        [0.6236],\n",
      "        [0.2364],\n",
      "        [0.6239],\n",
      "        [0.5627],\n",
      "        [0.8518],\n",
      "        [0.3958],\n",
      "        [0.6666],\n",
      "        [0.6651],\n",
      "        [0.2894],\n",
      "        [0.6466],\n",
      "        [0.8160],\n",
      "        [0.7337],\n",
      "        [0.3272],\n",
      "        [0.2923],\n",
      "        [0.6337],\n",
      "        [0.2842],\n",
      "        [0.8919],\n",
      "        [0.6201],\n",
      "        [0.8587],\n",
      "        [0.8719],\n",
      "        [0.4116],\n",
      "        [0.2745],\n",
      "        [0.2217],\n",
      "        [0.3608],\n",
      "        [0.2759],\n",
      "        [0.8658],\n",
      "        [0.2833],\n",
      "        [0.2854],\n",
      "        [0.2770],\n",
      "        [0.7124],\n",
      "        [0.2248],\n",
      "        [0.3325],\n",
      "        [0.3459],\n",
      "        [0.4000],\n",
      "        [0.5717],\n",
      "        [0.8519],\n",
      "        [0.2840],\n",
      "        [0.7442],\n",
      "        [0.2841],\n",
      "        [0.3231],\n",
      "        [0.3931],\n",
      "        [0.7904],\n",
      "        [0.8456],\n",
      "        [0.7948],\n",
      "        [0.8746],\n",
      "        [0.3947],\n",
      "        [0.6191],\n",
      "        [0.6312],\n",
      "        [0.7367]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0034],\n",
      "        [0.0038],\n",
      "        [0.0042],\n",
      "        [0.0057],\n",
      "        [0.0061],\n",
      "        [0.0065],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0084],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0098],\n",
      "        [0.0106],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0127],\n",
      "        [0.0128],\n",
      "        [0.0139],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0163],\n",
      "        [0.0165],\n",
      "        [0.0175],\n",
      "        [0.0181],\n",
      "        [0.0187],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0196],\n",
      "        [0.0203],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0216],\n",
      "        [0.0220],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0238],\n",
      "        [0.0243],\n",
      "        [0.0250],\n",
      "        [0.0257],\n",
      "        [0.0257],\n",
      "        [0.0259],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0264],\n",
      "        [0.0264],\n",
      "        [0.0265],\n",
      "        [0.0271],\n",
      "        [0.0274],\n",
      "        [0.0279],\n",
      "        [0.0280],\n",
      "        [0.0287],\n",
      "        [0.0293],\n",
      "        [0.0295],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0332],\n",
      "        [0.0332],\n",
      "        [0.0339],\n",
      "        [0.0351],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0368],\n",
      "        [0.0369],\n",
      "        [0.0390],\n",
      "        [0.0390],\n",
      "        [0.0397],\n",
      "        [0.0397],\n",
      "        [0.0407],\n",
      "        [0.0409],\n",
      "        [0.0411],\n",
      "        [0.0431],\n",
      "        [0.0434],\n",
      "        [0.0436],\n",
      "        [0.0463],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0495],\n",
      "        [0.0523],\n",
      "        [0.0523],\n",
      "        [0.0538],\n",
      "        [0.0543],\n",
      "        [0.0558],\n",
      "        [0.0563],\n",
      "        [0.0566],\n",
      "        [0.0573],\n",
      "        [0.0578],\n",
      "        [0.0624],\n",
      "        [0.0634],\n",
      "        [0.0642],\n",
      "        [0.0656],\n",
      "        [0.0663],\n",
      "        [0.0668],\n",
      "        [0.0673],\n",
      "        [0.0685],\n",
      "        [0.0687],\n",
      "        [0.0689],\n",
      "        [0.0691],\n",
      "        [0.0691],\n",
      "        [0.0695],\n",
      "        [0.0698],\n",
      "        [0.0707],\n",
      "        [0.0718],\n",
      "        [0.0721],\n",
      "        [0.0741],\n",
      "        [0.0745],\n",
      "        [0.0763],\n",
      "        [0.0765],\n",
      "        [0.0768],\n",
      "        [0.0834],\n",
      "        [0.0852],\n",
      "        [0.0869],\n",
      "        [0.0898],\n",
      "        [0.0903],\n",
      "        [0.0919],\n",
      "        [0.0936],\n",
      "        [0.0942],\n",
      "        [0.0945],\n",
      "        [0.0962],\n",
      "        [0.0981],\n",
      "        [0.1004],\n",
      "        [0.1040],\n",
      "        [0.1062],\n",
      "        [0.1132],\n",
      "        [0.1148],\n",
      "        [0.1166],\n",
      "        [0.1213],\n",
      "        [0.1229],\n",
      "        [0.1251],\n",
      "        [0.1256],\n",
      "        [0.1282],\n",
      "        [0.1320],\n",
      "        [0.1418],\n",
      "        [0.1456],\n",
      "        [0.1666],\n",
      "        [0.1673],\n",
      "        [0.1986],\n",
      "        [0.2075],\n",
      "        [0.2756]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0069],\n",
      "        [    0.0002],\n",
      "        [    0.0033],\n",
      "        [    0.0044],\n",
      "        [    0.0055],\n",
      "        [    0.0011],\n",
      "        [    0.0074],\n",
      "        [    0.0071],\n",
      "        [    0.0002],\n",
      "        [    0.0087],\n",
      "        [    0.0024],\n",
      "        [    0.0094],\n",
      "        [    0.0040],\n",
      "        [    0.0061],\n",
      "        [    0.0012],\n",
      "        [    0.0093],\n",
      "        [    0.0074],\n",
      "        [    0.0085],\n",
      "        [    0.0023],\n",
      "        [    0.0023],\n",
      "        [    0.0123],\n",
      "        [    0.0086],\n",
      "        [    0.0164],\n",
      "        [    0.0112],\n",
      "        [    0.0148],\n",
      "        [    0.0065],\n",
      "        [    0.0118],\n",
      "        [    0.0158],\n",
      "        [    0.0129],\n",
      "        [    0.0115],\n",
      "        [    0.0146],\n",
      "        [    0.0165],\n",
      "        [    0.0181],\n",
      "        [    0.0190],\n",
      "        [    0.0178],\n",
      "        [    0.0087],\n",
      "        [    0.0135],\n",
      "        [    0.0120],\n",
      "        [    0.0140],\n",
      "        [    0.0183],\n",
      "        [    0.0165],\n",
      "        [    0.0181],\n",
      "        [    0.0220],\n",
      "        [    0.0228],\n",
      "        [    0.0138],\n",
      "        [    0.0181],\n",
      "        [    0.0209],\n",
      "        [    0.0188],\n",
      "        [    0.0250],\n",
      "        [    0.0187],\n",
      "        [    0.0242],\n",
      "        [    0.0249],\n",
      "        [    0.0271],\n",
      "        [    0.0196],\n",
      "        [    0.0289],\n",
      "        [    0.0205],\n",
      "        [    0.0221],\n",
      "        [    0.0265],\n",
      "        [    0.0288],\n",
      "        [    0.0251],\n",
      "        [    0.0270],\n",
      "        [    0.0264],\n",
      "        [    0.0223],\n",
      "        [    0.0309],\n",
      "        [    0.0304],\n",
      "        [    0.0200],\n",
      "        [    0.0332],\n",
      "        [    0.0267],\n",
      "        [    0.0312],\n",
      "        [    0.0209],\n",
      "        [    0.0331],\n",
      "        [    0.0295],\n",
      "        [    0.0299],\n",
      "        [    0.0263],\n",
      "        [    0.0317],\n",
      "        [    0.0338],\n",
      "        [    0.0345],\n",
      "        [    0.0364],\n",
      "        [    0.0372],\n",
      "        [    0.0308],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0314],\n",
      "        [    0.0391],\n",
      "        [    0.0436],\n",
      "        [    0.0436],\n",
      "        [    0.0362],\n",
      "        [    0.0399],\n",
      "        [    0.0398],\n",
      "        [    0.0459],\n",
      "        [    0.0439],\n",
      "        [    0.0504],\n",
      "        [    0.0447],\n",
      "        [    0.0401],\n",
      "        [    0.0492],\n",
      "        [    0.0519],\n",
      "        [    0.0513],\n",
      "        [    0.0524],\n",
      "        [    0.0568],\n",
      "        [    0.0553],\n",
      "        [    0.0574],\n",
      "        [    0.0605],\n",
      "        [    0.0551],\n",
      "        [    0.0528],\n",
      "        [    0.0573],\n",
      "        [    0.0612],\n",
      "        [    0.0537],\n",
      "        [    0.0626],\n",
      "        [    0.0636],\n",
      "        [    0.0615],\n",
      "        [    0.0700],\n",
      "        [    0.0651],\n",
      "        [    0.0711],\n",
      "        [    0.0641],\n",
      "        [    0.0723],\n",
      "        [    0.0679],\n",
      "        [    0.0749],\n",
      "        [    0.0751],\n",
      "        [    0.0699],\n",
      "        [    0.0733],\n",
      "        [    0.0643],\n",
      "        [    0.0653],\n",
      "        [    0.0734],\n",
      "        [    0.0705],\n",
      "        [    0.0778],\n",
      "        [    0.0751],\n",
      "        [    0.0837],\n",
      "        [    0.0807],\n",
      "        [    0.0803],\n",
      "        [    0.0878],\n",
      "        [    0.0859],\n",
      "        [    0.0849],\n",
      "        [    0.0879],\n",
      "        [    0.0927],\n",
      "        [    0.0887],\n",
      "        [    0.0976],\n",
      "        [    0.0894],\n",
      "        [    0.0932],\n",
      "        [    0.0937],\n",
      "        [    0.0929],\n",
      "        [    0.0990],\n",
      "        [    0.1058],\n",
      "        [    0.1080],\n",
      "        [    0.1139],\n",
      "        [    0.1111],\n",
      "        [    0.1229],\n",
      "        [    0.1193],\n",
      "        [    0.1172],\n",
      "        [    0.1206],\n",
      "        [    0.1204],\n",
      "        [    0.1279],\n",
      "        [    0.1273],\n",
      "        [    0.1367],\n",
      "        [    0.1409],\n",
      "        [    0.1625],\n",
      "        [    0.1676],\n",
      "        [    0.1949],\n",
      "        [    0.2030],\n",
      "        [    0.2705]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 53.82750463485718\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 11 個區塊累積花費時間(s) 1.2189161777496338\n",
      "<<The performance of 11 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2189161777496338\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1270.15\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1700.17\n",
      "The accuracy(2000) for l = 1: 79.25%\n",
      "The accuracy(3000) for l = 1: 91.82%\n",
      "The maximum error: tensor(6905.4805)\n",
      "The minimum error: tensor(4.7656)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1083.3\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1158.1\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1739.03515625\n",
      "The minimum error: 658.40234375\n",
      "------------------------------------------------------------\n",
      "0.7924528301886793\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<12>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.483894772671192e-08, 111)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [111, 91, 102, 33, 47, 54, 142, 55, 100, 78, 7, 141, 80, 6, 53, 46, 112, 107, 103, 11, 10, 90, 41, 95, 121, 106, 19, 56, 99, 86, 9, 87, 76, 79, 67, 94, 66, 21, 52, 110, 92, 105, 70, 93, 30, 4, 0, 2, 101, 8, 84, 85, 151, 88, 137, 108, 83, 115, 157, 22, 124, 136, 104, 97, 20, 58, 12, 125, 98, 18, 24, 31, 68, 29, 40, 109, 38, 155, 5, 45, 131, 96, 150, 69, 57, 77, 138, 123, 71, 156, 65, 32, 89, 130, 48, 59, 81, 13, 60, 23, 3, 113, 82, 51, 127, 39, 42, 61, 149, 126, 120, 75, 28, 122, 34, 114, 158, 129, 64, 143, 63, 17, 49, 135, 43, 128, 44, 50, 14, 62, 146, 116, 16, 139, 148, 154, 132, 37, 144, 147, 15, 140, 134, 133, 74, 117, 36, 145, 152, 153, 27, 119, 25, 26, 1]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0023],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0055],\n",
      "        [0.0061],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0123],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0178],\n",
      "        [0.0181],\n",
      "        [0.0181],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0196],\n",
      "        [0.0200],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0228],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0263],\n",
      "        [0.0264],\n",
      "        [0.0265],\n",
      "        [0.0267],\n",
      "        [0.0270],\n",
      "        [0.0271],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0295],\n",
      "        [0.0299],\n",
      "        [0.0304],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0317],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0331],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0345],\n",
      "        [0.0362],\n",
      "        [0.0364],\n",
      "        [0.0372],\n",
      "        [0.0391],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0401],\n",
      "        [0.0430],\n",
      "        [0.0436],\n",
      "        [0.0436],\n",
      "        [0.0439],\n",
      "        [0.0447],\n",
      "        [0.0459],\n",
      "        [0.0492],\n",
      "        [0.0513],\n",
      "        [0.0519],\n",
      "        [0.0524],\n",
      "        [0.0528],\n",
      "        [0.0537],\n",
      "        [0.0551],\n",
      "        [0.0553],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0574],\n",
      "        [0.0605],\n",
      "        [0.0612],\n",
      "        [0.0615],\n",
      "        [0.0626],\n",
      "        [0.0636],\n",
      "        [0.0641],\n",
      "        [0.0643],\n",
      "        [0.0651],\n",
      "        [0.0653],\n",
      "        [0.0679],\n",
      "        [0.0681],\n",
      "        [0.0699],\n",
      "        [0.0700],\n",
      "        [0.0705],\n",
      "        [0.0711],\n",
      "        [0.0723],\n",
      "        [0.0733],\n",
      "        [0.0734],\n",
      "        [0.0749],\n",
      "        [0.0751],\n",
      "        [0.0751],\n",
      "        [0.0778],\n",
      "        [0.0803],\n",
      "        [0.0807],\n",
      "        [0.0849],\n",
      "        [0.0859],\n",
      "        [0.0878],\n",
      "        [0.0879],\n",
      "        [0.0887],\n",
      "        [0.0894],\n",
      "        [0.0927],\n",
      "        [0.0929],\n",
      "        [0.0932],\n",
      "        [0.0937],\n",
      "        [0.0976],\n",
      "        [0.0990],\n",
      "        [0.1058],\n",
      "        [0.1080],\n",
      "        [0.1111],\n",
      "        [0.1139],\n",
      "        [0.1172],\n",
      "        [0.1193],\n",
      "        [0.1204],\n",
      "        [0.1206],\n",
      "        [0.1273],\n",
      "        [0.1279],\n",
      "        [0.1367],\n",
      "        [0.1409],\n",
      "        [0.1625]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.483894772671192e-08, 111)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [111, 91, 102, 33, 47, 54, 142, 55, 100, 78, 7, 141, 80, 6, 53, 46, 112, 107, 103, 11, 10, 90, 41, 95, 121, 106, 19, 56, 99, 86, 9, 87, 76, 79, 67, 94, 66, 21, 52, 110, 92, 105, 70, 93, 30, 4, 0, 2, 101, 8, 84, 85, 151, 88, 137, 108, 83, 115, 157, 22, 124, 136, 104, 97, 20, 58, 12, 125, 98, 18, 24, 31, 68, 29, 40, 109, 38, 155, 5, 45, 131, 96, 150, 69, 57, 77, 138, 123, 71, 156, 65, 32, 89, 130, 48, 59, 81, 13, 60, 23, 3, 113, 82, 51, 127, 39, 42, 61, 149, 126, 120, 75, 28, 122, 34, 114, 158, 129, 64, 143, 63, 17, 49, 135, 43, 128, 44, 50, 14, 62, 146, 116, 16, 139, 148, 154, 132, 37, 144, 147, 15, 140, 134, 133, 74, 117, 36, 145, 152, 153, 27, 119, 25, 26, 1, 118] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4870],\n",
      "        [0.6213],\n",
      "        [0.5569],\n",
      "        [0.7230],\n",
      "        [0.6937],\n",
      "        [0.6795],\n",
      "        [0.2646],\n",
      "        [0.6564],\n",
      "        [0.5630],\n",
      "        [0.5470],\n",
      "        [0.8923],\n",
      "        [0.2649],\n",
      "        [0.5916],\n",
      "        [0.8823],\n",
      "        [0.6764],\n",
      "        [0.6960],\n",
      "        [0.4666],\n",
      "        [0.5255],\n",
      "        [0.5394],\n",
      "        [0.9009],\n",
      "        [0.9013],\n",
      "        [0.6337],\n",
      "        [0.6516],\n",
      "        [0.6054],\n",
      "        [0.2794],\n",
      "        [0.5248],\n",
      "        [0.8737],\n",
      "        [0.6230],\n",
      "        [0.5778],\n",
      "        [0.5942],\n",
      "        [0.8812],\n",
      "        [0.6445],\n",
      "        [0.4967],\n",
      "        [0.5957],\n",
      "        [0.6296],\n",
      "        [0.6235],\n",
      "        [0.6250],\n",
      "        [0.8179],\n",
      "        [0.6769],\n",
      "        [0.4891],\n",
      "        [0.6515],\n",
      "        [0.5222],\n",
      "        [0.6075],\n",
      "        [0.6474],\n",
      "        [0.7416],\n",
      "        [0.8659],\n",
      "        [0.9218],\n",
      "        [0.8637],\n",
      "        [0.5820],\n",
      "        [0.9044],\n",
      "        [0.5887],\n",
      "        [0.5859],\n",
      "        [0.2768],\n",
      "        [0.6539],\n",
      "        [0.3301],\n",
      "        [0.5080],\n",
      "        [0.5937],\n",
      "        [0.4153],\n",
      "        [0.2060],\n",
      "        [0.8618],\n",
      "        [0.2814],\n",
      "        [0.3380],\n",
      "        [0.4997],\n",
      "        [0.6008],\n",
      "        [0.8473],\n",
      "        [0.6124],\n",
      "        [0.8770],\n",
      "        [0.2776],\n",
      "        [0.5773],\n",
      "        [0.8529],\n",
      "        [0.8475],\n",
      "        [0.7154],\n",
      "        [0.6052],\n",
      "        [0.7769],\n",
      "        [0.6550],\n",
      "        [0.4913],\n",
      "        [0.6656],\n",
      "        [0.2507],\n",
      "        [0.8193],\n",
      "        [0.6785],\n",
      "        [0.3573],\n",
      "        [0.6095],\n",
      "        [0.2698],\n",
      "        [0.5996],\n",
      "        [0.6103],\n",
      "        [0.4875],\n",
      "        [0.2743],\n",
      "        [0.2793],\n",
      "        [0.6070],\n",
      "        [0.2351],\n",
      "        [0.6284],\n",
      "        [0.7150],\n",
      "        [0.6278],\n",
      "        [0.3187],\n",
      "        [0.6598],\n",
      "        [0.6099],\n",
      "        [0.5725],\n",
      "        [0.8681],\n",
      "        [0.6066],\n",
      "        [0.8464],\n",
      "        [0.8833],\n",
      "        [0.4169],\n",
      "        [0.5618],\n",
      "        [0.6482],\n",
      "        [0.3007],\n",
      "        [0.5979],\n",
      "        [0.6563],\n",
      "        [0.6182],\n",
      "        [0.2532],\n",
      "        [0.2953],\n",
      "        [0.3311],\n",
      "        [0.5594],\n",
      "        [0.8104],\n",
      "        [0.2375],\n",
      "        [0.7284],\n",
      "        [0.3967],\n",
      "        [0.2302],\n",
      "        [0.2887],\n",
      "        [0.6193],\n",
      "        [0.2906],\n",
      "        [0.6196],\n",
      "        [0.8480],\n",
      "        [0.6427],\n",
      "        [0.3255],\n",
      "        [0.6606],\n",
      "        [0.2836],\n",
      "        [0.6591],\n",
      "        [0.6300],\n",
      "        [0.8551],\n",
      "        [0.6159],\n",
      "        [0.2725],\n",
      "        [0.4123],\n",
      "        [0.8674],\n",
      "        [0.2235],\n",
      "        [0.2728],\n",
      "        [0.2785],\n",
      "        [0.3583],\n",
      "        [0.7072],\n",
      "        [0.2841],\n",
      "        [0.2745],\n",
      "        [0.8618],\n",
      "        [0.2263],\n",
      "        [0.3307],\n",
      "        [0.3441],\n",
      "        [0.5680],\n",
      "        [0.4006],\n",
      "        [0.7386],\n",
      "        [0.2820],\n",
      "        [0.3179],\n",
      "        [0.2796],\n",
      "        [0.7858],\n",
      "        [0.3928],\n",
      "        [0.8405],\n",
      "        [0.7902],\n",
      "        [0.8704],\n",
      "        [0.3949]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0023],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0055],\n",
      "        [0.0061],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0087],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0123],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0140],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0178],\n",
      "        [0.0181],\n",
      "        [0.0181],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0196],\n",
      "        [0.0200],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0228],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0263],\n",
      "        [0.0264],\n",
      "        [0.0265],\n",
      "        [0.0267],\n",
      "        [0.0270],\n",
      "        [0.0271],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0295],\n",
      "        [0.0299],\n",
      "        [0.0304],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0317],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0331],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0345],\n",
      "        [0.0362],\n",
      "        [0.0364],\n",
      "        [0.0372],\n",
      "        [0.0391],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0401],\n",
      "        [0.0430],\n",
      "        [0.0436],\n",
      "        [0.0436],\n",
      "        [0.0439],\n",
      "        [0.0447],\n",
      "        [0.0459],\n",
      "        [0.0492],\n",
      "        [0.0513],\n",
      "        [0.0519],\n",
      "        [0.0524],\n",
      "        [0.0528],\n",
      "        [0.0537],\n",
      "        [0.0551],\n",
      "        [0.0553],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0574],\n",
      "        [0.0605],\n",
      "        [0.0612],\n",
      "        [0.0615],\n",
      "        [0.0626],\n",
      "        [0.0636],\n",
      "        [0.0641],\n",
      "        [0.0643],\n",
      "        [0.0651],\n",
      "        [0.0653],\n",
      "        [0.0679],\n",
      "        [0.0681],\n",
      "        [0.0699],\n",
      "        [0.0700],\n",
      "        [0.0705],\n",
      "        [0.0711],\n",
      "        [0.0723],\n",
      "        [0.0733],\n",
      "        [0.0734],\n",
      "        [0.0749],\n",
      "        [0.0751],\n",
      "        [0.0751],\n",
      "        [0.0778],\n",
      "        [0.0803],\n",
      "        [0.0807],\n",
      "        [0.0849],\n",
      "        [0.0859],\n",
      "        [0.0878],\n",
      "        [0.0879],\n",
      "        [0.0887],\n",
      "        [0.0894],\n",
      "        [0.0927],\n",
      "        [0.0929],\n",
      "        [0.0932],\n",
      "        [0.0937],\n",
      "        [0.0976],\n",
      "        [0.0990],\n",
      "        [0.1058],\n",
      "        [0.1080],\n",
      "        [0.1111],\n",
      "        [0.1139],\n",
      "        [0.1172],\n",
      "        [0.1193],\n",
      "        [0.1204],\n",
      "        [0.1206],\n",
      "        [0.1273],\n",
      "        [0.1279],\n",
      "        [0.1367],\n",
      "        [0.1409],\n",
      "        [0.1625],\n",
      "        [0.1676]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0016],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0050],\n",
      "        [0.0056],\n",
      "        [0.0002],\n",
      "        [0.0005],\n",
      "        [0.0070],\n",
      "        [0.0025],\n",
      "        [0.0084],\n",
      "        [0.0075],\n",
      "        [0.0071],\n",
      "        [0.0090],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0057],\n",
      "        [0.0075],\n",
      "        [0.0054],\n",
      "        [0.0110],\n",
      "        [0.0058],\n",
      "        [0.0075],\n",
      "        [0.0065],\n",
      "        [0.0084],\n",
      "        [0.0114],\n",
      "        [0.0102],\n",
      "        [0.0131],\n",
      "        [0.0079],\n",
      "        [0.0101],\n",
      "        [0.0157],\n",
      "        [0.0109],\n",
      "        [0.0157],\n",
      "        [0.0117],\n",
      "        [0.0141],\n",
      "        [0.0121],\n",
      "        [0.0184],\n",
      "        [0.0128],\n",
      "        [0.0146],\n",
      "        [0.0155],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0161],\n",
      "        [0.0218],\n",
      "        [0.0202],\n",
      "        [0.0172],\n",
      "        [0.0188],\n",
      "        [0.0167],\n",
      "        [0.0203],\n",
      "        [0.0235],\n",
      "        [0.0178],\n",
      "        [0.0197],\n",
      "        [0.0246],\n",
      "        [0.0172],\n",
      "        [0.0211],\n",
      "        [0.0263],\n",
      "        [0.0245],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0207],\n",
      "        [0.0291],\n",
      "        [0.0290],\n",
      "        [0.0291],\n",
      "        [0.0237],\n",
      "        [0.0243],\n",
      "        [0.0252],\n",
      "        [0.0241],\n",
      "        [0.0301],\n",
      "        [0.0324],\n",
      "        [0.0268],\n",
      "        [0.0291],\n",
      "        [0.0332],\n",
      "        [0.0284],\n",
      "        [0.0271],\n",
      "        [0.0327],\n",
      "        [0.0344],\n",
      "        [0.0308],\n",
      "        [0.0352],\n",
      "        [0.0269],\n",
      "        [0.0344],\n",
      "        [0.0305],\n",
      "        [0.0379],\n",
      "        [0.0318],\n",
      "        [0.0314],\n",
      "        [0.0323],\n",
      "        [0.0328],\n",
      "        [0.0367],\n",
      "        [0.0399],\n",
      "        [0.0420],\n",
      "        [0.0434],\n",
      "        [0.0375],\n",
      "        [0.0408],\n",
      "        [0.0403],\n",
      "        [0.0422],\n",
      "        [0.0475],\n",
      "        [0.0422],\n",
      "        [0.0444],\n",
      "        [0.0500],\n",
      "        [0.0522],\n",
      "        [0.0478],\n",
      "        [0.0559],\n",
      "        [0.0528],\n",
      "        [0.0527],\n",
      "        [0.0535],\n",
      "        [0.0537],\n",
      "        [0.0595],\n",
      "        [0.0533],\n",
      "        [0.0580],\n",
      "        [0.0579],\n",
      "        [0.0574],\n",
      "        [0.0647],\n",
      "        [0.0637],\n",
      "        [0.0665],\n",
      "        [0.0655],\n",
      "        [0.0658],\n",
      "        [0.0679],\n",
      "        [0.0656],\n",
      "        [0.0628],\n",
      "        [0.0721],\n",
      "        [0.0669],\n",
      "        [0.0679],\n",
      "        [0.0680],\n",
      "        [0.0711],\n",
      "        [0.0692],\n",
      "        [0.0762],\n",
      "        [0.0720],\n",
      "        [0.0770],\n",
      "        [0.0724],\n",
      "        [0.0738],\n",
      "        [0.0798],\n",
      "        [0.0776],\n",
      "        [0.0818],\n",
      "        [0.0878],\n",
      "        [0.0870],\n",
      "        [0.0870],\n",
      "        [0.0845],\n",
      "        [0.0828],\n",
      "        [0.0965],\n",
      "        [0.0958],\n",
      "        [0.0911],\n",
      "        [0.0902],\n",
      "        [0.0967],\n",
      "        [0.0984],\n",
      "        [0.1088],\n",
      "        [0.1108],\n",
      "        [0.1133],\n",
      "        [0.1151],\n",
      "        [0.1198],\n",
      "        [0.1164],\n",
      "        [0.1142],\n",
      "        [0.1145],\n",
      "        [0.1294],\n",
      "        [0.1280],\n",
      "        [0.1382],\n",
      "        [0.1427],\n",
      "        [0.1616],\n",
      "        [0.1682]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 54.355021715164185\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.349775733520801e-08, 142)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [142, 55, 111, 91, 78, 33, 102, 53, 46, 47, 103, 54, 112, 10, 41, 100, 80, 141, 90, 107, 56, 7, 95, 6, 99, 106, 9, 11, 121, 76, 67, 66, 19, 79, 21, 52, 87, 86, 105, 0, 151, 30, 8, 94, 4, 110, 92, 84, 93, 2, 157, 88, 70, 83, 115, 101, 104, 58, 97, 108, 85, 20, 137, 98, 155, 68, 31, 124, 18, 22, 136, 12, 45, 109, 150, 96, 69, 125, 29, 57, 24, 40, 5, 38, 77, 156, 131, 138, 32, 65, 123, 89, 48, 71, 59, 130, 60, 81, 13, 113, 3, 39, 82, 51, 23, 149, 61, 42, 127, 158, 120, 126, 28, 114, 122, 75, 64, 34, 143, 63, 49, 17, 43, 129, 44, 50, 135, 128, 62, 14, 146, 154, 148, 139, 16, 116, 147, 144, 37, 132, 15, 140, 134, 133, 74, 152, 153, 117, 145, 36, 119, 27, 25, 26, 1, 118, 72] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2624],\n",
      "        [0.6602],\n",
      "        [0.4880],\n",
      "        [0.6232],\n",
      "        [0.5489],\n",
      "        [0.7259],\n",
      "        [0.5600],\n",
      "        [0.6794],\n",
      "        [0.6987],\n",
      "        [0.6964],\n",
      "        [0.5425],\n",
      "        [0.6827],\n",
      "        [0.4683],\n",
      "        [0.8984],\n",
      "        [0.6546],\n",
      "        [0.5660],\n",
      "        [0.5922],\n",
      "        [0.2635],\n",
      "        [0.6354],\n",
      "        [0.5265],\n",
      "        [0.6274],\n",
      "        [0.8895],\n",
      "        [0.6082],\n",
      "        [0.8802],\n",
      "        [0.5806],\n",
      "        [0.5264],\n",
      "        [0.8783],\n",
      "        [0.8986],\n",
      "        [0.2796],\n",
      "        [0.4997],\n",
      "        [0.6333],\n",
      "        [0.6287],\n",
      "        [0.8747],\n",
      "        [0.5964],\n",
      "        [0.8210],\n",
      "        [0.6795],\n",
      "        [0.6463],\n",
      "        [0.5964],\n",
      "        [0.5244],\n",
      "        [0.9185],\n",
      "        [0.2718],\n",
      "        [0.7434],\n",
      "        [0.9013],\n",
      "        [0.6254],\n",
      "        [0.8650],\n",
      "        [0.4901],\n",
      "        [0.6527],\n",
      "        [0.5910],\n",
      "        [0.6488],\n",
      "        [0.8636],\n",
      "        [0.2009],\n",
      "        [0.6556],\n",
      "        [0.6107],\n",
      "        [0.5957],\n",
      "        [0.4172],\n",
      "        [0.5846],\n",
      "        [0.5028],\n",
      "        [0.6170],\n",
      "        [0.6034],\n",
      "        [0.5085],\n",
      "        [0.5884],\n",
      "        [0.8491],\n",
      "        [0.3280],\n",
      "        [0.5805],\n",
      "        [0.2447],\n",
      "        [0.6093],\n",
      "        [0.7179],\n",
      "        [0.2788],\n",
      "        [0.8542],\n",
      "        [0.8646],\n",
      "        [0.3353],\n",
      "        [0.8758],\n",
      "        [0.6813],\n",
      "        [0.4923],\n",
      "        [0.2650],\n",
      "        [0.6122],\n",
      "        [0.6037],\n",
      "        [0.2747],\n",
      "        [0.7782],\n",
      "        [0.6146],\n",
      "        [0.8499],\n",
      "        [0.6580],\n",
      "        [0.8180],\n",
      "        [0.6687],\n",
      "        [0.4899],\n",
      "        [0.2296],\n",
      "        [0.3532],\n",
      "        [0.2743],\n",
      "        [0.7183],\n",
      "        [0.6312],\n",
      "        [0.2772],\n",
      "        [0.6295],\n",
      "        [0.6634],\n",
      "        [0.6103],\n",
      "        [0.6147],\n",
      "        [0.3160],\n",
      "        [0.6111],\n",
      "        [0.5739],\n",
      "        [0.8678],\n",
      "        [0.4192],\n",
      "        [0.8825],\n",
      "        [0.6020],\n",
      "        [0.5636],\n",
      "        [0.6513],\n",
      "        [0.8495],\n",
      "        [0.2491],\n",
      "        [0.6215],\n",
      "        [0.6588],\n",
      "        [0.2985],\n",
      "        [0.2249],\n",
      "        [0.3311],\n",
      "        [0.2932],\n",
      "        [0.8117],\n",
      "        [0.3989],\n",
      "        [0.2369],\n",
      "        [0.5618],\n",
      "        [0.6224],\n",
      "        [0.7309],\n",
      "        [0.2880],\n",
      "        [0.6227],\n",
      "        [0.6469],\n",
      "        [0.8492],\n",
      "        [0.6635],\n",
      "        [0.2865],\n",
      "        [0.6618],\n",
      "        [0.6340],\n",
      "        [0.3228],\n",
      "        [0.2817],\n",
      "        [0.6190],\n",
      "        [0.8556],\n",
      "        [0.2694],\n",
      "        [0.2720],\n",
      "        [0.2686],\n",
      "        [0.2245],\n",
      "        [0.8683],\n",
      "        [0.4141],\n",
      "        [0.2710],\n",
      "        [0.2819],\n",
      "        [0.7102],\n",
      "        [0.3545],\n",
      "        [0.8627],\n",
      "        [0.2269],\n",
      "        [0.3277],\n",
      "        [0.3414],\n",
      "        [0.5701],\n",
      "        [0.3117],\n",
      "        [0.2736],\n",
      "        [0.4019],\n",
      "        [0.2791],\n",
      "        [0.7411],\n",
      "        [0.3929],\n",
      "        [0.7879],\n",
      "        [0.8421],\n",
      "        [0.7919],\n",
      "        [0.8695],\n",
      "        [0.3956],\n",
      "        [0.6182]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0005],\n",
      "        [0.0008],\n",
      "        [0.0016],\n",
      "        [0.0025],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0075],\n",
      "        [0.0075],\n",
      "        [0.0079],\n",
      "        [0.0084],\n",
      "        [0.0084],\n",
      "        [0.0090],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0117],\n",
      "        [0.0121],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0141],\n",
      "        [0.0146],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0157],\n",
      "        [0.0161],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0172],\n",
      "        [0.0178],\n",
      "        [0.0184],\n",
      "        [0.0188],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0211],\n",
      "        [0.0218],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0241],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0246],\n",
      "        [0.0252],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0271],\n",
      "        [0.0284],\n",
      "        [0.0290],\n",
      "        [0.0291],\n",
      "        [0.0291],\n",
      "        [0.0291],\n",
      "        [0.0301],\n",
      "        [0.0305],\n",
      "        [0.0308],\n",
      "        [0.0314],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0332],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0352],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0379],\n",
      "        [0.0399],\n",
      "        [0.0403],\n",
      "        [0.0408],\n",
      "        [0.0420],\n",
      "        [0.0422],\n",
      "        [0.0422],\n",
      "        [0.0434],\n",
      "        [0.0444],\n",
      "        [0.0475],\n",
      "        [0.0478],\n",
      "        [0.0500],\n",
      "        [0.0522],\n",
      "        [0.0527],\n",
      "        [0.0528],\n",
      "        [0.0533],\n",
      "        [0.0535],\n",
      "        [0.0537],\n",
      "        [0.0559],\n",
      "        [0.0574],\n",
      "        [0.0579],\n",
      "        [0.0580],\n",
      "        [0.0595],\n",
      "        [0.0628],\n",
      "        [0.0637],\n",
      "        [0.0647],\n",
      "        [0.0655],\n",
      "        [0.0656],\n",
      "        [0.0658],\n",
      "        [0.0665],\n",
      "        [0.0669],\n",
      "        [0.0679],\n",
      "        [0.0679],\n",
      "        [0.0680],\n",
      "        [0.0692],\n",
      "        [0.0711],\n",
      "        [0.0720],\n",
      "        [0.0721],\n",
      "        [0.0724],\n",
      "        [0.0738],\n",
      "        [0.0762],\n",
      "        [0.0770],\n",
      "        [0.0776],\n",
      "        [0.0798],\n",
      "        [0.0818],\n",
      "        [0.0828],\n",
      "        [0.0845],\n",
      "        [0.0870],\n",
      "        [0.0870],\n",
      "        [0.0878],\n",
      "        [0.0902],\n",
      "        [0.0911],\n",
      "        [0.0958],\n",
      "        [0.0965],\n",
      "        [0.0967],\n",
      "        [0.0984],\n",
      "        [0.1088],\n",
      "        [0.1108],\n",
      "        [0.1133],\n",
      "        [0.1142],\n",
      "        [0.1145],\n",
      "        [0.1151],\n",
      "        [0.1164],\n",
      "        [0.1198],\n",
      "        [0.1280],\n",
      "        [0.1294],\n",
      "        [0.1382],\n",
      "        [0.1427],\n",
      "        [0.1616],\n",
      "        [0.1682],\n",
      "        [0.1977]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0009],\n",
      "        [    0.0004],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0037],\n",
      "        [    0.0038],\n",
      "        [    0.0044],\n",
      "        [    0.0048],\n",
      "        [    0.0058],\n",
      "        [    0.0038],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0059],\n",
      "        [    0.0005],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0048],\n",
      "        [    0.0080],\n",
      "        [    0.0092],\n",
      "        [    0.0087],\n",
      "        [    0.0073],\n",
      "        [    0.0139],\n",
      "        [    0.0090],\n",
      "        [    0.0139],\n",
      "        [    0.0103],\n",
      "        [    0.0109],\n",
      "        [    0.0055],\n",
      "        [    0.0159],\n",
      "        [    0.0113],\n",
      "        [    0.0117],\n",
      "        [    0.0125],\n",
      "        [    0.0131],\n",
      "        [    0.0108],\n",
      "        [    0.0163],\n",
      "        [    0.0151],\n",
      "        [    0.0163],\n",
      "        [    0.0138],\n",
      "        [    0.0145],\n",
      "        [    0.0162],\n",
      "        [    0.0109],\n",
      "        [    0.0134],\n",
      "        [    0.0184],\n",
      "        [    0.0121],\n",
      "        [    0.0170],\n",
      "        [    0.0154],\n",
      "        [    0.0182],\n",
      "        [    0.0172],\n",
      "        [    0.0207],\n",
      "        [    0.0183],\n",
      "        [    0.0174],\n",
      "        [    0.0173],\n",
      "        [    0.0230],\n",
      "        [    0.0210],\n",
      "        [    0.0243],\n",
      "        [    0.0229],\n",
      "        [    0.0233],\n",
      "        [    0.0229],\n",
      "        [    0.0232],\n",
      "        [    0.0248],\n",
      "        [    0.0260],\n",
      "        [    0.0239],\n",
      "        [    0.0268],\n",
      "        [    0.0275],\n",
      "        [    0.0267],\n",
      "        [    0.0224],\n",
      "        [    0.0271],\n",
      "        [    0.0288],\n",
      "        [    0.0302],\n",
      "        [    0.0310],\n",
      "        [    0.0280],\n",
      "        [    0.0307],\n",
      "        [    0.0340],\n",
      "        [    0.0315],\n",
      "        [    0.0318],\n",
      "        [    0.0280],\n",
      "        [    0.0323],\n",
      "        [    0.0323],\n",
      "        [    0.0336],\n",
      "        [    0.0311],\n",
      "        [    0.0321],\n",
      "        [    0.0318],\n",
      "        [    0.0339],\n",
      "        [    0.0383],\n",
      "        [    0.0351],\n",
      "        [    0.0370],\n",
      "        [    0.0337],\n",
      "        [    0.0407],\n",
      "        [    0.0392],\n",
      "        [    0.0404],\n",
      "        [    0.0418],\n",
      "        [    0.0431],\n",
      "        [    0.0439],\n",
      "        [    0.0424],\n",
      "        [    0.0429],\n",
      "        [    0.0434],\n",
      "        [    0.0492],\n",
      "        [    0.0470],\n",
      "        [    0.0516],\n",
      "        [    0.0552],\n",
      "        [    0.0520],\n",
      "        [    0.0493],\n",
      "        [    0.0525],\n",
      "        [    0.0547],\n",
      "        [    0.0541],\n",
      "        [    0.0551],\n",
      "        [    0.0547],\n",
      "        [    0.0581],\n",
      "        [    0.0590],\n",
      "        [    0.0606],\n",
      "        [    0.0591],\n",
      "        [    0.0633],\n",
      "        [    0.0656],\n",
      "        [    0.0637],\n",
      "        [    0.0649],\n",
      "        [    0.0657],\n",
      "        [    0.0656],\n",
      "        [    0.0675],\n",
      "        [    0.0672],\n",
      "        [    0.0662],\n",
      "        [    0.0686],\n",
      "        [    0.0687],\n",
      "        [    0.0730],\n",
      "        [    0.0728],\n",
      "        [    0.0732],\n",
      "        [    0.0734],\n",
      "        [    0.0733],\n",
      "        [    0.0779],\n",
      "        [    0.0780],\n",
      "        [    0.0781],\n",
      "        [    0.0821],\n",
      "        [    0.0797],\n",
      "        [    0.0779],\n",
      "        [    0.0814],\n",
      "        [    0.0854],\n",
      "        [    0.0894],\n",
      "        [    0.0883],\n",
      "        [    0.0877],\n",
      "        [    0.0896],\n",
      "        [    0.0954],\n",
      "        [    0.0990],\n",
      "        [    0.0989],\n",
      "        [    0.0971],\n",
      "        [    0.1107],\n",
      "        [    0.1126],\n",
      "        [    0.1121],\n",
      "        [    0.1094],\n",
      "        [    0.1099],\n",
      "        [    0.1154],\n",
      "        [    0.1144],\n",
      "        [    0.1189],\n",
      "        [    0.1273],\n",
      "        [    0.1282],\n",
      "        [    0.1362],\n",
      "        [    0.1413],\n",
      "        [    0.1580],\n",
      "        [    0.1681],\n",
      "        [    0.1968]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 54.64298725128174\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.647592995368541e-08, 111)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [111, 91, 55, 10, 142, 78, 47, 33, 102, 53, 80, 103, 54, 9, 46, 112, 41, 100, 56, 141, 107, 95, 90, 99, 19, 0, 106, 121, 76, 8, 67, 66, 151, 87, 7, 6, 86, 21, 4, 11, 105, 79, 52, 94, 92, 157, 2, 110, 93, 30, 84, 70, 155, 115, 104, 88, 58, 101, 85, 83, 97, 108, 98, 20, 68, 137, 22, 150, 31, 124, 136, 18, 29, 45, 109, 24, 57, 69, 96, 125, 156, 40, 12, 38, 77, 5, 138, 32, 131, 65, 48, 71, 123, 59, 89, 60, 130, 3, 81, 113, 39, 51, 149, 82, 23, 13, 61, 42, 158, 127, 120, 28, 114, 126, 75, 122, 143, 34, 64, 63, 49, 43, 17, 129, 50, 44, 135, 154, 128, 62, 146, 148, 14, 139, 147, 116, 16, 144, 37, 140, 15, 132, 152, 153, 134, 74, 133, 145, 117, 36, 119, 27, 25, 26, 1, 118, 72, 73] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4871],\n",
      "        [0.6218],\n",
      "        [0.6602],\n",
      "        [0.8931],\n",
      "        [0.2613],\n",
      "        [0.5477],\n",
      "        [0.6952],\n",
      "        [0.7256],\n",
      "        [0.5602],\n",
      "        [0.6787],\n",
      "        [0.5900],\n",
      "        [0.5430],\n",
      "        [0.6823],\n",
      "        [0.8729],\n",
      "        [0.6976],\n",
      "        [0.4681],\n",
      "        [0.6540],\n",
      "        [0.5661],\n",
      "        [0.6281],\n",
      "        [0.2630],\n",
      "        [0.5253],\n",
      "        [0.6076],\n",
      "        [0.6338],\n",
      "        [0.5804],\n",
      "        [0.8724],\n",
      "        [0.9128],\n",
      "        [0.5257],\n",
      "        [0.2797],\n",
      "        [0.4996],\n",
      "        [0.8956],\n",
      "        [0.6329],\n",
      "        [0.6284],\n",
      "        [0.2680],\n",
      "        [0.6444],\n",
      "        [0.8839],\n",
      "        [0.8753],\n",
      "        [0.5952],\n",
      "        [0.8205],\n",
      "        [0.8616],\n",
      "        [0.8937],\n",
      "        [0.5244],\n",
      "        [0.5942],\n",
      "        [0.6787],\n",
      "        [0.6240],\n",
      "        [0.6506],\n",
      "        [0.1976],\n",
      "        [0.8607],\n",
      "        [0.4892],\n",
      "        [0.6469],\n",
      "        [0.7422],\n",
      "        [0.5901],\n",
      "        [0.6099],\n",
      "        [0.2402],\n",
      "        [0.4176],\n",
      "        [0.5035],\n",
      "        [0.6537],\n",
      "        [0.6180],\n",
      "        [0.5844],\n",
      "        [0.5877],\n",
      "        [0.5944],\n",
      "        [0.6030],\n",
      "        [0.5069],\n",
      "        [0.5806],\n",
      "        [0.8475],\n",
      "        [0.6093],\n",
      "        [0.3268],\n",
      "        [0.8635],\n",
      "        [0.2616],\n",
      "        [0.7175],\n",
      "        [0.2776],\n",
      "        [0.3338],\n",
      "        [0.8523],\n",
      "        [0.7766],\n",
      "        [0.6802],\n",
      "        [0.4913],\n",
      "        [0.8485],\n",
      "        [0.6153],\n",
      "        [0.6037],\n",
      "        [0.6116],\n",
      "        [0.2735],\n",
      "        [0.2257],\n",
      "        [0.6575],\n",
      "        [0.8719],\n",
      "        [0.6686],\n",
      "        [0.4895],\n",
      "        [0.8142],\n",
      "        [0.2750],\n",
      "        [0.7182],\n",
      "        [0.3504],\n",
      "        [0.6302],\n",
      "        [0.6632],\n",
      "        [0.6098],\n",
      "        [0.2761],\n",
      "        [0.6158],\n",
      "        [0.6278],\n",
      "        [0.6120],\n",
      "        [0.3143],\n",
      "        [0.8790],\n",
      "        [0.5723],\n",
      "        [0.4199],\n",
      "        [0.6028],\n",
      "        [0.6509],\n",
      "        [0.2464],\n",
      "        [0.5623],\n",
      "        [0.8487],\n",
      "        [0.8648],\n",
      "        [0.6214],\n",
      "        [0.6578],\n",
      "        [0.2212],\n",
      "        [0.2974],\n",
      "        [0.3308],\n",
      "        [0.8099],\n",
      "        [0.3996],\n",
      "        [0.2923],\n",
      "        [0.5610],\n",
      "        [0.2370],\n",
      "        [0.2863],\n",
      "        [0.7302],\n",
      "        [0.6217],\n",
      "        [0.6221],\n",
      "        [0.6474],\n",
      "        [0.6627],\n",
      "        [0.8473],\n",
      "        [0.2853],\n",
      "        [0.6344],\n",
      "        [0.6609],\n",
      "        [0.3211],\n",
      "        [0.2671],\n",
      "        [0.2808],\n",
      "        [0.6185],\n",
      "        [0.2673],\n",
      "        [0.2655],\n",
      "        [0.8533],\n",
      "        [0.2261],\n",
      "        [0.2685],\n",
      "        [0.4146],\n",
      "        [0.8659],\n",
      "        [0.2805],\n",
      "        [0.7097],\n",
      "        [0.2281],\n",
      "        [0.8605],\n",
      "        [0.3520],\n",
      "        [0.3069],\n",
      "        [0.2690],\n",
      "        [0.3259],\n",
      "        [0.5690],\n",
      "        [0.3396],\n",
      "        [0.2771],\n",
      "        [0.4022],\n",
      "        [0.7402],\n",
      "        [0.3922],\n",
      "        [0.7867],\n",
      "        [0.8401],\n",
      "        [0.7905],\n",
      "        [0.8659],\n",
      "        [0.3955],\n",
      "        [0.6173],\n",
      "        [0.6270]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0009],\n",
      "        [    0.0037],\n",
      "        [    0.0038],\n",
      "        [    0.0038],\n",
      "        [    0.0044],\n",
      "        [    0.0048],\n",
      "        [    0.0048],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0055],\n",
      "        [    0.0058],\n",
      "        [    0.0059],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0080],\n",
      "        [    0.0087],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0103],\n",
      "        [    0.0108],\n",
      "        [    0.0109],\n",
      "        [    0.0109],\n",
      "        [    0.0113],\n",
      "        [    0.0117],\n",
      "        [    0.0121],\n",
      "        [    0.0125],\n",
      "        [    0.0131],\n",
      "        [    0.0134],\n",
      "        [    0.0138],\n",
      "        [    0.0139],\n",
      "        [    0.0139],\n",
      "        [    0.0145],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0163],\n",
      "        [    0.0163],\n",
      "        [    0.0170],\n",
      "        [    0.0172],\n",
      "        [    0.0173],\n",
      "        [    0.0174],\n",
      "        [    0.0182],\n",
      "        [    0.0183],\n",
      "        [    0.0184],\n",
      "        [    0.0207],\n",
      "        [    0.0210],\n",
      "        [    0.0224],\n",
      "        [    0.0229],\n",
      "        [    0.0229],\n",
      "        [    0.0230],\n",
      "        [    0.0232],\n",
      "        [    0.0233],\n",
      "        [    0.0239],\n",
      "        [    0.0243],\n",
      "        [    0.0248],\n",
      "        [    0.0260],\n",
      "        [    0.0267],\n",
      "        [    0.0268],\n",
      "        [    0.0271],\n",
      "        [    0.0275],\n",
      "        [    0.0280],\n",
      "        [    0.0280],\n",
      "        [    0.0288],\n",
      "        [    0.0302],\n",
      "        [    0.0307],\n",
      "        [    0.0310],\n",
      "        [    0.0311],\n",
      "        [    0.0315],\n",
      "        [    0.0318],\n",
      "        [    0.0318],\n",
      "        [    0.0321],\n",
      "        [    0.0323],\n",
      "        [    0.0323],\n",
      "        [    0.0336],\n",
      "        [    0.0337],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0351],\n",
      "        [    0.0370],\n",
      "        [    0.0383],\n",
      "        [    0.0392],\n",
      "        [    0.0404],\n",
      "        [    0.0407],\n",
      "        [    0.0418],\n",
      "        [    0.0424],\n",
      "        [    0.0429],\n",
      "        [    0.0431],\n",
      "        [    0.0434],\n",
      "        [    0.0439],\n",
      "        [    0.0470],\n",
      "        [    0.0492],\n",
      "        [    0.0493],\n",
      "        [    0.0516],\n",
      "        [    0.0520],\n",
      "        [    0.0525],\n",
      "        [    0.0541],\n",
      "        [    0.0547],\n",
      "        [    0.0547],\n",
      "        [    0.0551],\n",
      "        [    0.0552],\n",
      "        [    0.0581],\n",
      "        [    0.0590],\n",
      "        [    0.0591],\n",
      "        [    0.0606],\n",
      "        [    0.0633],\n",
      "        [    0.0637],\n",
      "        [    0.0649],\n",
      "        [    0.0656],\n",
      "        [    0.0656],\n",
      "        [    0.0657],\n",
      "        [    0.0662],\n",
      "        [    0.0672],\n",
      "        [    0.0675],\n",
      "        [    0.0686],\n",
      "        [    0.0687],\n",
      "        [    0.0728],\n",
      "        [    0.0730],\n",
      "        [    0.0732],\n",
      "        [    0.0733],\n",
      "        [    0.0734],\n",
      "        [    0.0779],\n",
      "        [    0.0779],\n",
      "        [    0.0780],\n",
      "        [    0.0781],\n",
      "        [    0.0797],\n",
      "        [    0.0814],\n",
      "        [    0.0821],\n",
      "        [    0.0854],\n",
      "        [    0.0877],\n",
      "        [    0.0883],\n",
      "        [    0.0894],\n",
      "        [    0.0896],\n",
      "        [    0.0954],\n",
      "        [    0.0971],\n",
      "        [    0.0989],\n",
      "        [    0.0990],\n",
      "        [    0.1094],\n",
      "        [    0.1099],\n",
      "        [    0.1107],\n",
      "        [    0.1121],\n",
      "        [    0.1126],\n",
      "        [    0.1144],\n",
      "        [    0.1154],\n",
      "        [    0.1189],\n",
      "        [    0.1273],\n",
      "        [    0.1282],\n",
      "        [    0.1362],\n",
      "        [    0.1413],\n",
      "        [    0.1580],\n",
      "        [    0.1681],\n",
      "        [    0.1968],\n",
      "        [    0.2033]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0009],\n",
      "        [0.0006],\n",
      "        [0.0024],\n",
      "        [0.0009],\n",
      "        [0.0048],\n",
      "        [0.0029],\n",
      "        [0.0045],\n",
      "        [0.0051],\n",
      "        [0.0051],\n",
      "        [0.0029],\n",
      "        [0.0039],\n",
      "        [0.0049],\n",
      "        [0.0025],\n",
      "        [0.0065],\n",
      "        [0.0056],\n",
      "        [0.0073],\n",
      "        [0.0078],\n",
      "        [0.0067],\n",
      "        [0.0073],\n",
      "        [0.0094],\n",
      "        [0.0092],\n",
      "        [0.0106],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0076],\n",
      "        [0.0110],\n",
      "        [0.0106],\n",
      "        [0.0118],\n",
      "        [0.0086],\n",
      "        [0.0131],\n",
      "        [0.0137],\n",
      "        [0.0109],\n",
      "        [0.0122],\n",
      "        [0.0173],\n",
      "        [0.0166],\n",
      "        [0.0134],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0185],\n",
      "        [0.0156],\n",
      "        [0.0182],\n",
      "        [0.0166],\n",
      "        [0.0161],\n",
      "        [0.0156],\n",
      "        [0.0151],\n",
      "        [0.0169],\n",
      "        [0.0177],\n",
      "        [0.0168],\n",
      "        [0.0181],\n",
      "        [0.0213],\n",
      "        [0.0200],\n",
      "        [0.0190],\n",
      "        [0.0219],\n",
      "        [0.0216],\n",
      "        [0.0245],\n",
      "        [0.0223],\n",
      "        [0.0236],\n",
      "        [0.0234],\n",
      "        [0.0253],\n",
      "        [0.0248],\n",
      "        [0.0270],\n",
      "        [0.0261],\n",
      "        [0.0267],\n",
      "        [0.0275],\n",
      "        [0.0272],\n",
      "        [0.0284],\n",
      "        [0.0258],\n",
      "        [0.0280],\n",
      "        [0.0298],\n",
      "        [0.0308],\n",
      "        [0.0312],\n",
      "        [0.0311],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0318],\n",
      "        [0.0313],\n",
      "        [0.0326],\n",
      "        [0.0324],\n",
      "        [0.0332],\n",
      "        [0.0310],\n",
      "        [0.0339],\n",
      "        [0.0356],\n",
      "        [0.0356],\n",
      "        [0.0373],\n",
      "        [0.0398],\n",
      "        [0.0373],\n",
      "        [0.0396],\n",
      "        [0.0418],\n",
      "        [0.0430],\n",
      "        [0.0425],\n",
      "        [0.0422],\n",
      "        [0.0430],\n",
      "        [0.0424],\n",
      "        [0.0453],\n",
      "        [0.0460],\n",
      "        [0.0494],\n",
      "        [0.0482],\n",
      "        [0.0530],\n",
      "        [0.0509],\n",
      "        [0.0512],\n",
      "        [0.0539],\n",
      "        [0.0532],\n",
      "        [0.0558],\n",
      "        [0.0558],\n",
      "        [0.0561],\n",
      "        [0.0581],\n",
      "        [0.0597],\n",
      "        [0.0565],\n",
      "        [0.0602],\n",
      "        [0.0635],\n",
      "        [0.0636],\n",
      "        [0.0638],\n",
      "        [0.0650],\n",
      "        [0.0648],\n",
      "        [0.0646],\n",
      "        [0.0657],\n",
      "        [0.0674],\n",
      "        [0.0683],\n",
      "        [0.0692],\n",
      "        [0.0678],\n",
      "        [0.0734],\n",
      "        [0.0731],\n",
      "        [0.0730],\n",
      "        [0.0726],\n",
      "        [0.0741],\n",
      "        [0.0781],\n",
      "        [0.0743],\n",
      "        [0.0776],\n",
      "        [0.0785],\n",
      "        [0.0787],\n",
      "        [0.0794],\n",
      "        [0.0823],\n",
      "        [0.0828],\n",
      "        [0.0863],\n",
      "        [0.0894],\n",
      "        [0.0901],\n",
      "        [0.0893],\n",
      "        [0.0957],\n",
      "        [0.0949],\n",
      "        [0.0993],\n",
      "        [0.0999],\n",
      "        [0.1058],\n",
      "        [0.1066],\n",
      "        [0.1110],\n",
      "        [0.1109],\n",
      "        [0.1128],\n",
      "        [0.1135],\n",
      "        [0.1164],\n",
      "        [0.1188],\n",
      "        [0.1273],\n",
      "        [0.1286],\n",
      "        [0.1357],\n",
      "        [0.1413],\n",
      "        [0.1567],\n",
      "        [0.1687],\n",
      "        [0.1958],\n",
      "        [0.2015]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 54.93023920059204\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.814104504977877e-07, 111)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [111, 55, 142, 91, 10, 9, 47, 80, 103, 33, 78, 54, 53, 102, 112, 46, 56, 141, 41, 0, 100, 8, 95, 107, 99, 19, 121, 90, 151, 106, 76, 87, 67, 86, 66, 21, 4, 157, 105, 92, 94, 52, 6, 93, 2, 7, 110, 30, 79, 11, 155, 70, 84, 104, 115, 58, 85, 101, 88, 97, 83, 150, 98, 20, 108, 137, 68, 31, 22, 124, 136, 156, 29, 18, 57, 24, 109, 45, 96, 69, 125, 40, 12, 38, 77, 138, 32, 5, 131, 71, 59, 48, 123, 65, 89, 60, 3, 130, 113, 39, 81, 149, 51, 23, 82, 13, 158, 61, 42, 127, 120, 28, 114, 122, 75, 126, 143, 34, 49, 64, 63, 50, 129, 17, 43, 44, 154, 128, 135, 62, 146, 148, 14, 139, 147, 144, 116, 16, 140, 37, 15, 132, 152, 153, 74, 134, 133, 145, 117, 36, 119, 27, 25, 26, 1, 118, 72, 73, 35] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4867],\n",
      "        [0.6603],\n",
      "        [0.2613],\n",
      "        [0.6207],\n",
      "        [0.8901],\n",
      "        [0.8699],\n",
      "        [0.6943],\n",
      "        [0.5880],\n",
      "        [0.5440],\n",
      "        [0.7263],\n",
      "        [0.5466],\n",
      "        [0.6820],\n",
      "        [0.6785],\n",
      "        [0.5609],\n",
      "        [0.4683],\n",
      "        [0.6968],\n",
      "        [0.6287],\n",
      "        [0.2636],\n",
      "        [0.6537],\n",
      "        [0.9095],\n",
      "        [0.5668],\n",
      "        [0.8921],\n",
      "        [0.6073],\n",
      "        [0.5246],\n",
      "        [0.5807],\n",
      "        [0.8720],\n",
      "        [0.2804],\n",
      "        [0.6324],\n",
      "        [0.2654],\n",
      "        [0.5256],\n",
      "        [0.4995],\n",
      "        [0.6427],\n",
      "        [0.6323],\n",
      "        [0.5941],\n",
      "        [0.6279],\n",
      "        [0.8215],\n",
      "        [0.8606],\n",
      "        [0.1953],\n",
      "        [0.5250],\n",
      "        [0.6490],\n",
      "        [0.6231],\n",
      "        [0.6785],\n",
      "        [0.8726],\n",
      "        [0.6454],\n",
      "        [0.8602],\n",
      "        [0.8806],\n",
      "        [0.4887],\n",
      "        [0.7425],\n",
      "        [0.5923],\n",
      "        [0.8911],\n",
      "        [0.2368],\n",
      "        [0.6089],\n",
      "        [0.5894],\n",
      "        [0.5048],\n",
      "        [0.4185],\n",
      "        [0.6188],\n",
      "        [0.5872],\n",
      "        [0.5846],\n",
      "        [0.6523],\n",
      "        [0.6030],\n",
      "        [0.5934],\n",
      "        [0.2594],\n",
      "        [0.5811],\n",
      "        [0.8476],\n",
      "        [0.5060],\n",
      "        [0.3271],\n",
      "        [0.6089],\n",
      "        [0.7183],\n",
      "        [0.8639],\n",
      "        [0.2780],\n",
      "        [0.3337],\n",
      "        [0.2231],\n",
      "        [0.7766],\n",
      "        [0.8521],\n",
      "        [0.6161],\n",
      "        [0.8484],\n",
      "        [0.4907],\n",
      "        [0.6794],\n",
      "        [0.6116],\n",
      "        [0.6034],\n",
      "        [0.2739],\n",
      "        [0.6574],\n",
      "        [0.8703],\n",
      "        [0.6691],\n",
      "        [0.4892],\n",
      "        [0.2768],\n",
      "        [0.7190],\n",
      "        [0.8126],\n",
      "        [0.3493],\n",
      "        [0.6091],\n",
      "        [0.6168],\n",
      "        [0.6632],\n",
      "        [0.2763],\n",
      "        [0.6290],\n",
      "        [0.6265],\n",
      "        [0.6130],\n",
      "        [0.8778],\n",
      "        [0.3141],\n",
      "        [0.4210],\n",
      "        [0.6041],\n",
      "        [0.5708],\n",
      "        [0.2449],\n",
      "        [0.6511],\n",
      "        [0.8494],\n",
      "        [0.5612],\n",
      "        [0.8639],\n",
      "        [0.2186],\n",
      "        [0.6213],\n",
      "        [0.6571],\n",
      "        [0.2978],\n",
      "        [0.3310],\n",
      "        [0.8097],\n",
      "        [0.4007],\n",
      "        [0.2380],\n",
      "        [0.5601],\n",
      "        [0.2929],\n",
      "        [0.2858],\n",
      "        [0.7305],\n",
      "        [0.6482],\n",
      "        [0.6209],\n",
      "        [0.6215],\n",
      "        [0.6352],\n",
      "        [0.2855],\n",
      "        [0.8472],\n",
      "        [0.6621],\n",
      "        [0.6602],\n",
      "        [0.2634],\n",
      "        [0.2811],\n",
      "        [0.3209],\n",
      "        [0.6181],\n",
      "        [0.2664],\n",
      "        [0.2635],\n",
      "        [0.8531],\n",
      "        [0.2287],\n",
      "        [0.2671],\n",
      "        [0.2802],\n",
      "        [0.4157],\n",
      "        [0.8652],\n",
      "        [0.2303],\n",
      "        [0.7100],\n",
      "        [0.8601],\n",
      "        [0.3512],\n",
      "        [0.3034],\n",
      "        [0.2657],\n",
      "        [0.5677],\n",
      "        [0.3255],\n",
      "        [0.3393],\n",
      "        [0.2762],\n",
      "        [0.4032],\n",
      "        [0.7402],\n",
      "        [0.3923],\n",
      "        [0.7871],\n",
      "        [0.8395],\n",
      "        [0.7905],\n",
      "        [0.8646],\n",
      "        [0.3961],\n",
      "        [0.6163],\n",
      "        [0.6252],\n",
      "        [0.7346]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0051],\n",
      "        [0.0056],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0078],\n",
      "        [0.0086],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0131],\n",
      "        [0.0134],\n",
      "        [0.0137],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0151],\n",
      "        [0.0156],\n",
      "        [0.0156],\n",
      "        [0.0161],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0173],\n",
      "        [0.0177],\n",
      "        [0.0181],\n",
      "        [0.0182],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0200],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0223],\n",
      "        [0.0234],\n",
      "        [0.0236],\n",
      "        [0.0245],\n",
      "        [0.0248],\n",
      "        [0.0253],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0267],\n",
      "        [0.0270],\n",
      "        [0.0272],\n",
      "        [0.0275],\n",
      "        [0.0280],\n",
      "        [0.0284],\n",
      "        [0.0298],\n",
      "        [0.0308],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0312],\n",
      "        [0.0313],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0332],\n",
      "        [0.0339],\n",
      "        [0.0356],\n",
      "        [0.0356],\n",
      "        [0.0373],\n",
      "        [0.0373],\n",
      "        [0.0396],\n",
      "        [0.0398],\n",
      "        [0.0418],\n",
      "        [0.0422],\n",
      "        [0.0424],\n",
      "        [0.0425],\n",
      "        [0.0430],\n",
      "        [0.0430],\n",
      "        [0.0453],\n",
      "        [0.0460],\n",
      "        [0.0482],\n",
      "        [0.0494],\n",
      "        [0.0509],\n",
      "        [0.0512],\n",
      "        [0.0530],\n",
      "        [0.0532],\n",
      "        [0.0539],\n",
      "        [0.0558],\n",
      "        [0.0558],\n",
      "        [0.0561],\n",
      "        [0.0565],\n",
      "        [0.0581],\n",
      "        [0.0597],\n",
      "        [0.0602],\n",
      "        [0.0635],\n",
      "        [0.0636],\n",
      "        [0.0638],\n",
      "        [0.0646],\n",
      "        [0.0648],\n",
      "        [0.0650],\n",
      "        [0.0657],\n",
      "        [0.0674],\n",
      "        [0.0678],\n",
      "        [0.0683],\n",
      "        [0.0692],\n",
      "        [0.0726],\n",
      "        [0.0730],\n",
      "        [0.0731],\n",
      "        [0.0734],\n",
      "        [0.0741],\n",
      "        [0.0743],\n",
      "        [0.0776],\n",
      "        [0.0781],\n",
      "        [0.0785],\n",
      "        [0.0787],\n",
      "        [0.0794],\n",
      "        [0.0823],\n",
      "        [0.0828],\n",
      "        [0.0863],\n",
      "        [0.0893],\n",
      "        [0.0894],\n",
      "        [0.0901],\n",
      "        [0.0949],\n",
      "        [0.0957],\n",
      "        [0.0993],\n",
      "        [0.0999],\n",
      "        [0.1058],\n",
      "        [0.1066],\n",
      "        [0.1109],\n",
      "        [0.1110],\n",
      "        [0.1128],\n",
      "        [0.1135],\n",
      "        [0.1164],\n",
      "        [0.1188],\n",
      "        [0.1273],\n",
      "        [0.1286],\n",
      "        [0.1357],\n",
      "        [0.1413],\n",
      "        [0.1567],\n",
      "        [0.1687],\n",
      "        [0.1958],\n",
      "        [0.2015],\n",
      "        [0.2735]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0024],\n",
      "        [0.0006],\n",
      "        [0.0024],\n",
      "        [0.0077],\n",
      "        [0.0027],\n",
      "        [0.0014],\n",
      "        [0.0004],\n",
      "        [0.0031],\n",
      "        [0.0013],\n",
      "        [0.0064],\n",
      "        [0.0015],\n",
      "        [0.0085],\n",
      "        [0.0056],\n",
      "        [0.0052],\n",
      "        [0.0106],\n",
      "        [0.0089],\n",
      "        [0.0066],\n",
      "        [0.0109],\n",
      "        [0.0021],\n",
      "        [0.0082],\n",
      "        [0.0031],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0100],\n",
      "        [0.0068],\n",
      "        [0.0090],\n",
      "        [0.0123],\n",
      "        [0.0091],\n",
      "        [0.0111],\n",
      "        [0.0128],\n",
      "        [0.0099],\n",
      "        [0.0157],\n",
      "        [0.0118],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0112],\n",
      "        [0.0139],\n",
      "        [0.0151],\n",
      "        [0.0136],\n",
      "        [0.0148],\n",
      "        [0.0201],\n",
      "        [0.0214],\n",
      "        [0.0149],\n",
      "        [0.0141],\n",
      "        [0.0226],\n",
      "        [0.0174],\n",
      "        [0.0215],\n",
      "        [0.0206],\n",
      "        [0.0234],\n",
      "        [0.0167],\n",
      "        [0.0175],\n",
      "        [0.0227],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0242],\n",
      "        [0.0222],\n",
      "        [0.0236],\n",
      "        [0.0266],\n",
      "        [0.0251],\n",
      "        [0.0270],\n",
      "        [0.0244],\n",
      "        [0.0259],\n",
      "        [0.0296],\n",
      "        [0.0277],\n",
      "        [0.0271],\n",
      "        [0.0296],\n",
      "        [0.0309],\n",
      "        [0.0257],\n",
      "        [0.0287],\n",
      "        [0.0310],\n",
      "        [0.0292],\n",
      "        [0.0273],\n",
      "        [0.0343],\n",
      "        [0.0333],\n",
      "        [0.0286],\n",
      "        [0.0326],\n",
      "        [0.0365],\n",
      "        [0.0329],\n",
      "        [0.0347],\n",
      "        [0.0321],\n",
      "        [0.0306],\n",
      "        [0.0395],\n",
      "        [0.0326],\n",
      "        [0.0382],\n",
      "        [0.0356],\n",
      "        [0.0427],\n",
      "        [0.0433],\n",
      "        [0.0428],\n",
      "        [0.0400],\n",
      "        [0.0440],\n",
      "        [0.0458],\n",
      "        [0.0419],\n",
      "        [0.0462],\n",
      "        [0.0470],\n",
      "        [0.0476],\n",
      "        [0.0447],\n",
      "        [0.0494],\n",
      "        [0.0497],\n",
      "        [0.0531],\n",
      "        [0.0551],\n",
      "        [0.0524],\n",
      "        [0.0568],\n",
      "        [0.0532],\n",
      "        [0.0575],\n",
      "        [0.0593],\n",
      "        [0.0550],\n",
      "        [0.0605],\n",
      "        [0.0637],\n",
      "        [0.0595],\n",
      "        [0.0645],\n",
      "        [0.0596],\n",
      "        [0.0624],\n",
      "        [0.0626],\n",
      "        [0.0628],\n",
      "        [0.0639],\n",
      "        [0.0655],\n",
      "        [0.0638],\n",
      "        [0.0702],\n",
      "        [0.0712],\n",
      "        [0.0720],\n",
      "        [0.0749],\n",
      "        [0.0725],\n",
      "        [0.0761],\n",
      "        [0.0773],\n",
      "        [0.0781],\n",
      "        [0.0717],\n",
      "        [0.0769],\n",
      "        [0.0783],\n",
      "        [0.0812],\n",
      "        [0.0783],\n",
      "        [0.0782],\n",
      "        [0.0851],\n",
      "        [0.0802],\n",
      "        [0.0856],\n",
      "        [0.0894],\n",
      "        [0.0908],\n",
      "        [0.0935],\n",
      "        [0.0927],\n",
      "        [0.0921],\n",
      "        [0.1024],\n",
      "        [0.1008],\n",
      "        [0.1031],\n",
      "        [0.1043],\n",
      "        [0.1085],\n",
      "        [0.1114],\n",
      "        [0.1132],\n",
      "        [0.1130],\n",
      "        [0.1177],\n",
      "        [0.1148],\n",
      "        [0.1279],\n",
      "        [0.1253],\n",
      "        [0.1319],\n",
      "        [0.1379],\n",
      "        [0.1532],\n",
      "        [0.1698],\n",
      "        [0.1933],\n",
      "        [0.1983],\n",
      "        [0.2700]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 55.217581033706665\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 12 個區塊累積花費時間(s) 1.2178938388824463\n",
      "<<The performance of 12 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2178938388824463\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1221.97\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1649.63\n",
      "The accuracy(2000) for l = 1: 81.76%\n",
      "The accuracy(3000) for l = 1: 93.71%\n",
      "The maximum error: tensor(6892.5352)\n",
      "The minimum error: tensor(11.3945)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 4841.9\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 4921.7\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "The maximum error: 5905.6015625\n",
      "The minimum error: 3596.2265625\n",
      "------------------------------------------------------------\n",
      "0.8176100628930818\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<13>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.9914796212106012e-07, 76)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [76, 138, 107, 29, 43, 50, 51, 87, 5, 4, 99, 108, 98, 74, 137, 15, 6, 96, 49, 52, 117, 147, 91, 83, 103, 95, 42, 37, 102, 0, 82, 86, 72, 88, 153, 90, 89, 101, 63, 62, 17, 151, 106, 66, 48, 100, 75, 111, 2, 26, 81, 3, 80, 7, 97, 54, 146, 93, 18, 94, 84, 79, 133, 25, 104, 20, 120, 152, 64, 16, 36, 27, 132, 121, 105, 34, 92, 53, 14, 65, 134, 41, 73, 8, 67, 119, 28, 127, 1, 55, 44, 61, 85, 56, 126, 109, 145, 35, 19, 154, 77, 47, 78, 9, 123, 24, 57, 110, 118, 71, 38, 30, 122, 116, 139, 45, 60, 150, 59, 125, 46, 13, 124, 39, 40, 144, 131, 142, 135, 58, 10, 143, 140, 112, 33, 136, 12, 128, 11, 148, 149, 70, 130, 141, 129, 32, 113, 23, 115, 21, 22, 155, 114, 158, 68]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0068],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0100],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0128],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0201],\n",
      "        [0.0204],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0222],\n",
      "        [0.0226],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0236],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0251],\n",
      "        [0.0257],\n",
      "        [0.0259],\n",
      "        [0.0266],\n",
      "        [0.0270],\n",
      "        [0.0271],\n",
      "        [0.0273],\n",
      "        [0.0277],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0292],\n",
      "        [0.0296],\n",
      "        [0.0296],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0321],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0329],\n",
      "        [0.0333],\n",
      "        [0.0343],\n",
      "        [0.0347],\n",
      "        [0.0356],\n",
      "        [0.0365],\n",
      "        [0.0382],\n",
      "        [0.0395],\n",
      "        [0.0400],\n",
      "        [0.0419],\n",
      "        [0.0427],\n",
      "        [0.0428],\n",
      "        [0.0433],\n",
      "        [0.0440],\n",
      "        [0.0458],\n",
      "        [0.0462],\n",
      "        [0.0470],\n",
      "        [0.0476],\n",
      "        [0.0494],\n",
      "        [0.0497],\n",
      "        [0.0524],\n",
      "        [0.0531],\n",
      "        [0.0532],\n",
      "        [0.0550],\n",
      "        [0.0551],\n",
      "        [0.0568],\n",
      "        [0.0575],\n",
      "        [0.0593],\n",
      "        [0.0595],\n",
      "        [0.0596],\n",
      "        [0.0605],\n",
      "        [0.0624],\n",
      "        [0.0626],\n",
      "        [0.0628],\n",
      "        [0.0637],\n",
      "        [0.0638],\n",
      "        [0.0639],\n",
      "        [0.0645],\n",
      "        [0.0655],\n",
      "        [0.0702],\n",
      "        [0.0712],\n",
      "        [0.0717],\n",
      "        [0.0720],\n",
      "        [0.0725],\n",
      "        [0.0749],\n",
      "        [0.0761],\n",
      "        [0.0769],\n",
      "        [0.0773],\n",
      "        [0.0781],\n",
      "        [0.0782],\n",
      "        [0.0783],\n",
      "        [0.0783],\n",
      "        [0.0802],\n",
      "        [0.0812],\n",
      "        [0.0851],\n",
      "        [0.0856],\n",
      "        [0.0894],\n",
      "        [0.0908],\n",
      "        [0.0921],\n",
      "        [0.0927],\n",
      "        [0.0935],\n",
      "        [0.1008],\n",
      "        [0.1024],\n",
      "        [0.1031],\n",
      "        [0.1043],\n",
      "        [0.1085],\n",
      "        [0.1114],\n",
      "        [0.1130],\n",
      "        [0.1132],\n",
      "        [0.1148],\n",
      "        [0.1177],\n",
      "        [0.1253],\n",
      "        [0.1279],\n",
      "        [0.1319],\n",
      "        [0.1379],\n",
      "        [0.1409],\n",
      "        [0.1698],\n",
      "        [0.1754],\n",
      "        [0.1933]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.9914796212106012e-07, 76)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [76, 138, 107, 29, 43, 50, 51, 87, 5, 4, 99, 108, 98, 74, 137, 15, 6, 96, 49, 52, 117, 147, 91, 83, 103, 95, 42, 37, 102, 0, 82, 86, 72, 88, 153, 90, 89, 101, 63, 62, 17, 151, 106, 66, 48, 100, 75, 111, 2, 26, 81, 3, 80, 7, 97, 54, 146, 93, 18, 94, 84, 79, 133, 25, 104, 20, 120, 152, 64, 16, 36, 27, 132, 121, 105, 34, 92, 53, 14, 65, 134, 41, 73, 8, 67, 119, 28, 127, 1, 55, 44, 61, 85, 56, 126, 109, 145, 35, 19, 154, 77, 47, 78, 9, 123, 24, 57, 110, 118, 71, 38, 30, 122, 116, 139, 45, 60, 150, 59, 125, 46, 13, 124, 39, 40, 144, 131, 142, 135, 58, 10, 143, 140, 112, 33, 136, 12, 128, 11, 148, 149, 70, 130, 141, 129, 32, 113, 23, 115, 21, 22, 155, 114, 158, 68, 69] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5856],\n",
      "        [0.2616],\n",
      "        [0.4864],\n",
      "        [0.7231],\n",
      "        [0.6900],\n",
      "        [0.6786],\n",
      "        [0.6574],\n",
      "        [0.6192],\n",
      "        [0.8647],\n",
      "        [0.8865],\n",
      "        [0.5448],\n",
      "        [0.4687],\n",
      "        [0.5614],\n",
      "        [0.5450],\n",
      "        [0.2644],\n",
      "        [0.8684],\n",
      "        [0.8849],\n",
      "        [0.5671],\n",
      "        [0.6750],\n",
      "        [0.6264],\n",
      "        [0.2820],\n",
      "        [0.2637],\n",
      "        [0.6067],\n",
      "        [0.6405],\n",
      "        [0.5240],\n",
      "        [0.5807],\n",
      "        [0.6927],\n",
      "        [0.6502],\n",
      "        [0.5255],\n",
      "        [0.8574],\n",
      "        [0.5924],\n",
      "        [0.6306],\n",
      "        [0.4986],\n",
      "        [0.6469],\n",
      "        [0.1941],\n",
      "        [0.6217],\n",
      "        [0.6436],\n",
      "        [0.5255],\n",
      "        [0.6297],\n",
      "        [0.6253],\n",
      "        [0.8193],\n",
      "        [0.2345],\n",
      "        [0.4884],\n",
      "        [0.6063],\n",
      "        [0.6749],\n",
      "        [0.5061],\n",
      "        [0.5899],\n",
      "        [0.4197],\n",
      "        [0.8679],\n",
      "        [0.7391],\n",
      "        [0.5860],\n",
      "        [0.8752],\n",
      "        [0.5880],\n",
      "        [0.8862],\n",
      "        [0.5846],\n",
      "        [0.6170],\n",
      "        [0.2580],\n",
      "        [0.6026],\n",
      "        [0.8612],\n",
      "        [0.5814],\n",
      "        [0.6502],\n",
      "        [0.5917],\n",
      "        [0.3271],\n",
      "        [0.7728],\n",
      "        [0.5052],\n",
      "        [0.8452],\n",
      "        [0.2791],\n",
      "        [0.2213],\n",
      "        [0.6068],\n",
      "        [0.8447],\n",
      "        [0.6542],\n",
      "        [0.7154],\n",
      "        [0.3335],\n",
      "        [0.2750],\n",
      "        [0.4905],\n",
      "        [0.6661],\n",
      "        [0.6111],\n",
      "        [0.6142],\n",
      "        [0.8490],\n",
      "        [0.6013],\n",
      "        [0.2785],\n",
      "        [0.6753],\n",
      "        [0.4883],\n",
      "        [0.8664],\n",
      "        [0.6069],\n",
      "        [0.2773],\n",
      "        [0.7159],\n",
      "        [0.3483],\n",
      "        [0.8091],\n",
      "        [0.6152],\n",
      "        [0.6598],\n",
      "        [0.6259],\n",
      "        [0.6247],\n",
      "        [0.6114],\n",
      "        [0.3141],\n",
      "        [0.4222],\n",
      "        [0.2441],\n",
      "        [0.6022],\n",
      "        [0.8469],\n",
      "        [0.2171],\n",
      "        [0.5687],\n",
      "        [0.6482],\n",
      "        [0.5595],\n",
      "        [0.8606],\n",
      "        [0.2985],\n",
      "        [0.8057],\n",
      "        [0.6189],\n",
      "        [0.4021],\n",
      "        [0.2400],\n",
      "        [0.5581],\n",
      "        [0.6530],\n",
      "        [0.7268],\n",
      "        [0.2940],\n",
      "        [0.3320],\n",
      "        [0.2856],\n",
      "        [0.6458],\n",
      "        [0.6180],\n",
      "        [0.2609],\n",
      "        [0.6187],\n",
      "        [0.2861],\n",
      "        [0.6329],\n",
      "        [0.8442],\n",
      "        [0.2819],\n",
      "        [0.6581],\n",
      "        [0.6562],\n",
      "        [0.2623],\n",
      "        [0.3206],\n",
      "        [0.2660],\n",
      "        [0.2313],\n",
      "        [0.6154],\n",
      "        [0.8504],\n",
      "        [0.2664],\n",
      "        [0.2803],\n",
      "        [0.4171],\n",
      "        [0.7064],\n",
      "        [0.2326],\n",
      "        [0.8617],\n",
      "        [0.3502],\n",
      "        [0.8570],\n",
      "        [0.3006],\n",
      "        [0.2634],\n",
      "        [0.5653],\n",
      "        [0.3251],\n",
      "        [0.2758],\n",
      "        [0.3390],\n",
      "        [0.7362],\n",
      "        [0.4045],\n",
      "        [0.7838],\n",
      "        [0.3928],\n",
      "        [0.8357],\n",
      "        [0.7872],\n",
      "        [0.2590],\n",
      "        [0.3972],\n",
      "        [0.2024],\n",
      "        [0.6138],\n",
      "        [0.6220]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0068],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0100],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0128],\n",
      "        [0.0136],\n",
      "        [0.0139],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0151],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0201],\n",
      "        [0.0204],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0214],\n",
      "        [0.0215],\n",
      "        [0.0222],\n",
      "        [0.0226],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0236],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0251],\n",
      "        [0.0257],\n",
      "        [0.0259],\n",
      "        [0.0266],\n",
      "        [0.0270],\n",
      "        [0.0271],\n",
      "        [0.0273],\n",
      "        [0.0277],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0292],\n",
      "        [0.0296],\n",
      "        [0.0296],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0321],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0329],\n",
      "        [0.0333],\n",
      "        [0.0343],\n",
      "        [0.0347],\n",
      "        [0.0356],\n",
      "        [0.0365],\n",
      "        [0.0382],\n",
      "        [0.0395],\n",
      "        [0.0400],\n",
      "        [0.0419],\n",
      "        [0.0427],\n",
      "        [0.0428],\n",
      "        [0.0433],\n",
      "        [0.0440],\n",
      "        [0.0458],\n",
      "        [0.0462],\n",
      "        [0.0470],\n",
      "        [0.0476],\n",
      "        [0.0494],\n",
      "        [0.0497],\n",
      "        [0.0524],\n",
      "        [0.0531],\n",
      "        [0.0532],\n",
      "        [0.0550],\n",
      "        [0.0551],\n",
      "        [0.0568],\n",
      "        [0.0575],\n",
      "        [0.0593],\n",
      "        [0.0595],\n",
      "        [0.0596],\n",
      "        [0.0605],\n",
      "        [0.0624],\n",
      "        [0.0626],\n",
      "        [0.0628],\n",
      "        [0.0637],\n",
      "        [0.0638],\n",
      "        [0.0639],\n",
      "        [0.0645],\n",
      "        [0.0655],\n",
      "        [0.0702],\n",
      "        [0.0712],\n",
      "        [0.0717],\n",
      "        [0.0720],\n",
      "        [0.0725],\n",
      "        [0.0749],\n",
      "        [0.0761],\n",
      "        [0.0769],\n",
      "        [0.0773],\n",
      "        [0.0781],\n",
      "        [0.0782],\n",
      "        [0.0783],\n",
      "        [0.0783],\n",
      "        [0.0802],\n",
      "        [0.0812],\n",
      "        [0.0851],\n",
      "        [0.0856],\n",
      "        [0.0894],\n",
      "        [0.0908],\n",
      "        [0.0921],\n",
      "        [0.0927],\n",
      "        [0.0935],\n",
      "        [0.1008],\n",
      "        [0.1024],\n",
      "        [0.1031],\n",
      "        [0.1043],\n",
      "        [0.1085],\n",
      "        [0.1114],\n",
      "        [0.1130],\n",
      "        [0.1132],\n",
      "        [0.1148],\n",
      "        [0.1177],\n",
      "        [0.1253],\n",
      "        [0.1279],\n",
      "        [0.1319],\n",
      "        [0.1379],\n",
      "        [0.1409],\n",
      "        [0.1698],\n",
      "        [0.1754],\n",
      "        [0.1933],\n",
      "        [0.1983]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0015],\n",
      "        [    0.0076],\n",
      "        [    0.0000],\n",
      "        [    0.0102],\n",
      "        [    0.0038],\n",
      "        [    0.0074],\n",
      "        [    0.0032],\n",
      "        [    0.0017],\n",
      "        [    0.0098],\n",
      "        [    0.0158],\n",
      "        [    0.0015],\n",
      "        [    0.0042],\n",
      "        [    0.0104],\n",
      "        [    0.0056],\n",
      "        [    0.0126],\n",
      "        [    0.0221],\n",
      "        [    0.0059],\n",
      "        [    0.0131],\n",
      "        [    0.0028],\n",
      "        [    0.0038],\n",
      "        [    0.0143],\n",
      "        [    0.0023],\n",
      "        [    0.0046],\n",
      "        [    0.0137],\n",
      "        [    0.0081],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0067],\n",
      "        [    0.0086],\n",
      "        [    0.0269],\n",
      "        [    0.0147],\n",
      "        [    0.0081],\n",
      "        [    0.0119],\n",
      "        [    0.0182],\n",
      "        [    0.0004],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0116],\n",
      "        [    0.0114],\n",
      "        [    0.0122],\n",
      "        [    0.0012],\n",
      "        [    0.0027],\n",
      "        [    0.0181],\n",
      "        [    0.0208],\n",
      "        [    0.0143],\n",
      "        [    0.0165],\n",
      "        [    0.0195],\n",
      "        [    0.0208],\n",
      "        [    0.0079],\n",
      "        [    0.0116],\n",
      "        [    0.0257],\n",
      "        [    0.0100],\n",
      "        [    0.0194],\n",
      "        [    0.0094],\n",
      "        [    0.0287],\n",
      "        [    0.0189],\n",
      "        [    0.0134],\n",
      "        [    0.0198],\n",
      "        [    0.0414],\n",
      "        [    0.0205],\n",
      "        [    0.0219],\n",
      "        [    0.0241],\n",
      "        [    0.0316],\n",
      "        [    0.0385],\n",
      "        [    0.0269],\n",
      "        [    0.0428],\n",
      "        [    0.0348],\n",
      "        [    0.0161],\n",
      "        [    0.0256],\n",
      "        [    0.0144],\n",
      "        [    0.0357],\n",
      "        [    0.0215],\n",
      "        [    0.0356],\n",
      "        [    0.0383],\n",
      "        [    0.0317],\n",
      "        [    0.0396],\n",
      "        [    0.0273],\n",
      "        [    0.0281],\n",
      "        [    0.0192],\n",
      "        [    0.0308],\n",
      "        [    0.0398],\n",
      "        [    0.0317],\n",
      "        [    0.0380],\n",
      "        [    0.0250],\n",
      "        [    0.0437],\n",
      "        [    0.0484],\n",
      "        [    0.0340],\n",
      "        [    0.0485],\n",
      "        [    0.0300],\n",
      "        [    0.0386],\n",
      "        [    0.0404],\n",
      "        [    0.0429],\n",
      "        [    0.0431],\n",
      "        [    0.0424],\n",
      "        [    0.0553],\n",
      "        [    0.0493],\n",
      "        [    0.0421],\n",
      "        [    0.0478],\n",
      "        [    0.0686],\n",
      "        [    0.0419],\n",
      "        [    0.0540],\n",
      "        [    0.0511],\n",
      "        [    0.0561],\n",
      "        [    0.0443],\n",
      "        [    0.0653],\n",
      "        [    0.0718],\n",
      "        [    0.0562],\n",
      "        [    0.0628],\n",
      "        [    0.0690],\n",
      "        [    0.0647],\n",
      "        [    0.0601],\n",
      "        [    0.0725],\n",
      "        [    0.0695],\n",
      "        [    0.0604],\n",
      "        [    0.0584],\n",
      "        [    0.0640],\n",
      "        [    0.0679],\n",
      "        [    0.0586],\n",
      "        [    0.0684],\n",
      "        [    0.0789],\n",
      "        [    0.0690],\n",
      "        [    0.0610],\n",
      "        [    0.0833],\n",
      "        [    0.0731],\n",
      "        [    0.0739],\n",
      "        [    0.0680],\n",
      "        [    0.0837],\n",
      "        [    0.0698],\n",
      "        [    0.0850],\n",
      "        [    0.0775],\n",
      "        [    0.0697],\n",
      "        [    0.0765],\n",
      "        [    0.0822],\n",
      "        [    0.0908],\n",
      "        [    0.1001],\n",
      "        [    0.0980],\n",
      "        [    0.0788],\n",
      "        [    0.1062],\n",
      "        [    0.0874],\n",
      "        [    0.0916],\n",
      "        [    0.0919],\n",
      "        [    0.1102],\n",
      "        [    0.1168],\n",
      "        [    0.1048],\n",
      "        [    0.1181],\n",
      "        [    0.1236],\n",
      "        [    0.1170],\n",
      "        [    0.1376],\n",
      "        [    0.1255],\n",
      "        [    0.1451],\n",
      "        [    0.1502],\n",
      "        [    0.1277],\n",
      "        [    0.1684],\n",
      "        [    0.1619],\n",
      "        [    0.1968],\n",
      "        [    0.2011]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 55.7383508682251\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.4141578453272814e-10, 107)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [107, 153, 17, 76, 99, 87, 147, 151, 49, 51, 52, 43, 108, 91, 95, 42, 74, 6, 37, 50, 138, 2, 86, 103, 102, 7, 5, 3, 29, 98, 63, 26, 101, 72, 62, 137, 96, 146, 83, 48, 117, 16, 82, 4, 152, 100, 106, 88, 54, 14, 80, 75, 90, 93, 89, 94, 111, 66, 27, 84, 15, 79, 8, 64, 81, 0, 104, 92, 53, 97, 1, 65, 133, 41, 105, 28, 120, 132, 36, 73, 121, 25, 55, 34, 134, 44, 18, 154, 145, 56, 20, 61, 85, 67, 9, 35, 119, 127, 109, 47, 77, 126, 78, 57, 139, 150, 38, 116, 13, 110, 45, 71, 123, 60, 144, 59, 19, 46, 118, 122, 10, 142, 24, 30, 39, 40, 143, 58, 12, 125, 140, 124, 131, 135, 11, 112, 148, 149, 136, 33, 141, 128, 70, 130, 113, 129, 32, 115, 155, 23, 21, 22, 158, 114, 68, 156, 69] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4872],\n",
      "        [0.1806],\n",
      "        [0.8345],\n",
      "        [0.5866],\n",
      "        [0.5495],\n",
      "        [0.6233],\n",
      "        [0.2523],\n",
      "        [0.2206],\n",
      "        [0.6808],\n",
      "        [0.6630],\n",
      "        [0.6316],\n",
      "        [0.6952],\n",
      "        [0.4697],\n",
      "        [0.6120],\n",
      "        [0.5857],\n",
      "        [0.6982],\n",
      "        [0.5458],\n",
      "        [0.8984],\n",
      "        [0.6544],\n",
      "        [0.6845],\n",
      "        [0.2546],\n",
      "        [0.8813],\n",
      "        [0.6349],\n",
      "        [0.5259],\n",
      "        [0.5280],\n",
      "        [0.9002],\n",
      "        [0.8772],\n",
      "        [0.8878],\n",
      "        [0.7320],\n",
      "        [0.5663],\n",
      "        [0.6340],\n",
      "        [0.7490],\n",
      "        [0.5289],\n",
      "        [0.4995],\n",
      "        [0.6294],\n",
      "        [0.2583],\n",
      "        [0.5721],\n",
      "        [0.2470],\n",
      "        [0.6442],\n",
      "        [0.6808],\n",
      "        [0.2767],\n",
      "        [0.8600],\n",
      "        [0.5954],\n",
      "        [0.8992],\n",
      "        [0.2082],\n",
      "        [0.5099],\n",
      "        [0.4890],\n",
      "        [0.6515],\n",
      "        [0.6222],\n",
      "        [0.8641],\n",
      "        [0.5913],\n",
      "        [0.5910],\n",
      "        [0.6267],\n",
      "        [0.6080],\n",
      "        [0.6484],\n",
      "        [0.5867],\n",
      "        [0.4197],\n",
      "        [0.6096],\n",
      "        [0.7248],\n",
      "        [0.6548],\n",
      "        [0.8837],\n",
      "        [0.5947],\n",
      "        [0.8809],\n",
      "        [0.6108],\n",
      "        [0.5894],\n",
      "        [0.8731],\n",
      "        [0.5060],\n",
      "        [0.6166],\n",
      "        [0.6193],\n",
      "        [0.5898],\n",
      "        [0.8224],\n",
      "        [0.6052],\n",
      "        [0.3226],\n",
      "        [0.6800],\n",
      "        [0.4913],\n",
      "        [0.7246],\n",
      "        [0.2730],\n",
      "        [0.3289],\n",
      "        [0.6592],\n",
      "        [0.4885],\n",
      "        [0.2688],\n",
      "        [0.7840],\n",
      "        [0.6205],\n",
      "        [0.6731],\n",
      "        [0.2743],\n",
      "        [0.6652],\n",
      "        [0.8769],\n",
      "        [0.2039],\n",
      "        [0.2338],\n",
      "        [0.6166],\n",
      "        [0.8594],\n",
      "        [0.6291],\n",
      "        [0.6286],\n",
      "        [0.6106],\n",
      "        [0.8757],\n",
      "        [0.6075],\n",
      "        [0.2709],\n",
      "        [0.3426],\n",
      "        [0.4226],\n",
      "        [0.6539],\n",
      "        [0.5698],\n",
      "        [0.3082],\n",
      "        [0.5610],\n",
      "        [0.6232],\n",
      "        [0.2786],\n",
      "        [0.2478],\n",
      "        [0.6567],\n",
      "        [0.3278],\n",
      "        [0.8593],\n",
      "        [0.4018],\n",
      "        [0.6521],\n",
      "        [0.5601],\n",
      "        [0.2928],\n",
      "        [0.6213],\n",
      "        [0.2520],\n",
      "        [0.6223],\n",
      "        [0.8622],\n",
      "        [0.6388],\n",
      "        [0.2336],\n",
      "        [0.2884],\n",
      "        [0.8658],\n",
      "        [0.2574],\n",
      "        [0.8180],\n",
      "        [0.7355],\n",
      "        [0.6624],\n",
      "        [0.6604],\n",
      "        [0.2573],\n",
      "        [0.6191],\n",
      "        [0.8764],\n",
      "        [0.2797],\n",
      "        [0.2730],\n",
      "        [0.2754],\n",
      "        [0.3153],\n",
      "        [0.2264],\n",
      "        [0.8719],\n",
      "        [0.4171],\n",
      "        [0.2891],\n",
      "        [0.2510],\n",
      "        [0.2273],\n",
      "        [0.7145],\n",
      "        [0.2676],\n",
      "        [0.3449],\n",
      "        [0.5670],\n",
      "        [0.3197],\n",
      "        [0.4038],\n",
      "        [0.3341],\n",
      "        [0.7450],\n",
      "        [0.3905],\n",
      "        [0.2458],\n",
      "        [0.7961],\n",
      "        [0.8489],\n",
      "        [0.7994],\n",
      "        [0.1890],\n",
      "        [0.3957],\n",
      "        [0.6173],\n",
      "        [0.2287],\n",
      "        [0.6248]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0004],\n",
      "        [    0.0012],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0023],\n",
      "        [    0.0027],\n",
      "        [    0.0028],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0038],\n",
      "        [    0.0042],\n",
      "        [    0.0046],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0056],\n",
      "        [    0.0059],\n",
      "        [    0.0067],\n",
      "        [    0.0074],\n",
      "        [    0.0076],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0081],\n",
      "        [    0.0086],\n",
      "        [    0.0094],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0104],\n",
      "        [    0.0114],\n",
      "        [    0.0116],\n",
      "        [    0.0116],\n",
      "        [    0.0119],\n",
      "        [    0.0122],\n",
      "        [    0.0126],\n",
      "        [    0.0131],\n",
      "        [    0.0134],\n",
      "        [    0.0137],\n",
      "        [    0.0143],\n",
      "        [    0.0143],\n",
      "        [    0.0144],\n",
      "        [    0.0147],\n",
      "        [    0.0158],\n",
      "        [    0.0161],\n",
      "        [    0.0165],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0189],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0195],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0198],\n",
      "        [    0.0205],\n",
      "        [    0.0208],\n",
      "        [    0.0208],\n",
      "        [    0.0215],\n",
      "        [    0.0219],\n",
      "        [    0.0221],\n",
      "        [    0.0241],\n",
      "        [    0.0250],\n",
      "        [    0.0256],\n",
      "        [    0.0257],\n",
      "        [    0.0269],\n",
      "        [    0.0269],\n",
      "        [    0.0273],\n",
      "        [    0.0281],\n",
      "        [    0.0287],\n",
      "        [    0.0300],\n",
      "        [    0.0308],\n",
      "        [    0.0316],\n",
      "        [    0.0317],\n",
      "        [    0.0317],\n",
      "        [    0.0340],\n",
      "        [    0.0348],\n",
      "        [    0.0356],\n",
      "        [    0.0357],\n",
      "        [    0.0380],\n",
      "        [    0.0383],\n",
      "        [    0.0385],\n",
      "        [    0.0386],\n",
      "        [    0.0396],\n",
      "        [    0.0398],\n",
      "        [    0.0404],\n",
      "        [    0.0414],\n",
      "        [    0.0419],\n",
      "        [    0.0421],\n",
      "        [    0.0424],\n",
      "        [    0.0428],\n",
      "        [    0.0429],\n",
      "        [    0.0431],\n",
      "        [    0.0437],\n",
      "        [    0.0443],\n",
      "        [    0.0478],\n",
      "        [    0.0484],\n",
      "        [    0.0485],\n",
      "        [    0.0493],\n",
      "        [    0.0511],\n",
      "        [    0.0540],\n",
      "        [    0.0553],\n",
      "        [    0.0561],\n",
      "        [    0.0562],\n",
      "        [    0.0584],\n",
      "        [    0.0586],\n",
      "        [    0.0601],\n",
      "        [    0.0604],\n",
      "        [    0.0610],\n",
      "        [    0.0628],\n",
      "        [    0.0640],\n",
      "        [    0.0647],\n",
      "        [    0.0653],\n",
      "        [    0.0679],\n",
      "        [    0.0680],\n",
      "        [    0.0684],\n",
      "        [    0.0686],\n",
      "        [    0.0690],\n",
      "        [    0.0690],\n",
      "        [    0.0695],\n",
      "        [    0.0697],\n",
      "        [    0.0698],\n",
      "        [    0.0718],\n",
      "        [    0.0725],\n",
      "        [    0.0731],\n",
      "        [    0.0739],\n",
      "        [    0.0765],\n",
      "        [    0.0775],\n",
      "        [    0.0788],\n",
      "        [    0.0789],\n",
      "        [    0.0822],\n",
      "        [    0.0833],\n",
      "        [    0.0837],\n",
      "        [    0.0850],\n",
      "        [    0.0874],\n",
      "        [    0.0908],\n",
      "        [    0.0916],\n",
      "        [    0.0919],\n",
      "        [    0.0980],\n",
      "        [    0.1001],\n",
      "        [    0.1048],\n",
      "        [    0.1062],\n",
      "        [    0.1102],\n",
      "        [    0.1168],\n",
      "        [    0.1170],\n",
      "        [    0.1181],\n",
      "        [    0.1236],\n",
      "        [    0.1255],\n",
      "        [    0.1277],\n",
      "        [    0.1376],\n",
      "        [    0.1451],\n",
      "        [    0.1502],\n",
      "        [    0.1619],\n",
      "        [    0.1684],\n",
      "        [    0.1968],\n",
      "        [    0.1974],\n",
      "        [    0.2011]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0075],\n",
      "        [0.0027],\n",
      "        [0.0016],\n",
      "        [0.0032],\n",
      "        [0.0010],\n",
      "        [0.0094],\n",
      "        [0.0065],\n",
      "        [0.0038],\n",
      "        [0.0026],\n",
      "        [0.0042],\n",
      "        [0.0021],\n",
      "        [0.0044],\n",
      "        [0.0040],\n",
      "        [0.0039],\n",
      "        [0.0066],\n",
      "        [0.0079],\n",
      "        [0.0063],\n",
      "        [0.0084],\n",
      "        [0.0065],\n",
      "        [0.0107],\n",
      "        [0.0072],\n",
      "        [0.0090],\n",
      "        [0.0087],\n",
      "        [0.0086],\n",
      "        [0.0086],\n",
      "        [0.0098],\n",
      "        [0.0103],\n",
      "        [0.0109],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0106],\n",
      "        [0.0107],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0149],\n",
      "        [0.0144],\n",
      "        [0.0069],\n",
      "        [0.0120],\n",
      "        [0.0152],\n",
      "        [0.0164],\n",
      "        [0.0111],\n",
      "        [0.0132],\n",
      "        [0.0153],\n",
      "        [0.0079],\n",
      "        [0.0148],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0189],\n",
      "        [0.0162],\n",
      "        [0.0204],\n",
      "        [0.0225],\n",
      "        [0.0196],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0207],\n",
      "        [0.0192],\n",
      "        [0.0203],\n",
      "        [0.0229],\n",
      "        [0.0248],\n",
      "        [0.0255],\n",
      "        [0.0232],\n",
      "        [0.0264],\n",
      "        [0.0250],\n",
      "        [0.0299],\n",
      "        [0.0280],\n",
      "        [0.0265],\n",
      "        [0.0283],\n",
      "        [0.0297],\n",
      "        [0.0283],\n",
      "        [0.0316],\n",
      "        [0.0338],\n",
      "        [0.0333],\n",
      "        [0.0326],\n",
      "        [0.0332],\n",
      "        [0.0377],\n",
      "        [0.0380],\n",
      "        [0.0346],\n",
      "        [0.0399],\n",
      "        [0.0411],\n",
      "        [0.0398],\n",
      "        [0.0386],\n",
      "        [0.0399],\n",
      "        [0.0405],\n",
      "        [0.0413],\n",
      "        [0.0449],\n",
      "        [0.0337],\n",
      "        [0.0364],\n",
      "        [0.0424],\n",
      "        [0.0454],\n",
      "        [0.0449],\n",
      "        [0.0442],\n",
      "        [0.0427],\n",
      "        [0.0417],\n",
      "        [0.0475],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0490],\n",
      "        [0.0515],\n",
      "        [0.0567],\n",
      "        [0.0585],\n",
      "        [0.0582],\n",
      "        [0.0572],\n",
      "        [0.0548],\n",
      "        [0.0497],\n",
      "        [0.0623],\n",
      "        [0.0582],\n",
      "        [0.0579],\n",
      "        [0.0627],\n",
      "        [0.0637],\n",
      "        [0.0630],\n",
      "        [0.0680],\n",
      "        [0.0698],\n",
      "        [0.0619],\n",
      "        [0.0700],\n",
      "        [0.0723],\n",
      "        [0.0688],\n",
      "        [0.0712],\n",
      "        [0.0720],\n",
      "        [0.0665],\n",
      "        [0.0652],\n",
      "        [0.0734],\n",
      "        [0.0728],\n",
      "        [0.0748],\n",
      "        [0.0756],\n",
      "        [0.0714],\n",
      "        [0.0790],\n",
      "        [0.0764],\n",
      "        [0.0820],\n",
      "        [0.0786],\n",
      "        [0.0864],\n",
      "        [0.0865],\n",
      "        [0.0852],\n",
      "        [0.0847],\n",
      "        [0.0908],\n",
      "        [0.0835],\n",
      "        [0.0835],\n",
      "        [0.0987],\n",
      "        [0.1005],\n",
      "        [0.1004],\n",
      "        [0.1097],\n",
      "        [0.1081],\n",
      "        [0.1199],\n",
      "        [0.1167],\n",
      "        [0.1209],\n",
      "        [0.1239],\n",
      "        [0.1238],\n",
      "        [0.1186],\n",
      "        [0.1397],\n",
      "        [0.1469],\n",
      "        [0.1522],\n",
      "        [0.1537],\n",
      "        [0.1674],\n",
      "        [0.1954],\n",
      "        [0.1882],\n",
      "        [0.1988]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 56.025988817214966\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.287220794045425e-07, 107)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [107, 87, 76, 43, 51, 17, 99, 49, 95, 91, 52, 108, 6, 50, 151, 42, 146, 2, 153, 74, 152, 37, 102, 7, 103, 86, 147, 5, 3, 26, 138, 101, 29, 16, 98, 83, 63, 82, 72, 62, 96, 100, 137, 48, 4, 14, 117, 106, 88, 54, 93, 89, 66, 94, 90, 27, 80, 111, 75, 84, 8, 15, 81, 79, 64, 92, 104, 53, 1, 97, 0, 65, 105, 28, 41, 154, 133, 36, 145, 120, 132, 55, 25, 73, 34, 134, 121, 44, 9, 56, 67, 85, 61, 18, 20, 35, 109, 150, 119, 47, 127, 139, 77, 57, 13, 116, 78, 126, 144, 38, 110, 71, 45, 142, 10, 123, 46, 60, 59, 118, 143, 122, 19, 30, 24, 39, 40, 12, 140, 58, 125, 148, 149, 11, 135, 124, 131, 112, 136, 141, 33, 70, 128, 113, 155, 130, 129, 115, 32, 23, 21, 22, 158, 114, 156, 68, 69, 157] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4863],\n",
      "        [0.6225],\n",
      "        [0.5835],\n",
      "        [0.6935],\n",
      "        [0.6623],\n",
      "        [0.8383],\n",
      "        [0.5511],\n",
      "        [0.6798],\n",
      "        [0.5868],\n",
      "        [0.6125],\n",
      "        [0.6312],\n",
      "        [0.4695],\n",
      "        [0.8989],\n",
      "        [0.6836],\n",
      "        [0.2113],\n",
      "        [0.6968],\n",
      "        [0.2405],\n",
      "        [0.8821],\n",
      "        [0.1727],\n",
      "        [0.5436],\n",
      "        [0.1999],\n",
      "        [0.6527],\n",
      "        [0.5280],\n",
      "        [0.9010],\n",
      "        [0.5253],\n",
      "        [0.6340],\n",
      "        [0.2451],\n",
      "        [0.8772],\n",
      "        [0.8875],\n",
      "        [0.7500],\n",
      "        [0.2515],\n",
      "        [0.5299],\n",
      "        [0.7327],\n",
      "        [0.8632],\n",
      "        [0.5677],\n",
      "        [0.6426],\n",
      "        [0.6329],\n",
      "        [0.5939],\n",
      "        [0.4981],\n",
      "        [0.6281],\n",
      "        [0.5734],\n",
      "        [0.5116],\n",
      "        [0.2561],\n",
      "        [0.6798],\n",
      "        [0.8988],\n",
      "        [0.8671],\n",
      "        [0.2745],\n",
      "        [0.4880],\n",
      "        [0.6506],\n",
      "        [0.6222],\n",
      "        [0.6088],\n",
      "        [0.6477],\n",
      "        [0.6081],\n",
      "        [0.5880],\n",
      "        [0.6265],\n",
      "        [0.7260],\n",
      "        [0.5904],\n",
      "        [0.4197],\n",
      "        [0.5880],\n",
      "        [0.6538],\n",
      "        [0.8828],\n",
      "        [0.8864],\n",
      "        [0.5888],\n",
      "        [0.5932],\n",
      "        [0.6100],\n",
      "        [0.6174],\n",
      "        [0.5049],\n",
      "        [0.6192],\n",
      "        [0.8241],\n",
      "        [0.5908],\n",
      "        [0.8762],\n",
      "        [0.6044],\n",
      "        [0.4905],\n",
      "        [0.7254],\n",
      "        [0.6784],\n",
      "        [0.1958],\n",
      "        [0.3205],\n",
      "        [0.6582],\n",
      "        [0.2281],\n",
      "        [0.2701],\n",
      "        [0.3265],\n",
      "        [0.6206],\n",
      "        [0.7854],\n",
      "        [0.4867],\n",
      "        [0.6734],\n",
      "        [0.2737],\n",
      "        [0.2660],\n",
      "        [0.6644],\n",
      "        [0.8782],\n",
      "        [0.6166],\n",
      "        [0.6096],\n",
      "        [0.6276],\n",
      "        [0.6271],\n",
      "        [0.8804],\n",
      "        [0.8621],\n",
      "        [0.6078],\n",
      "        [0.4229],\n",
      "        [0.2388],\n",
      "        [0.2677],\n",
      "        [0.6535],\n",
      "        [0.3389],\n",
      "        [0.2749],\n",
      "        [0.5672],\n",
      "        [0.6222],\n",
      "        [0.8624],\n",
      "        [0.3256],\n",
      "        [0.5589],\n",
      "        [0.3050],\n",
      "        [0.2460],\n",
      "        [0.6545],\n",
      "        [0.4018],\n",
      "        [0.5584],\n",
      "        [0.6524],\n",
      "        [0.2528],\n",
      "        [0.8689],\n",
      "        [0.2900],\n",
      "        [0.6389],\n",
      "        [0.6195],\n",
      "        [0.6207],\n",
      "        [0.2315],\n",
      "        [0.2522],\n",
      "        [0.2859],\n",
      "        [0.8659],\n",
      "        [0.7358],\n",
      "        [0.8195],\n",
      "        [0.6606],\n",
      "        [0.6586],\n",
      "        [0.8789],\n",
      "        [0.2695],\n",
      "        [0.6176],\n",
      "        [0.2765],\n",
      "        [0.2810],\n",
      "        [0.2426],\n",
      "        [0.8747],\n",
      "        [0.2263],\n",
      "        [0.2723],\n",
      "        [0.3124],\n",
      "        [0.4172],\n",
      "        [0.2266],\n",
      "        [0.2631],\n",
      "        [0.7149],\n",
      "        [0.5649],\n",
      "        [0.3414],\n",
      "        [0.4035],\n",
      "        [0.2367],\n",
      "        [0.3166],\n",
      "        [0.3312],\n",
      "        [0.3887],\n",
      "        [0.7453],\n",
      "        [0.7982],\n",
      "        [0.8508],\n",
      "        [0.8015],\n",
      "        [0.1808],\n",
      "        [0.3948],\n",
      "        [0.2195],\n",
      "        [0.6159],\n",
      "        [0.6225],\n",
      "        [0.2093]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0044],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0075],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0084],\n",
      "        [0.0086],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0107],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0125],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0144],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0225],\n",
      "        [0.0229],\n",
      "        [0.0232],\n",
      "        [0.0248],\n",
      "        [0.0250],\n",
      "        [0.0255],\n",
      "        [0.0264],\n",
      "        [0.0265],\n",
      "        [0.0280],\n",
      "        [0.0283],\n",
      "        [0.0283],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0316],\n",
      "        [0.0326],\n",
      "        [0.0332],\n",
      "        [0.0333],\n",
      "        [0.0337],\n",
      "        [0.0338],\n",
      "        [0.0346],\n",
      "        [0.0364],\n",
      "        [0.0377],\n",
      "        [0.0380],\n",
      "        [0.0386],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0399],\n",
      "        [0.0405],\n",
      "        [0.0411],\n",
      "        [0.0413],\n",
      "        [0.0417],\n",
      "        [0.0424],\n",
      "        [0.0427],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0454],\n",
      "        [0.0475],\n",
      "        [0.0490],\n",
      "        [0.0497],\n",
      "        [0.0515],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0548],\n",
      "        [0.0567],\n",
      "        [0.0572],\n",
      "        [0.0579],\n",
      "        [0.0582],\n",
      "        [0.0582],\n",
      "        [0.0585],\n",
      "        [0.0619],\n",
      "        [0.0623],\n",
      "        [0.0627],\n",
      "        [0.0630],\n",
      "        [0.0637],\n",
      "        [0.0652],\n",
      "        [0.0665],\n",
      "        [0.0680],\n",
      "        [0.0688],\n",
      "        [0.0698],\n",
      "        [0.0700],\n",
      "        [0.0712],\n",
      "        [0.0714],\n",
      "        [0.0720],\n",
      "        [0.0723],\n",
      "        [0.0728],\n",
      "        [0.0734],\n",
      "        [0.0748],\n",
      "        [0.0756],\n",
      "        [0.0764],\n",
      "        [0.0786],\n",
      "        [0.0790],\n",
      "        [0.0820],\n",
      "        [0.0835],\n",
      "        [0.0835],\n",
      "        [0.0847],\n",
      "        [0.0852],\n",
      "        [0.0864],\n",
      "        [0.0865],\n",
      "        [0.0908],\n",
      "        [0.0987],\n",
      "        [0.1004],\n",
      "        [0.1005],\n",
      "        [0.1081],\n",
      "        [0.1097],\n",
      "        [0.1167],\n",
      "        [0.1186],\n",
      "        [0.1199],\n",
      "        [0.1209],\n",
      "        [0.1238],\n",
      "        [0.1239],\n",
      "        [0.1397],\n",
      "        [0.1469],\n",
      "        [0.1522],\n",
      "        [0.1537],\n",
      "        [0.1674],\n",
      "        [0.1882],\n",
      "        [0.1954],\n",
      "        [0.1988],\n",
      "        [0.2093]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0038],\n",
      "        [0.0019],\n",
      "        [0.0068],\n",
      "        [0.0019],\n",
      "        [0.0005],\n",
      "        [0.0032],\n",
      "        [0.0027],\n",
      "        [0.0072],\n",
      "        [0.0051],\n",
      "        [0.0058],\n",
      "        [0.0069],\n",
      "        [0.0065],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0173],\n",
      "        [0.0103],\n",
      "        [0.0010],\n",
      "        [0.0100],\n",
      "        [0.0169],\n",
      "        [0.0122],\n",
      "        [0.0019],\n",
      "        [0.0124],\n",
      "        [0.0106],\n",
      "        [0.0114],\n",
      "        [0.0113],\n",
      "        [0.0120],\n",
      "        [0.0179],\n",
      "        [0.0062],\n",
      "        [0.0142],\n",
      "        [0.0125],\n",
      "        [0.0153],\n",
      "        [0.0118],\n",
      "        [0.0091],\n",
      "        [0.0114],\n",
      "        [0.0111],\n",
      "        [0.0082],\n",
      "        [0.0158],\n",
      "        [0.0096],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0135],\n",
      "        [0.0152],\n",
      "        [0.0188],\n",
      "        [0.0186],\n",
      "        [0.0114],\n",
      "        [0.0166],\n",
      "        [0.0202],\n",
      "        [0.0141],\n",
      "        [0.0140],\n",
      "        [0.0212],\n",
      "        [0.0204],\n",
      "        [0.0161],\n",
      "        [0.0155],\n",
      "        [0.0202],\n",
      "        [0.0171],\n",
      "        [0.0220],\n",
      "        [0.0235],\n",
      "        [0.0226],\n",
      "        [0.0277],\n",
      "        [0.0262],\n",
      "        [0.0249],\n",
      "        [0.0240],\n",
      "        [0.0221],\n",
      "        [0.0292],\n",
      "        [0.0293],\n",
      "        [0.0280],\n",
      "        [0.0311],\n",
      "        [0.0308],\n",
      "        [0.0301],\n",
      "        [0.0285],\n",
      "        [0.0293],\n",
      "        [0.0346],\n",
      "        [0.0353],\n",
      "        [0.0350],\n",
      "        [0.0372],\n",
      "        [0.0240],\n",
      "        [0.0375],\n",
      "        [0.0312],\n",
      "        [0.0293],\n",
      "        [0.0424],\n",
      "        [0.0420],\n",
      "        [0.0409],\n",
      "        [0.0382],\n",
      "        [0.0437],\n",
      "        [0.0378],\n",
      "        [0.0427],\n",
      "        [0.0457],\n",
      "        [0.0444],\n",
      "        [0.0427],\n",
      "        [0.0447],\n",
      "        [0.0394],\n",
      "        [0.0474],\n",
      "        [0.0492],\n",
      "        [0.0452],\n",
      "        [0.0450],\n",
      "        [0.0495],\n",
      "        [0.0505],\n",
      "        [0.0391],\n",
      "        [0.0563],\n",
      "        [0.0544],\n",
      "        [0.0575],\n",
      "        [0.0496],\n",
      "        [0.0614],\n",
      "        [0.0606],\n",
      "        [0.0581],\n",
      "        [0.0542],\n",
      "        [0.0624],\n",
      "        [0.0632],\n",
      "        [0.0544],\n",
      "        [0.0667],\n",
      "        [0.0644],\n",
      "        [0.0592],\n",
      "        [0.0657],\n",
      "        [0.0591],\n",
      "        [0.0669],\n",
      "        [0.0725],\n",
      "        [0.0711],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.0750],\n",
      "        [0.0648],\n",
      "        [0.0762],\n",
      "        [0.0727],\n",
      "        [0.0704],\n",
      "        [0.0719],\n",
      "        [0.0788],\n",
      "        [0.0796],\n",
      "        [0.0772],\n",
      "        [0.0735],\n",
      "        [0.0827],\n",
      "        [0.0868],\n",
      "        [0.0739],\n",
      "        [0.0737],\n",
      "        [0.0853],\n",
      "        [0.0868],\n",
      "        [0.0911],\n",
      "        [0.0910],\n",
      "        [0.0891],\n",
      "        [0.1009],\n",
      "        [0.0944],\n",
      "        [0.0984],\n",
      "        [0.1038],\n",
      "        [0.1148],\n",
      "        [0.1146],\n",
      "        [0.1077],\n",
      "        [0.1245],\n",
      "        [0.1254],\n",
      "        [0.1201],\n",
      "        [0.1216],\n",
      "        [0.1389],\n",
      "        [0.1456],\n",
      "        [0.1512],\n",
      "        [0.1439],\n",
      "        [0.1646],\n",
      "        [0.1774],\n",
      "        [0.1918],\n",
      "        [0.1942],\n",
      "        [0.1989]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 56.31249117851257\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.7819453407573747e-07, 51)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [51, 146, 152, 87, 43, 99, 50, 6, 17, 107, 95, 91, 5, 108, 76, 52, 49, 83, 29, 82, 2, 42, 102, 98, 103, 16, 4, 7, 101, 86, 74, 37, 26, 96, 88, 106, 3, 100, 138, 66, 63, 89, 14, 72, 62, 153, 90, 151, 147, 48, 137, 94, 117, 93, 54, 27, 81, 111, 80, 154, 15, 8, 84, 75, 92, 97, 79, 0, 64, 145, 1, 53, 104, 36, 65, 28, 105, 41, 133, 34, 25, 150, 67, 55, 132, 120, 134, 9, 73, 44, 56, 20, 18, 121, 85, 61, 35, 139, 109, 116, 144, 47, 119, 127, 13, 142, 71, 57, 77, 78, 126, 110, 143, 45, 38, 10, 30, 46, 24, 123, 19, 140, 149, 60, 59, 148, 118, 122, 12, 39, 40, 58, 11, 125, 135, 112, 131, 124, 141, 33, 136, 70, 155, 113, 128, 115, 32, 130, 129, 23, 158, 21, 22, 114, 156, 68, 69, 157, 31] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6592],\n",
      "        [0.2326],\n",
      "        [0.1902],\n",
      "        [0.6196],\n",
      "        [0.6895],\n",
      "        [0.5507],\n",
      "        [0.6803],\n",
      "        [0.8957],\n",
      "        [0.8389],\n",
      "        [0.4834],\n",
      "        [0.5856],\n",
      "        [0.6108],\n",
      "        [0.8736],\n",
      "        [0.4674],\n",
      "        [0.5783],\n",
      "        [0.6284],\n",
      "        [0.6763],\n",
      "        [0.6387],\n",
      "        [0.7309],\n",
      "        [0.5903],\n",
      "        [0.8792],\n",
      "        [0.6930],\n",
      "        [0.5260],\n",
      "        [0.5669],\n",
      "        [0.5227],\n",
      "        [0.8630],\n",
      "        [0.8948],\n",
      "        [0.8982],\n",
      "        [0.5288],\n",
      "        [0.6309],\n",
      "        [0.5393],\n",
      "        [0.6487],\n",
      "        [0.7481],\n",
      "        [0.5725],\n",
      "        [0.6474],\n",
      "        [0.4850],\n",
      "        [0.8836],\n",
      "        [0.5113],\n",
      "        [0.2469],\n",
      "        [0.6043],\n",
      "        [0.6296],\n",
      "        [0.6447],\n",
      "        [0.8667],\n",
      "        [0.4946],\n",
      "        [0.6247],\n",
      "        [0.1633],\n",
      "        [0.6241],\n",
      "        [0.2005],\n",
      "        [0.2366],\n",
      "        [0.6764],\n",
      "        [0.2522],\n",
      "        [0.5871],\n",
      "        [0.2708],\n",
      "        [0.6074],\n",
      "        [0.6199],\n",
      "        [0.7243],\n",
      "        [0.5859],\n",
      "        [0.4179],\n",
      "        [0.5872],\n",
      "        [0.1860],\n",
      "        [0.8857],\n",
      "        [0.8810],\n",
      "        [0.6506],\n",
      "        [0.5828],\n",
      "        [0.6160],\n",
      "        [0.5896],\n",
      "        [0.5896],\n",
      "        [0.8755],\n",
      "        [0.6071],\n",
      "        [0.2210],\n",
      "        [0.8223],\n",
      "        [0.6167],\n",
      "        [0.5018],\n",
      "        [0.6547],\n",
      "        [0.6014],\n",
      "        [0.7236],\n",
      "        [0.4877],\n",
      "        [0.6745],\n",
      "        [0.3168],\n",
      "        [0.6713],\n",
      "        [0.7837],\n",
      "        [0.2282],\n",
      "        [0.6063],\n",
      "        [0.6183],\n",
      "        [0.3225],\n",
      "        [0.2654],\n",
      "        [0.2715],\n",
      "        [0.8773],\n",
      "        [0.4828],\n",
      "        [0.6613],\n",
      "        [0.6143],\n",
      "        [0.8616],\n",
      "        [0.8806],\n",
      "        [0.2614],\n",
      "        [0.6243],\n",
      "        [0.6229],\n",
      "        [0.6058],\n",
      "        [0.2698],\n",
      "        [0.4214],\n",
      "        [0.3217],\n",
      "        [0.2385],\n",
      "        [0.6506],\n",
      "        [0.2629],\n",
      "        [0.3336],\n",
      "        [0.8622],\n",
      "        [0.2467],\n",
      "        [0.5545],\n",
      "        [0.6189],\n",
      "        [0.5624],\n",
      "        [0.5546],\n",
      "        [0.3002],\n",
      "        [0.4001],\n",
      "        [0.2456],\n",
      "        [0.6504],\n",
      "        [0.6501],\n",
      "        [0.8686],\n",
      "        [0.7334],\n",
      "        [0.6367],\n",
      "        [0.8180],\n",
      "        [0.2855],\n",
      "        [0.8664],\n",
      "        [0.2644],\n",
      "        [0.2328],\n",
      "        [0.6155],\n",
      "        [0.6169],\n",
      "        [0.2715],\n",
      "        [0.2277],\n",
      "        [0.2817],\n",
      "        [0.8780],\n",
      "        [0.6567],\n",
      "        [0.6546],\n",
      "        [0.6139],\n",
      "        [0.8741],\n",
      "        [0.2718],\n",
      "        [0.2246],\n",
      "        [0.4155],\n",
      "        [0.3079],\n",
      "        [0.2676],\n",
      "        [0.2572],\n",
      "        [0.7127],\n",
      "        [0.2244],\n",
      "        [0.5607],\n",
      "        [0.2258],\n",
      "        [0.4014],\n",
      "        [0.3362],\n",
      "        [0.3851],\n",
      "        [0.7430],\n",
      "        [0.3120],\n",
      "        [0.3267],\n",
      "        [0.7974],\n",
      "        [0.1710],\n",
      "        [0.8495],\n",
      "        [0.8004],\n",
      "        [0.3919],\n",
      "        [0.2087],\n",
      "        [0.6123],\n",
      "        [0.6179],\n",
      "        [0.1989],\n",
      "        [0.7393]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0010],\n",
      "        [0.0019],\n",
      "        [0.0019],\n",
      "        [0.0019],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0051],\n",
      "        [0.0058],\n",
      "        [0.0062],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0082],\n",
      "        [0.0091],\n",
      "        [0.0096],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0111],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0124],\n",
      "        [0.0125],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0141],\n",
      "        [0.0142],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0161],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0173],\n",
      "        [0.0179],\n",
      "        [0.0186],\n",
      "        [0.0188],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0204],\n",
      "        [0.0212],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0235],\n",
      "        [0.0240],\n",
      "        [0.0240],\n",
      "        [0.0249],\n",
      "        [0.0262],\n",
      "        [0.0277],\n",
      "        [0.0280],\n",
      "        [0.0285],\n",
      "        [0.0292],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0301],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0312],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0353],\n",
      "        [0.0372],\n",
      "        [0.0375],\n",
      "        [0.0378],\n",
      "        [0.0382],\n",
      "        [0.0391],\n",
      "        [0.0394],\n",
      "        [0.0409],\n",
      "        [0.0420],\n",
      "        [0.0424],\n",
      "        [0.0427],\n",
      "        [0.0427],\n",
      "        [0.0437],\n",
      "        [0.0444],\n",
      "        [0.0447],\n",
      "        [0.0450],\n",
      "        [0.0452],\n",
      "        [0.0457],\n",
      "        [0.0474],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0496],\n",
      "        [0.0505],\n",
      "        [0.0542],\n",
      "        [0.0544],\n",
      "        [0.0544],\n",
      "        [0.0563],\n",
      "        [0.0575],\n",
      "        [0.0581],\n",
      "        [0.0591],\n",
      "        [0.0592],\n",
      "        [0.0606],\n",
      "        [0.0614],\n",
      "        [0.0624],\n",
      "        [0.0632],\n",
      "        [0.0644],\n",
      "        [0.0648],\n",
      "        [0.0657],\n",
      "        [0.0667],\n",
      "        [0.0669],\n",
      "        [0.0704],\n",
      "        [0.0711],\n",
      "        [0.0719],\n",
      "        [0.0725],\n",
      "        [0.0727],\n",
      "        [0.0735],\n",
      "        [0.0737],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.0739],\n",
      "        [0.0750],\n",
      "        [0.0762],\n",
      "        [0.0772],\n",
      "        [0.0788],\n",
      "        [0.0796],\n",
      "        [0.0827],\n",
      "        [0.0853],\n",
      "        [0.0868],\n",
      "        [0.0868],\n",
      "        [0.0891],\n",
      "        [0.0910],\n",
      "        [0.0911],\n",
      "        [0.0944],\n",
      "        [0.0984],\n",
      "        [0.1009],\n",
      "        [0.1038],\n",
      "        [0.1077],\n",
      "        [0.1146],\n",
      "        [0.1148],\n",
      "        [0.1201],\n",
      "        [0.1216],\n",
      "        [0.1245],\n",
      "        [0.1254],\n",
      "        [0.1389],\n",
      "        [0.1439],\n",
      "        [0.1456],\n",
      "        [0.1512],\n",
      "        [0.1646],\n",
      "        [0.1774],\n",
      "        [0.1918],\n",
      "        [0.1942],\n",
      "        [0.1989],\n",
      "        [0.2781]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0043],\n",
      "        [    0.0023],\n",
      "        [    0.0045],\n",
      "        [    0.0031],\n",
      "        [    0.0070],\n",
      "        [    0.0046],\n",
      "        [    0.0012],\n",
      "        [    0.0028],\n",
      "        [    0.0009],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0058],\n",
      "        [    0.0002],\n",
      "        [    0.0053],\n",
      "        [    0.0100],\n",
      "        [    0.0099],\n",
      "        [    0.0116],\n",
      "        [    0.0057],\n",
      "        [    0.0051],\n",
      "        [    0.0078],\n",
      "        [    0.0154],\n",
      "        [    0.0152],\n",
      "        [    0.0098],\n",
      "        [    0.0125],\n",
      "        [    0.0110],\n",
      "        [    0.0147],\n",
      "        [    0.0048],\n",
      "        [    0.0171],\n",
      "        [    0.0103],\n",
      "        [    0.0135],\n",
      "        [    0.0141],\n",
      "        [    0.0170],\n",
      "        [    0.0167],\n",
      "        [    0.0147],\n",
      "        [    0.0123],\n",
      "        [    0.0144],\n",
      "        [    0.0205],\n",
      "        [    0.0128],\n",
      "        [    0.0142],\n",
      "        [    0.0127],\n",
      "        [    0.0186],\n",
      "        [    0.0145],\n",
      "        [    0.0201],\n",
      "        [    0.0178],\n",
      "        [    0.0199],\n",
      "        [    0.0188],\n",
      "        [    0.0162],\n",
      "        [    0.0208],\n",
      "        [    0.0198],\n",
      "        [    0.0230],\n",
      "        [    0.0171],\n",
      "        [    0.0192],\n",
      "        [    0.0182],\n",
      "        [    0.0201],\n",
      "        [    0.0236],\n",
      "        [    0.0257],\n",
      "        [    0.0210],\n",
      "        [    0.0204],\n",
      "        [    0.0250],\n",
      "        [    0.0215],\n",
      "        [    0.0199],\n",
      "        [    0.0294],\n",
      "        [    0.0281],\n",
      "        [    0.0307],\n",
      "        [    0.0279],\n",
      "        [    0.0293],\n",
      "        [    0.0311],\n",
      "        [    0.0259],\n",
      "        [    0.0315],\n",
      "        [    0.0288],\n",
      "        [    0.0340],\n",
      "        [    0.0334],\n",
      "        [    0.0311],\n",
      "        [    0.0270],\n",
      "        [    0.0367],\n",
      "        [    0.0389],\n",
      "        [    0.0348],\n",
      "        [    0.0421],\n",
      "        [    0.0365],\n",
      "        [    0.0342],\n",
      "        [    0.0338],\n",
      "        [    0.0356],\n",
      "        [    0.0371],\n",
      "        [    0.0431],\n",
      "        [    0.0413],\n",
      "        [    0.0412],\n",
      "        [    0.0398],\n",
      "        [    0.0464],\n",
      "        [    0.0448],\n",
      "        [    0.0483],\n",
      "        [    0.0469],\n",
      "        [    0.0415],\n",
      "        [    0.0422],\n",
      "        [    0.0444],\n",
      "        [    0.0490],\n",
      "        [    0.0529],\n",
      "        [    0.0520],\n",
      "        [    0.0502],\n",
      "        [    0.0482],\n",
      "        [    0.0556],\n",
      "        [    0.0533],\n",
      "        [    0.0579],\n",
      "        [    0.0552],\n",
      "        [    0.0580],\n",
      "        [    0.0613],\n",
      "        [    0.0591],\n",
      "        [    0.0570],\n",
      "        [    0.0637],\n",
      "        [    0.0640],\n",
      "        [    0.0645],\n",
      "        [    0.0628],\n",
      "        [    0.0621],\n",
      "        [    0.0644],\n",
      "        [    0.0686],\n",
      "        [    0.0717],\n",
      "        [    0.0700],\n",
      "        [    0.0658],\n",
      "        [    0.0739],\n",
      "        [    0.0673],\n",
      "        [    0.0714],\n",
      "        [    0.0701],\n",
      "        [    0.0742],\n",
      "        [    0.0707],\n",
      "        [    0.0773],\n",
      "        [    0.0772],\n",
      "        [    0.0708],\n",
      "        [    0.0726],\n",
      "        [    0.0747],\n",
      "        [    0.0811],\n",
      "        [    0.0835],\n",
      "        [    0.0843],\n",
      "        [    0.0860],\n",
      "        [    0.0888],\n",
      "        [    0.0859],\n",
      "        [    0.0828],\n",
      "        [    0.0913],\n",
      "        [    0.0906],\n",
      "        [    0.0901],\n",
      "        [    0.0944],\n",
      "        [    0.0942],\n",
      "        [    0.0974],\n",
      "        [    0.1012],\n",
      "        [    0.1040],\n",
      "        [    0.1167],\n",
      "        [    0.1152],\n",
      "        [    0.1210],\n",
      "        [    0.1169],\n",
      "        [    0.1243],\n",
      "        [    0.1252],\n",
      "        [    0.1352],\n",
      "        [    0.1417],\n",
      "        [    0.1414],\n",
      "        [    0.1475],\n",
      "        [    0.1661],\n",
      "        [    0.1738],\n",
      "        [    0.1892],\n",
      "        [    0.1907],\n",
      "        [    0.1958],\n",
      "        [    0.2739]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 56.59981632232666\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 13 個區塊累積花費時間(s) 1.2151100635528564\n",
      "<<The performance of 13 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2151100635528564\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1282.95\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1731.73\n",
      "The accuracy(2000) for l = 1: 80.50%\n",
      "The accuracy(3000) for l = 1: 91.82%\n",
      "The maximum error: tensor(6993.3438)\n",
      "The minimum error: tensor(4.0508)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 2355.0\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 2451.1\n",
      "The accuracy(2000) for l = 1: 50.0%\n",
      "The accuracy(3000) for l = 1: 75.0%\n",
      "The maximum error: 3501.39453125\n",
      "The minimum error: 1772.0703125\n",
      "------------------------------------------------------------\n",
      "0.8050314465408805\n",
      "<class 'float'>\n",
      "0.5\n",
      "<class 'float'>\n",
      "The <<14>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.5156484895205722e-08, 1)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [1, 13, 46, 142, 2, 83, 103, 91, 47, 148, 95, 0, 25, 104, 79, 87, 39, 78, 98, 48, 72, 97, 99, 45, 84, 94, 62, 96, 82, 70, 134, 102, 85, 12, 92, 38, 86, 22, 33, 3, 133, 68, 113, 59, 149, 90, 143, 58, 11, 89, 10, 107, 147, 77, 150, 44, 50, 76, 23, 32, 88, 80, 141, 93, 4, 71, 100, 75, 60, 49, 21, 30, 101, 146, 129, 61, 63, 24, 130, 116, 128, 16, 37, 14, 51, 117, 69, 5, 52, 105, 40, 81, 135, 31, 57, 140, 115, 112, 67, 43, 123, 138, 9, 106, 122, 53, 73, 139, 74, 26, 20, 41, 158, 6, 15, 145, 144, 119, 34, 114, 42, 136, 118, 157, 55, 56, 8, 131, 35, 36, 121, 54, 156, 7, 120, 127, 108, 29, 137, 132, 66, 151, 124, 109, 28, 111, 126, 125, 19, 155, 17, 154, 18, 110, 152]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0023],\n",
      "        [    0.0028],\n",
      "        [    0.0031],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0043],\n",
      "        [    0.0045],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0051],\n",
      "        [    0.0053],\n",
      "        [    0.0057],\n",
      "        [    0.0058],\n",
      "        [    0.0070],\n",
      "        [    0.0078],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0116],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0127],\n",
      "        [    0.0128],\n",
      "        [    0.0135],\n",
      "        [    0.0141],\n",
      "        [    0.0142],\n",
      "        [    0.0144],\n",
      "        [    0.0145],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0152],\n",
      "        [    0.0162],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0171],\n",
      "        [    0.0178],\n",
      "        [    0.0182],\n",
      "        [    0.0186],\n",
      "        [    0.0188],\n",
      "        [    0.0192],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0199],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0204],\n",
      "        [    0.0208],\n",
      "        [    0.0210],\n",
      "        [    0.0215],\n",
      "        [    0.0230],\n",
      "        [    0.0236],\n",
      "        [    0.0250],\n",
      "        [    0.0257],\n",
      "        [    0.0270],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0288],\n",
      "        [    0.0293],\n",
      "        [    0.0294],\n",
      "        [    0.0307],\n",
      "        [    0.0311],\n",
      "        [    0.0311],\n",
      "        [    0.0315],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0342],\n",
      "        [    0.0348],\n",
      "        [    0.0356],\n",
      "        [    0.0365],\n",
      "        [    0.0367],\n",
      "        [    0.0371],\n",
      "        [    0.0389],\n",
      "        [    0.0398],\n",
      "        [    0.0412],\n",
      "        [    0.0413],\n",
      "        [    0.0415],\n",
      "        [    0.0421],\n",
      "        [    0.0422],\n",
      "        [    0.0431],\n",
      "        [    0.0444],\n",
      "        [    0.0448],\n",
      "        [    0.0464],\n",
      "        [    0.0469],\n",
      "        [    0.0482],\n",
      "        [    0.0483],\n",
      "        [    0.0490],\n",
      "        [    0.0502],\n",
      "        [    0.0520],\n",
      "        [    0.0529],\n",
      "        [    0.0533],\n",
      "        [    0.0552],\n",
      "        [    0.0556],\n",
      "        [    0.0570],\n",
      "        [    0.0579],\n",
      "        [    0.0580],\n",
      "        [    0.0591],\n",
      "        [    0.0613],\n",
      "        [    0.0621],\n",
      "        [    0.0628],\n",
      "        [    0.0637],\n",
      "        [    0.0640],\n",
      "        [    0.0644],\n",
      "        [    0.0645],\n",
      "        [    0.0658],\n",
      "        [    0.0673],\n",
      "        [    0.0686],\n",
      "        [    0.0694],\n",
      "        [    0.0700],\n",
      "        [    0.0701],\n",
      "        [    0.0707],\n",
      "        [    0.0708],\n",
      "        [    0.0714],\n",
      "        [    0.0717],\n",
      "        [    0.0726],\n",
      "        [    0.0739],\n",
      "        [    0.0742],\n",
      "        [    0.0747],\n",
      "        [    0.0761],\n",
      "        [    0.0772],\n",
      "        [    0.0773],\n",
      "        [    0.0811],\n",
      "        [    0.0828],\n",
      "        [    0.0835],\n",
      "        [    0.0843],\n",
      "        [    0.0859],\n",
      "        [    0.0860],\n",
      "        [    0.0864],\n",
      "        [    0.0888],\n",
      "        [    0.0901],\n",
      "        [    0.0906],\n",
      "        [    0.0913],\n",
      "        [    0.0942],\n",
      "        [    0.0944],\n",
      "        [    0.0974],\n",
      "        [    0.1012],\n",
      "        [    0.1040],\n",
      "        [    0.1152],\n",
      "        [    0.1167],\n",
      "        [    0.1169],\n",
      "        [    0.1210],\n",
      "        [    0.1243],\n",
      "        [    0.1252],\n",
      "        [    0.1352],\n",
      "        [    0.1371],\n",
      "        [    0.1414],\n",
      "        [    0.1417],\n",
      "        [    0.1475],\n",
      "        [    0.1661],\n",
      "        [    0.1738]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.5156484895205722e-08, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [1, 13, 46, 142, 2, 83, 103, 91, 47, 148, 95, 0, 25, 104, 79, 87, 39, 78, 98, 48, 72, 97, 99, 45, 84, 94, 62, 96, 82, 70, 134, 102, 85, 12, 92, 38, 86, 22, 33, 3, 133, 68, 113, 59, 149, 90, 143, 58, 11, 89, 10, 107, 147, 77, 150, 44, 50, 76, 23, 32, 88, 80, 141, 93, 4, 71, 100, 75, 60, 49, 21, 30, 101, 146, 129, 61, 63, 24, 130, 116, 128, 16, 37, 14, 51, 117, 69, 5, 52, 105, 40, 81, 135, 31, 57, 140, 115, 112, 67, 43, 123, 138, 9, 106, 122, 53, 73, 139, 74, 26, 20, 41, 158, 6, 15, 145, 144, 119, 34, 114, 42, 136, 118, 157, 55, 56, 8, 131, 35, 36, 121, 54, 156, 7, 120, 127, 108, 29, 137, 132, 66, 151, 124, 109, 28, 111, 126, 125, 19, 155, 17, 154, 18, 110, 152, 64] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8676],\n",
      "        [0.8365],\n",
      "        [0.6760],\n",
      "        [0.2313],\n",
      "        [0.8898],\n",
      "        [0.6184],\n",
      "        [0.4837],\n",
      "        [0.5865],\n",
      "        [0.6555],\n",
      "        [0.1876],\n",
      "        [0.5526],\n",
      "        [0.8883],\n",
      "        [0.7269],\n",
      "        [0.4686],\n",
      "        [0.6363],\n",
      "        [0.6108],\n",
      "        [0.6844],\n",
      "        [0.5885],\n",
      "        [0.5268],\n",
      "        [0.6254],\n",
      "        [0.5752],\n",
      "        [0.5303],\n",
      "        [0.5230],\n",
      "        [0.6720],\n",
      "        [0.6456],\n",
      "        [0.5683],\n",
      "        [0.6015],\n",
      "        [0.5137],\n",
      "        [0.6295],\n",
      "        [0.5374],\n",
      "        [0.2480],\n",
      "        [0.4853],\n",
      "        [0.6432],\n",
      "        [0.8596],\n",
      "        [0.5737],\n",
      "        [0.6881],\n",
      "        [0.6232],\n",
      "        [0.7439],\n",
      "        [0.6441],\n",
      "        [0.8925],\n",
      "        [0.2539],\n",
      "        [0.4935],\n",
      "        [0.2728],\n",
      "        [0.6267],\n",
      "        [0.1614],\n",
      "        [0.5881],\n",
      "        [0.2347],\n",
      "        [0.6216],\n",
      "        [0.8816],\n",
      "        [0.6077],\n",
      "        [0.8632],\n",
      "        [0.4200],\n",
      "        [0.1970],\n",
      "        [0.5848],\n",
      "        [0.1836],\n",
      "        [0.6721],\n",
      "        [0.6175],\n",
      "        [0.5858],\n",
      "        [0.7206],\n",
      "        [0.6506],\n",
      "        [0.6161],\n",
      "        [0.6486],\n",
      "        [0.2205],\n",
      "        [0.5903],\n",
      "        [0.8765],\n",
      "        [0.5798],\n",
      "        [0.5019],\n",
      "        [0.5876],\n",
      "        [0.6049],\n",
      "        [0.6141],\n",
      "        [0.7793],\n",
      "        [0.6677],\n",
      "        [0.4882],\n",
      "        [0.2247],\n",
      "        [0.3178],\n",
      "        [0.5993],\n",
      "        [0.6040],\n",
      "        [0.7197],\n",
      "        [0.2744],\n",
      "        [0.2666],\n",
      "        [0.3231],\n",
      "        [0.8581],\n",
      "        [0.6697],\n",
      "        [0.8777],\n",
      "        [0.6160],\n",
      "        [0.2627],\n",
      "        [0.4817],\n",
      "        [0.8736],\n",
      "        [0.6121],\n",
      "        [0.4237],\n",
      "        [0.6573],\n",
      "        [0.6227],\n",
      "        [0.2703],\n",
      "        [0.6033],\n",
      "        [0.6191],\n",
      "        [0.2373],\n",
      "        [0.2641],\n",
      "        [0.3230],\n",
      "        [0.5524],\n",
      "        [0.6471],\n",
      "        [0.3331],\n",
      "        [0.2467],\n",
      "        [0.8590],\n",
      "        [0.4025],\n",
      "        [0.3007],\n",
      "        [0.6158],\n",
      "        [0.5599],\n",
      "        [0.2452],\n",
      "        [0.5526],\n",
      "        [0.7289],\n",
      "        [0.8134],\n",
      "        [0.6475],\n",
      "        [0.1449],\n",
      "        [0.8655],\n",
      "        [0.8637],\n",
      "        [0.2298],\n",
      "        [0.2683],\n",
      "        [0.2866],\n",
      "        [0.6450],\n",
      "        [0.2301],\n",
      "        [0.6339],\n",
      "        [0.2651],\n",
      "        [0.2831],\n",
      "        [0.1452],\n",
      "        [0.6135],\n",
      "        [0.6120],\n",
      "        [0.8742],\n",
      "        [0.2286],\n",
      "        [0.6519],\n",
      "        [0.6499],\n",
      "        [0.2727],\n",
      "        [0.6106],\n",
      "        [0.1411],\n",
      "        [0.8706],\n",
      "        [0.2686],\n",
      "        [0.3084],\n",
      "        [0.4177],\n",
      "        [0.7085],\n",
      "        [0.2571],\n",
      "        [0.2278],\n",
      "        [0.5581],\n",
      "        [0.2221],\n",
      "        [0.3358],\n",
      "        [0.4034],\n",
      "        [0.7383],\n",
      "        [0.3860],\n",
      "        [0.3122],\n",
      "        [0.3269],\n",
      "        [0.7937],\n",
      "        [0.1700],\n",
      "        [0.8453],\n",
      "        [0.1688],\n",
      "        [0.7967],\n",
      "        [0.3935],\n",
      "        [0.2051],\n",
      "        [0.6097]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0023],\n",
      "        [    0.0028],\n",
      "        [    0.0031],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0043],\n",
      "        [    0.0045],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0051],\n",
      "        [    0.0053],\n",
      "        [    0.0057],\n",
      "        [    0.0058],\n",
      "        [    0.0070],\n",
      "        [    0.0078],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0116],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0127],\n",
      "        [    0.0128],\n",
      "        [    0.0135],\n",
      "        [    0.0141],\n",
      "        [    0.0142],\n",
      "        [    0.0144],\n",
      "        [    0.0145],\n",
      "        [    0.0147],\n",
      "        [    0.0147],\n",
      "        [    0.0152],\n",
      "        [    0.0162],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0171],\n",
      "        [    0.0178],\n",
      "        [    0.0182],\n",
      "        [    0.0186],\n",
      "        [    0.0188],\n",
      "        [    0.0192],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0199],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0204],\n",
      "        [    0.0208],\n",
      "        [    0.0210],\n",
      "        [    0.0215],\n",
      "        [    0.0230],\n",
      "        [    0.0236],\n",
      "        [    0.0250],\n",
      "        [    0.0257],\n",
      "        [    0.0270],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0288],\n",
      "        [    0.0293],\n",
      "        [    0.0294],\n",
      "        [    0.0307],\n",
      "        [    0.0311],\n",
      "        [    0.0311],\n",
      "        [    0.0315],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0342],\n",
      "        [    0.0348],\n",
      "        [    0.0356],\n",
      "        [    0.0365],\n",
      "        [    0.0367],\n",
      "        [    0.0371],\n",
      "        [    0.0389],\n",
      "        [    0.0398],\n",
      "        [    0.0412],\n",
      "        [    0.0413],\n",
      "        [    0.0415],\n",
      "        [    0.0421],\n",
      "        [    0.0422],\n",
      "        [    0.0431],\n",
      "        [    0.0444],\n",
      "        [    0.0448],\n",
      "        [    0.0464],\n",
      "        [    0.0469],\n",
      "        [    0.0482],\n",
      "        [    0.0483],\n",
      "        [    0.0490],\n",
      "        [    0.0502],\n",
      "        [    0.0520],\n",
      "        [    0.0529],\n",
      "        [    0.0533],\n",
      "        [    0.0552],\n",
      "        [    0.0556],\n",
      "        [    0.0570],\n",
      "        [    0.0579],\n",
      "        [    0.0580],\n",
      "        [    0.0591],\n",
      "        [    0.0613],\n",
      "        [    0.0621],\n",
      "        [    0.0628],\n",
      "        [    0.0637],\n",
      "        [    0.0640],\n",
      "        [    0.0644],\n",
      "        [    0.0645],\n",
      "        [    0.0658],\n",
      "        [    0.0673],\n",
      "        [    0.0686],\n",
      "        [    0.0694],\n",
      "        [    0.0700],\n",
      "        [    0.0701],\n",
      "        [    0.0707],\n",
      "        [    0.0708],\n",
      "        [    0.0714],\n",
      "        [    0.0717],\n",
      "        [    0.0726],\n",
      "        [    0.0739],\n",
      "        [    0.0742],\n",
      "        [    0.0747],\n",
      "        [    0.0761],\n",
      "        [    0.0772],\n",
      "        [    0.0773],\n",
      "        [    0.0811],\n",
      "        [    0.0828],\n",
      "        [    0.0835],\n",
      "        [    0.0843],\n",
      "        [    0.0859],\n",
      "        [    0.0860],\n",
      "        [    0.0864],\n",
      "        [    0.0888],\n",
      "        [    0.0901],\n",
      "        [    0.0906],\n",
      "        [    0.0913],\n",
      "        [    0.0942],\n",
      "        [    0.0944],\n",
      "        [    0.0974],\n",
      "        [    0.1012],\n",
      "        [    0.1040],\n",
      "        [    0.1152],\n",
      "        [    0.1167],\n",
      "        [    0.1169],\n",
      "        [    0.1210],\n",
      "        [    0.1243],\n",
      "        [    0.1252],\n",
      "        [    0.1352],\n",
      "        [    0.1371],\n",
      "        [    0.1414],\n",
      "        [    0.1417],\n",
      "        [    0.1475],\n",
      "        [    0.1661],\n",
      "        [    0.1738],\n",
      "        [    0.1892]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0042],\n",
      "        [    0.0092],\n",
      "        [    0.0038],\n",
      "        [    0.0078],\n",
      "        [    0.0019],\n",
      "        [    0.0002],\n",
      "        [    0.0021],\n",
      "        [    0.0002],\n",
      "        [    0.0008],\n",
      "        [    0.0120],\n",
      "        [    0.0089],\n",
      "        [    0.0087],\n",
      "        [    0.0117],\n",
      "        [    0.0033],\n",
      "        [    0.0082],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0101],\n",
      "        [    0.0072],\n",
      "        [    0.0049],\n",
      "        [    0.0092],\n",
      "        [    0.0070],\n",
      "        [    0.0089],\n",
      "        [    0.0067],\n",
      "        [    0.0154],\n",
      "        [    0.0168],\n",
      "        [    0.0160],\n",
      "        [    0.0089],\n",
      "        [    0.0106],\n",
      "        [    0.0129],\n",
      "        [    0.0165],\n",
      "        [    0.0157],\n",
      "        [    0.0177],\n",
      "        [    0.0069],\n",
      "        [    0.0189],\n",
      "        [    0.0105],\n",
      "        [    0.0195],\n",
      "        [    0.0108],\n",
      "        [    0.0130],\n",
      "        [    0.0121],\n",
      "        [    0.0187],\n",
      "        [    0.0161],\n",
      "        [    0.0200],\n",
      "        [    0.0144],\n",
      "        [    0.0265],\n",
      "        [    0.0149],\n",
      "        [    0.0257],\n",
      "        [    0.0159],\n",
      "        [    0.0274],\n",
      "        [    0.0161],\n",
      "        [    0.0125],\n",
      "        [    0.0187],\n",
      "        [    0.0291],\n",
      "        [    0.0239],\n",
      "        [    0.0138],\n",
      "        [    0.0182],\n",
      "        [    0.0184],\n",
      "        [    0.0223],\n",
      "        [    0.0198],\n",
      "        [    0.0311],\n",
      "        [    0.0239],\n",
      "        [    0.0252],\n",
      "        [    0.0239],\n",
      "        [    0.0334],\n",
      "        [    0.0237],\n",
      "        [    0.0298],\n",
      "        [    0.0297],\n",
      "        [    0.0288],\n",
      "        [    0.0272],\n",
      "        [    0.0283],\n",
      "        [    0.0400],\n",
      "        [    0.0397],\n",
      "        [    0.0333],\n",
      "        [    0.0276],\n",
      "        [    0.0369],\n",
      "        [    0.0325],\n",
      "        [    0.0407],\n",
      "        [    0.0323],\n",
      "        [    0.0393],\n",
      "        [    0.0441],\n",
      "        [    0.0421],\n",
      "        [    0.0494],\n",
      "        [    0.0376],\n",
      "        [    0.0506],\n",
      "        [    0.0380],\n",
      "        [    0.0473],\n",
      "        [    0.0438],\n",
      "        [    0.0398],\n",
      "        [    0.0419],\n",
      "        [    0.0459],\n",
      "        [    0.0431],\n",
      "        [    0.0464],\n",
      "        [    0.0475],\n",
      "        [    0.0474],\n",
      "        [    0.0496],\n",
      "        [    0.0482],\n",
      "        [    0.0582],\n",
      "        [    0.0541],\n",
      "        [    0.0594],\n",
      "        [    0.0533],\n",
      "        [    0.0602],\n",
      "        [    0.0554],\n",
      "        [    0.0537],\n",
      "        [    0.0603],\n",
      "        [    0.0649],\n",
      "        [    0.0597],\n",
      "        [    0.0628],\n",
      "        [    0.0602],\n",
      "        [    0.0631],\n",
      "        [    0.0719],\n",
      "        [    0.0741],\n",
      "        [    0.0629],\n",
      "        [    0.0578],\n",
      "        [    0.0629],\n",
      "        [    0.0786],\n",
      "        [    0.0634],\n",
      "        [    0.0643],\n",
      "        [    0.0735],\n",
      "        [    0.0678],\n",
      "        [    0.0751],\n",
      "        [    0.0687],\n",
      "        [    0.0717],\n",
      "        [    0.0769],\n",
      "        [    0.0650],\n",
      "        [    0.0734],\n",
      "        [    0.0737],\n",
      "        [    0.0738],\n",
      "        [    0.0825],\n",
      "        [    0.0793],\n",
      "        [    0.0802],\n",
      "        [    0.0883],\n",
      "        [    0.0823],\n",
      "        [    0.0759],\n",
      "        [    0.0814],\n",
      "        [    0.0925],\n",
      "        [    0.0921],\n",
      "        [    0.0931],\n",
      "        [    0.1003],\n",
      "        [    0.0909],\n",
      "        [    0.0977],\n",
      "        [    0.1034],\n",
      "        [    0.0956],\n",
      "        [    0.1172],\n",
      "        [    0.1180],\n",
      "        [    0.1232],\n",
      "        [    0.1207],\n",
      "        [    0.1260],\n",
      "        [    0.1267],\n",
      "        [    0.1423],\n",
      "        [    0.1276],\n",
      "        [    0.1485],\n",
      "        [    0.1334],\n",
      "        [    0.1544],\n",
      "        [    0.1665],\n",
      "        [    0.1652],\n",
      "        [    0.1925]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 57.12299108505249\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.701921968968236e-08, 83)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [83, 91, 47, 2, 103, 87, 39, 104, 46, 1, 48, 45, 12, 97, 98, 142, 79, 0, 99, 96, 95, 72, 13, 78, 38, 82, 22, 25, 148, 3, 10, 70, 33, 150, 59, 90, 84, 102, 58, 62, 89, 68, 134, 94, 85, 44, 50, 107, 133, 92, 86, 23, 113, 76, 4, 141, 88, 77, 80, 143, 149, 60, 11, 146, 49, 75, 147, 100, 71, 32, 24, 61, 101, 93, 129, 37, 51, 130, 30, 5, 21, 63, 52, 128, 40, 69, 116, 105, 81, 117, 31, 135, 140, 16, 57, 14, 43, 9, 112, 138, 158, 115, 67, 53, 139, 123, 106, 73, 41, 6, 74, 145, 144, 122, 157, 34, 42, 136, 26, 55, 119, 56, 8, 20, 114, 156, 118, 15, 35, 36, 7, 54, 131, 121, 137, 127, 120, 108, 151, 132, 29, 66, 124, 109, 111, 28, 126, 125, 155, 154, 19, 17, 18, 152, 110, 153, 64] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6214],\n",
      "        [0.5905],\n",
      "        [0.6605],\n",
      "        [0.8945],\n",
      "        [0.4852],\n",
      "        [0.6145],\n",
      "        [0.6891],\n",
      "        [0.4707],\n",
      "        [0.6810],\n",
      "        [0.8716],\n",
      "        [0.6304],\n",
      "        [0.6768],\n",
      "        [0.8674],\n",
      "        [0.5336],\n",
      "        [0.5294],\n",
      "        [0.2258],\n",
      "        [0.6387],\n",
      "        [0.8922],\n",
      "        [0.5251],\n",
      "        [0.5175],\n",
      "        [0.5569],\n",
      "        [0.5759],\n",
      "        [0.8448],\n",
      "        [0.5908],\n",
      "        [0.6928],\n",
      "        [0.6323],\n",
      "        [0.7498],\n",
      "        [0.7335],\n",
      "        [0.1801],\n",
      "        [0.8975],\n",
      "        [0.8708],\n",
      "        [0.5386],\n",
      "        [0.6480],\n",
      "        [0.1758],\n",
      "        [0.6310],\n",
      "        [0.5923],\n",
      "        [0.6487],\n",
      "        [0.4867],\n",
      "        [0.6257],\n",
      "        [0.6048],\n",
      "        [0.6117],\n",
      "        [0.4952],\n",
      "        [0.2457],\n",
      "        [0.5726],\n",
      "        [0.6464],\n",
      "        [0.6768],\n",
      "        [0.6228],\n",
      "        [0.4217],\n",
      "        [0.2523],\n",
      "        [0.5779],\n",
      "        [0.6265],\n",
      "        [0.7265],\n",
      "        [0.2709],\n",
      "        [0.5884],\n",
      "        [0.8822],\n",
      "        [0.2156],\n",
      "        [0.6201],\n",
      "        [0.5877],\n",
      "        [0.6515],\n",
      "        [0.2288],\n",
      "        [0.1537],\n",
      "        [0.6092],\n",
      "        [0.8890],\n",
      "        [0.2167],\n",
      "        [0.6191],\n",
      "        [0.5899],\n",
      "        [0.1887],\n",
      "        [0.5033],\n",
      "        [0.5807],\n",
      "        [0.6547],\n",
      "        [0.7263],\n",
      "        [0.6035],\n",
      "        [0.4898],\n",
      "        [0.5944],\n",
      "        [0.3174],\n",
      "        [0.6742],\n",
      "        [0.6211],\n",
      "        [0.2749],\n",
      "        [0.6732],\n",
      "        [0.8801],\n",
      "        [0.7855],\n",
      "        [0.6076],\n",
      "        [0.6171],\n",
      "        [0.3223],\n",
      "        [0.6625],\n",
      "        [0.4827],\n",
      "        [0.2636],\n",
      "        [0.4260],\n",
      "        [0.6253],\n",
      "        [0.2598],\n",
      "        [0.6079],\n",
      "        [0.2676],\n",
      "        [0.2322],\n",
      "        [0.8660],\n",
      "        [0.6225],\n",
      "        [0.8860],\n",
      "        [0.6518],\n",
      "        [0.8666],\n",
      "        [0.3216],\n",
      "        [0.2430],\n",
      "        [0.1334],\n",
      "        [0.2610],\n",
      "        [0.5547],\n",
      "        [0.6198],\n",
      "        [0.2410],\n",
      "        [0.3309],\n",
      "        [0.4042],\n",
      "        [0.5611],\n",
      "        [0.6532],\n",
      "        [0.8725],\n",
      "        [0.5540],\n",
      "        [0.2225],\n",
      "        [0.2618],\n",
      "        [0.2986],\n",
      "        [0.1341],\n",
      "        [0.6490],\n",
      "        [0.6391],\n",
      "        [0.2626],\n",
      "        [0.7349],\n",
      "        [0.6173],\n",
      "        [0.2845],\n",
      "        [0.6156],\n",
      "        [0.8814],\n",
      "        [0.8202],\n",
      "        [0.2276],\n",
      "        [0.1306],\n",
      "        [0.2810],\n",
      "        [0.8723],\n",
      "        [0.6562],\n",
      "        [0.6541],\n",
      "        [0.8779],\n",
      "        [0.6143],\n",
      "        [0.2289],\n",
      "        [0.2702],\n",
      "        [0.2537],\n",
      "        [0.3069],\n",
      "        [0.2662],\n",
      "        [0.4194],\n",
      "        [0.2137],\n",
      "        [0.2275],\n",
      "        [0.7146],\n",
      "        [0.5603],\n",
      "        [0.3339],\n",
      "        [0.4047],\n",
      "        [0.3856],\n",
      "        [0.7446],\n",
      "        [0.3105],\n",
      "        [0.3255],\n",
      "        [0.1605],\n",
      "        [0.1605],\n",
      "        [0.8007],\n",
      "        [0.8523],\n",
      "        [0.8037],\n",
      "        [0.1965],\n",
      "        [0.3939],\n",
      "        [0.1874],\n",
      "        [0.6130]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0008],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0033],\n",
      "        [    0.0038],\n",
      "        [    0.0042],\n",
      "        [    0.0049],\n",
      "        [    0.0067],\n",
      "        [    0.0069],\n",
      "        [    0.0070],\n",
      "        [    0.0072],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0089],\n",
      "        [    0.0089],\n",
      "        [    0.0092],\n",
      "        [    0.0092],\n",
      "        [    0.0101],\n",
      "        [    0.0105],\n",
      "        [    0.0106],\n",
      "        [    0.0108],\n",
      "        [    0.0117],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0125],\n",
      "        [    0.0129],\n",
      "        [    0.0130],\n",
      "        [    0.0138],\n",
      "        [    0.0144],\n",
      "        [    0.0149],\n",
      "        [    0.0154],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0160],\n",
      "        [    0.0161],\n",
      "        [    0.0161],\n",
      "        [    0.0165],\n",
      "        [    0.0168],\n",
      "        [    0.0177],\n",
      "        [    0.0182],\n",
      "        [    0.0184],\n",
      "        [    0.0187],\n",
      "        [    0.0187],\n",
      "        [    0.0189],\n",
      "        [    0.0195],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0223],\n",
      "        [    0.0237],\n",
      "        [    0.0239],\n",
      "        [    0.0239],\n",
      "        [    0.0239],\n",
      "        [    0.0252],\n",
      "        [    0.0257],\n",
      "        [    0.0265],\n",
      "        [    0.0272],\n",
      "        [    0.0274],\n",
      "        [    0.0276],\n",
      "        [    0.0283],\n",
      "        [    0.0288],\n",
      "        [    0.0291],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0311],\n",
      "        [    0.0323],\n",
      "        [    0.0325],\n",
      "        [    0.0333],\n",
      "        [    0.0334],\n",
      "        [    0.0369],\n",
      "        [    0.0376],\n",
      "        [    0.0380],\n",
      "        [    0.0393],\n",
      "        [    0.0397],\n",
      "        [    0.0398],\n",
      "        [    0.0400],\n",
      "        [    0.0407],\n",
      "        [    0.0419],\n",
      "        [    0.0421],\n",
      "        [    0.0431],\n",
      "        [    0.0438],\n",
      "        [    0.0441],\n",
      "        [    0.0459],\n",
      "        [    0.0464],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0475],\n",
      "        [    0.0482],\n",
      "        [    0.0494],\n",
      "        [    0.0496],\n",
      "        [    0.0506],\n",
      "        [    0.0533],\n",
      "        [    0.0537],\n",
      "        [    0.0541],\n",
      "        [    0.0554],\n",
      "        [    0.0578],\n",
      "        [    0.0582],\n",
      "        [    0.0594],\n",
      "        [    0.0597],\n",
      "        [    0.0602],\n",
      "        [    0.0602],\n",
      "        [    0.0603],\n",
      "        [    0.0628],\n",
      "        [    0.0629],\n",
      "        [    0.0629],\n",
      "        [    0.0631],\n",
      "        [    0.0634],\n",
      "        [    0.0643],\n",
      "        [    0.0649],\n",
      "        [    0.0650],\n",
      "        [    0.0678],\n",
      "        [    0.0687],\n",
      "        [    0.0717],\n",
      "        [    0.0719],\n",
      "        [    0.0734],\n",
      "        [    0.0735],\n",
      "        [    0.0737],\n",
      "        [    0.0738],\n",
      "        [    0.0741],\n",
      "        [    0.0751],\n",
      "        [    0.0759],\n",
      "        [    0.0769],\n",
      "        [    0.0786],\n",
      "        [    0.0793],\n",
      "        [    0.0802],\n",
      "        [    0.0814],\n",
      "        [    0.0823],\n",
      "        [    0.0825],\n",
      "        [    0.0883],\n",
      "        [    0.0909],\n",
      "        [    0.0921],\n",
      "        [    0.0925],\n",
      "        [    0.0931],\n",
      "        [    0.0956],\n",
      "        [    0.0977],\n",
      "        [    0.1003],\n",
      "        [    0.1034],\n",
      "        [    0.1172],\n",
      "        [    0.1180],\n",
      "        [    0.1207],\n",
      "        [    0.1232],\n",
      "        [    0.1260],\n",
      "        [    0.1267],\n",
      "        [    0.1276],\n",
      "        [    0.1334],\n",
      "        [    0.1423],\n",
      "        [    0.1485],\n",
      "        [    0.1544],\n",
      "        [    0.1652],\n",
      "        [    0.1665],\n",
      "        [    0.1874],\n",
      "        [    0.1925]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0018],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0034],\n",
      "        [0.0008],\n",
      "        [0.0006],\n",
      "        [0.0003],\n",
      "        [0.0012],\n",
      "        [0.0068],\n",
      "        [0.0051],\n",
      "        [0.0016],\n",
      "        [0.0039],\n",
      "        [0.0025],\n",
      "        [0.0040],\n",
      "        [0.0050],\n",
      "        [0.0115],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0070],\n",
      "        [0.0054],\n",
      "        [0.0127],\n",
      "        [0.0092],\n",
      "        [0.0143],\n",
      "        [0.0112],\n",
      "        [0.0078],\n",
      "        [0.0088],\n",
      "        [0.0074],\n",
      "        [0.0161],\n",
      "        [0.0179],\n",
      "        [0.0103],\n",
      "        [0.0081],\n",
      "        [0.0124],\n",
      "        [0.0108],\n",
      "        [0.0077],\n",
      "        [0.0117],\n",
      "        [0.0116],\n",
      "        [0.0173],\n",
      "        [0.0169],\n",
      "        [0.0134],\n",
      "        [0.0180],\n",
      "        [0.0131],\n",
      "        [0.0152],\n",
      "        [0.0175],\n",
      "        [0.0204],\n",
      "        [0.0197],\n",
      "        [0.0155],\n",
      "        [0.0147],\n",
      "        [0.0167],\n",
      "        [0.0190],\n",
      "        [0.0224],\n",
      "        [0.0216],\n",
      "        [0.0163],\n",
      "        [0.0208],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0207],\n",
      "        [0.0210],\n",
      "        [0.0258],\n",
      "        [0.0235],\n",
      "        [0.0299],\n",
      "        [0.0324],\n",
      "        [0.0242],\n",
      "        [0.0315],\n",
      "        [0.0212],\n",
      "        [0.0249],\n",
      "        [0.0276],\n",
      "        [0.0358],\n",
      "        [0.0284],\n",
      "        [0.0298],\n",
      "        [0.0334],\n",
      "        [0.0279],\n",
      "        [0.0297],\n",
      "        [0.0318],\n",
      "        [0.0367],\n",
      "        [0.0363],\n",
      "        [0.0350],\n",
      "        [0.0346],\n",
      "        [0.0376],\n",
      "        [0.0432],\n",
      "        [0.0364],\n",
      "        [0.0434],\n",
      "        [0.0429],\n",
      "        [0.0386],\n",
      "        [0.0418],\n",
      "        [0.0398],\n",
      "        [0.0433],\n",
      "        [0.0457],\n",
      "        [0.0434],\n",
      "        [0.0450],\n",
      "        [0.0487],\n",
      "        [0.0445],\n",
      "        [0.0462],\n",
      "        [0.0446],\n",
      "        [0.0542],\n",
      "        [0.0477],\n",
      "        [0.0556],\n",
      "        [0.0505],\n",
      "        [0.0492],\n",
      "        [0.0535],\n",
      "        [0.0531],\n",
      "        [0.0486],\n",
      "        [0.0599],\n",
      "        [0.0607],\n",
      "        [0.0573],\n",
      "        [0.0575],\n",
      "        [0.0612],\n",
      "        [0.0582],\n",
      "        [0.0624],\n",
      "        [0.0591],\n",
      "        [0.0590],\n",
      "        [0.0626],\n",
      "        [0.0578],\n",
      "        [0.0595],\n",
      "        [0.0657],\n",
      "        [0.0562],\n",
      "        [0.0655],\n",
      "        [0.0653],\n",
      "        [0.0705],\n",
      "        [0.0756],\n",
      "        [0.0712],\n",
      "        [0.0743],\n",
      "        [0.0716],\n",
      "        [0.0697],\n",
      "        [0.0779],\n",
      "        [0.0761],\n",
      "        [0.0676],\n",
      "        [0.0776],\n",
      "        [0.0840],\n",
      "        [0.0768],\n",
      "        [0.0777],\n",
      "        [0.0773],\n",
      "        [0.0801],\n",
      "        [0.0809],\n",
      "        [0.0894],\n",
      "        [0.0889],\n",
      "        [0.0924],\n",
      "        [0.0936],\n",
      "        [0.0951],\n",
      "        [0.0889],\n",
      "        [0.0967],\n",
      "        [0.1042],\n",
      "        [0.1046],\n",
      "        [0.1180],\n",
      "        [0.1197],\n",
      "        [0.1209],\n",
      "        [0.1271],\n",
      "        [0.1266],\n",
      "        [0.1271],\n",
      "        [0.1200],\n",
      "        [0.1269],\n",
      "        [0.1465],\n",
      "        [0.1525],\n",
      "        [0.1586],\n",
      "        [0.1583],\n",
      "        [0.1674],\n",
      "        [0.1806],\n",
      "        [0.1945]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 57.41036629676819\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.127255230443552e-08, 39)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [39, 87, 103, 104, 48, 83, 12, 91, 2, 47, 45, 97, 98, 1, 96, 46, 99, 22, 150, 38, 10, 82, 72, 79, 0, 3, 33, 78, 142, 90, 59, 70, 95, 89, 58, 13, 50, 68, 44, 25, 23, 107, 102, 84, 134, 148, 62, 133, 85, 94, 141, 76, 113, 88, 4, 146, 86, 92, 80, 60, 49, 77, 75, 24, 100, 61, 71, 143, 11, 101, 149, 32, 51, 37, 147, 129, 5, 93, 130, 52, 40, 128, 63, 30, 69, 105, 21, 31, 140, 81, 116, 135, 57, 158, 117, 9, 43, 138, 112, 16, 14, 157, 53, 139, 145, 106, 6, 41, 144, 115, 67, 123, 73, 74, 42, 34, 122, 156, 8, 136, 55, 56, 119, 26, 114, 35, 7, 118, 36, 20, 54, 131, 15, 137, 151, 121, 127, 120, 108, 132, 29, 66, 124, 109, 155, 111, 126, 154, 125, 28, 19, 17, 152, 18, 110, 153, 65, 64] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6917],\n",
      "        [0.6171],\n",
      "        [0.4865],\n",
      "        [0.4727],\n",
      "        [0.6337],\n",
      "        [0.6233],\n",
      "        [0.8719],\n",
      "        [0.5937],\n",
      "        [0.8960],\n",
      "        [0.6636],\n",
      "        [0.6797],\n",
      "        [0.5366],\n",
      "        [0.5317],\n",
      "        [0.8725],\n",
      "        [0.5210],\n",
      "        [0.6840],\n",
      "        [0.5270],\n",
      "        [0.7532],\n",
      "        [0.1698],\n",
      "        [0.6955],\n",
      "        [0.8752],\n",
      "        [0.6341],\n",
      "        [0.5759],\n",
      "        [0.6399],\n",
      "        [0.8930],\n",
      "        [0.8993],\n",
      "        [0.6502],\n",
      "        [0.5919],\n",
      "        [0.2221],\n",
      "        [0.5956],\n",
      "        [0.6337],\n",
      "        [0.5390],\n",
      "        [0.5606],\n",
      "        [0.6146],\n",
      "        [0.6282],\n",
      "        [0.8499],\n",
      "        [0.6264],\n",
      "        [0.4961],\n",
      "        [0.6795],\n",
      "        [0.7378],\n",
      "        [0.7300],\n",
      "        [0.4238],\n",
      "        [0.4879],\n",
      "        [0.6507],\n",
      "        [0.2447],\n",
      "        [0.1742],\n",
      "        [0.6068],\n",
      "        [0.2519],\n",
      "        [0.6483],\n",
      "        [0.5762],\n",
      "        [0.2124],\n",
      "        [0.5900],\n",
      "        [0.2702],\n",
      "        [0.6230],\n",
      "        [0.8848],\n",
      "        [0.2103],\n",
      "        [0.6286],\n",
      "        [0.5814],\n",
      "        [0.6533],\n",
      "        [0.6122],\n",
      "        [0.6225],\n",
      "        [0.5896],\n",
      "        [0.5911],\n",
      "        [0.7307],\n",
      "        [0.5046],\n",
      "        [0.6063],\n",
      "        [0.5807],\n",
      "        [0.2247],\n",
      "        [0.8932],\n",
      "        [0.4912],\n",
      "        [0.1478],\n",
      "        [0.6570],\n",
      "        [0.6245],\n",
      "        [0.6768],\n",
      "        [0.1820],\n",
      "        [0.3180],\n",
      "        [0.8835],\n",
      "        [0.5977],\n",
      "        [0.2765],\n",
      "        [0.6204],\n",
      "        [0.6658],\n",
      "        [0.3227],\n",
      "        [0.6098],\n",
      "        [0.6767],\n",
      "        [0.4832],\n",
      "        [0.4285],\n",
      "        [0.7889],\n",
      "        [0.6108],\n",
      "        [0.2287],\n",
      "        [0.6268],\n",
      "        [0.2621],\n",
      "        [0.2663],\n",
      "        [0.6243],\n",
      "        [0.1241],\n",
      "        [0.2583],\n",
      "        [0.8711],\n",
      "        [0.6545],\n",
      "        [0.2407],\n",
      "        [0.3210],\n",
      "        [0.8708],\n",
      "        [0.8911],\n",
      "        [0.1253],\n",
      "        [0.6222],\n",
      "        [0.2383],\n",
      "        [0.2169],\n",
      "        [0.4064],\n",
      "        [0.8765],\n",
      "        [0.6570],\n",
      "        [0.2570],\n",
      "        [0.2593],\n",
      "        [0.5561],\n",
      "        [0.3299],\n",
      "        [0.5615],\n",
      "        [0.5545],\n",
      "        [0.6424],\n",
      "        [0.6513],\n",
      "        [0.2978],\n",
      "        [0.1223],\n",
      "        [0.8855],\n",
      "        [0.2614],\n",
      "        [0.6196],\n",
      "        [0.6176],\n",
      "        [0.2837],\n",
      "        [0.7386],\n",
      "        [0.2266],\n",
      "        [0.6587],\n",
      "        [0.8821],\n",
      "        [0.2803],\n",
      "        [0.6565],\n",
      "        [0.8241],\n",
      "        [0.6165],\n",
      "        [0.2306],\n",
      "        [0.8776],\n",
      "        [0.2516],\n",
      "        [0.2070],\n",
      "        [0.2691],\n",
      "        [0.3066],\n",
      "        [0.2651],\n",
      "        [0.4215],\n",
      "        [0.2286],\n",
      "        [0.7185],\n",
      "        [0.5615],\n",
      "        [0.3331],\n",
      "        [0.4065],\n",
      "        [0.1528],\n",
      "        [0.3858],\n",
      "        [0.3099],\n",
      "        [0.1540],\n",
      "        [0.3251],\n",
      "        [0.7484],\n",
      "        [0.8050],\n",
      "        [0.8564],\n",
      "        [0.1896],\n",
      "        [0.8078],\n",
      "        [0.3948],\n",
      "        [0.1806],\n",
      "        [0.6182],\n",
      "        [0.6150]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0016],\n",
      "        [0.0018],\n",
      "        [0.0025],\n",
      "        [0.0030],\n",
      "        [0.0034],\n",
      "        [0.0039],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0081],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0103],\n",
      "        [0.0108],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0117],\n",
      "        [0.0124],\n",
      "        [0.0127],\n",
      "        [0.0131],\n",
      "        [0.0134],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0152],\n",
      "        [0.0155],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0224],\n",
      "        [0.0235],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0258],\n",
      "        [0.0276],\n",
      "        [0.0279],\n",
      "        [0.0284],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0315],\n",
      "        [0.0318],\n",
      "        [0.0324],\n",
      "        [0.0334],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0358],\n",
      "        [0.0363],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0376],\n",
      "        [0.0386],\n",
      "        [0.0398],\n",
      "        [0.0418],\n",
      "        [0.0429],\n",
      "        [0.0432],\n",
      "        [0.0433],\n",
      "        [0.0434],\n",
      "        [0.0434],\n",
      "        [0.0445],\n",
      "        [0.0446],\n",
      "        [0.0450],\n",
      "        [0.0457],\n",
      "        [0.0462],\n",
      "        [0.0477],\n",
      "        [0.0486],\n",
      "        [0.0487],\n",
      "        [0.0492],\n",
      "        [0.0505],\n",
      "        [0.0531],\n",
      "        [0.0535],\n",
      "        [0.0542],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0573],\n",
      "        [0.0575],\n",
      "        [0.0578],\n",
      "        [0.0582],\n",
      "        [0.0590],\n",
      "        [0.0591],\n",
      "        [0.0595],\n",
      "        [0.0599],\n",
      "        [0.0607],\n",
      "        [0.0612],\n",
      "        [0.0624],\n",
      "        [0.0626],\n",
      "        [0.0653],\n",
      "        [0.0655],\n",
      "        [0.0657],\n",
      "        [0.0676],\n",
      "        [0.0697],\n",
      "        [0.0705],\n",
      "        [0.0712],\n",
      "        [0.0716],\n",
      "        [0.0743],\n",
      "        [0.0756],\n",
      "        [0.0761],\n",
      "        [0.0768],\n",
      "        [0.0773],\n",
      "        [0.0776],\n",
      "        [0.0777],\n",
      "        [0.0779],\n",
      "        [0.0801],\n",
      "        [0.0809],\n",
      "        [0.0840],\n",
      "        [0.0889],\n",
      "        [0.0889],\n",
      "        [0.0894],\n",
      "        [0.0924],\n",
      "        [0.0936],\n",
      "        [0.0951],\n",
      "        [0.0967],\n",
      "        [0.1042],\n",
      "        [0.1046],\n",
      "        [0.1180],\n",
      "        [0.1197],\n",
      "        [0.1200],\n",
      "        [0.1209],\n",
      "        [0.1266],\n",
      "        [0.1269],\n",
      "        [0.1271],\n",
      "        [0.1271],\n",
      "        [0.1465],\n",
      "        [0.1525],\n",
      "        [0.1583],\n",
      "        [0.1586],\n",
      "        [0.1674],\n",
      "        [0.1806],\n",
      "        [0.1945],\n",
      "        [0.1945]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0043],\n",
      "        [    0.0033],\n",
      "        [    0.0036],\n",
      "        [    0.0032],\n",
      "        [    0.0050],\n",
      "        [    0.0027],\n",
      "        [    0.0076],\n",
      "        [    0.0001],\n",
      "        [    0.0044],\n",
      "        [    0.0000],\n",
      "        [    0.0080],\n",
      "        [    0.0060],\n",
      "        [    0.0076],\n",
      "        [    0.0030],\n",
      "        [    0.0068],\n",
      "        [    0.0027],\n",
      "        [    0.0099],\n",
      "        [    0.0116],\n",
      "        [    0.0039],\n",
      "        [    0.0123],\n",
      "        [    0.0133],\n",
      "        [    0.0136],\n",
      "        [    0.0147],\n",
      "        [    0.0039],\n",
      "        [    0.0009],\n",
      "        [    0.0180],\n",
      "        [    0.0150],\n",
      "        [    0.0064],\n",
      "        [    0.0139],\n",
      "        [    0.0143],\n",
      "        [    0.0159],\n",
      "        [    0.0170],\n",
      "        [    0.0109],\n",
      "        [    0.0165],\n",
      "        [    0.0177],\n",
      "        [    0.0099],\n",
      "        [    0.0177],\n",
      "        [    0.0190],\n",
      "        [    0.0196],\n",
      "        [    0.0130],\n",
      "        [    0.0201],\n",
      "        [    0.0178],\n",
      "        [    0.0139],\n",
      "        [    0.0126],\n",
      "        [    0.0180],\n",
      "        [    0.0216],\n",
      "        [    0.0134],\n",
      "        [    0.0190],\n",
      "        [    0.0149],\n",
      "        [    0.0182],\n",
      "        [    0.0188],\n",
      "        [    0.0250],\n",
      "        [    0.0220],\n",
      "        [    0.0246],\n",
      "        [    0.0280],\n",
      "        [    0.0166],\n",
      "        [    0.0172],\n",
      "        [    0.0200],\n",
      "        [    0.0285],\n",
      "        [    0.0280],\n",
      "        [    0.0280],\n",
      "        [    0.0217],\n",
      "        [    0.0322],\n",
      "        [    0.0309],\n",
      "        [    0.0315],\n",
      "        [    0.0336],\n",
      "        [    0.0352],\n",
      "        [    0.0328],\n",
      "        [    0.0258],\n",
      "        [    0.0346],\n",
      "        [    0.0358],\n",
      "        [    0.0292],\n",
      "        [    0.0377],\n",
      "        [    0.0393],\n",
      "        [    0.0404],\n",
      "        [    0.0361],\n",
      "        [    0.0425],\n",
      "        [    0.0339],\n",
      "        [    0.0361],\n",
      "        [    0.0417],\n",
      "        [    0.0434],\n",
      "        [    0.0419],\n",
      "        [    0.0387],\n",
      "        [    0.0399],\n",
      "        [    0.0472],\n",
      "        [    0.0443],\n",
      "        [    0.0389],\n",
      "        [    0.0475],\n",
      "        [    0.0422],\n",
      "        [    0.0499],\n",
      "        [    0.0468],\n",
      "        [    0.0452],\n",
      "        [    0.0525],\n",
      "        [    0.0427],\n",
      "        [    0.0494],\n",
      "        [    0.0542],\n",
      "        [    0.0544],\n",
      "        [    0.0516],\n",
      "        [    0.0516],\n",
      "        [    0.0493],\n",
      "        [    0.0506],\n",
      "        [    0.0505],\n",
      "        [    0.0612],\n",
      "        [    0.0556],\n",
      "        [    0.0538],\n",
      "        [    0.0591],\n",
      "        [    0.0645],\n",
      "        [    0.0621],\n",
      "        [    0.0557],\n",
      "        [    0.0612],\n",
      "        [    0.0565],\n",
      "        [    0.0624],\n",
      "        [    0.0673],\n",
      "        [    0.0673],\n",
      "        [    0.0686],\n",
      "        [    0.0695],\n",
      "        [    0.0665],\n",
      "        [    0.0624],\n",
      "        [    0.0753],\n",
      "        [    0.0696],\n",
      "        [    0.0753],\n",
      "        [    0.0760],\n",
      "        [    0.0748],\n",
      "        [    0.0717],\n",
      "        [    0.0767],\n",
      "        [    0.0808],\n",
      "        [    0.0827],\n",
      "        [    0.0781],\n",
      "        [    0.0819],\n",
      "        [    0.0732],\n",
      "        [    0.0842],\n",
      "        [    0.0789],\n",
      "        [    0.0795],\n",
      "        [    0.0873],\n",
      "        [    0.0840],\n",
      "        [    0.0902],\n",
      "        [    0.0929],\n",
      "        [    0.0943],\n",
      "        [    0.0941],\n",
      "        [    0.0953],\n",
      "        [    0.1006],\n",
      "        [    0.1001],\n",
      "        [    0.1191],\n",
      "        [    0.1188],\n",
      "        [    0.1149],\n",
      "        [    0.1188],\n",
      "        [    0.1273],\n",
      "        [    0.1227],\n",
      "        [    0.1279],\n",
      "        [    0.1232],\n",
      "        [    0.1422],\n",
      "        [    0.1473],\n",
      "        [    0.1535],\n",
      "        [    0.1541],\n",
      "        [    0.1659],\n",
      "        [    0.1759],\n",
      "        [    0.1892],\n",
      "        [    0.1900]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 57.69702911376953\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.535394613318203e-11, 47)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [47, 91, 0, 46, 83, 1, 104, 87, 103, 150, 79, 39, 2, 48, 97, 78, 96, 98, 12, 45, 13, 99, 95, 22, 38, 84, 25, 10, 62, 82, 142, 102, 90, 72, 85, 33, 59, 89, 146, 70, 86, 50, 58, 107, 3, 134, 94, 141, 68, 133, 44, 92, 23, 148, 77, 113, 88, 76, 11, 49, 60, 4, 80, 32, 24, 100, 75, 143, 61, 93, 101, 71, 149, 129, 130, 51, 63, 21, 37, 30, 147, 52, 128, 140, 5, 158, 40, 105, 135, 116, 69, 31, 16, 117, 81, 157, 14, 138, 112, 57, 145, 9, 43, 139, 144, 67, 106, 53, 115, 41, 123, 156, 6, 122, 73, 74, 42, 34, 136, 26, 20, 119, 55, 8, 56, 114, 118, 131, 15, 35, 36, 7, 151, 54, 137, 121, 127, 108, 120, 132, 66, 29, 155, 109, 111, 124, 154, 28, 126, 125, 19, 17, 152, 18, 110, 153, 65, 64, 27] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6597],\n",
      "        [0.5908],\n",
      "        [0.8844],\n",
      "        [0.6798],\n",
      "        [0.6189],\n",
      "        [0.8644],\n",
      "        [0.4707],\n",
      "        [0.6133],\n",
      "        [0.4836],\n",
      "        [0.1659],\n",
      "        [0.6345],\n",
      "        [0.6871],\n",
      "        [0.8882],\n",
      "        [0.6303],\n",
      "        [0.5346],\n",
      "        [0.5871],\n",
      "        [0.5197],\n",
      "        [0.5290],\n",
      "        [0.8667],\n",
      "        [0.6756],\n",
      "        [0.8455],\n",
      "        [0.5241],\n",
      "        [0.5588],\n",
      "        [0.7490],\n",
      "        [0.6911],\n",
      "        [0.6459],\n",
      "        [0.7347],\n",
      "        [0.8700],\n",
      "        [0.6022],\n",
      "        [0.6294],\n",
      "        [0.2198],\n",
      "        [0.4849],\n",
      "        [0.5929],\n",
      "        [0.5704],\n",
      "        [0.6436],\n",
      "        [0.6460],\n",
      "        [0.6295],\n",
      "        [0.6113],\n",
      "        [0.2057],\n",
      "        [0.5345],\n",
      "        [0.6242],\n",
      "        [0.6235],\n",
      "        [0.6238],\n",
      "        [0.4226],\n",
      "        [0.8916],\n",
      "        [0.2442],\n",
      "        [0.5740],\n",
      "        [0.2105],\n",
      "        [0.4924],\n",
      "        [0.2519],\n",
      "        [0.6754],\n",
      "        [0.5790],\n",
      "        [0.7262],\n",
      "        [0.1705],\n",
      "        [0.5855],\n",
      "        [0.2690],\n",
      "        [0.6194],\n",
      "        [0.5858],\n",
      "        [0.8874],\n",
      "        [0.6194],\n",
      "        [0.6084],\n",
      "        [0.8779],\n",
      "        [0.6482],\n",
      "        [0.6528],\n",
      "        [0.7277],\n",
      "        [0.5014],\n",
      "        [0.5865],\n",
      "        [0.2218],\n",
      "        [0.6024],\n",
      "        [0.5950],\n",
      "        [0.4884],\n",
      "        [0.5753],\n",
      "        [0.1444],\n",
      "        [0.3182],\n",
      "        [0.2780],\n",
      "        [0.6214],\n",
      "        [0.6056],\n",
      "        [0.7844],\n",
      "        [0.6725],\n",
      "        [0.6734],\n",
      "        [0.1774],\n",
      "        [0.6173],\n",
      "        [0.3226],\n",
      "        [0.2262],\n",
      "        [0.8775],\n",
      "        [0.1182],\n",
      "        [0.6622],\n",
      "        [0.4276],\n",
      "        [0.2653],\n",
      "        [0.2610],\n",
      "        [0.4793],\n",
      "        [0.6078],\n",
      "        [0.8659],\n",
      "        [0.2577],\n",
      "        [0.6218],\n",
      "        [0.1196],\n",
      "        [0.8861],\n",
      "        [0.2392],\n",
      "        [0.3191],\n",
      "        [0.6196],\n",
      "        [0.2128],\n",
      "        [0.8661],\n",
      "        [0.6506],\n",
      "        [0.2364],\n",
      "        [0.2532],\n",
      "        [0.5518],\n",
      "        [0.4054],\n",
      "        [0.6182],\n",
      "        [0.2580],\n",
      "        [0.6540],\n",
      "        [0.3287],\n",
      "        [0.1171],\n",
      "        [0.8710],\n",
      "        [0.2970],\n",
      "        [0.5566],\n",
      "        [0.5497],\n",
      "        [0.6392],\n",
      "        [0.6473],\n",
      "        [0.2605],\n",
      "        [0.7348],\n",
      "        [0.8194],\n",
      "        [0.2832],\n",
      "        [0.6155],\n",
      "        [0.8800],\n",
      "        [0.6133],\n",
      "        [0.2260],\n",
      "        [0.2798],\n",
      "        [0.2326],\n",
      "        [0.8731],\n",
      "        [0.6546],\n",
      "        [0.6524],\n",
      "        [0.8767],\n",
      "        [0.2021],\n",
      "        [0.6124],\n",
      "        [0.2501],\n",
      "        [0.2683],\n",
      "        [0.3061],\n",
      "        [0.4205],\n",
      "        [0.2644],\n",
      "        [0.2300],\n",
      "        [0.5570],\n",
      "        [0.7150],\n",
      "        [0.1477],\n",
      "        [0.4056],\n",
      "        [0.3838],\n",
      "        [0.3319],\n",
      "        [0.1498],\n",
      "        [0.7445],\n",
      "        [0.3092],\n",
      "        [0.3243],\n",
      "        [0.8007],\n",
      "        [0.8511],\n",
      "        [0.1848],\n",
      "        [0.8033],\n",
      "        [0.3932],\n",
      "        [0.1759],\n",
      "        [0.6129],\n",
      "        [0.6105],\n",
      "        [0.7418]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0009],\n",
      "        [    0.0027],\n",
      "        [    0.0027],\n",
      "        [    0.0030],\n",
      "        [    0.0032],\n",
      "        [    0.0033],\n",
      "        [    0.0036],\n",
      "        [    0.0039],\n",
      "        [    0.0039],\n",
      "        [    0.0043],\n",
      "        [    0.0044],\n",
      "        [    0.0050],\n",
      "        [    0.0060],\n",
      "        [    0.0064],\n",
      "        [    0.0068],\n",
      "        [    0.0076],\n",
      "        [    0.0076],\n",
      "        [    0.0080],\n",
      "        [    0.0099],\n",
      "        [    0.0099],\n",
      "        [    0.0109],\n",
      "        [    0.0116],\n",
      "        [    0.0123],\n",
      "        [    0.0126],\n",
      "        [    0.0130],\n",
      "        [    0.0133],\n",
      "        [    0.0134],\n",
      "        [    0.0136],\n",
      "        [    0.0139],\n",
      "        [    0.0139],\n",
      "        [    0.0143],\n",
      "        [    0.0147],\n",
      "        [    0.0149],\n",
      "        [    0.0150],\n",
      "        [    0.0159],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0170],\n",
      "        [    0.0172],\n",
      "        [    0.0177],\n",
      "        [    0.0177],\n",
      "        [    0.0178],\n",
      "        [    0.0180],\n",
      "        [    0.0180],\n",
      "        [    0.0182],\n",
      "        [    0.0188],\n",
      "        [    0.0190],\n",
      "        [    0.0190],\n",
      "        [    0.0196],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0216],\n",
      "        [    0.0217],\n",
      "        [    0.0220],\n",
      "        [    0.0246],\n",
      "        [    0.0250],\n",
      "        [    0.0258],\n",
      "        [    0.0280],\n",
      "        [    0.0280],\n",
      "        [    0.0280],\n",
      "        [    0.0285],\n",
      "        [    0.0292],\n",
      "        [    0.0309],\n",
      "        [    0.0315],\n",
      "        [    0.0322],\n",
      "        [    0.0328],\n",
      "        [    0.0336],\n",
      "        [    0.0339],\n",
      "        [    0.0346],\n",
      "        [    0.0352],\n",
      "        [    0.0358],\n",
      "        [    0.0361],\n",
      "        [    0.0361],\n",
      "        [    0.0377],\n",
      "        [    0.0387],\n",
      "        [    0.0389],\n",
      "        [    0.0393],\n",
      "        [    0.0399],\n",
      "        [    0.0404],\n",
      "        [    0.0417],\n",
      "        [    0.0419],\n",
      "        [    0.0422],\n",
      "        [    0.0425],\n",
      "        [    0.0427],\n",
      "        [    0.0434],\n",
      "        [    0.0443],\n",
      "        [    0.0452],\n",
      "        [    0.0468],\n",
      "        [    0.0472],\n",
      "        [    0.0475],\n",
      "        [    0.0493],\n",
      "        [    0.0494],\n",
      "        [    0.0499],\n",
      "        [    0.0505],\n",
      "        [    0.0506],\n",
      "        [    0.0516],\n",
      "        [    0.0516],\n",
      "        [    0.0525],\n",
      "        [    0.0538],\n",
      "        [    0.0542],\n",
      "        [    0.0544],\n",
      "        [    0.0556],\n",
      "        [    0.0557],\n",
      "        [    0.0565],\n",
      "        [    0.0591],\n",
      "        [    0.0612],\n",
      "        [    0.0612],\n",
      "        [    0.0621],\n",
      "        [    0.0624],\n",
      "        [    0.0624],\n",
      "        [    0.0645],\n",
      "        [    0.0665],\n",
      "        [    0.0673],\n",
      "        [    0.0673],\n",
      "        [    0.0686],\n",
      "        [    0.0695],\n",
      "        [    0.0696],\n",
      "        [    0.0717],\n",
      "        [    0.0732],\n",
      "        [    0.0748],\n",
      "        [    0.0753],\n",
      "        [    0.0753],\n",
      "        [    0.0760],\n",
      "        [    0.0767],\n",
      "        [    0.0781],\n",
      "        [    0.0789],\n",
      "        [    0.0795],\n",
      "        [    0.0808],\n",
      "        [    0.0819],\n",
      "        [    0.0827],\n",
      "        [    0.0840],\n",
      "        [    0.0842],\n",
      "        [    0.0873],\n",
      "        [    0.0902],\n",
      "        [    0.0929],\n",
      "        [    0.0941],\n",
      "        [    0.0943],\n",
      "        [    0.0953],\n",
      "        [    0.1001],\n",
      "        [    0.1006],\n",
      "        [    0.1149],\n",
      "        [    0.1188],\n",
      "        [    0.1188],\n",
      "        [    0.1191],\n",
      "        [    0.1227],\n",
      "        [    0.1232],\n",
      "        [    0.1273],\n",
      "        [    0.1279],\n",
      "        [    0.1422],\n",
      "        [    0.1473],\n",
      "        [    0.1535],\n",
      "        [    0.1541],\n",
      "        [    0.1659],\n",
      "        [    0.1759],\n",
      "        [    0.1892],\n",
      "        [    0.1900],\n",
      "        [    0.2807]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0024],\n",
      "        [    0.0008],\n",
      "        [    0.0046],\n",
      "        [    0.0001],\n",
      "        [    0.0035],\n",
      "        [    0.0082],\n",
      "        [    0.0021],\n",
      "        [    0.0035],\n",
      "        [    0.0032],\n",
      "        [    0.0030],\n",
      "        [    0.0021],\n",
      "        [    0.0075],\n",
      "        [    0.0094],\n",
      "        [    0.0069],\n",
      "        [    0.0047],\n",
      "        [    0.0050],\n",
      "        [    0.0049],\n",
      "        [    0.0069],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0074],\n",
      "        [    0.0094],\n",
      "        [    0.0124],\n",
      "        [    0.0148],\n",
      "        [    0.0155],\n",
      "        [    0.0114],\n",
      "        [    0.0105],\n",
      "        [    0.0165],\n",
      "        [    0.0114],\n",
      "        [    0.0147],\n",
      "        [    0.0136],\n",
      "        [    0.0143],\n",
      "        [    0.0136],\n",
      "        [    0.0167],\n",
      "        [    0.0138],\n",
      "        [    0.0180],\n",
      "        [    0.0177],\n",
      "        [    0.0163],\n",
      "        [    0.0152],\n",
      "        [    0.0183],\n",
      "        [    0.0164],\n",
      "        [    0.0192],\n",
      "        [    0.0197],\n",
      "        [    0.0159],\n",
      "        [    0.0229],\n",
      "        [    0.0163],\n",
      "        [    0.0193],\n",
      "        [    0.0196],\n",
      "        [    0.0199],\n",
      "        [    0.0170],\n",
      "        [    0.0225],\n",
      "        [    0.0210],\n",
      "        [    0.0229],\n",
      "        [    0.0224],\n",
      "        [    0.0210],\n",
      "        [    0.0201],\n",
      "        [    0.0246],\n",
      "        [    0.0260],\n",
      "        [    0.0221],\n",
      "        [    0.0296],\n",
      "        [    0.0294],\n",
      "        [    0.0321],\n",
      "        [    0.0300],\n",
      "        [    0.0263],\n",
      "        [    0.0333],\n",
      "        [    0.0312],\n",
      "        [    0.0336],\n",
      "        [    0.0329],\n",
      "        [    0.0350],\n",
      "        [    0.0346],\n",
      "        [    0.0340],\n",
      "        [    0.0371],\n",
      "        [    0.0362],\n",
      "        [    0.0340],\n",
      "        [    0.0329],\n",
      "        [    0.0392],\n",
      "        [    0.0370],\n",
      "        [    0.0353],\n",
      "        [    0.0424],\n",
      "        [    0.0374],\n",
      "        [    0.0419],\n",
      "        [    0.0431],\n",
      "        [    0.0401],\n",
      "        [    0.0424],\n",
      "        [    0.0460],\n",
      "        [    0.0407],\n",
      "        [    0.0459],\n",
      "        [    0.0422],\n",
      "        [    0.0465],\n",
      "        [    0.0450],\n",
      "        [    0.0481],\n",
      "        [    0.0494],\n",
      "        [    0.0463],\n",
      "        [    0.0475],\n",
      "        [    0.0512],\n",
      "        [    0.0486],\n",
      "        [    0.0477],\n",
      "        [    0.0526],\n",
      "        [    0.0529],\n",
      "        [    0.0549],\n",
      "        [    0.0528],\n",
      "        [    0.0572],\n",
      "        [    0.0569],\n",
      "        [    0.0563],\n",
      "        [    0.0547],\n",
      "        [    0.0550],\n",
      "        [    0.0571],\n",
      "        [    0.0632],\n",
      "        [    0.0597],\n",
      "        [    0.0640],\n",
      "        [    0.0614],\n",
      "        [    0.0608],\n",
      "        [    0.0675],\n",
      "        [    0.0650],\n",
      "        [    0.0689],\n",
      "        [    0.0688],\n",
      "        [    0.0705],\n",
      "        [    0.0725],\n",
      "        [    0.0712],\n",
      "        [    0.0687],\n",
      "        [    0.0696],\n",
      "        [    0.0730],\n",
      "        [    0.0773],\n",
      "        [    0.0786],\n",
      "        [    0.0782],\n",
      "        [    0.0744],\n",
      "        [    0.0760],\n",
      "        [    0.0751],\n",
      "        [    0.0769],\n",
      "        [    0.0839],\n",
      "        [    0.0849],\n",
      "        [    0.0858],\n",
      "        [    0.0823],\n",
      "        [    0.0864],\n",
      "        [    0.0883],\n",
      "        [    0.0886],\n",
      "        [    0.0914],\n",
      "        [    0.0961],\n",
      "        [    0.0926],\n",
      "        [    0.0920],\n",
      "        [    0.0983],\n",
      "        [    0.0978],\n",
      "        [    0.1133],\n",
      "        [    0.1209],\n",
      "        [    0.1199],\n",
      "        [    0.1182],\n",
      "        [    0.1219],\n",
      "        [    0.1200],\n",
      "        [    0.1260],\n",
      "        [    0.1266],\n",
      "        [    0.1392],\n",
      "        [    0.1438],\n",
      "        [    0.1520],\n",
      "        [    0.1510],\n",
      "        [    0.1674],\n",
      "        [    0.1745],\n",
      "        [    0.1868],\n",
      "        [    0.1882],\n",
      "        [    0.2777]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 57.98351526260376\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 14 個區塊累積花費時間(s) 1.2137880325317383\n",
      "<<The performance of 14 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2137880325317383\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1294.89\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1725.00\n",
      "The accuracy(2000) for l = 1: 80.50%\n",
      "The accuracy(3000) for l = 1: 89.94%\n",
      "The maximum error: tensor(7088.7227)\n",
      "The minimum error: tensor(3.)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 2141.7\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 2205.1\n",
      "The accuracy(2000) for l = 1: 50.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 2724.86328125\n",
      "The minimum error: 1364.6328125\n",
      "------------------------------------------------------------\n",
      "0.8050314465408805\n",
      "<class 'float'>\n",
      "0.5\n",
      "<class 'float'>\n",
      "The <<15>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.3801727760665017e-08, 42)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [42, 87, 100, 75, 43, 146, 99, 83, 79, 93, 92, 74, 94, 44, 9, 35, 95, 21, 41, 8, 58, 80, 91, 138, 86, 81, 98, 78, 18, 142, 34, 103, 85, 130, 82, 6, 68, 129, 55, 29, 66, 46, 90, 137, 54, 64, 109, 73, 88, 7, 144, 40, 19, 84, 72, 28, 56, 45, 76, 96, 0, 139, 126, 20, 71, 125, 97, 89, 57, 17, 145, 59, 67, 26, 47, 124, 154, 143, 101, 136, 33, 48, 112, 36, 1, 12, 131, 113, 10, 65, 153, 27, 77, 134, 141, 108, 155, 140, 53, 63, 135, 39, 102, 5, 111, 152, 119, 49, 37, 118, 2, 22, 70, 69, 16, 38, 132, 30, 115, 110, 127, 114, 11, 158, 51, 52, 4, 147, 31, 32, 3, 50, 133, 117, 123, 128, 116, 104, 25, 156, 62, 157, 151, 120, 107, 24, 105, 150, 122, 121, 15, 13, 14, 148, 106]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0008],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0030],\n",
      "        [    0.0032],\n",
      "        [    0.0035],\n",
      "        [    0.0035],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0050],\n",
      "        [    0.0069],\n",
      "        [    0.0069],\n",
      "        [    0.0074],\n",
      "        [    0.0075],\n",
      "        [    0.0094],\n",
      "        [    0.0105],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0114],\n",
      "        [    0.0114],\n",
      "        [    0.0124],\n",
      "        [    0.0136],\n",
      "        [    0.0136],\n",
      "        [    0.0138],\n",
      "        [    0.0143],\n",
      "        [    0.0147],\n",
      "        [    0.0148],\n",
      "        [    0.0152],\n",
      "        [    0.0155],\n",
      "        [    0.0159],\n",
      "        [    0.0163],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0165],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0177],\n",
      "        [    0.0180],\n",
      "        [    0.0183],\n",
      "        [    0.0192],\n",
      "        [    0.0193],\n",
      "        [    0.0196],\n",
      "        [    0.0197],\n",
      "        [    0.0199],\n",
      "        [    0.0201],\n",
      "        [    0.0210],\n",
      "        [    0.0210],\n",
      "        [    0.0221],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0246],\n",
      "        [    0.0260],\n",
      "        [    0.0263],\n",
      "        [    0.0294],\n",
      "        [    0.0296],\n",
      "        [    0.0300],\n",
      "        [    0.0312],\n",
      "        [    0.0321],\n",
      "        [    0.0329],\n",
      "        [    0.0329],\n",
      "        [    0.0333],\n",
      "        [    0.0336],\n",
      "        [    0.0340],\n",
      "        [    0.0340],\n",
      "        [    0.0346],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0362],\n",
      "        [    0.0370],\n",
      "        [    0.0371],\n",
      "        [    0.0374],\n",
      "        [    0.0392],\n",
      "        [    0.0401],\n",
      "        [    0.0407],\n",
      "        [    0.0419],\n",
      "        [    0.0422],\n",
      "        [    0.0424],\n",
      "        [    0.0424],\n",
      "        [    0.0431],\n",
      "        [    0.0450],\n",
      "        [    0.0459],\n",
      "        [    0.0460],\n",
      "        [    0.0463],\n",
      "        [    0.0465],\n",
      "        [    0.0475],\n",
      "        [    0.0477],\n",
      "        [    0.0481],\n",
      "        [    0.0486],\n",
      "        [    0.0494],\n",
      "        [    0.0512],\n",
      "        [    0.0526],\n",
      "        [    0.0528],\n",
      "        [    0.0529],\n",
      "        [    0.0535],\n",
      "        [    0.0547],\n",
      "        [    0.0549],\n",
      "        [    0.0550],\n",
      "        [    0.0563],\n",
      "        [    0.0569],\n",
      "        [    0.0571],\n",
      "        [    0.0572],\n",
      "        [    0.0597],\n",
      "        [    0.0608],\n",
      "        [    0.0614],\n",
      "        [    0.0632],\n",
      "        [    0.0640],\n",
      "        [    0.0650],\n",
      "        [    0.0675],\n",
      "        [    0.0687],\n",
      "        [    0.0688],\n",
      "        [    0.0689],\n",
      "        [    0.0696],\n",
      "        [    0.0705],\n",
      "        [    0.0712],\n",
      "        [    0.0725],\n",
      "        [    0.0730],\n",
      "        [    0.0744],\n",
      "        [    0.0751],\n",
      "        [    0.0760],\n",
      "        [    0.0769],\n",
      "        [    0.0773],\n",
      "        [    0.0773],\n",
      "        [    0.0782],\n",
      "        [    0.0786],\n",
      "        [    0.0823],\n",
      "        [    0.0839],\n",
      "        [    0.0849],\n",
      "        [    0.0858],\n",
      "        [    0.0864],\n",
      "        [    0.0883],\n",
      "        [    0.0886],\n",
      "        [    0.0914],\n",
      "        [    0.0920],\n",
      "        [    0.0926],\n",
      "        [    0.0961],\n",
      "        [    0.0978],\n",
      "        [    0.0981],\n",
      "        [    0.0983],\n",
      "        [    0.1067],\n",
      "        [    0.1133],\n",
      "        [    0.1182],\n",
      "        [    0.1199],\n",
      "        [    0.1200],\n",
      "        [    0.1209],\n",
      "        [    0.1219],\n",
      "        [    0.1260],\n",
      "        [    0.1266],\n",
      "        [    0.1392],\n",
      "        [    0.1438],\n",
      "        [    0.1510],\n",
      "        [    0.1520],\n",
      "        [    0.1674]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.3801727760665017e-08, 42)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [42, 87, 100, 75, 43, 146, 99, 83, 79, 93, 92, 74, 94, 44, 9, 35, 95, 21, 41, 8, 58, 80, 91, 138, 86, 81, 98, 78, 18, 142, 34, 103, 85, 130, 82, 6, 68, 129, 55, 29, 66, 46, 90, 137, 54, 64, 109, 73, 88, 7, 144, 40, 19, 84, 72, 28, 56, 45, 76, 96, 0, 139, 126, 20, 71, 125, 97, 89, 57, 17, 145, 59, 67, 26, 47, 124, 154, 143, 101, 136, 33, 48, 112, 36, 1, 12, 131, 113, 10, 65, 153, 27, 77, 134, 141, 108, 155, 140, 53, 63, 135, 39, 102, 5, 111, 152, 119, 49, 37, 118, 2, 22, 70, 69, 16, 38, 132, 30, 115, 110, 127, 114, 11, 158, 51, 52, 4, 147, 31, 32, 3, 50, 133, 117, 123, 128, 116, 104, 25, 156, 62, 157, 151, 120, 107, 24, 105, 150, 122, 121, 15, 13, 14, 148, 106, 149] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6770],\n",
      "        [0.5914],\n",
      "        [0.4719],\n",
      "        [0.6327],\n",
      "        [0.6573],\n",
      "        [0.1651],\n",
      "        [0.4841],\n",
      "        [0.6131],\n",
      "        [0.6180],\n",
      "        [0.5359],\n",
      "        [0.5215],\n",
      "        [0.5857],\n",
      "        [0.5298],\n",
      "        [0.6284],\n",
      "        [0.8430],\n",
      "        [0.6839],\n",
      "        [0.5246],\n",
      "        [0.7323],\n",
      "        [0.6727],\n",
      "        [0.8635],\n",
      "        [0.6002],\n",
      "        [0.6448],\n",
      "        [0.5603],\n",
      "        [0.2200],\n",
      "        [0.5936],\n",
      "        [0.6425],\n",
      "        [0.4853],\n",
      "        [0.6283],\n",
      "        [0.7458],\n",
      "        [0.2043],\n",
      "        [0.6879],\n",
      "        [0.4245],\n",
      "        [0.6115],\n",
      "        [0.2459],\n",
      "        [0.6234],\n",
      "        [0.8669],\n",
      "        [0.5685],\n",
      "        [0.2540],\n",
      "        [0.6276],\n",
      "        [0.6430],\n",
      "        [0.5332],\n",
      "        [0.6220],\n",
      "        [0.5752],\n",
      "        [0.2113],\n",
      "        [0.6218],\n",
      "        [0.4915],\n",
      "        [0.2708],\n",
      "        [0.5847],\n",
      "        [0.5800],\n",
      "        [0.8838],\n",
      "        [0.1697],\n",
      "        [0.6725],\n",
      "        [0.7234],\n",
      "        [0.6193],\n",
      "        [0.5848],\n",
      "        [0.6499],\n",
      "        [0.6070],\n",
      "        [0.6178],\n",
      "        [0.6467],\n",
      "        [0.5017],\n",
      "        [0.8738],\n",
      "        [0.2217],\n",
      "        [0.2813],\n",
      "        [0.7253],\n",
      "        [0.5852],\n",
      "        [0.3203],\n",
      "        [0.4890],\n",
      "        [0.5956],\n",
      "        [0.6009],\n",
      "        [0.7809],\n",
      "        [0.1440],\n",
      "        [0.6039],\n",
      "        [0.5734],\n",
      "        [0.6709],\n",
      "        [0.6200],\n",
      "        [0.3244],\n",
      "        [0.1162],\n",
      "        [0.1760],\n",
      "        [0.4297],\n",
      "        [0.2264],\n",
      "        [0.6693],\n",
      "        [0.6159],\n",
      "        [0.2628],\n",
      "        [0.6597],\n",
      "        [0.8740],\n",
      "        [0.8629],\n",
      "        [0.2666],\n",
      "        [0.2595],\n",
      "        [0.8832],\n",
      "        [0.4785],\n",
      "        [0.1178],\n",
      "        [0.6059],\n",
      "        [0.6205],\n",
      "        [0.2402],\n",
      "        [0.2119],\n",
      "        [0.3204],\n",
      "        [0.1217],\n",
      "        [0.2523],\n",
      "        [0.6171],\n",
      "        [0.5503],\n",
      "        [0.2371],\n",
      "        [0.6481],\n",
      "        [0.4075],\n",
      "        [0.8631],\n",
      "        [0.2596],\n",
      "        [0.1156],\n",
      "        [0.3297],\n",
      "        [0.6162],\n",
      "        [0.6521],\n",
      "        [0.2984],\n",
      "        [0.8680],\n",
      "        [0.7317],\n",
      "        [0.5483],\n",
      "        [0.5549],\n",
      "        [0.8157],\n",
      "        [0.6372],\n",
      "        [0.2620],\n",
      "        [0.6442],\n",
      "        [0.2851],\n",
      "        [0.2283],\n",
      "        [0.2363],\n",
      "        [0.2819],\n",
      "        [0.8705],\n",
      "        [0.1087],\n",
      "        [0.6134],\n",
      "        [0.6111],\n",
      "        [0.8766],\n",
      "        [0.2004],\n",
      "        [0.6516],\n",
      "        [0.6493],\n",
      "        [0.8735],\n",
      "        [0.6102],\n",
      "        [0.2510],\n",
      "        [0.2700],\n",
      "        [0.3075],\n",
      "        [0.2333],\n",
      "        [0.2661],\n",
      "        [0.4225],\n",
      "        [0.7121],\n",
      "        [0.1220],\n",
      "        [0.5552],\n",
      "        [0.1276],\n",
      "        [0.1462],\n",
      "        [0.3329],\n",
      "        [0.3849],\n",
      "        [0.7413],\n",
      "        [0.4076],\n",
      "        [0.1490],\n",
      "        [0.3105],\n",
      "        [0.3256],\n",
      "        [0.7977],\n",
      "        [0.8476],\n",
      "        [0.8003],\n",
      "        [0.1833],\n",
      "        [0.3948],\n",
      "        [0.1745]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0008],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0030],\n",
      "        [    0.0032],\n",
      "        [    0.0035],\n",
      "        [    0.0035],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0050],\n",
      "        [    0.0069],\n",
      "        [    0.0069],\n",
      "        [    0.0074],\n",
      "        [    0.0075],\n",
      "        [    0.0094],\n",
      "        [    0.0105],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0114],\n",
      "        [    0.0114],\n",
      "        [    0.0124],\n",
      "        [    0.0136],\n",
      "        [    0.0136],\n",
      "        [    0.0138],\n",
      "        [    0.0143],\n",
      "        [    0.0147],\n",
      "        [    0.0148],\n",
      "        [    0.0152],\n",
      "        [    0.0155],\n",
      "        [    0.0159],\n",
      "        [    0.0163],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0165],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0177],\n",
      "        [    0.0180],\n",
      "        [    0.0183],\n",
      "        [    0.0192],\n",
      "        [    0.0193],\n",
      "        [    0.0196],\n",
      "        [    0.0197],\n",
      "        [    0.0199],\n",
      "        [    0.0201],\n",
      "        [    0.0210],\n",
      "        [    0.0210],\n",
      "        [    0.0221],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0246],\n",
      "        [    0.0260],\n",
      "        [    0.0263],\n",
      "        [    0.0294],\n",
      "        [    0.0296],\n",
      "        [    0.0300],\n",
      "        [    0.0312],\n",
      "        [    0.0321],\n",
      "        [    0.0329],\n",
      "        [    0.0329],\n",
      "        [    0.0333],\n",
      "        [    0.0336],\n",
      "        [    0.0340],\n",
      "        [    0.0340],\n",
      "        [    0.0346],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0362],\n",
      "        [    0.0370],\n",
      "        [    0.0371],\n",
      "        [    0.0374],\n",
      "        [    0.0392],\n",
      "        [    0.0401],\n",
      "        [    0.0407],\n",
      "        [    0.0419],\n",
      "        [    0.0422],\n",
      "        [    0.0424],\n",
      "        [    0.0424],\n",
      "        [    0.0431],\n",
      "        [    0.0450],\n",
      "        [    0.0459],\n",
      "        [    0.0460],\n",
      "        [    0.0463],\n",
      "        [    0.0465],\n",
      "        [    0.0475],\n",
      "        [    0.0477],\n",
      "        [    0.0481],\n",
      "        [    0.0486],\n",
      "        [    0.0494],\n",
      "        [    0.0512],\n",
      "        [    0.0526],\n",
      "        [    0.0528],\n",
      "        [    0.0529],\n",
      "        [    0.0535],\n",
      "        [    0.0547],\n",
      "        [    0.0549],\n",
      "        [    0.0550],\n",
      "        [    0.0563],\n",
      "        [    0.0569],\n",
      "        [    0.0571],\n",
      "        [    0.0572],\n",
      "        [    0.0597],\n",
      "        [    0.0608],\n",
      "        [    0.0614],\n",
      "        [    0.0632],\n",
      "        [    0.0640],\n",
      "        [    0.0650],\n",
      "        [    0.0675],\n",
      "        [    0.0687],\n",
      "        [    0.0688],\n",
      "        [    0.0689],\n",
      "        [    0.0696],\n",
      "        [    0.0705],\n",
      "        [    0.0712],\n",
      "        [    0.0725],\n",
      "        [    0.0730],\n",
      "        [    0.0744],\n",
      "        [    0.0751],\n",
      "        [    0.0760],\n",
      "        [    0.0769],\n",
      "        [    0.0773],\n",
      "        [    0.0773],\n",
      "        [    0.0782],\n",
      "        [    0.0786],\n",
      "        [    0.0823],\n",
      "        [    0.0839],\n",
      "        [    0.0849],\n",
      "        [    0.0858],\n",
      "        [    0.0864],\n",
      "        [    0.0883],\n",
      "        [    0.0886],\n",
      "        [    0.0914],\n",
      "        [    0.0920],\n",
      "        [    0.0926],\n",
      "        [    0.0961],\n",
      "        [    0.0978],\n",
      "        [    0.0981],\n",
      "        [    0.0983],\n",
      "        [    0.1067],\n",
      "        [    0.1133],\n",
      "        [    0.1182],\n",
      "        [    0.1199],\n",
      "        [    0.1200],\n",
      "        [    0.1209],\n",
      "        [    0.1219],\n",
      "        [    0.1260],\n",
      "        [    0.1266],\n",
      "        [    0.1392],\n",
      "        [    0.1438],\n",
      "        [    0.1510],\n",
      "        [    0.1520],\n",
      "        [    0.1674],\n",
      "        [    0.1745]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0063],\n",
      "        [0.0055],\n",
      "        [0.0002],\n",
      "        [0.0055],\n",
      "        [0.0040],\n",
      "        [0.0077],\n",
      "        [0.0020],\n",
      "        [0.0010],\n",
      "        [0.0002],\n",
      "        [0.0011],\n",
      "        [0.0009],\n",
      "        [0.0077],\n",
      "        [0.0040],\n",
      "        [0.0006],\n",
      "        [0.0175],\n",
      "        [0.0017],\n",
      "        [0.0071],\n",
      "        [0.0186],\n",
      "        [0.0047],\n",
      "        [0.0012],\n",
      "        [0.0159],\n",
      "        [0.0153],\n",
      "        [0.0172],\n",
      "        [0.0213],\n",
      "        [0.0088],\n",
      "        [0.0178],\n",
      "        [0.0156],\n",
      "        [0.0109],\n",
      "        [0.0075],\n",
      "        [0.0042],\n",
      "        [0.0097],\n",
      "        [0.0146],\n",
      "        [0.0116],\n",
      "        [0.0206],\n",
      "        [0.0204],\n",
      "        [0.0070],\n",
      "        [0.0154],\n",
      "        [0.0204],\n",
      "        [0.0120],\n",
      "        [0.0132],\n",
      "        [0.0165],\n",
      "        [0.0125],\n",
      "        [0.0243],\n",
      "        [0.0123],\n",
      "        [0.0144],\n",
      "        [0.0178],\n",
      "        [0.0234],\n",
      "        [0.0243],\n",
      "        [0.0259],\n",
      "        [0.0314],\n",
      "        [0.0330],\n",
      "        [0.0166],\n",
      "        [0.0157],\n",
      "        [0.0198],\n",
      "        [0.0229],\n",
      "        [0.0313],\n",
      "        [0.0237],\n",
      "        [0.0234],\n",
      "        [0.0260],\n",
      "        [0.0298],\n",
      "        [0.0251],\n",
      "        [0.0411],\n",
      "        [0.0339],\n",
      "        [0.0251],\n",
      "        [0.0309],\n",
      "        [0.0358],\n",
      "        [0.0326],\n",
      "        [0.0395],\n",
      "        [0.0295],\n",
      "        [0.0428],\n",
      "        [0.0471],\n",
      "        [0.0418],\n",
      "        [0.0356],\n",
      "        [0.0441],\n",
      "        [0.0328],\n",
      "        [0.0422],\n",
      "        [0.0252],\n",
      "        [0.0534],\n",
      "        [0.0403],\n",
      "        [0.0350],\n",
      "        [0.0369],\n",
      "        [0.0369],\n",
      "        [0.0500],\n",
      "        [0.0395],\n",
      "        [0.0380],\n",
      "        [0.0560],\n",
      "        [0.0421],\n",
      "        [0.0525],\n",
      "        [0.0580],\n",
      "        [0.0467],\n",
      "        [0.0338],\n",
      "        [0.0443],\n",
      "        [0.0478],\n",
      "        [0.0468],\n",
      "        [0.0427],\n",
      "        [0.0503],\n",
      "        [0.0370],\n",
      "        [0.0461],\n",
      "        [0.0504],\n",
      "        [0.0583],\n",
      "        [0.0500],\n",
      "        [0.0514],\n",
      "        [0.0558],\n",
      "        [0.0478],\n",
      "        [0.0644],\n",
      "        [0.0468],\n",
      "        [0.0651],\n",
      "        [0.0581],\n",
      "        [0.0572],\n",
      "        [0.0687],\n",
      "        [0.0589],\n",
      "        [0.0761],\n",
      "        [0.0672],\n",
      "        [0.0672],\n",
      "        [0.0779],\n",
      "        [0.0644],\n",
      "        [0.0669],\n",
      "        [0.0677],\n",
      "        [0.0768],\n",
      "        [0.0785],\n",
      "        [0.0765],\n",
      "        [0.0799],\n",
      "        [0.0874],\n",
      "        [0.0604],\n",
      "        [0.0724],\n",
      "        [0.0734],\n",
      "        [0.0695],\n",
      "        [0.0710],\n",
      "        [0.0787],\n",
      "        [0.0799],\n",
      "        [0.0768],\n",
      "        [0.0816],\n",
      "        [0.0829],\n",
      "        [0.0929],\n",
      "        [0.0943],\n",
      "        [0.0940],\n",
      "        [0.0969],\n",
      "        [0.0974],\n",
      "        [0.1053],\n",
      "        [0.0816],\n",
      "        [0.1015],\n",
      "        [0.0903],\n",
      "        [0.1004],\n",
      "        [0.1215],\n",
      "        [0.1189],\n",
      "        [0.1278],\n",
      "        [0.1217],\n",
      "        [0.1103],\n",
      "        [0.1292],\n",
      "        [0.1294],\n",
      "        [0.1478],\n",
      "        [0.1526],\n",
      "        [0.1595],\n",
      "        [0.1403],\n",
      "        [0.1672],\n",
      "        [0.1628]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 58.50907611846924\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.656191438494716e-08, 79)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [79, 100, 44, 92, 83, 93, 8, 35, 99, 43, 94, 142, 41, 87, 75, 42, 6, 95, 18, 74, 146, 86, 34, 78, 85, 55, 137, 46, 29, 54, 103, 80, 68, 98, 19, 58, 66, 40, 91, 9, 64, 81, 21, 84, 82, 129, 130, 138, 72, 109, 45, 56, 90, 73, 20, 0, 154, 88, 76, 57, 96, 71, 28, 7, 97, 47, 144, 153, 126, 136, 67, 125, 48, 33, 155, 1, 89, 36, 101, 139, 59, 131, 124, 141, 17, 26, 27, 140, 65, 152, 134, 145, 77, 5, 112, 135, 108, 53, 39, 113, 143, 102, 12, 37, 10, 49, 63, 2, 158, 38, 111, 119, 132, 70, 69, 30, 118, 4, 147, 51, 52, 22, 127, 3, 115, 16, 110, 31, 32, 114, 156, 50, 133, 11, 157, 117, 128, 123, 116, 104, 151, 62, 25, 150, 107, 120, 105, 24, 122, 121, 148, 15, 13, 14, 149, 106, 61] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6218],\n",
      "        [0.4737],\n",
      "        [0.6347],\n",
      "        [0.5255],\n",
      "        [0.6176],\n",
      "        [0.5395],\n",
      "        [0.8731],\n",
      "        [0.6897],\n",
      "        [0.4853],\n",
      "        [0.6637],\n",
      "        [0.5326],\n",
      "        [0.1933],\n",
      "        [0.6789],\n",
      "        [0.5961],\n",
      "        [0.6360],\n",
      "        [0.6835],\n",
      "        [0.8763],\n",
      "        [0.5269],\n",
      "        [0.7531],\n",
      "        [0.5884],\n",
      "        [0.1543],\n",
      "        [0.5985],\n",
      "        [0.6937],\n",
      "        [0.6320],\n",
      "        [0.6162],\n",
      "        [0.6333],\n",
      "        [0.2040],\n",
      "        [0.6286],\n",
      "        [0.6478],\n",
      "        [0.6272],\n",
      "        [0.4258],\n",
      "        [0.6487],\n",
      "        [0.5697],\n",
      "        [0.4865],\n",
      "        [0.7306],\n",
      "        [0.6047],\n",
      "        [0.5349],\n",
      "        [0.6784],\n",
      "        [0.5652],\n",
      "        [0.8531],\n",
      "        [0.4936],\n",
      "        [0.6464],\n",
      "        [0.7404],\n",
      "        [0.6242],\n",
      "        [0.6273],\n",
      "        [0.2506],\n",
      "        [0.2416],\n",
      "        [0.2123],\n",
      "        [0.5879],\n",
      "        [0.2676],\n",
      "        [0.6240],\n",
      "        [0.6127],\n",
      "        [0.5801],\n",
      "        [0.5881],\n",
      "        [0.7335],\n",
      "        [0.8808],\n",
      "        [0.1007],\n",
      "        [0.5849],\n",
      "        [0.6507],\n",
      "        [0.6065],\n",
      "        [0.5031],\n",
      "        [0.5878],\n",
      "        [0.6549],\n",
      "        [0.8931],\n",
      "        [0.4905],\n",
      "        [0.6264],\n",
      "        [0.1591],\n",
      "        [0.1029],\n",
      "        [0.2803],\n",
      "        [0.2191],\n",
      "        [0.5749],\n",
      "        [0.3185],\n",
      "        [0.6221],\n",
      "        [0.6748],\n",
      "        [0.1053],\n",
      "        [0.8820],\n",
      "        [0.6005],\n",
      "        [0.6662],\n",
      "        [0.4317],\n",
      "        [0.2135],\n",
      "        [0.6087],\n",
      "        [0.2622],\n",
      "        [0.3223],\n",
      "        [0.2018],\n",
      "        [0.7883],\n",
      "        [0.6776],\n",
      "        [0.6110],\n",
      "        [0.2436],\n",
      "        [0.4798],\n",
      "        [0.1015],\n",
      "        [0.2344],\n",
      "        [0.1331],\n",
      "        [0.6239],\n",
      "        [0.8725],\n",
      "        [0.2578],\n",
      "        [0.2308],\n",
      "        [0.3177],\n",
      "        [0.6216],\n",
      "        [0.6537],\n",
      "        [0.2545],\n",
      "        [0.1644],\n",
      "        [0.4087],\n",
      "        [0.8727],\n",
      "        [0.6589],\n",
      "        [0.8935],\n",
      "        [0.6213],\n",
      "        [0.5536],\n",
      "        [0.8765],\n",
      "        [0.0918],\n",
      "        [0.6433],\n",
      "        [0.2548],\n",
      "        [0.3260],\n",
      "        [0.2578],\n",
      "        [0.5499],\n",
      "        [0.5566],\n",
      "        [0.6490],\n",
      "        [0.2947],\n",
      "        [0.8857],\n",
      "        [0.1891],\n",
      "        [0.6183],\n",
      "        [0.6159],\n",
      "        [0.7391],\n",
      "        [0.2349],\n",
      "        [0.8826],\n",
      "        [0.2812],\n",
      "        [0.8241],\n",
      "        [0.2241],\n",
      "        [0.6568],\n",
      "        [0.6544],\n",
      "        [0.2779],\n",
      "        [0.1055],\n",
      "        [0.6150],\n",
      "        [0.2457],\n",
      "        [0.8810],\n",
      "        [0.1112],\n",
      "        [0.2657],\n",
      "        [0.2312],\n",
      "        [0.3046],\n",
      "        [0.2618],\n",
      "        [0.4237],\n",
      "        [0.1333],\n",
      "        [0.5584],\n",
      "        [0.7197],\n",
      "        [0.1374],\n",
      "        [0.3838],\n",
      "        [0.3295],\n",
      "        [0.4085],\n",
      "        [0.7492],\n",
      "        [0.3073],\n",
      "        [0.3227],\n",
      "        [0.1716],\n",
      "        [0.8063],\n",
      "        [0.8564],\n",
      "        [0.8087],\n",
      "        [0.1628],\n",
      "        [0.3946],\n",
      "        [0.6142]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0040],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0063],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0088],\n",
      "        [0.0097],\n",
      "        [0.0109],\n",
      "        [0.0116],\n",
      "        [0.0120],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0132],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0157],\n",
      "        [0.0159],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0172],\n",
      "        [0.0175],\n",
      "        [0.0178],\n",
      "        [0.0178],\n",
      "        [0.0186],\n",
      "        [0.0198],\n",
      "        [0.0204],\n",
      "        [0.0204],\n",
      "        [0.0206],\n",
      "        [0.0213],\n",
      "        [0.0229],\n",
      "        [0.0234],\n",
      "        [0.0234],\n",
      "        [0.0237],\n",
      "        [0.0243],\n",
      "        [0.0243],\n",
      "        [0.0251],\n",
      "        [0.0251],\n",
      "        [0.0252],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0295],\n",
      "        [0.0298],\n",
      "        [0.0309],\n",
      "        [0.0313],\n",
      "        [0.0314],\n",
      "        [0.0326],\n",
      "        [0.0328],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0339],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0358],\n",
      "        [0.0369],\n",
      "        [0.0369],\n",
      "        [0.0370],\n",
      "        [0.0380],\n",
      "        [0.0395],\n",
      "        [0.0395],\n",
      "        [0.0403],\n",
      "        [0.0411],\n",
      "        [0.0418],\n",
      "        [0.0421],\n",
      "        [0.0422],\n",
      "        [0.0427],\n",
      "        [0.0428],\n",
      "        [0.0441],\n",
      "        [0.0443],\n",
      "        [0.0461],\n",
      "        [0.0467],\n",
      "        [0.0468],\n",
      "        [0.0468],\n",
      "        [0.0471],\n",
      "        [0.0478],\n",
      "        [0.0478],\n",
      "        [0.0500],\n",
      "        [0.0500],\n",
      "        [0.0503],\n",
      "        [0.0504],\n",
      "        [0.0514],\n",
      "        [0.0525],\n",
      "        [0.0534],\n",
      "        [0.0558],\n",
      "        [0.0560],\n",
      "        [0.0572],\n",
      "        [0.0580],\n",
      "        [0.0581],\n",
      "        [0.0583],\n",
      "        [0.0589],\n",
      "        [0.0604],\n",
      "        [0.0644],\n",
      "        [0.0644],\n",
      "        [0.0651],\n",
      "        [0.0669],\n",
      "        [0.0672],\n",
      "        [0.0672],\n",
      "        [0.0677],\n",
      "        [0.0687],\n",
      "        [0.0695],\n",
      "        [0.0710],\n",
      "        [0.0724],\n",
      "        [0.0734],\n",
      "        [0.0761],\n",
      "        [0.0765],\n",
      "        [0.0768],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0785],\n",
      "        [0.0787],\n",
      "        [0.0799],\n",
      "        [0.0799],\n",
      "        [0.0816],\n",
      "        [0.0816],\n",
      "        [0.0829],\n",
      "        [0.0874],\n",
      "        [0.0903],\n",
      "        [0.0929],\n",
      "        [0.0940],\n",
      "        [0.0943],\n",
      "        [0.0969],\n",
      "        [0.0974],\n",
      "        [0.1004],\n",
      "        [0.1015],\n",
      "        [0.1053],\n",
      "        [0.1103],\n",
      "        [0.1189],\n",
      "        [0.1215],\n",
      "        [0.1217],\n",
      "        [0.1278],\n",
      "        [0.1292],\n",
      "        [0.1294],\n",
      "        [0.1403],\n",
      "        [0.1478],\n",
      "        [0.1526],\n",
      "        [0.1595],\n",
      "        [0.1628],\n",
      "        [0.1672],\n",
      "        [0.1905]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0016],\n",
      "        [0.0013],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0055],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0034],\n",
      "        [0.0070],\n",
      "        [0.0050],\n",
      "        [0.0077],\n",
      "        [0.0070],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0076],\n",
      "        [0.0086],\n",
      "        [0.0072],\n",
      "        [0.0089],\n",
      "        [0.0109],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0134],\n",
      "        [0.0102],\n",
      "        [0.0123],\n",
      "        [0.0133],\n",
      "        [0.0121],\n",
      "        [0.0153],\n",
      "        [0.0160],\n",
      "        [0.0166],\n",
      "        [0.0143],\n",
      "        [0.0166],\n",
      "        [0.0163],\n",
      "        [0.0154],\n",
      "        [0.0196],\n",
      "        [0.0182],\n",
      "        [0.0168],\n",
      "        [0.0177],\n",
      "        [0.0207],\n",
      "        [0.0188],\n",
      "        [0.0205],\n",
      "        [0.0174],\n",
      "        [0.0180],\n",
      "        [0.0206],\n",
      "        [0.0226],\n",
      "        [0.0214],\n",
      "        [0.0213],\n",
      "        [0.0221],\n",
      "        [0.0264],\n",
      "        [0.0248],\n",
      "        [0.0228],\n",
      "        [0.0270],\n",
      "        [0.0219],\n",
      "        [0.0279],\n",
      "        [0.0262],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0309],\n",
      "        [0.0322],\n",
      "        [0.0310],\n",
      "        [0.0314],\n",
      "        [0.0306],\n",
      "        [0.0337],\n",
      "        [0.0308],\n",
      "        [0.0294],\n",
      "        [0.0357],\n",
      "        [0.0361],\n",
      "        [0.0324],\n",
      "        [0.0348],\n",
      "        [0.0360],\n",
      "        [0.0330],\n",
      "        [0.0390],\n",
      "        [0.0411],\n",
      "        [0.0378],\n",
      "        [0.0375],\n",
      "        [0.0408],\n",
      "        [0.0428],\n",
      "        [0.0442],\n",
      "        [0.0391],\n",
      "        [0.0418],\n",
      "        [0.0434],\n",
      "        [0.0459],\n",
      "        [0.0425],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0443],\n",
      "        [0.0483],\n",
      "        [0.0476],\n",
      "        [0.0480],\n",
      "        [0.0477],\n",
      "        [0.0481],\n",
      "        [0.0513],\n",
      "        [0.0517],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0505],\n",
      "        [0.0549],\n",
      "        [0.0532],\n",
      "        [0.0564],\n",
      "        [0.0551],\n",
      "        [0.0583],\n",
      "        [0.0568],\n",
      "        [0.0591],\n",
      "        [0.0595],\n",
      "        [0.0561],\n",
      "        [0.0626],\n",
      "        [0.0627],\n",
      "        [0.0631],\n",
      "        [0.0691],\n",
      "        [0.0674],\n",
      "        [0.0674],\n",
      "        [0.0666],\n",
      "        [0.0665],\n",
      "        [0.0698],\n",
      "        [0.0693],\n",
      "        [0.0712],\n",
      "        [0.0724],\n",
      "        [0.0775],\n",
      "        [0.0718],\n",
      "        [0.0770],\n",
      "        [0.0744],\n",
      "        [0.0784],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0789],\n",
      "        [0.0775],\n",
      "        [0.0776],\n",
      "        [0.0804],\n",
      "        [0.0845],\n",
      "        [0.0881],\n",
      "        [0.0862],\n",
      "        [0.0907],\n",
      "        [0.0898],\n",
      "        [0.0916],\n",
      "        [0.0946],\n",
      "        [0.0999],\n",
      "        [0.0983],\n",
      "        [0.1022],\n",
      "        [0.1071],\n",
      "        [0.1090],\n",
      "        [0.1205],\n",
      "        [0.1194],\n",
      "        [0.1244],\n",
      "        [0.1293],\n",
      "        [0.1267],\n",
      "        [0.1269],\n",
      "        [0.1385],\n",
      "        [0.1487],\n",
      "        [0.1526],\n",
      "        [0.1602],\n",
      "        [0.1611],\n",
      "        [0.1692],\n",
      "        [0.1906]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 58.794848680496216\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5279285037195223e-07, 79)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [79, 35, 99, 93, 8, 44, 100, 92, 83, 94, 142, 41, 75, 43, 95, 18, 87, 6, 86, 74, 42, 146, 34, 46, 85, 55, 78, 103, 29, 54, 137, 19, 80, 40, 68, 66, 58, 98, 64, 129, 81, 130, 9, 84, 91, 82, 138, 21, 45, 109, 154, 56, 72, 20, 73, 76, 90, 0, 88, 57, 96, 126, 47, 153, 71, 7, 97, 28, 125, 155, 144, 48, 136, 33, 67, 101, 36, 1, 124, 139, 89, 141, 27, 59, 17, 131, 152, 140, 26, 65, 145, 5, 77, 112, 134, 53, 39, 113, 135, 108, 102, 143, 37, 158, 12, 49, 10, 63, 2, 38, 111, 119, 118, 30, 69, 70, 132, 147, 4, 51, 127, 52, 115, 110, 3, 22, 114, 31, 156, 16, 32, 50, 133, 157, 11, 128, 117, 123, 116, 151, 104, 62, 25, 150, 120, 107, 105, 122, 121, 24, 148, 15, 13, 14, 149, 106, 61, 60] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6220],\n",
      "        [0.6906],\n",
      "        [0.4864],\n",
      "        [0.5416],\n",
      "        [0.8731],\n",
      "        [0.6367],\n",
      "        [0.4755],\n",
      "        [0.5281],\n",
      "        [0.6183],\n",
      "        [0.5342],\n",
      "        [0.1918],\n",
      "        [0.6802],\n",
      "        [0.6356],\n",
      "        [0.6653],\n",
      "        [0.5282],\n",
      "        [0.7541],\n",
      "        [0.5977],\n",
      "        [0.8763],\n",
      "        [0.6000],\n",
      "        [0.5883],\n",
      "        [0.6848],\n",
      "        [0.1535],\n",
      "        [0.6945],\n",
      "        [0.6309],\n",
      "        [0.6173],\n",
      "        [0.6346],\n",
      "        [0.6321],\n",
      "        [0.4283],\n",
      "        [0.6488],\n",
      "        [0.6283],\n",
      "        [0.2051],\n",
      "        [0.7320],\n",
      "        [0.6486],\n",
      "        [0.6796],\n",
      "        [0.5691],\n",
      "        [0.5351],\n",
      "        [0.6054],\n",
      "        [0.4876],\n",
      "        [0.4946],\n",
      "        [0.2536],\n",
      "        [0.6464],\n",
      "        [0.2442],\n",
      "        [0.8538],\n",
      "        [0.6252],\n",
      "        [0.5676],\n",
      "        [0.6275],\n",
      "        [0.2130],\n",
      "        [0.7425],\n",
      "        [0.6261],\n",
      "        [0.2696],\n",
      "        [0.0974],\n",
      "        [0.6142],\n",
      "        [0.5881],\n",
      "        [0.7358],\n",
      "        [0.5886],\n",
      "        [0.6506],\n",
      "        [0.5822],\n",
      "        [0.8789],\n",
      "        [0.5869],\n",
      "        [0.6079],\n",
      "        [0.5041],\n",
      "        [0.2848],\n",
      "        [0.6285],\n",
      "        [0.1000],\n",
      "        [0.5878],\n",
      "        [0.8927],\n",
      "        [0.4917],\n",
      "        [0.6558],\n",
      "        [0.3219],\n",
      "        [0.1013],\n",
      "        [0.1584],\n",
      "        [0.6242],\n",
      "        [0.2197],\n",
      "        [0.6758],\n",
      "        [0.5744],\n",
      "        [0.4344],\n",
      "        [0.6679],\n",
      "        [0.8810],\n",
      "        [0.3254],\n",
      "        [0.2138],\n",
      "        [0.6022],\n",
      "        [0.2009],\n",
      "        [0.6128],\n",
      "        [0.6097],\n",
      "        [0.7889],\n",
      "        [0.2643],\n",
      "        [0.0990],\n",
      "        [0.2432],\n",
      "        [0.6794],\n",
      "        [0.4805],\n",
      "        [0.1326],\n",
      "        [0.8726],\n",
      "        [0.6237],\n",
      "        [0.2597],\n",
      "        [0.2359],\n",
      "        [0.6222],\n",
      "        [0.6549],\n",
      "        [0.2566],\n",
      "        [0.2321],\n",
      "        [0.3192],\n",
      "        [0.4114],\n",
      "        [0.1629],\n",
      "        [0.6610],\n",
      "        [0.0875],\n",
      "        [0.8731],\n",
      "        [0.6226],\n",
      "        [0.8938],\n",
      "        [0.5545],\n",
      "        [0.8760],\n",
      "        [0.6451],\n",
      "        [0.2565],\n",
      "        [0.3280],\n",
      "        [0.2970],\n",
      "        [0.6502],\n",
      "        [0.5565],\n",
      "        [0.5497],\n",
      "        [0.2600],\n",
      "        [0.1874],\n",
      "        [0.8854],\n",
      "        [0.6195],\n",
      "        [0.2397],\n",
      "        [0.6169],\n",
      "        [0.2836],\n",
      "        [0.2266],\n",
      "        [0.8823],\n",
      "        [0.7405],\n",
      "        [0.2804],\n",
      "        [0.6579],\n",
      "        [0.1016],\n",
      "        [0.8246],\n",
      "        [0.6554],\n",
      "        [0.6162],\n",
      "        [0.2472],\n",
      "        [0.1071],\n",
      "        [0.8817],\n",
      "        [0.2355],\n",
      "        [0.2679],\n",
      "        [0.3073],\n",
      "        [0.2641],\n",
      "        [0.1311],\n",
      "        [0.4263],\n",
      "        [0.5591],\n",
      "        [0.7214],\n",
      "        [0.1361],\n",
      "        [0.3316],\n",
      "        [0.3854],\n",
      "        [0.4112],\n",
      "        [0.3098],\n",
      "        [0.3252],\n",
      "        [0.7506],\n",
      "        [0.1698],\n",
      "        [0.8072],\n",
      "        [0.8564],\n",
      "        [0.8094],\n",
      "        [0.1611],\n",
      "        [0.3966],\n",
      "        [0.6143],\n",
      "        [0.6140]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0016],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0034],\n",
      "        [0.0050],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0070],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0086],\n",
      "        [0.0089],\n",
      "        [0.0102],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0143],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0166],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0174],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0188],\n",
      "        [0.0196],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0248],\n",
      "        [0.0262],\n",
      "        [0.0264],\n",
      "        [0.0270],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0294],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0314],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0330],\n",
      "        [0.0337],\n",
      "        [0.0348],\n",
      "        [0.0357],\n",
      "        [0.0360],\n",
      "        [0.0361],\n",
      "        [0.0375],\n",
      "        [0.0378],\n",
      "        [0.0390],\n",
      "        [0.0391],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0418],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0434],\n",
      "        [0.0442],\n",
      "        [0.0443],\n",
      "        [0.0457],\n",
      "        [0.0459],\n",
      "        [0.0460],\n",
      "        [0.0476],\n",
      "        [0.0477],\n",
      "        [0.0480],\n",
      "        [0.0481],\n",
      "        [0.0483],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0505],\n",
      "        [0.0513],\n",
      "        [0.0517],\n",
      "        [0.0532],\n",
      "        [0.0549],\n",
      "        [0.0551],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0568],\n",
      "        [0.0583],\n",
      "        [0.0591],\n",
      "        [0.0595],\n",
      "        [0.0626],\n",
      "        [0.0627],\n",
      "        [0.0631],\n",
      "        [0.0665],\n",
      "        [0.0666],\n",
      "        [0.0674],\n",
      "        [0.0674],\n",
      "        [0.0691],\n",
      "        [0.0693],\n",
      "        [0.0698],\n",
      "        [0.0712],\n",
      "        [0.0718],\n",
      "        [0.0724],\n",
      "        [0.0744],\n",
      "        [0.0761],\n",
      "        [0.0770],\n",
      "        [0.0775],\n",
      "        [0.0775],\n",
      "        [0.0776],\n",
      "        [0.0776],\n",
      "        [0.0784],\n",
      "        [0.0789],\n",
      "        [0.0804],\n",
      "        [0.0845],\n",
      "        [0.0862],\n",
      "        [0.0881],\n",
      "        [0.0898],\n",
      "        [0.0907],\n",
      "        [0.0916],\n",
      "        [0.0946],\n",
      "        [0.0983],\n",
      "        [0.0999],\n",
      "        [0.1022],\n",
      "        [0.1071],\n",
      "        [0.1090],\n",
      "        [0.1194],\n",
      "        [0.1205],\n",
      "        [0.1244],\n",
      "        [0.1267],\n",
      "        [0.1269],\n",
      "        [0.1293],\n",
      "        [0.1385],\n",
      "        [0.1487],\n",
      "        [0.1526],\n",
      "        [0.1602],\n",
      "        [0.1611],\n",
      "        [0.1692],\n",
      "        [0.1906],\n",
      "        [0.1935]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0045],\n",
      "        [0.0054],\n",
      "        [0.0042],\n",
      "        [0.0017],\n",
      "        [0.0072],\n",
      "        [0.0020],\n",
      "        [0.0010],\n",
      "        [0.0005],\n",
      "        [0.0027],\n",
      "        [0.0056],\n",
      "        [0.0006],\n",
      "        [0.0074],\n",
      "        [0.0006],\n",
      "        [0.0017],\n",
      "        [0.0092],\n",
      "        [0.0108],\n",
      "        [0.0034],\n",
      "        [0.0131],\n",
      "        [0.0107],\n",
      "        [0.0025],\n",
      "        [0.0036],\n",
      "        [0.0112],\n",
      "        [0.0134],\n",
      "        [0.0133],\n",
      "        [0.0145],\n",
      "        [0.0151],\n",
      "        [0.0160],\n",
      "        [0.0138],\n",
      "        [0.0163],\n",
      "        [0.0177],\n",
      "        [0.0123],\n",
      "        [0.0182],\n",
      "        [0.0101],\n",
      "        [0.0195],\n",
      "        [0.0212],\n",
      "        [0.0207],\n",
      "        [0.0119],\n",
      "        [0.0132],\n",
      "        [0.0204],\n",
      "        [0.0172],\n",
      "        [0.0125],\n",
      "        [0.0181],\n",
      "        [0.0128],\n",
      "        [0.0230],\n",
      "        [0.0170],\n",
      "        [0.0155],\n",
      "        [0.0220],\n",
      "        [0.0174],\n",
      "        [0.0244],\n",
      "        [0.0224],\n",
      "        [0.0176],\n",
      "        [0.0261],\n",
      "        [0.0272],\n",
      "        [0.0260],\n",
      "        [0.0204],\n",
      "        [0.0316],\n",
      "        [0.0235],\n",
      "        [0.0345],\n",
      "        [0.0248],\n",
      "        [0.0322],\n",
      "        [0.0323],\n",
      "        [0.0280],\n",
      "        [0.0338],\n",
      "        [0.0267],\n",
      "        [0.0358],\n",
      "        [0.0246],\n",
      "        [0.0346],\n",
      "        [0.0280],\n",
      "        [0.0320],\n",
      "        [0.0282],\n",
      "        [0.0362],\n",
      "        [0.0379],\n",
      "        [0.0340],\n",
      "        [0.0403],\n",
      "        [0.0412],\n",
      "        [0.0390],\n",
      "        [0.0415],\n",
      "        [0.0459],\n",
      "        [0.0389],\n",
      "        [0.0426],\n",
      "        [0.0377],\n",
      "        [0.0390],\n",
      "        [0.0458],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0436],\n",
      "        [0.0406],\n",
      "        [0.0430],\n",
      "        [0.0424],\n",
      "        [0.0497],\n",
      "        [0.0498],\n",
      "        [0.0536],\n",
      "        [0.0533],\n",
      "        [0.0487],\n",
      "        [0.0474],\n",
      "        [0.0545],\n",
      "        [0.0541],\n",
      "        [0.0508],\n",
      "        [0.0501],\n",
      "        [0.0499],\n",
      "        [0.0547],\n",
      "        [0.0582],\n",
      "        [0.0584],\n",
      "        [0.0510],\n",
      "        [0.0507],\n",
      "        [0.0607],\n",
      "        [0.0524],\n",
      "        [0.0550],\n",
      "        [0.0658],\n",
      "        [0.0661],\n",
      "        [0.0635],\n",
      "        [0.0638],\n",
      "        [0.0669],\n",
      "        [0.0705],\n",
      "        [0.0722],\n",
      "        [0.0723],\n",
      "        [0.0687],\n",
      "        [0.0658],\n",
      "        [0.0762],\n",
      "        [0.0752],\n",
      "        [0.0699],\n",
      "        [0.0766],\n",
      "        [0.0746],\n",
      "        [0.0763],\n",
      "        [0.0833],\n",
      "        [0.0735],\n",
      "        [0.0777],\n",
      "        [0.0816],\n",
      "        [0.0728],\n",
      "        [0.0734],\n",
      "        [0.0830],\n",
      "        [0.0845],\n",
      "        [0.0835],\n",
      "        [0.0811],\n",
      "        [0.0827],\n",
      "        [0.0884],\n",
      "        [0.0911],\n",
      "        [0.0917],\n",
      "        [0.0949],\n",
      "        [0.0946],\n",
      "        [0.0984],\n",
      "        [0.0978],\n",
      "        [0.1034],\n",
      "        [0.1061],\n",
      "        [0.1201],\n",
      "        [0.1184],\n",
      "        [0.1232],\n",
      "        [0.1271],\n",
      "        [0.1273],\n",
      "        [0.1252],\n",
      "        [0.1351],\n",
      "        [0.1439],\n",
      "        [0.1467],\n",
      "        [0.1552],\n",
      "        [0.1577],\n",
      "        [0.1675],\n",
      "        [0.1854],\n",
      "        [0.1889]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 59.08136796951294\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.610505021038989e-07, 92)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [92, 142, 75, 100, 43, 93, 44, 74, 83, 87, 42, 99, 79, 35, 94, 8, 41, 95, 80, 86, 18, 146, 58, 137, 81, 9, 6, 98, 46, 34, 103, 85, 55, 82, 78, 29, 91, 129, 21, 154, 54, 130, 19, 40, 73, 64, 66, 68, 138, 109, 84, 90, 45, 7, 88, 20, 56, 153, 72, 126, 28, 155, 76, 125, 57, 96, 47, 136, 0, 97, 71, 144, 89, 48, 59, 17, 124, 101, 141, 33, 152, 67, 36, 26, 139, 140, 131, 27, 1, 134, 112, 65, 145, 108, 135, 12, 113, 158, 10, 77, 5, 39, 53, 102, 63, 143, 37, 49, 111, 119, 147, 2, 38, 118, 132, 127, 30, 69, 70, 156, 16, 22, 115, 51, 4, 110, 52, 114, 157, 31, 11, 32, 3, 133, 50, 128, 117, 123, 151, 116, 62, 104, 25, 150, 107, 120, 105, 24, 122, 121, 148, 15, 13, 14, 149, 106, 61, 60, 23] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5259],\n",
      "        [0.1886],\n",
      "        [0.6299],\n",
      "        [0.4729],\n",
      "        [0.6614],\n",
      "        [0.5389],\n",
      "        [0.6333],\n",
      "        [0.5832],\n",
      "        [0.6139],\n",
      "        [0.5941],\n",
      "        [0.6808],\n",
      "        [0.4831],\n",
      "        [0.6171],\n",
      "        [0.6860],\n",
      "        [0.5310],\n",
      "        [0.8671],\n",
      "        [0.6761],\n",
      "        [0.5248],\n",
      "        [0.6434],\n",
      "        [0.5965],\n",
      "        [0.7498],\n",
      "        [0.1508],\n",
      "        [0.6008],\n",
      "        [0.2040],\n",
      "        [0.6411],\n",
      "        [0.8485],\n",
      "        [0.8702],\n",
      "        [0.4842],\n",
      "        [0.6279],\n",
      "        [0.6900],\n",
      "        [0.4266],\n",
      "        [0.6133],\n",
      "        [0.6303],\n",
      "        [0.6225],\n",
      "        [0.6269],\n",
      "        [0.6447],\n",
      "        [0.5650],\n",
      "        [0.2538],\n",
      "        [0.7392],\n",
      "        [0.0931],\n",
      "        [0.6239],\n",
      "        [0.2441],\n",
      "        [0.7281],\n",
      "        [0.6756],\n",
      "        [0.5841],\n",
      "        [0.4909],\n",
      "        [0.5307],\n",
      "        [0.5639],\n",
      "        [0.2116],\n",
      "        [0.2685],\n",
      "        [0.6209],\n",
      "        [0.5793],\n",
      "        [0.6230],\n",
      "        [0.8862],\n",
      "        [0.5837],\n",
      "        [0.7326],\n",
      "        [0.6103],\n",
      "        [0.0958],\n",
      "        [0.5836],\n",
      "        [0.2862],\n",
      "        [0.6516],\n",
      "        [0.0964],\n",
      "        [0.6452],\n",
      "        [0.3222],\n",
      "        [0.6038],\n",
      "        [0.5006],\n",
      "        [0.6253],\n",
      "        [0.2181],\n",
      "        [0.8714],\n",
      "        [0.4884],\n",
      "        [0.5830],\n",
      "        [0.1559],\n",
      "        [0.5988],\n",
      "        [0.6211],\n",
      "        [0.6053],\n",
      "        [0.7840],\n",
      "        [0.3256],\n",
      "        [0.4329],\n",
      "        [0.1981],\n",
      "        [0.6715],\n",
      "        [0.0954],\n",
      "        [0.5693],\n",
      "        [0.6642],\n",
      "        [0.6759],\n",
      "        [0.2120],\n",
      "        [0.2406],\n",
      "        [0.2637],\n",
      "        [0.6095],\n",
      "        [0.8741],\n",
      "        [0.2350],\n",
      "        [0.2591],\n",
      "        [0.4768],\n",
      "        [0.1304],\n",
      "        [0.3173],\n",
      "        [0.2309],\n",
      "        [0.8674],\n",
      "        [0.2563],\n",
      "        [0.0823],\n",
      "        [0.8879],\n",
      "        [0.6185],\n",
      "        [0.8667],\n",
      "        [0.6509],\n",
      "        [0.6175],\n",
      "        [0.4099],\n",
      "        [0.5503],\n",
      "        [0.1596],\n",
      "        [0.6577],\n",
      "        [0.6187],\n",
      "        [0.2557],\n",
      "        [0.3273],\n",
      "        [0.1839],\n",
      "        [0.8696],\n",
      "        [0.6417],\n",
      "        [0.2966],\n",
      "        [0.2595],\n",
      "        [0.2415],\n",
      "        [0.6463],\n",
      "        [0.5517],\n",
      "        [0.5448],\n",
      "        [0.0967],\n",
      "        [0.8195],\n",
      "        [0.7366],\n",
      "        [0.2834],\n",
      "        [0.6155],\n",
      "        [0.8791],\n",
      "        [0.2264],\n",
      "        [0.6127],\n",
      "        [0.2802],\n",
      "        [0.1020],\n",
      "        [0.6538],\n",
      "        [0.8763],\n",
      "        [0.6512],\n",
      "        [0.8761],\n",
      "        [0.2463],\n",
      "        [0.6121],\n",
      "        [0.2369],\n",
      "        [0.2675],\n",
      "        [0.3072],\n",
      "        [0.1275],\n",
      "        [0.2638],\n",
      "        [0.5547],\n",
      "        [0.4247],\n",
      "        [0.7177],\n",
      "        [0.1331],\n",
      "        [0.3833],\n",
      "        [0.3310],\n",
      "        [0.4099],\n",
      "        [0.7466],\n",
      "        [0.3094],\n",
      "        [0.3248],\n",
      "        [0.1664],\n",
      "        [0.8024],\n",
      "        [0.8505],\n",
      "        [0.8044],\n",
      "        [0.1577],\n",
      "        [0.3949],\n",
      "        [0.6091],\n",
      "        [0.6094],\n",
      "        [0.7444]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0010],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0072],\n",
      "        [0.0074],\n",
      "        [0.0092],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0108],\n",
      "        [0.0112],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0128],\n",
      "        [0.0131],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0145],\n",
      "        [0.0151],\n",
      "        [0.0155],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0181],\n",
      "        [0.0182],\n",
      "        [0.0195],\n",
      "        [0.0204],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0230],\n",
      "        [0.0235],\n",
      "        [0.0244],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0260],\n",
      "        [0.0261],\n",
      "        [0.0267],\n",
      "        [0.0272],\n",
      "        [0.0280],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0322],\n",
      "        [0.0323],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0358],\n",
      "        [0.0362],\n",
      "        [0.0377],\n",
      "        [0.0379],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0389],\n",
      "        [0.0390],\n",
      "        [0.0390],\n",
      "        [0.0403],\n",
      "        [0.0406],\n",
      "        [0.0412],\n",
      "        [0.0415],\n",
      "        [0.0424],\n",
      "        [0.0426],\n",
      "        [0.0430],\n",
      "        [0.0436],\n",
      "        [0.0458],\n",
      "        [0.0459],\n",
      "        [0.0474],\n",
      "        [0.0487],\n",
      "        [0.0497],\n",
      "        [0.0498],\n",
      "        [0.0499],\n",
      "        [0.0501],\n",
      "        [0.0507],\n",
      "        [0.0508],\n",
      "        [0.0510],\n",
      "        [0.0524],\n",
      "        [0.0533],\n",
      "        [0.0536],\n",
      "        [0.0541],\n",
      "        [0.0545],\n",
      "        [0.0547],\n",
      "        [0.0550],\n",
      "        [0.0582],\n",
      "        [0.0584],\n",
      "        [0.0607],\n",
      "        [0.0635],\n",
      "        [0.0638],\n",
      "        [0.0658],\n",
      "        [0.0658],\n",
      "        [0.0661],\n",
      "        [0.0669],\n",
      "        [0.0687],\n",
      "        [0.0699],\n",
      "        [0.0705],\n",
      "        [0.0722],\n",
      "        [0.0723],\n",
      "        [0.0728],\n",
      "        [0.0734],\n",
      "        [0.0735],\n",
      "        [0.0746],\n",
      "        [0.0752],\n",
      "        [0.0762],\n",
      "        [0.0763],\n",
      "        [0.0766],\n",
      "        [0.0777],\n",
      "        [0.0811],\n",
      "        [0.0816],\n",
      "        [0.0827],\n",
      "        [0.0830],\n",
      "        [0.0833],\n",
      "        [0.0835],\n",
      "        [0.0845],\n",
      "        [0.0884],\n",
      "        [0.0911],\n",
      "        [0.0917],\n",
      "        [0.0946],\n",
      "        [0.0949],\n",
      "        [0.0978],\n",
      "        [0.0984],\n",
      "        [0.1034],\n",
      "        [0.1061],\n",
      "        [0.1184],\n",
      "        [0.1201],\n",
      "        [0.1232],\n",
      "        [0.1252],\n",
      "        [0.1271],\n",
      "        [0.1273],\n",
      "        [0.1351],\n",
      "        [0.1439],\n",
      "        [0.1467],\n",
      "        [0.1552],\n",
      "        [0.1577],\n",
      "        [0.1675],\n",
      "        [0.1854],\n",
      "        [0.1889],\n",
      "        [0.2832]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0040],\n",
      "        [0.0009],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0007],\n",
      "        [0.0041],\n",
      "        [0.0058],\n",
      "        [0.0088],\n",
      "        [0.0053],\n",
      "        [0.0114],\n",
      "        [0.0103],\n",
      "        [0.0091],\n",
      "        [0.0085],\n",
      "        [0.0106],\n",
      "        [0.0143],\n",
      "        [0.0113],\n",
      "        [0.0097],\n",
      "        [0.0136],\n",
      "        [0.0109],\n",
      "        [0.0093],\n",
      "        [0.0173],\n",
      "        [0.0133],\n",
      "        [0.0148],\n",
      "        [0.0168],\n",
      "        [0.0122],\n",
      "        [0.0149],\n",
      "        [0.0172],\n",
      "        [0.0142],\n",
      "        [0.0175],\n",
      "        [0.0194],\n",
      "        [0.0180],\n",
      "        [0.0149],\n",
      "        [0.0147],\n",
      "        [0.0165],\n",
      "        [0.0198],\n",
      "        [0.0162],\n",
      "        [0.0213],\n",
      "        [0.0224],\n",
      "        [0.0193],\n",
      "        [0.0213],\n",
      "        [0.0220],\n",
      "        [0.0232],\n",
      "        [0.0211],\n",
      "        [0.0204],\n",
      "        [0.0236],\n",
      "        [0.0240],\n",
      "        [0.0262],\n",
      "        [0.0199],\n",
      "        [0.0252],\n",
      "        [0.0286],\n",
      "        [0.0277],\n",
      "        [0.0257],\n",
      "        [0.0285],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0265],\n",
      "        [0.0334],\n",
      "        [0.0299],\n",
      "        [0.0338],\n",
      "        [0.0324],\n",
      "        [0.0353],\n",
      "        [0.0348],\n",
      "        [0.0398],\n",
      "        [0.0344],\n",
      "        [0.0374],\n",
      "        [0.0362],\n",
      "        [0.0378],\n",
      "        [0.0394],\n",
      "        [0.0365],\n",
      "        [0.0344],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0388],\n",
      "        [0.0436],\n",
      "        [0.0399],\n",
      "        [0.0431],\n",
      "        [0.0441],\n",
      "        [0.0397],\n",
      "        [0.0421],\n",
      "        [0.0428],\n",
      "        [0.0452],\n",
      "        [0.0479],\n",
      "        [0.0505],\n",
      "        [0.0488],\n",
      "        [0.0468],\n",
      "        [0.0505],\n",
      "        [0.0495],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0468],\n",
      "        [0.0487],\n",
      "        [0.0492],\n",
      "        [0.0485],\n",
      "        [0.0549],\n",
      "        [0.0576],\n",
      "        [0.0568],\n",
      "        [0.0571],\n",
      "        [0.0530],\n",
      "        [0.0535],\n",
      "        [0.0588],\n",
      "        [0.0605],\n",
      "        [0.0629],\n",
      "        [0.0616],\n",
      "        [0.0626],\n",
      "        [0.0649],\n",
      "        [0.0700],\n",
      "        [0.0683],\n",
      "        [0.0651],\n",
      "        [0.0705],\n",
      "        [0.0661],\n",
      "        [0.0735],\n",
      "        [0.0738],\n",
      "        [0.0739],\n",
      "        [0.0712],\n",
      "        [0.0691],\n",
      "        [0.0702],\n",
      "        [0.0725],\n",
      "        [0.0774],\n",
      "        [0.0805],\n",
      "        [0.0737],\n",
      "        [0.0788],\n",
      "        [0.0754],\n",
      "        [0.0794],\n",
      "        [0.0848],\n",
      "        [0.0791],\n",
      "        [0.0862],\n",
      "        [0.0875],\n",
      "        [0.0849],\n",
      "        [0.0867],\n",
      "        [0.0849],\n",
      "        [0.0892],\n",
      "        [0.0900],\n",
      "        [0.0938],\n",
      "        [0.0929],\n",
      "        [0.0961],\n",
      "        [0.1001],\n",
      "        [0.1002],\n",
      "        [0.1060],\n",
      "        [0.1194],\n",
      "        [0.1189],\n",
      "        [0.1250],\n",
      "        [0.1217],\n",
      "        [0.1256],\n",
      "        [0.1259],\n",
      "        [0.1343],\n",
      "        [0.1402],\n",
      "        [0.1424],\n",
      "        [0.1514],\n",
      "        [0.1570],\n",
      "        [0.1690],\n",
      "        [0.1828],\n",
      "        [0.1869],\n",
      "        [0.2799]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 59.36728811264038\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 15 個區塊累積花費時間(s) 1.2129075527191162\n",
      "<<The performance of 15 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2129075527191162\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1299.24\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1712.53\n",
      "The accuracy(2000) for l = 1: 79.25%\n",
      "The accuracy(3000) for l = 1: 90.57%\n",
      "The maximum error: tensor(7145.5664)\n",
      "The minimum error: tensor(8.7695)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 489.6\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 518.4\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 695.265625\n",
      "The minimum error: 233.46484375\n",
      "------------------------------------------------------------\n",
      "0.7924528301886793\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<16>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1797285282000303e-07, 96)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [96, 38, 88, 70, 39, 89, 138, 71, 83, 79, 40, 95, 90, 75, 76, 31, 91, 158, 5, 54, 37, 82, 77, 142, 4, 99, 94, 133, 78, 14, 17, 42, 81, 125, 126, 150, 30, 51, 2, 74, 157, 87, 69, 25, 50, 3, 105, 134, 60, 15, 62, 36, 156, 64, 80, 86, 122, 24, 84, 149, 41, 151, 155, 52, 68, 16, 121, 92, 72, 53, 93, 13, 132, 43, 140, 55, 120, 97, 67, 85, 137, 44, 22, 148, 135, 136, 63, 29, 32, 127, 108, 8, 23, 6, 109, 130, 154, 141, 61, 104, 131, 98, 59, 73, 35, 49, 1, 139, 33, 107, 115, 45, 143, 114, 123, 34, 12, 18, 128, 152, 111, 26, 106, 65, 66, 110, 47, 48, 7, 153, 0, 27, 129, 124, 28, 46, 113, 119, 112, 147, 58, 100, 21, 146, 116, 103, 20, 101, 118, 117, 144, 11, 9, 10, 145]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0122],\n",
      "        [0.0133],\n",
      "        [0.0136],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0168],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0240],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0257],\n",
      "        [0.0262],\n",
      "        [0.0265],\n",
      "        [0.0272],\n",
      "        [0.0277],\n",
      "        [0.0285],\n",
      "        [0.0286],\n",
      "        [0.0299],\n",
      "        [0.0324],\n",
      "        [0.0334],\n",
      "        [0.0338],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0348],\n",
      "        [0.0353],\n",
      "        [0.0362],\n",
      "        [0.0365],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0374],\n",
      "        [0.0378],\n",
      "        [0.0388],\n",
      "        [0.0394],\n",
      "        [0.0397],\n",
      "        [0.0399],\n",
      "        [0.0421],\n",
      "        [0.0428],\n",
      "        [0.0431],\n",
      "        [0.0436],\n",
      "        [0.0441],\n",
      "        [0.0452],\n",
      "        [0.0468],\n",
      "        [0.0468],\n",
      "        [0.0479],\n",
      "        [0.0485],\n",
      "        [0.0487],\n",
      "        [0.0488],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0505],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0530],\n",
      "        [0.0535],\n",
      "        [0.0549],\n",
      "        [0.0568],\n",
      "        [0.0571],\n",
      "        [0.0576],\n",
      "        [0.0588],\n",
      "        [0.0605],\n",
      "        [0.0616],\n",
      "        [0.0626],\n",
      "        [0.0629],\n",
      "        [0.0649],\n",
      "        [0.0651],\n",
      "        [0.0661],\n",
      "        [0.0683],\n",
      "        [0.0691],\n",
      "        [0.0702],\n",
      "        [0.0705],\n",
      "        [0.0712],\n",
      "        [0.0725],\n",
      "        [0.0735],\n",
      "        [0.0737],\n",
      "        [0.0738],\n",
      "        [0.0739],\n",
      "        [0.0754],\n",
      "        [0.0774],\n",
      "        [0.0788],\n",
      "        [0.0791],\n",
      "        [0.0794],\n",
      "        [0.0805],\n",
      "        [0.0848],\n",
      "        [0.0849],\n",
      "        [0.0849],\n",
      "        [0.0862],\n",
      "        [0.0867],\n",
      "        [0.0892],\n",
      "        [0.0900],\n",
      "        [0.0929],\n",
      "        [0.0938],\n",
      "        [0.0961],\n",
      "        [0.1001],\n",
      "        [0.1002],\n",
      "        [0.1060],\n",
      "        [0.1189],\n",
      "        [0.1194],\n",
      "        [0.1217],\n",
      "        [0.1250],\n",
      "        [0.1256],\n",
      "        [0.1259],\n",
      "        [0.1343],\n",
      "        [0.1402],\n",
      "        [0.1424],\n",
      "        [0.1514],\n",
      "        [0.1570]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1797285282000303e-07, 96)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [96, 38, 88, 70, 39, 89, 138, 71, 83, 79, 40, 95, 90, 75, 76, 31, 91, 158, 5, 54, 37, 82, 77, 142, 4, 99, 94, 133, 78, 14, 17, 42, 81, 125, 126, 150, 30, 51, 2, 74, 157, 87, 69, 25, 50, 3, 105, 134, 60, 15, 62, 36, 156, 64, 80, 86, 122, 24, 84, 149, 41, 151, 155, 52, 68, 16, 121, 92, 72, 53, 93, 13, 132, 43, 140, 55, 120, 97, 67, 85, 137, 44, 22, 148, 135, 136, 63, 29, 32, 127, 108, 8, 23, 6, 109, 130, 154, 141, 61, 104, 131, 98, 59, 73, 35, 49, 1, 139, 33, 107, 115, 45, 143, 114, 123, 34, 12, 18, 128, 152, 111, 26, 106, 65, 66, 110, 47, 48, 7, 153, 0, 27, 129, 124, 28, 46, 113, 119, 112, 147, 58, 100, 21, 146, 116, 103, 20, 101, 118, 117, 144, 11, 9, 10, 145, 102] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4736],\n",
      "        [0.6779],\n",
      "        [0.5272],\n",
      "        [0.5816],\n",
      "        [0.6589],\n",
      "        [0.5396],\n",
      "        [0.1879],\n",
      "        [0.6279],\n",
      "        [0.5941],\n",
      "        [0.6131],\n",
      "        [0.6313],\n",
      "        [0.4831],\n",
      "        [0.5313],\n",
      "        [0.6158],\n",
      "        [0.6418],\n",
      "        [0.6826],\n",
      "        [0.5249],\n",
      "        [0.0448],\n",
      "        [0.8450],\n",
      "        [0.5986],\n",
      "        [0.6732],\n",
      "        [0.5966],\n",
      "        [0.6395],\n",
      "        [0.1508],\n",
      "        [0.8629],\n",
      "        [0.4282],\n",
      "        [0.4842],\n",
      "        [0.2053],\n",
      "        [0.6212],\n",
      "        [0.7463],\n",
      "        [0.7365],\n",
      "        [0.6263],\n",
      "        [0.6129],\n",
      "        [0.2560],\n",
      "        [0.2460],\n",
      "        [0.0920],\n",
      "        [0.6866],\n",
      "        [0.6282],\n",
      "        [0.8660],\n",
      "        [0.6255],\n",
      "        [0.0217],\n",
      "        [0.5659],\n",
      "        [0.5830],\n",
      "        [0.6417],\n",
      "        [0.6217],\n",
      "        [0.8816],\n",
      "        [0.2705],\n",
      "        [0.2125],\n",
      "        [0.4901],\n",
      "        [0.7250],\n",
      "        [0.5294],\n",
      "        [0.6726],\n",
      "        [0.0287],\n",
      "        [0.5620],\n",
      "        [0.6203],\n",
      "        [0.5799],\n",
      "        [0.2895],\n",
      "        [0.6486],\n",
      "        [0.5842],\n",
      "        [0.0948],\n",
      "        [0.6213],\n",
      "        [0.0948],\n",
      "        [0.0697],\n",
      "        [0.6087],\n",
      "        [0.5823],\n",
      "        [0.7300],\n",
      "        [0.3244],\n",
      "        [0.5005],\n",
      "        [0.6433],\n",
      "        [0.6022],\n",
      "        [0.4887],\n",
      "        [0.7799],\n",
      "        [0.2189],\n",
      "        [0.6238],\n",
      "        [0.1559],\n",
      "        [0.6034],\n",
      "        [0.3275],\n",
      "        [0.4346],\n",
      "        [0.5813],\n",
      "        [0.5989],\n",
      "        [0.1979],\n",
      "        [0.6196],\n",
      "        [0.6732],\n",
      "        [0.0947],\n",
      "        [0.2125],\n",
      "        [0.2403],\n",
      "        [0.5674],\n",
      "        [0.6682],\n",
      "        [0.6615],\n",
      "        [0.2654],\n",
      "        [0.2610],\n",
      "        [0.8634],\n",
      "        [0.6074],\n",
      "        [0.8839],\n",
      "        [0.2584],\n",
      "        [0.2364],\n",
      "        [0.0806],\n",
      "        [0.1308],\n",
      "        [0.4761],\n",
      "        [0.3186],\n",
      "        [0.2320],\n",
      "        [0.4116],\n",
      "        [0.5488],\n",
      "        [0.6169],\n",
      "        [0.6482],\n",
      "        [0.6150],\n",
      "        [0.8627],\n",
      "        [0.1590],\n",
      "        [0.6556],\n",
      "        [0.2576],\n",
      "        [0.3285],\n",
      "        [0.6166],\n",
      "        [0.1830],\n",
      "        [0.2983],\n",
      "        [0.2453],\n",
      "        [0.6395],\n",
      "        [0.8153],\n",
      "        [0.7332],\n",
      "        [0.2614],\n",
      "        [0.0951],\n",
      "        [0.2855],\n",
      "        [0.6432],\n",
      "        [0.2290],\n",
      "        [0.5501],\n",
      "        [0.5432],\n",
      "        [0.2825],\n",
      "        [0.6133],\n",
      "        [0.6104],\n",
      "        [0.8727],\n",
      "        [0.1003],\n",
      "        [0.8747],\n",
      "        [0.6507],\n",
      "        [0.2476],\n",
      "        [0.2403],\n",
      "        [0.6481],\n",
      "        [0.6099],\n",
      "        [0.2694],\n",
      "        [0.3089],\n",
      "        [0.2658],\n",
      "        [0.1267],\n",
      "        [0.5529],\n",
      "        [0.4264],\n",
      "        [0.7146],\n",
      "        [0.1330],\n",
      "        [0.3322],\n",
      "        [0.3844],\n",
      "        [0.7431],\n",
      "        [0.4118],\n",
      "        [0.3110],\n",
      "        [0.3263],\n",
      "        [0.1656],\n",
      "        [0.7987],\n",
      "        [0.8462],\n",
      "        [0.8006],\n",
      "        [0.1570],\n",
      "        [0.3963]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0027],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0103],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0122],\n",
      "        [0.0133],\n",
      "        [0.0136],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0168],\n",
      "        [0.0172],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0220],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0236],\n",
      "        [0.0240],\n",
      "        [0.0247],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0257],\n",
      "        [0.0262],\n",
      "        [0.0265],\n",
      "        [0.0272],\n",
      "        [0.0277],\n",
      "        [0.0285],\n",
      "        [0.0286],\n",
      "        [0.0299],\n",
      "        [0.0324],\n",
      "        [0.0334],\n",
      "        [0.0338],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0348],\n",
      "        [0.0353],\n",
      "        [0.0362],\n",
      "        [0.0365],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0374],\n",
      "        [0.0378],\n",
      "        [0.0388],\n",
      "        [0.0394],\n",
      "        [0.0397],\n",
      "        [0.0399],\n",
      "        [0.0421],\n",
      "        [0.0428],\n",
      "        [0.0431],\n",
      "        [0.0436],\n",
      "        [0.0441],\n",
      "        [0.0452],\n",
      "        [0.0468],\n",
      "        [0.0468],\n",
      "        [0.0479],\n",
      "        [0.0485],\n",
      "        [0.0487],\n",
      "        [0.0488],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0505],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0530],\n",
      "        [0.0535],\n",
      "        [0.0549],\n",
      "        [0.0568],\n",
      "        [0.0571],\n",
      "        [0.0576],\n",
      "        [0.0588],\n",
      "        [0.0605],\n",
      "        [0.0616],\n",
      "        [0.0626],\n",
      "        [0.0629],\n",
      "        [0.0649],\n",
      "        [0.0651],\n",
      "        [0.0661],\n",
      "        [0.0683],\n",
      "        [0.0691],\n",
      "        [0.0702],\n",
      "        [0.0705],\n",
      "        [0.0712],\n",
      "        [0.0725],\n",
      "        [0.0735],\n",
      "        [0.0737],\n",
      "        [0.0738],\n",
      "        [0.0739],\n",
      "        [0.0754],\n",
      "        [0.0774],\n",
      "        [0.0788],\n",
      "        [0.0791],\n",
      "        [0.0794],\n",
      "        [0.0805],\n",
      "        [0.0848],\n",
      "        [0.0849],\n",
      "        [0.0849],\n",
      "        [0.0862],\n",
      "        [0.0867],\n",
      "        [0.0892],\n",
      "        [0.0900],\n",
      "        [0.0929],\n",
      "        [0.0938],\n",
      "        [0.0961],\n",
      "        [0.1001],\n",
      "        [0.1002],\n",
      "        [0.1060],\n",
      "        [0.1189],\n",
      "        [0.1194],\n",
      "        [0.1217],\n",
      "        [0.1250],\n",
      "        [0.1256],\n",
      "        [0.1259],\n",
      "        [0.1343],\n",
      "        [0.1402],\n",
      "        [0.1424],\n",
      "        [0.1514],\n",
      "        [0.1570],\n",
      "        [0.1690]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0038],\n",
      "        [0.0070],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0055],\n",
      "        [0.0033],\n",
      "        [0.0006],\n",
      "        [0.0016],\n",
      "        [0.0078],\n",
      "        [0.0007],\n",
      "        [0.0025],\n",
      "        [0.0005],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0126],\n",
      "        [0.0023],\n",
      "        [0.0051],\n",
      "        [0.0111],\n",
      "        [0.0124],\n",
      "        [0.0153],\n",
      "        [0.0042],\n",
      "        [0.0062],\n",
      "        [0.0149],\n",
      "        [0.0094],\n",
      "        [0.0091],\n",
      "        [0.0080],\n",
      "        [0.0170],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0099],\n",
      "        [0.0208],\n",
      "        [0.0081],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0124],\n",
      "        [0.0165],\n",
      "        [0.0104],\n",
      "        [0.0111],\n",
      "        [0.0150],\n",
      "        [0.0132],\n",
      "        [0.0165],\n",
      "        [0.0225],\n",
      "        [0.0239],\n",
      "        [0.0130],\n",
      "        [0.0138],\n",
      "        [0.0220],\n",
      "        [0.0171],\n",
      "        [0.0178],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0172],\n",
      "        [0.0166],\n",
      "        [0.0231],\n",
      "        [0.0188],\n",
      "        [0.0194],\n",
      "        [0.0286],\n",
      "        [0.0197],\n",
      "        [0.0309],\n",
      "        [0.0297],\n",
      "        [0.0259],\n",
      "        [0.0197],\n",
      "        [0.0262],\n",
      "        [0.0266],\n",
      "        [0.0214],\n",
      "        [0.0239],\n",
      "        [0.0222],\n",
      "        [0.0255],\n",
      "        [0.0287],\n",
      "        [0.0291],\n",
      "        [0.0277],\n",
      "        [0.0305],\n",
      "        [0.0382],\n",
      "        [0.0379],\n",
      "        [0.0289],\n",
      "        [0.0341],\n",
      "        [0.0421],\n",
      "        [0.0329],\n",
      "        [0.0328],\n",
      "        [0.0330],\n",
      "        [0.0421],\n",
      "        [0.0410],\n",
      "        [0.0331],\n",
      "        [0.0455],\n",
      "        [0.0403],\n",
      "        [0.0389],\n",
      "        [0.0456],\n",
      "        [0.0386],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0490],\n",
      "        [0.0443],\n",
      "        [0.0504],\n",
      "        [0.0422],\n",
      "        [0.0515],\n",
      "        [0.0461],\n",
      "        [0.0523],\n",
      "        [0.0487],\n",
      "        [0.0474],\n",
      "        [0.0458],\n",
      "        [0.0544],\n",
      "        [0.0546],\n",
      "        [0.0486],\n",
      "        [0.0586],\n",
      "        [0.0507],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0554],\n",
      "        [0.0570],\n",
      "        [0.0540],\n",
      "        [0.0589],\n",
      "        [0.0592],\n",
      "        [0.0570],\n",
      "        [0.0663],\n",
      "        [0.0615],\n",
      "        [0.0611],\n",
      "        [0.0623],\n",
      "        [0.0729],\n",
      "        [0.0758],\n",
      "        [0.0745],\n",
      "        [0.0709],\n",
      "        [0.0690],\n",
      "        [0.0667],\n",
      "        [0.0705],\n",
      "        [0.0691],\n",
      "        [0.0695],\n",
      "        [0.0721],\n",
      "        [0.0713],\n",
      "        [0.0728],\n",
      "        [0.0825],\n",
      "        [0.0789],\n",
      "        [0.0783],\n",
      "        [0.0781],\n",
      "        [0.0885],\n",
      "        [0.0802],\n",
      "        [0.0796],\n",
      "        [0.0807],\n",
      "        [0.0857],\n",
      "        [0.0862],\n",
      "        [0.0894],\n",
      "        [0.0945],\n",
      "        [0.1012],\n",
      "        [0.1042],\n",
      "        [0.1062],\n",
      "        [0.1073],\n",
      "        [0.1153],\n",
      "        [0.1228],\n",
      "        [0.1275],\n",
      "        [0.1292],\n",
      "        [0.1219],\n",
      "        [0.1221],\n",
      "        [0.1357],\n",
      "        [0.1442],\n",
      "        [0.1457],\n",
      "        [0.1551],\n",
      "        [0.1582],\n",
      "        [0.1725]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 59.888436794281006\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.3505448609739688e-07, 95)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [95, 138, 79, 90, 75, 71, 31, 40, 89, 96, 37, 91, 88, 70, 39, 82, 38, 83, 99, 42, 4, 142, 14, 30, 81, 125, 51, 158, 5, 126, 76, 25, 74, 50, 77, 2, 54, 60, 157, 15, 150, 36, 133, 94, 105, 62, 134, 78, 64, 80, 41, 122, 17, 52, 3, 16, 87, 156, 69, 68, 121, 149, 151, 155, 53, 86, 92, 43, 72, 84, 93, 24, 97, 120, 67, 44, 140, 29, 32, 132, 13, 63, 135, 148, 137, 55, 85, 23, 108, 22, 136, 61, 109, 141, 98, 154, 127, 8, 73, 35, 49, 6, 130, 33, 104, 131, 1, 139, 45, 59, 107, 115, 123, 114, 34, 143, 26, 111, 65, 66, 106, 152, 47, 110, 48, 12, 128, 18, 27, 0, 153, 28, 124, 46, 7, 113, 119, 129, 112, 147, 58, 100, 21, 146, 116, 118, 117, 103, 20, 101, 144, 11, 9, 10, 145, 102, 57] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4867],\n",
      "        [0.1897],\n",
      "        [0.6172],\n",
      "        [0.5354],\n",
      "        [0.6201],\n",
      "        [0.6321],\n",
      "        [0.6891],\n",
      "        [0.6378],\n",
      "        [0.5439],\n",
      "        [0.4777],\n",
      "        [0.6793],\n",
      "        [0.5289],\n",
      "        [0.5316],\n",
      "        [0.5859],\n",
      "        [0.6652],\n",
      "        [0.6010],\n",
      "        [0.6842],\n",
      "        [0.5985],\n",
      "        [0.4324],\n",
      "        [0.6330],\n",
      "        [0.8652],\n",
      "        [0.1527],\n",
      "        [0.7507],\n",
      "        [0.6930],\n",
      "        [0.6171],\n",
      "        [0.2600],\n",
      "        [0.6343],\n",
      "        [0.0429],\n",
      "        [0.8480],\n",
      "        [0.2498],\n",
      "        [0.6459],\n",
      "        [0.6481],\n",
      "        [0.6297],\n",
      "        [0.6278],\n",
      "        [0.6435],\n",
      "        [0.8683],\n",
      "        [0.6041],\n",
      "        [0.4949],\n",
      "        [0.0231],\n",
      "        [0.7298],\n",
      "        [0.0921],\n",
      "        [0.6785],\n",
      "        [0.2086],\n",
      "        [0.4880],\n",
      "        [0.2739],\n",
      "        [0.5342],\n",
      "        [0.2158],\n",
      "        [0.6250],\n",
      "        [0.5663],\n",
      "        [0.6245],\n",
      "        [0.6277],\n",
      "        [0.2944],\n",
      "        [0.7426],\n",
      "        [0.6150],\n",
      "        [0.8836],\n",
      "        [0.7364],\n",
      "        [0.5705],\n",
      "        [0.0282],\n",
      "        [0.5876],\n",
      "        [0.5868],\n",
      "        [0.3288],\n",
      "        [0.0951],\n",
      "        [0.0945],\n",
      "        [0.0690],\n",
      "        [0.6083],\n",
      "        [0.5844],\n",
      "        [0.5043],\n",
      "        [0.6303],\n",
      "        [0.6476],\n",
      "        [0.5887],\n",
      "        [0.4925],\n",
      "        [0.6545],\n",
      "        [0.4392],\n",
      "        [0.3316],\n",
      "        [0.5858],\n",
      "        [0.6259],\n",
      "        [0.1580],\n",
      "        [0.6748],\n",
      "        [0.6683],\n",
      "        [0.2220],\n",
      "        [0.7837],\n",
      "        [0.5719],\n",
      "        [0.2157],\n",
      "        [0.0951],\n",
      "        [0.2001],\n",
      "        [0.6090],\n",
      "        [0.6032],\n",
      "        [0.6131],\n",
      "        [0.2635],\n",
      "        [0.6790],\n",
      "        [0.2431],\n",
      "        [0.4807],\n",
      "        [0.2610],\n",
      "        [0.1328],\n",
      "        [0.4160],\n",
      "        [0.0800],\n",
      "        [0.2691],\n",
      "        [0.8670],\n",
      "        [0.6210],\n",
      "        [0.6538],\n",
      "        [0.6208],\n",
      "        [0.8869],\n",
      "        [0.2399],\n",
      "        [0.6621],\n",
      "        [0.3218],\n",
      "        [0.2354],\n",
      "        [0.8649],\n",
      "        [0.1608],\n",
      "        [0.6224],\n",
      "        [0.5539],\n",
      "        [0.2603],\n",
      "        [0.3319],\n",
      "        [0.2503],\n",
      "        [0.3020],\n",
      "        [0.6455],\n",
      "        [0.1844],\n",
      "        [0.6501],\n",
      "        [0.2890],\n",
      "        [0.5548],\n",
      "        [0.5476],\n",
      "        [0.2322],\n",
      "        [0.0948],\n",
      "        [0.6195],\n",
      "        [0.2858],\n",
      "        [0.6165],\n",
      "        [0.8191],\n",
      "        [0.2654],\n",
      "        [0.7389],\n",
      "        [0.6574],\n",
      "        [0.8770],\n",
      "        [0.0999],\n",
      "        [0.6546],\n",
      "        [0.2451],\n",
      "        [0.6158],\n",
      "        [0.8761],\n",
      "        [0.2729],\n",
      "        [0.3127],\n",
      "        [0.2513],\n",
      "        [0.2693],\n",
      "        [0.1274],\n",
      "        [0.5581],\n",
      "        [0.4306],\n",
      "        [0.7205],\n",
      "        [0.1343],\n",
      "        [0.3357],\n",
      "        [0.3146],\n",
      "        [0.3301],\n",
      "        [0.3878],\n",
      "        [0.7489],\n",
      "        [0.4160],\n",
      "        [0.1670],\n",
      "        [0.8026],\n",
      "        [0.8496],\n",
      "        [0.8043],\n",
      "        [0.1582],\n",
      "        [0.3999],\n",
      "        [0.6116]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0033],\n",
      "        [0.0038],\n",
      "        [0.0042],\n",
      "        [0.0051],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0055],\n",
      "        [0.0062],\n",
      "        [0.0070],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0099],\n",
      "        [0.0104],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0111],\n",
      "        [0.0111],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0126],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0153],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0178],\n",
      "        [0.0180],\n",
      "        [0.0188],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0208],\n",
      "        [0.0214],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0225],\n",
      "        [0.0231],\n",
      "        [0.0239],\n",
      "        [0.0239],\n",
      "        [0.0255],\n",
      "        [0.0259],\n",
      "        [0.0262],\n",
      "        [0.0266],\n",
      "        [0.0277],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0289],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0309],\n",
      "        [0.0328],\n",
      "        [0.0329],\n",
      "        [0.0330],\n",
      "        [0.0331],\n",
      "        [0.0341],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0379],\n",
      "        [0.0382],\n",
      "        [0.0386],\n",
      "        [0.0389],\n",
      "        [0.0403],\n",
      "        [0.0410],\n",
      "        [0.0421],\n",
      "        [0.0421],\n",
      "        [0.0422],\n",
      "        [0.0443],\n",
      "        [0.0455],\n",
      "        [0.0456],\n",
      "        [0.0458],\n",
      "        [0.0461],\n",
      "        [0.0474],\n",
      "        [0.0486],\n",
      "        [0.0487],\n",
      "        [0.0490],\n",
      "        [0.0504],\n",
      "        [0.0507],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0515],\n",
      "        [0.0523],\n",
      "        [0.0540],\n",
      "        [0.0544],\n",
      "        [0.0546],\n",
      "        [0.0554],\n",
      "        [0.0570],\n",
      "        [0.0570],\n",
      "        [0.0586],\n",
      "        [0.0589],\n",
      "        [0.0592],\n",
      "        [0.0611],\n",
      "        [0.0615],\n",
      "        [0.0623],\n",
      "        [0.0663],\n",
      "        [0.0667],\n",
      "        [0.0690],\n",
      "        [0.0691],\n",
      "        [0.0695],\n",
      "        [0.0705],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0721],\n",
      "        [0.0728],\n",
      "        [0.0729],\n",
      "        [0.0745],\n",
      "        [0.0758],\n",
      "        [0.0781],\n",
      "        [0.0783],\n",
      "        [0.0789],\n",
      "        [0.0796],\n",
      "        [0.0802],\n",
      "        [0.0807],\n",
      "        [0.0825],\n",
      "        [0.0857],\n",
      "        [0.0862],\n",
      "        [0.0885],\n",
      "        [0.0894],\n",
      "        [0.0945],\n",
      "        [0.1012],\n",
      "        [0.1042],\n",
      "        [0.1062],\n",
      "        [0.1073],\n",
      "        [0.1153],\n",
      "        [0.1219],\n",
      "        [0.1221],\n",
      "        [0.1228],\n",
      "        [0.1275],\n",
      "        [0.1292],\n",
      "        [0.1357],\n",
      "        [0.1442],\n",
      "        [0.1457],\n",
      "        [0.1551],\n",
      "        [0.1582],\n",
      "        [0.1725],\n",
      "        [0.1879]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0021],\n",
      "        [0.0005],\n",
      "        [0.0016],\n",
      "        [0.0027],\n",
      "        [0.0035],\n",
      "        [0.0007],\n",
      "        [0.0030],\n",
      "        [0.0021],\n",
      "        [0.0019],\n",
      "        [0.0026],\n",
      "        [0.0051],\n",
      "        [0.0066],\n",
      "        [0.0040],\n",
      "        [0.0032],\n",
      "        [0.0048],\n",
      "        [0.0080],\n",
      "        [0.0062],\n",
      "        [0.0060],\n",
      "        [0.0086],\n",
      "        [0.0083],\n",
      "        [0.0143],\n",
      "        [0.0101],\n",
      "        [0.0123],\n",
      "        [0.0111],\n",
      "        [0.0127],\n",
      "        [0.0103],\n",
      "        [0.0119],\n",
      "        [0.0147],\n",
      "        [0.0079],\n",
      "        [0.0119],\n",
      "        [0.0103],\n",
      "        [0.0134],\n",
      "        [0.0154],\n",
      "        [0.0147],\n",
      "        [0.0123],\n",
      "        [0.0202],\n",
      "        [0.0139],\n",
      "        [0.0175],\n",
      "        [0.0161],\n",
      "        [0.0184],\n",
      "        [0.0146],\n",
      "        [0.0176],\n",
      "        [0.0173],\n",
      "        [0.0154],\n",
      "        [0.0174],\n",
      "        [0.0185],\n",
      "        [0.0174],\n",
      "        [0.0154],\n",
      "        [0.0206],\n",
      "        [0.0216],\n",
      "        [0.0201],\n",
      "        [0.0183],\n",
      "        [0.0201],\n",
      "        [0.0221],\n",
      "        [0.0165],\n",
      "        [0.0228],\n",
      "        [0.0213],\n",
      "        [0.0253],\n",
      "        [0.0223],\n",
      "        [0.0256],\n",
      "        [0.0248],\n",
      "        [0.0240],\n",
      "        [0.0240],\n",
      "        [0.0240],\n",
      "        [0.0285],\n",
      "        [0.0271],\n",
      "        [0.0304],\n",
      "        [0.0292],\n",
      "        [0.0315],\n",
      "        [0.0281],\n",
      "        [0.0321],\n",
      "        [0.0300],\n",
      "        [0.0331],\n",
      "        [0.0323],\n",
      "        [0.0347],\n",
      "        [0.0335],\n",
      "        [0.0347],\n",
      "        [0.0374],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0351],\n",
      "        [0.0403],\n",
      "        [0.0387],\n",
      "        [0.0386],\n",
      "        [0.0404],\n",
      "        [0.0408],\n",
      "        [0.0404],\n",
      "        [0.0429],\n",
      "        [0.0447],\n",
      "        [0.0445],\n",
      "        [0.0453],\n",
      "        [0.0469],\n",
      "        [0.0463],\n",
      "        [0.0480],\n",
      "        [0.0490],\n",
      "        [0.0461],\n",
      "        [0.0493],\n",
      "        [0.0463],\n",
      "        [0.0531],\n",
      "        [0.0524],\n",
      "        [0.0523],\n",
      "        [0.0467],\n",
      "        [0.0526],\n",
      "        [0.0545],\n",
      "        [0.0536],\n",
      "        [0.0548],\n",
      "        [0.0605],\n",
      "        [0.0580],\n",
      "        [0.0579],\n",
      "        [0.0574],\n",
      "        [0.0593],\n",
      "        [0.0592],\n",
      "        [0.0595],\n",
      "        [0.0611],\n",
      "        [0.0630],\n",
      "        [0.0649],\n",
      "        [0.0666],\n",
      "        [0.0688],\n",
      "        [0.0705],\n",
      "        [0.0711],\n",
      "        [0.0704],\n",
      "        [0.0686],\n",
      "        [0.0719],\n",
      "        [0.0719],\n",
      "        [0.0735],\n",
      "        [0.0696],\n",
      "        [0.0751],\n",
      "        [0.0745],\n",
      "        [0.0784],\n",
      "        [0.0835],\n",
      "        [0.0765],\n",
      "        [0.0799],\n",
      "        [0.0788],\n",
      "        [0.0815],\n",
      "        [0.0782],\n",
      "        [0.0854],\n",
      "        [0.0858],\n",
      "        [0.0888],\n",
      "        [0.0890],\n",
      "        [0.0928],\n",
      "        [0.0999],\n",
      "        [0.1037],\n",
      "        [0.1052],\n",
      "        [0.1061],\n",
      "        [0.1153],\n",
      "        [0.1217],\n",
      "        [0.1219],\n",
      "        [0.1219],\n",
      "        [0.1262],\n",
      "        [0.1288],\n",
      "        [0.1344],\n",
      "        [0.1410],\n",
      "        [0.1416],\n",
      "        [0.1516],\n",
      "        [0.1568],\n",
      "        [0.1717],\n",
      "        [0.1862]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 60.1758246421814\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.3712522079222254e-07, 138)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [138, 71, 79, 89, 95, 40, 96, 90, 31, 70, 75, 88, 39, 37, 83, 38, 91, 5, 82, 42, 99, 142, 76, 125, 30, 126, 51, 14, 77, 81, 25, 54, 4, 150, 158, 50, 94, 74, 78, 157, 3, 133, 105, 134, 60, 36, 122, 15, 62, 41, 17, 2, 64, 87, 80, 52, 69, 16, 151, 155, 149, 121, 156, 68, 86, 84, 53, 43, 24, 92, 72, 93, 120, 97, 44, 140, 67, 13, 29, 32, 132, 148, 135, 63, 85, 137, 55, 23, 22, 108, 136, 154, 8, 109, 6, 61, 141, 98, 127, 49, 35, 130, 73, 104, 33, 131, 59, 45, 139, 115, 107, 123, 1, 114, 34, 143, 26, 152, 111, 12, 106, 65, 66, 110, 47, 48, 18, 128, 153, 7, 27, 124, 28, 46, 0, 113, 119, 129, 112, 147, 58, 100, 21, 146, 116, 118, 103, 117, 20, 101, 144, 11, 9, 10, 145, 102, 57, 56] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1887],\n",
      "        [0.6298],\n",
      "        [0.6150],\n",
      "        [0.5425],\n",
      "        [0.4851],\n",
      "        [0.6375],\n",
      "        [0.4766],\n",
      "        [0.5339],\n",
      "        [0.6884],\n",
      "        [0.5839],\n",
      "        [0.6181],\n",
      "        [0.5304],\n",
      "        [0.6646],\n",
      "        [0.6785],\n",
      "        [0.5967],\n",
      "        [0.6834],\n",
      "        [0.5274],\n",
      "        [0.8435],\n",
      "        [0.5993],\n",
      "        [0.6329],\n",
      "        [0.4318],\n",
      "        [0.1519],\n",
      "        [0.6436],\n",
      "        [0.2607],\n",
      "        [0.6922],\n",
      "        [0.2503],\n",
      "        [0.6334],\n",
      "        [0.7483],\n",
      "        [0.6410],\n",
      "        [0.6150],\n",
      "        [0.6477],\n",
      "        [0.6028],\n",
      "        [0.8600],\n",
      "        [0.0901],\n",
      "        [0.0392],\n",
      "        [0.6268],\n",
      "        [0.4863],\n",
      "        [0.6275],\n",
      "        [0.6224],\n",
      "        [0.0235],\n",
      "        [0.8781],\n",
      "        [0.2090],\n",
      "        [0.2736],\n",
      "        [0.2162],\n",
      "        [0.4939],\n",
      "        [0.6774],\n",
      "        [0.2959],\n",
      "        [0.7279],\n",
      "        [0.5329],\n",
      "        [0.6274],\n",
      "        [0.7418],\n",
      "        [0.8631],\n",
      "        [0.5645],\n",
      "        [0.5692],\n",
      "        [0.6224],\n",
      "        [0.6143],\n",
      "        [0.5860],\n",
      "        [0.7358],\n",
      "        [0.0922],\n",
      "        [0.0664],\n",
      "        [0.0932],\n",
      "        [0.3295],\n",
      "        [0.0259],\n",
      "        [0.5851],\n",
      "        [0.5830],\n",
      "        [0.5871],\n",
      "        [0.6075],\n",
      "        [0.6299],\n",
      "        [0.6536],\n",
      "        [0.5026],\n",
      "        [0.6452],\n",
      "        [0.4910],\n",
      "        [0.3322],\n",
      "        [0.4388],\n",
      "        [0.6255],\n",
      "        [0.1574],\n",
      "        [0.5840],\n",
      "        [0.7806],\n",
      "        [0.6744],\n",
      "        [0.6680],\n",
      "        [0.2220],\n",
      "        [0.0933],\n",
      "        [0.2159],\n",
      "        [0.5702],\n",
      "        [0.6015],\n",
      "        [0.1995],\n",
      "        [0.6077],\n",
      "        [0.6124],\n",
      "        [0.6780],\n",
      "        [0.2631],\n",
      "        [0.2428],\n",
      "        [0.0775],\n",
      "        [0.8629],\n",
      "        [0.2608],\n",
      "        [0.8822],\n",
      "        [0.4797],\n",
      "        [0.1323],\n",
      "        [0.4155],\n",
      "        [0.2694],\n",
      "        [0.6197],\n",
      "        [0.6526],\n",
      "        [0.2402],\n",
      "        [0.6187],\n",
      "        [0.3210],\n",
      "        [0.6616],\n",
      "        [0.2356],\n",
      "        [0.5527],\n",
      "        [0.6215],\n",
      "        [0.1599],\n",
      "        [0.3319],\n",
      "        [0.2600],\n",
      "        [0.2520],\n",
      "        [0.8598],\n",
      "        [0.3023],\n",
      "        [0.6447],\n",
      "        [0.1830],\n",
      "        [0.6502],\n",
      "        [0.0926],\n",
      "        [0.2893],\n",
      "        [0.8157],\n",
      "        [0.2322],\n",
      "        [0.5534],\n",
      "        [0.5459],\n",
      "        [0.2860],\n",
      "        [0.6188],\n",
      "        [0.6158],\n",
      "        [0.7375],\n",
      "        [0.2660],\n",
      "        [0.0974],\n",
      "        [0.8718],\n",
      "        [0.6571],\n",
      "        [0.2465],\n",
      "        [0.6543],\n",
      "        [0.6151],\n",
      "        [0.8717],\n",
      "        [0.2732],\n",
      "        [0.3131],\n",
      "        [0.2516],\n",
      "        [0.2697],\n",
      "        [0.1257],\n",
      "        [0.5568],\n",
      "        [0.4301],\n",
      "        [0.7195],\n",
      "        [0.1331],\n",
      "        [0.3357],\n",
      "        [0.3148],\n",
      "        [0.3868],\n",
      "        [0.3302],\n",
      "        [0.7475],\n",
      "        [0.4156],\n",
      "        [0.1657],\n",
      "        [0.7994],\n",
      "        [0.8454],\n",
      "        [0.8009],\n",
      "        [0.1568],\n",
      "        [0.3991],\n",
      "        [0.6099],\n",
      "        [0.6114]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0007],\n",
      "        [0.0016],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0021],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0040],\n",
      "        [0.0048],\n",
      "        [0.0051],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0086],\n",
      "        [0.0101],\n",
      "        [0.0103],\n",
      "        [0.0103],\n",
      "        [0.0111],\n",
      "        [0.0119],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0127],\n",
      "        [0.0134],\n",
      "        [0.0139],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.0147],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0161],\n",
      "        [0.0165],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0213],\n",
      "        [0.0216],\n",
      "        [0.0221],\n",
      "        [0.0223],\n",
      "        [0.0228],\n",
      "        [0.0240],\n",
      "        [0.0240],\n",
      "        [0.0240],\n",
      "        [0.0248],\n",
      "        [0.0253],\n",
      "        [0.0256],\n",
      "        [0.0271],\n",
      "        [0.0281],\n",
      "        [0.0285],\n",
      "        [0.0292],\n",
      "        [0.0300],\n",
      "        [0.0304],\n",
      "        [0.0315],\n",
      "        [0.0321],\n",
      "        [0.0323],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0347],\n",
      "        [0.0347],\n",
      "        [0.0351],\n",
      "        [0.0374],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0386],\n",
      "        [0.0387],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0404],\n",
      "        [0.0408],\n",
      "        [0.0429],\n",
      "        [0.0445],\n",
      "        [0.0447],\n",
      "        [0.0453],\n",
      "        [0.0461],\n",
      "        [0.0463],\n",
      "        [0.0463],\n",
      "        [0.0467],\n",
      "        [0.0469],\n",
      "        [0.0480],\n",
      "        [0.0490],\n",
      "        [0.0493],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0526],\n",
      "        [0.0531],\n",
      "        [0.0536],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.0574],\n",
      "        [0.0579],\n",
      "        [0.0580],\n",
      "        [0.0592],\n",
      "        [0.0593],\n",
      "        [0.0595],\n",
      "        [0.0605],\n",
      "        [0.0611],\n",
      "        [0.0630],\n",
      "        [0.0649],\n",
      "        [0.0666],\n",
      "        [0.0686],\n",
      "        [0.0688],\n",
      "        [0.0696],\n",
      "        [0.0704],\n",
      "        [0.0705],\n",
      "        [0.0711],\n",
      "        [0.0719],\n",
      "        [0.0719],\n",
      "        [0.0735],\n",
      "        [0.0745],\n",
      "        [0.0751],\n",
      "        [0.0765],\n",
      "        [0.0782],\n",
      "        [0.0784],\n",
      "        [0.0788],\n",
      "        [0.0799],\n",
      "        [0.0815],\n",
      "        [0.0835],\n",
      "        [0.0854],\n",
      "        [0.0858],\n",
      "        [0.0888],\n",
      "        [0.0890],\n",
      "        [0.0928],\n",
      "        [0.0999],\n",
      "        [0.1037],\n",
      "        [0.1052],\n",
      "        [0.1061],\n",
      "        [0.1153],\n",
      "        [0.1217],\n",
      "        [0.1219],\n",
      "        [0.1219],\n",
      "        [0.1262],\n",
      "        [0.1288],\n",
      "        [0.1344],\n",
      "        [0.1410],\n",
      "        [0.1416],\n",
      "        [0.1516],\n",
      "        [0.1568],\n",
      "        [0.1717],\n",
      "        [0.1862],\n",
      "        [0.1910]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0021],\n",
      "        [    0.0046],\n",
      "        [    0.0052],\n",
      "        [    0.0006],\n",
      "        [    0.0050],\n",
      "        [    0.0000],\n",
      "        [    0.0002],\n",
      "        [    0.0055],\n",
      "        [    0.0055],\n",
      "        [    0.0003],\n",
      "        [    0.0070],\n",
      "        [    0.0017],\n",
      "        [    0.0023],\n",
      "        [    0.0077],\n",
      "        [    0.0028],\n",
      "        [    0.0036],\n",
      "        [    0.0094],\n",
      "        [    0.0021],\n",
      "        [    0.0111],\n",
      "        [    0.0102],\n",
      "        [    0.0104],\n",
      "        [    0.0114],\n",
      "        [    0.0064],\n",
      "        [    0.0103],\n",
      "        [    0.0136],\n",
      "        [    0.0121],\n",
      "        [    0.0148],\n",
      "        [    0.0159],\n",
      "        [    0.0084],\n",
      "        [    0.0162],\n",
      "        [    0.0154],\n",
      "        [    0.0107],\n",
      "        [    0.0207],\n",
      "        [    0.0124],\n",
      "        [    0.0185],\n",
      "        [    0.0176],\n",
      "        [    0.0125],\n",
      "        [    0.0191],\n",
      "        [    0.0115],\n",
      "        [    0.0161],\n",
      "        [    0.0097],\n",
      "        [    0.0171],\n",
      "        [    0.0185],\n",
      "        [    0.0177],\n",
      "        [    0.0201],\n",
      "        [    0.0203],\n",
      "        [    0.0176],\n",
      "        [    0.0216],\n",
      "        [    0.0213],\n",
      "        [    0.0221],\n",
      "        [    0.0178],\n",
      "        [    0.0266],\n",
      "        [    0.0238],\n",
      "        [    0.0187],\n",
      "        [    0.0251],\n",
      "        [    0.0247],\n",
      "        [    0.0192],\n",
      "        [    0.0249],\n",
      "        [    0.0214],\n",
      "        [    0.0211],\n",
      "        [    0.0218],\n",
      "        [    0.0248],\n",
      "        [    0.0278],\n",
      "        [    0.0287],\n",
      "        [    0.0243],\n",
      "        [    0.0252],\n",
      "        [    0.0312],\n",
      "        [    0.0314],\n",
      "        [    0.0275],\n",
      "        [    0.0333],\n",
      "        [    0.0354],\n",
      "        [    0.0349],\n",
      "        [    0.0325],\n",
      "        [    0.0348],\n",
      "        [    0.0356],\n",
      "        [    0.0358],\n",
      "        [    0.0380],\n",
      "        [    0.0307],\n",
      "        [    0.0396],\n",
      "        [    0.0397],\n",
      "        [    0.0373],\n",
      "        [    0.0365],\n",
      "        [    0.0392],\n",
      "        [    0.0435],\n",
      "        [    0.0373],\n",
      "        [    0.0392],\n",
      "        [    0.0377],\n",
      "        [    0.0450],\n",
      "        [    0.0421],\n",
      "        [    0.0455],\n",
      "        [    0.0442],\n",
      "        [    0.0433],\n",
      "        [    0.0408],\n",
      "        [    0.0468],\n",
      "        [    0.0406],\n",
      "        [    0.0494],\n",
      "        [    0.0490],\n",
      "        [    0.0506],\n",
      "        [    0.0489],\n",
      "        [    0.0553],\n",
      "        [    0.0552],\n",
      "        [    0.0522],\n",
      "        [    0.0569],\n",
      "        [    0.0518],\n",
      "        [    0.0566],\n",
      "        [    0.0542],\n",
      "        [    0.0545],\n",
      "        [    0.0605],\n",
      "        [    0.0595],\n",
      "        [    0.0598],\n",
      "        [    0.0602],\n",
      "        [    0.0586],\n",
      "        [    0.0669],\n",
      "        [    0.0615],\n",
      "        [    0.0654],\n",
      "        [    0.0631],\n",
      "        [    0.0683],\n",
      "        [    0.0661],\n",
      "        [    0.0690],\n",
      "        [    0.0649],\n",
      "        [    0.0710],\n",
      "        [    0.0734],\n",
      "        [    0.0742],\n",
      "        [    0.0722],\n",
      "        [    0.0744],\n",
      "        [    0.0761],\n",
      "        [    0.0716],\n",
      "        [    0.0749],\n",
      "        [    0.0738],\n",
      "        [    0.0726],\n",
      "        [    0.0803],\n",
      "        [    0.0781],\n",
      "        [    0.0820],\n",
      "        [    0.0841],\n",
      "        [    0.0900],\n",
      "        [    0.0857],\n",
      "        [    0.0862],\n",
      "        [    0.0884],\n",
      "        [    0.0893],\n",
      "        [    0.0907],\n",
      "        [    0.0969],\n",
      "        [    0.1021],\n",
      "        [    0.1025],\n",
      "        [    0.1044],\n",
      "        [    0.1159],\n",
      "        [    0.1222],\n",
      "        [    0.1199],\n",
      "        [    0.1224],\n",
      "        [    0.1233],\n",
      "        [    0.1273],\n",
      "        [    0.1326],\n",
      "        [    0.1365],\n",
      "        [    0.1360],\n",
      "        [    0.1468],\n",
      "        [    0.1549],\n",
      "        [    0.1698],\n",
      "        [    0.1826],\n",
      "        [    0.1877]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 60.46266484260559\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6816557035781443e-09, 40)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [40, 96, 70, 89, 88, 5, 138, 39, 83, 38, 71, 95, 79, 90, 31, 76, 75, 37, 77, 91, 3, 42, 125, 99, 54, 82, 142, 78, 126, 150, 94, 30, 51, 25, 14, 157, 81, 133, 50, 122, 134, 17, 158, 105, 87, 74, 69, 60, 36, 4, 155, 62, 151, 15, 149, 41, 64, 86, 52, 121, 16, 80, 84, 2, 24, 156, 68, 13, 53, 43, 120, 92, 97, 93, 72, 44, 140, 148, 85, 132, 55, 67, 137, 135, 29, 32, 6, 8, 22, 154, 63, 136, 23, 108, 109, 127, 141, 61, 98, 104, 130, 131, 59, 35, 49, 33, 73, 123, 139, 115, 107, 45, 114, 143, 12, 34, 152, 1, 26, 111, 106, 18, 110, 7, 65, 153, 66, 47, 128, 48, 124, 27, 28, 46, 113, 119, 129, 112, 0, 147, 58, 100, 21, 146, 116, 103, 118, 117, 20, 101, 144, 9, 11, 10, 145, 102, 57, 56, 19] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6353],\n",
      "        [0.4742],\n",
      "        [0.5804],\n",
      "        [0.5399],\n",
      "        [0.5281],\n",
      "        [0.8377],\n",
      "        [0.1871],\n",
      "        [0.6621],\n",
      "        [0.5935],\n",
      "        [0.6807],\n",
      "        [0.6259],\n",
      "        [0.4822],\n",
      "        [0.6113],\n",
      "        [0.5311],\n",
      "        [0.6859],\n",
      "        [0.6398],\n",
      "        [0.6145],\n",
      "        [0.6758],\n",
      "        [0.6370],\n",
      "        [0.5246],\n",
      "        [0.8714],\n",
      "        [0.6310],\n",
      "        [0.2606],\n",
      "        [0.4301],\n",
      "        [0.5995],\n",
      "        [0.5962],\n",
      "        [0.1506],\n",
      "        [0.6184],\n",
      "        [0.2501],\n",
      "        [0.0879],\n",
      "        [0.4835],\n",
      "        [0.6897],\n",
      "        [0.6306],\n",
      "        [0.6457],\n",
      "        [0.7446],\n",
      "        [0.0235],\n",
      "        [0.6116],\n",
      "        [0.2088],\n",
      "        [0.6239],\n",
      "        [0.2965],\n",
      "        [0.2159],\n",
      "        [0.7395],\n",
      "        [0.0355],\n",
      "        [0.2724],\n",
      "        [0.5666],\n",
      "        [0.6238],\n",
      "        [0.5830],\n",
      "        [0.4913],\n",
      "        [0.6747],\n",
      "        [0.8536],\n",
      "        [0.0636],\n",
      "        [0.5301],\n",
      "        [0.0897],\n",
      "        [0.7247],\n",
      "        [0.0909],\n",
      "        [0.6253],\n",
      "        [0.5613],\n",
      "        [0.5802],\n",
      "        [0.6117],\n",
      "        [0.3295],\n",
      "        [0.7337],\n",
      "        [0.6188],\n",
      "        [0.5842],\n",
      "        [0.8567],\n",
      "        [0.6511],\n",
      "        [0.0235],\n",
      "        [0.5821],\n",
      "        [0.7762],\n",
      "        [0.6048],\n",
      "        [0.6278],\n",
      "        [0.3320],\n",
      "        [0.4997],\n",
      "        [0.4372],\n",
      "        [0.4882],\n",
      "        [0.6414],\n",
      "        [0.6234],\n",
      "        [0.1563],\n",
      "        [0.0912],\n",
      "        [0.5984],\n",
      "        [0.2214],\n",
      "        [0.6046],\n",
      "        [0.5808],\n",
      "        [0.1983],\n",
      "        [0.2154],\n",
      "        [0.6722],\n",
      "        [0.6659],\n",
      "        [0.8761],\n",
      "        [0.8574],\n",
      "        [0.6755],\n",
      "        [0.0746],\n",
      "        [0.5670],\n",
      "        [0.2417],\n",
      "        [0.6103],\n",
      "        [0.2623],\n",
      "        [0.2602],\n",
      "        [0.2690],\n",
      "        [0.1312],\n",
      "        [0.4771],\n",
      "        [0.4139],\n",
      "        [0.3193],\n",
      "        [0.2398],\n",
      "        [0.2350],\n",
      "        [0.5498],\n",
      "        [0.6499],\n",
      "        [0.6168],\n",
      "        [0.6594],\n",
      "        [0.6149],\n",
      "        [0.2529],\n",
      "        [0.1583],\n",
      "        [0.3313],\n",
      "        [0.2590],\n",
      "        [0.6189],\n",
      "        [0.3020],\n",
      "        [0.1812],\n",
      "        [0.8111],\n",
      "        [0.6423],\n",
      "        [0.0900],\n",
      "        [0.8534],\n",
      "        [0.6485],\n",
      "        [0.2890],\n",
      "        [0.2316],\n",
      "        [0.7346],\n",
      "        [0.2857],\n",
      "        [0.8662],\n",
      "        [0.5504],\n",
      "        [0.0947],\n",
      "        [0.5428],\n",
      "        [0.6163],\n",
      "        [0.2658],\n",
      "        [0.6131],\n",
      "        [0.2472],\n",
      "        [0.6551],\n",
      "        [0.6522],\n",
      "        [0.6125],\n",
      "        [0.2728],\n",
      "        [0.3127],\n",
      "        [0.2512],\n",
      "        [0.2694],\n",
      "        [0.8652],\n",
      "        [0.1236],\n",
      "        [0.5537],\n",
      "        [0.4284],\n",
      "        [0.7168],\n",
      "        [0.1315],\n",
      "        [0.3351],\n",
      "        [0.3848],\n",
      "        [0.3143],\n",
      "        [0.3297],\n",
      "        [0.7446],\n",
      "        [0.4141],\n",
      "        [0.1639],\n",
      "        [0.8399],\n",
      "        [0.7949],\n",
      "        [0.7961],\n",
      "        [0.1549],\n",
      "        [0.3972],\n",
      "        [0.6063],\n",
      "        [0.6082],\n",
      "        [0.7423]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0006],\n",
      "        [    0.0017],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0023],\n",
      "        [    0.0028],\n",
      "        [    0.0036],\n",
      "        [    0.0046],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0055],\n",
      "        [    0.0055],\n",
      "        [    0.0064],\n",
      "        [    0.0070],\n",
      "        [    0.0077],\n",
      "        [    0.0084],\n",
      "        [    0.0094],\n",
      "        [    0.0097],\n",
      "        [    0.0102],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0107],\n",
      "        [    0.0111],\n",
      "        [    0.0114],\n",
      "        [    0.0115],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0125],\n",
      "        [    0.0136],\n",
      "        [    0.0148],\n",
      "        [    0.0154],\n",
      "        [    0.0159],\n",
      "        [    0.0161],\n",
      "        [    0.0162],\n",
      "        [    0.0171],\n",
      "        [    0.0176],\n",
      "        [    0.0176],\n",
      "        [    0.0177],\n",
      "        [    0.0178],\n",
      "        [    0.0185],\n",
      "        [    0.0185],\n",
      "        [    0.0187],\n",
      "        [    0.0191],\n",
      "        [    0.0192],\n",
      "        [    0.0201],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0211],\n",
      "        [    0.0213],\n",
      "        [    0.0214],\n",
      "        [    0.0216],\n",
      "        [    0.0218],\n",
      "        [    0.0221],\n",
      "        [    0.0238],\n",
      "        [    0.0243],\n",
      "        [    0.0247],\n",
      "        [    0.0248],\n",
      "        [    0.0249],\n",
      "        [    0.0251],\n",
      "        [    0.0252],\n",
      "        [    0.0266],\n",
      "        [    0.0275],\n",
      "        [    0.0278],\n",
      "        [    0.0287],\n",
      "        [    0.0307],\n",
      "        [    0.0312],\n",
      "        [    0.0314],\n",
      "        [    0.0325],\n",
      "        [    0.0333],\n",
      "        [    0.0348],\n",
      "        [    0.0349],\n",
      "        [    0.0354],\n",
      "        [    0.0356],\n",
      "        [    0.0358],\n",
      "        [    0.0365],\n",
      "        [    0.0373],\n",
      "        [    0.0373],\n",
      "        [    0.0377],\n",
      "        [    0.0380],\n",
      "        [    0.0392],\n",
      "        [    0.0392],\n",
      "        [    0.0396],\n",
      "        [    0.0397],\n",
      "        [    0.0406],\n",
      "        [    0.0408],\n",
      "        [    0.0421],\n",
      "        [    0.0433],\n",
      "        [    0.0435],\n",
      "        [    0.0442],\n",
      "        [    0.0450],\n",
      "        [    0.0455],\n",
      "        [    0.0468],\n",
      "        [    0.0489],\n",
      "        [    0.0490],\n",
      "        [    0.0494],\n",
      "        [    0.0506],\n",
      "        [    0.0518],\n",
      "        [    0.0522],\n",
      "        [    0.0542],\n",
      "        [    0.0545],\n",
      "        [    0.0552],\n",
      "        [    0.0553],\n",
      "        [    0.0566],\n",
      "        [    0.0569],\n",
      "        [    0.0586],\n",
      "        [    0.0595],\n",
      "        [    0.0598],\n",
      "        [    0.0602],\n",
      "        [    0.0605],\n",
      "        [    0.0615],\n",
      "        [    0.0631],\n",
      "        [    0.0649],\n",
      "        [    0.0654],\n",
      "        [    0.0661],\n",
      "        [    0.0669],\n",
      "        [    0.0683],\n",
      "        [    0.0690],\n",
      "        [    0.0710],\n",
      "        [    0.0716],\n",
      "        [    0.0722],\n",
      "        [    0.0726],\n",
      "        [    0.0734],\n",
      "        [    0.0738],\n",
      "        [    0.0742],\n",
      "        [    0.0744],\n",
      "        [    0.0749],\n",
      "        [    0.0761],\n",
      "        [    0.0781],\n",
      "        [    0.0803],\n",
      "        [    0.0820],\n",
      "        [    0.0841],\n",
      "        [    0.0857],\n",
      "        [    0.0862],\n",
      "        [    0.0884],\n",
      "        [    0.0893],\n",
      "        [    0.0900],\n",
      "        [    0.0907],\n",
      "        [    0.0969],\n",
      "        [    0.1021],\n",
      "        [    0.1025],\n",
      "        [    0.1044],\n",
      "        [    0.1159],\n",
      "        [    0.1199],\n",
      "        [    0.1222],\n",
      "        [    0.1224],\n",
      "        [    0.1233],\n",
      "        [    0.1273],\n",
      "        [    0.1326],\n",
      "        [    0.1360],\n",
      "        [    0.1365],\n",
      "        [    0.1468],\n",
      "        [    0.1549],\n",
      "        [    0.1698],\n",
      "        [    0.1826],\n",
      "        [    0.1877],\n",
      "        [    0.2811]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0014],\n",
      "        [    0.0007],\n",
      "        [    0.0011],\n",
      "        [    0.0003],\n",
      "        [    0.0024],\n",
      "        [    0.0023],\n",
      "        [    0.0018],\n",
      "        [    0.0005],\n",
      "        [    0.0027],\n",
      "        [    0.0015],\n",
      "        [    0.0057],\n",
      "        [    0.0050],\n",
      "        [    0.0059],\n",
      "        [    0.0053],\n",
      "        [    0.0076],\n",
      "        [    0.0055],\n",
      "        [    0.0076],\n",
      "        [    0.0099],\n",
      "        [    0.0073],\n",
      "        [    0.0093],\n",
      "        [    0.0044],\n",
      "        [    0.0112],\n",
      "        [    0.0087],\n",
      "        [    0.0093],\n",
      "        [    0.0093],\n",
      "        [    0.0111],\n",
      "        [    0.0108],\n",
      "        [    0.0105],\n",
      "        [    0.0105],\n",
      "        [    0.0125],\n",
      "        [    0.0126],\n",
      "        [    0.0158],\n",
      "        [    0.0160],\n",
      "        [    0.0171],\n",
      "        [    0.0193],\n",
      "        [    0.0135],\n",
      "        [    0.0166],\n",
      "        [    0.0186],\n",
      "        [    0.0189],\n",
      "        [    0.0153],\n",
      "        [    0.0163],\n",
      "        [    0.0155],\n",
      "        [    0.0194],\n",
      "        [    0.0169],\n",
      "        [    0.0191],\n",
      "        [    0.0199],\n",
      "        [    0.0188],\n",
      "        [    0.0205],\n",
      "        [    0.0227],\n",
      "        [    0.0258],\n",
      "        [    0.0209],\n",
      "        [    0.0216],\n",
      "        [    0.0212],\n",
      "        [    0.0245],\n",
      "        [    0.0219],\n",
      "        [    0.0234],\n",
      "        [    0.0246],\n",
      "        [    0.0246],\n",
      "        [    0.0256],\n",
      "        [    0.0234],\n",
      "        [    0.0269],\n",
      "        [    0.0257],\n",
      "        [    0.0254],\n",
      "        [    0.0316],\n",
      "        [    0.0256],\n",
      "        [    0.0251],\n",
      "        [    0.0292],\n",
      "        [    0.0267],\n",
      "        [    0.0322],\n",
      "        [    0.0324],\n",
      "        [    0.0312],\n",
      "        [    0.0333],\n",
      "        [    0.0336],\n",
      "        [    0.0347],\n",
      "        [    0.0364],\n",
      "        [    0.0366],\n",
      "        [    0.0352],\n",
      "        [    0.0367],\n",
      "        [    0.0372],\n",
      "        [    0.0385],\n",
      "        [    0.0365],\n",
      "        [    0.0387],\n",
      "        [    0.0398],\n",
      "        [    0.0379],\n",
      "        [    0.0415],\n",
      "        [    0.0414],\n",
      "        [    0.0359],\n",
      "        [    0.0365],\n",
      "        [    0.0398],\n",
      "        [    0.0430],\n",
      "        [    0.0442],\n",
      "        [    0.0448],\n",
      "        [    0.0466],\n",
      "        [    0.0440],\n",
      "        [    0.0452],\n",
      "        [    0.0503],\n",
      "        [    0.0481],\n",
      "        [    0.0496],\n",
      "        [    0.0494],\n",
      "        [    0.0529],\n",
      "        [    0.0536],\n",
      "        [    0.0555],\n",
      "        [    0.0536],\n",
      "        [    0.0573],\n",
      "        [    0.0568],\n",
      "        [    0.0582],\n",
      "        [    0.0577],\n",
      "        [    0.0559],\n",
      "        [    0.0592],\n",
      "        [    0.0589],\n",
      "        [    0.0586],\n",
      "        [    0.0620],\n",
      "        [    0.0600],\n",
      "        [    0.0630],\n",
      "        [    0.0607],\n",
      "        [    0.0672],\n",
      "        [    0.0660],\n",
      "        [    0.0717],\n",
      "        [    0.0698],\n",
      "        [    0.0673],\n",
      "        [    0.0690],\n",
      "        [    0.0689],\n",
      "        [    0.0704],\n",
      "        [    0.0683],\n",
      "        [    0.0739],\n",
      "        [    0.0735],\n",
      "        [    0.0748],\n",
      "        [    0.0757],\n",
      "        [    0.0767],\n",
      "        [    0.0774],\n",
      "        [    0.0756],\n",
      "        [    0.0821],\n",
      "        [    0.0838],\n",
      "        [    0.0855],\n",
      "        [    0.0841],\n",
      "        [    0.0850],\n",
      "        [    0.0899],\n",
      "        [    0.0876],\n",
      "        [    0.0950],\n",
      "        [    0.0908],\n",
      "        [    0.0958],\n",
      "        [    0.1033],\n",
      "        [    0.1000],\n",
      "        [    0.1049],\n",
      "        [    0.1150],\n",
      "        [    0.1206],\n",
      "        [    0.1211],\n",
      "        [    0.1214],\n",
      "        [    0.1204],\n",
      "        [    0.1287],\n",
      "        [    0.1327],\n",
      "        [    0.1315],\n",
      "        [    0.1326],\n",
      "        [    0.1428],\n",
      "        [    0.1550],\n",
      "        [    0.1708],\n",
      "        [    0.1810],\n",
      "        [    0.1865],\n",
      "        [    0.2783]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 60.74922037124634\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 16 個區塊累積花費時間(s) 1.2137699127197266\n",
      "<<The performance of 16 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2137699127197266\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1238.89\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1661.61\n",
      "The accuracy(2000) for l = 1: 82.39%\n",
      "The accuracy(3000) for l = 1: 91.19%\n",
      "The maximum error: tensor(7104.8750)\n",
      "The minimum error: tensor(6.3906)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 2027.8\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 2121.9\n",
      "The accuracy(2000) for l = 1: 25.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 2664.421875\n",
      "The minimum error: 1010.82421875\n",
      "------------------------------------------------------------\n",
      "0.8238993710691824\n",
      "<class 'float'>\n",
      "0.25\n",
      "<class 'float'>\n",
      "The <<17>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.266986929404084e-08, 85)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [85, 35, 92, 66, 36, 34, 134, 1, 84, 79, 91, 86, 72, 67, 75, 73, 71, 27, 121, 95, 50, 87, 33, 74, 122, 138, 78, 38, 146, 90, 153, 118, 13, 26, 47, 130, 77, 101, 21, 129, 65, 46, 83, 10, 154, 70, 56, 151, 147, 58, 145, 32, 117, 37, 11, 82, 60, 152, 80, 48, 20, 76, 0, 9, 12, 64, 116, 49, 39, 88, 93, 89, 136, 2, 68, 51, 4, 40, 144, 81, 131, 128, 63, 155, 133, 18, 28, 25, 150, 104, 59, 132, 105, 19, 137, 94, 57, 123, 100, 55, 126, 127, 119, 45, 31, 69, 29, 103, 111, 135, 110, 8, 41, 139, 148, 30, 107, 3, 14, 102, 22, 106, 149, 61, 62, 120, 43, 124, 44, 156, 23, 24, 109, 115, 42, 108, 125, 143, 157, 54, 17, 96, 158, 142, 112, 16, 99, 114, 113, 97, 5, 7, 140, 6, 141]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0087],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0099],\n",
      "        [0.0105],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0135],\n",
      "        [0.0153],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0186],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0234],\n",
      "        [0.0245],\n",
      "        [0.0246],\n",
      "        [0.0246],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0256],\n",
      "        [0.0257],\n",
      "        [0.0258],\n",
      "        [0.0267],\n",
      "        [0.0269],\n",
      "        [0.0292],\n",
      "        [0.0312],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0333],\n",
      "        [0.0336],\n",
      "        [0.0347],\n",
      "        [0.0352],\n",
      "        [0.0359],\n",
      "        [0.0364],\n",
      "        [0.0365],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0372],\n",
      "        [0.0379],\n",
      "        [0.0385],\n",
      "        [0.0387],\n",
      "        [0.0396],\n",
      "        [0.0398],\n",
      "        [0.0398],\n",
      "        [0.0414],\n",
      "        [0.0415],\n",
      "        [0.0430],\n",
      "        [0.0440],\n",
      "        [0.0442],\n",
      "        [0.0448],\n",
      "        [0.0452],\n",
      "        [0.0466],\n",
      "        [0.0481],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0503],\n",
      "        [0.0529],\n",
      "        [0.0536],\n",
      "        [0.0536],\n",
      "        [0.0555],\n",
      "        [0.0559],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0577],\n",
      "        [0.0582],\n",
      "        [0.0586],\n",
      "        [0.0589],\n",
      "        [0.0592],\n",
      "        [0.0600],\n",
      "        [0.0607],\n",
      "        [0.0620],\n",
      "        [0.0630],\n",
      "        [0.0660],\n",
      "        [0.0672],\n",
      "        [0.0673],\n",
      "        [0.0683],\n",
      "        [0.0689],\n",
      "        [0.0690],\n",
      "        [0.0698],\n",
      "        [0.0704],\n",
      "        [0.0735],\n",
      "        [0.0739],\n",
      "        [0.0748],\n",
      "        [0.0756],\n",
      "        [0.0757],\n",
      "        [0.0767],\n",
      "        [0.0774],\n",
      "        [0.0807],\n",
      "        [0.0821],\n",
      "        [0.0838],\n",
      "        [0.0841],\n",
      "        [0.0850],\n",
      "        [0.0855],\n",
      "        [0.0876],\n",
      "        [0.0899],\n",
      "        [0.0908],\n",
      "        [0.0931],\n",
      "        [0.0958],\n",
      "        [0.1000],\n",
      "        [0.1033],\n",
      "        [0.1044],\n",
      "        [0.1049],\n",
      "        [0.1150],\n",
      "        [0.1204],\n",
      "        [0.1206],\n",
      "        [0.1211],\n",
      "        [0.1214],\n",
      "        [0.1287],\n",
      "        [0.1315],\n",
      "        [0.1326],\n",
      "        [0.1327],\n",
      "        [0.1428],\n",
      "        [0.1550]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.266986929404084e-08, 85)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [85, 35, 92, 66, 36, 34, 134, 1, 84, 79, 91, 86, 72, 67, 75, 73, 71, 27, 121, 95, 50, 87, 33, 74, 122, 138, 78, 38, 146, 90, 153, 118, 13, 26, 47, 130, 77, 101, 21, 129, 65, 46, 83, 10, 154, 70, 56, 151, 147, 58, 145, 32, 117, 37, 11, 82, 60, 152, 80, 48, 20, 76, 0, 9, 12, 64, 116, 49, 39, 88, 93, 89, 136, 2, 68, 51, 4, 40, 144, 81, 131, 128, 63, 155, 133, 18, 28, 25, 150, 104, 59, 132, 105, 19, 137, 94, 57, 123, 100, 55, 126, 127, 119, 45, 31, 69, 29, 103, 111, 135, 110, 8, 41, 139, 148, 30, 107, 3, 14, 102, 22, 106, 149, 61, 62, 120, 43, 124, 44, 156, 23, 24, 109, 115, 42, 108, 125, 143, 157, 54, 17, 96, 158, 142, 112, 16, 99, 114, 113, 97, 5, 7, 140, 6, 141, 98] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5403],\n",
      "        [0.6603],\n",
      "        [0.4746],\n",
      "        [0.5796],\n",
      "        [0.6339],\n",
      "        [0.6787],\n",
      "        [0.1873],\n",
      "        [0.8334],\n",
      "        [0.5288],\n",
      "        [0.5934],\n",
      "        [0.4822],\n",
      "        [0.5313],\n",
      "        [0.6388],\n",
      "        [0.6248],\n",
      "        [0.6107],\n",
      "        [0.6359],\n",
      "        [0.6140],\n",
      "        [0.6838],\n",
      "        [0.2623],\n",
      "        [0.4312],\n",
      "        [0.5981],\n",
      "        [0.5247],\n",
      "        [0.6737],\n",
      "        [0.6174],\n",
      "        [0.2517],\n",
      "        [0.1512],\n",
      "        [0.5961],\n",
      "        [0.6299],\n",
      "        [0.0880],\n",
      "        [0.4835],\n",
      "        [0.0261],\n",
      "        [0.2988],\n",
      "        [0.7373],\n",
      "        [0.6876],\n",
      "        [0.6294],\n",
      "        [0.2173],\n",
      "        [0.6112],\n",
      "        [0.2740],\n",
      "        [0.6440],\n",
      "        [0.2103],\n",
      "        [0.5826],\n",
      "        [0.6227],\n",
      "        [0.5671],\n",
      "        [0.7413],\n",
      "        [0.0346],\n",
      "        [0.6231],\n",
      "        [0.4909],\n",
      "        [0.0633],\n",
      "        [0.0895],\n",
      "        [0.5298],\n",
      "        [0.0911],\n",
      "        [0.6724],\n",
      "        [0.3309],\n",
      "        [0.6241],\n",
      "        [0.7218],\n",
      "        [0.5804],\n",
      "        [0.5605],\n",
      "        [0.0261],\n",
      "        [0.5844],\n",
      "        [0.6108],\n",
      "        [0.6492],\n",
      "        [0.6183],\n",
      "        [0.8486],\n",
      "        [0.7722],\n",
      "        [0.7317],\n",
      "        [0.5816],\n",
      "        [0.3332],\n",
      "        [0.6038],\n",
      "        [0.6267],\n",
      "        [0.4996],\n",
      "        [0.4383],\n",
      "        [0.4884],\n",
      "        [0.1569],\n",
      "        [0.8714],\n",
      "        [0.6403],\n",
      "        [0.6034],\n",
      "        [0.8531],\n",
      "        [0.6224],\n",
      "        [0.0914],\n",
      "        [0.5983],\n",
      "        [0.2166],\n",
      "        [0.2225],\n",
      "        [0.5801],\n",
      "        [0.0315],\n",
      "        [0.1989],\n",
      "        [0.6733],\n",
      "        [0.6642],\n",
      "        [0.6703],\n",
      "        [0.0743],\n",
      "        [0.2638],\n",
      "        [0.5663],\n",
      "        [0.2424],\n",
      "        [0.2619],\n",
      "        [0.6087],\n",
      "        [0.1321],\n",
      "        [0.4152],\n",
      "        [0.4769],\n",
      "        [0.2704],\n",
      "        [0.3203],\n",
      "        [0.5490],\n",
      "        [0.2413],\n",
      "        [0.2364],\n",
      "        [0.2556],\n",
      "        [0.6153],\n",
      "        [0.6477],\n",
      "        [0.6140],\n",
      "        [0.6578],\n",
      "        [0.2606],\n",
      "        [0.3323],\n",
      "        [0.1586],\n",
      "        [0.3034],\n",
      "        [0.8068],\n",
      "        [0.6174],\n",
      "        [0.1811],\n",
      "        [0.0899],\n",
      "        [0.6406],\n",
      "        [0.2907],\n",
      "        [0.8620],\n",
      "        [0.7319],\n",
      "        [0.2337],\n",
      "        [0.6470],\n",
      "        [0.2875],\n",
      "        [0.0944],\n",
      "        [0.5499],\n",
      "        [0.5422],\n",
      "        [0.2497],\n",
      "        [0.6150],\n",
      "        [0.2675],\n",
      "        [0.6118],\n",
      "        [0.0261],\n",
      "        [0.6534],\n",
      "        [0.6505],\n",
      "        [0.2745],\n",
      "        [0.3139],\n",
      "        [0.6111],\n",
      "        [0.2711],\n",
      "        [0.2526],\n",
      "        [0.1236],\n",
      "        [0.0261],\n",
      "        [0.5527],\n",
      "        [0.7143],\n",
      "        [0.4296],\n",
      "        [0.0261],\n",
      "        [0.1320],\n",
      "        [0.3360],\n",
      "        [0.7417],\n",
      "        [0.3855],\n",
      "        [0.3154],\n",
      "        [0.3308],\n",
      "        [0.4154],\n",
      "        [0.8354],\n",
      "        [0.7910],\n",
      "        [0.1640],\n",
      "        [0.7920],\n",
      "        [0.1550],\n",
      "        [0.3981]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0027],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0073],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0087],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0099],\n",
      "        [0.0105],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0135],\n",
      "        [0.0153],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0186],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0234],\n",
      "        [0.0245],\n",
      "        [0.0246],\n",
      "        [0.0246],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0256],\n",
      "        [0.0257],\n",
      "        [0.0258],\n",
      "        [0.0267],\n",
      "        [0.0269],\n",
      "        [0.0292],\n",
      "        [0.0312],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0333],\n",
      "        [0.0336],\n",
      "        [0.0347],\n",
      "        [0.0352],\n",
      "        [0.0359],\n",
      "        [0.0364],\n",
      "        [0.0365],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0372],\n",
      "        [0.0379],\n",
      "        [0.0385],\n",
      "        [0.0387],\n",
      "        [0.0396],\n",
      "        [0.0398],\n",
      "        [0.0398],\n",
      "        [0.0414],\n",
      "        [0.0415],\n",
      "        [0.0430],\n",
      "        [0.0440],\n",
      "        [0.0442],\n",
      "        [0.0448],\n",
      "        [0.0452],\n",
      "        [0.0466],\n",
      "        [0.0481],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0503],\n",
      "        [0.0529],\n",
      "        [0.0536],\n",
      "        [0.0536],\n",
      "        [0.0555],\n",
      "        [0.0559],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0577],\n",
      "        [0.0582],\n",
      "        [0.0586],\n",
      "        [0.0589],\n",
      "        [0.0592],\n",
      "        [0.0600],\n",
      "        [0.0607],\n",
      "        [0.0620],\n",
      "        [0.0630],\n",
      "        [0.0660],\n",
      "        [0.0672],\n",
      "        [0.0673],\n",
      "        [0.0683],\n",
      "        [0.0689],\n",
      "        [0.0690],\n",
      "        [0.0698],\n",
      "        [0.0704],\n",
      "        [0.0735],\n",
      "        [0.0739],\n",
      "        [0.0748],\n",
      "        [0.0756],\n",
      "        [0.0757],\n",
      "        [0.0767],\n",
      "        [0.0774],\n",
      "        [0.0807],\n",
      "        [0.0821],\n",
      "        [0.0838],\n",
      "        [0.0841],\n",
      "        [0.0850],\n",
      "        [0.0855],\n",
      "        [0.0876],\n",
      "        [0.0899],\n",
      "        [0.0908],\n",
      "        [0.0931],\n",
      "        [0.0958],\n",
      "        [0.1000],\n",
      "        [0.1033],\n",
      "        [0.1044],\n",
      "        [0.1049],\n",
      "        [0.1150],\n",
      "        [0.1204],\n",
      "        [0.1206],\n",
      "        [0.1211],\n",
      "        [0.1214],\n",
      "        [0.1287],\n",
      "        [0.1315],\n",
      "        [0.1326],\n",
      "        [0.1327],\n",
      "        [0.1428],\n",
      "        [0.1550],\n",
      "        [0.1708]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0041],\n",
      "        [0.0069],\n",
      "        [0.0056],\n",
      "        [0.0038],\n",
      "        [0.0052],\n",
      "        [0.0078],\n",
      "        [0.0026],\n",
      "        [0.0011],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0004],\n",
      "        [0.0009],\n",
      "        [0.0101],\n",
      "        [0.0010],\n",
      "        [0.0018],\n",
      "        [0.0115],\n",
      "        [0.0028],\n",
      "        [0.0011],\n",
      "        [0.0034],\n",
      "        [0.0042],\n",
      "        [0.0154],\n",
      "        [0.0047],\n",
      "        [0.0039],\n",
      "        [0.0143],\n",
      "        [0.0052],\n",
      "        [0.0059],\n",
      "        [0.0070],\n",
      "        [0.0043],\n",
      "        [0.0166],\n",
      "        [0.0173],\n",
      "        [0.0032],\n",
      "        [0.0098],\n",
      "        [0.0218],\n",
      "        [0.0094],\n",
      "        [0.0093],\n",
      "        [0.0102],\n",
      "        [0.0125],\n",
      "        [0.0114],\n",
      "        [0.0101],\n",
      "        [0.0245],\n",
      "        [0.0239],\n",
      "        [0.0123],\n",
      "        [0.0233],\n",
      "        [0.0152],\n",
      "        [0.0112],\n",
      "        [0.0151],\n",
      "        [0.0145],\n",
      "        [0.0243],\n",
      "        [0.0252],\n",
      "        [0.0156],\n",
      "        [0.0260],\n",
      "        [0.0171],\n",
      "        [0.0180],\n",
      "        [0.0168],\n",
      "        [0.0197],\n",
      "        [0.0289],\n",
      "        [0.0189],\n",
      "        [0.0084],\n",
      "        [0.0297],\n",
      "        [0.0188],\n",
      "        [0.0318],\n",
      "        [0.0216],\n",
      "        [0.0250],\n",
      "        [0.0298],\n",
      "        [0.0202],\n",
      "        [0.0238],\n",
      "        [0.0259],\n",
      "        [0.0256],\n",
      "        [0.0258],\n",
      "        [0.0288],\n",
      "        [0.0282],\n",
      "        [0.0301],\n",
      "        [0.0304],\n",
      "        [0.0366],\n",
      "        [0.0318],\n",
      "        [0.0425],\n",
      "        [0.0383],\n",
      "        [0.0301],\n",
      "        [0.0407],\n",
      "        [0.0415],\n",
      "        [0.0319],\n",
      "        [0.0441],\n",
      "        [0.0331],\n",
      "        [0.0283],\n",
      "        [0.0447],\n",
      "        [0.0456],\n",
      "        [0.0348],\n",
      "        [0.0345],\n",
      "        [0.0465],\n",
      "        [0.0389],\n",
      "        [0.0382],\n",
      "        [0.0505],\n",
      "        [0.0402],\n",
      "        [0.0410],\n",
      "        [0.0434],\n",
      "        [0.0440],\n",
      "        [0.0435],\n",
      "        [0.0556],\n",
      "        [0.0579],\n",
      "        [0.0597],\n",
      "        [0.0591],\n",
      "        [0.0610],\n",
      "        [0.0504],\n",
      "        [0.0502],\n",
      "        [0.0522],\n",
      "        [0.0531],\n",
      "        [0.0524],\n",
      "        [0.0531],\n",
      "        [0.0538],\n",
      "        [0.0548],\n",
      "        [0.0546],\n",
      "        [0.0636],\n",
      "        [0.0558],\n",
      "        [0.0675],\n",
      "        [0.0698],\n",
      "        [0.0618],\n",
      "        [0.0619],\n",
      "        [0.0696],\n",
      "        [0.0747],\n",
      "        [0.0633],\n",
      "        [0.0622],\n",
      "        [0.0652],\n",
      "        [0.0772],\n",
      "        [0.0678],\n",
      "        [0.0692],\n",
      "        [0.0700],\n",
      "        [0.0689],\n",
      "        [0.0823],\n",
      "        [0.0707],\n",
      "        [0.0640],\n",
      "        [0.0750],\n",
      "        [0.0768],\n",
      "        [0.0787],\n",
      "        [0.0797],\n",
      "        [0.0790],\n",
      "        [0.0820],\n",
      "        [0.0954],\n",
      "        [0.0949],\n",
      "        [0.0764],\n",
      "        [0.1019],\n",
      "        [0.1058],\n",
      "        [0.1084],\n",
      "        [0.0877],\n",
      "        [0.1094],\n",
      "        [0.1100],\n",
      "        [0.1262],\n",
      "        [0.1254],\n",
      "        [0.1159],\n",
      "        [0.1162],\n",
      "        [0.1338],\n",
      "        [0.1335],\n",
      "        [0.1354],\n",
      "        [0.1371],\n",
      "        [0.1454],\n",
      "        [0.1594],\n",
      "        [0.1754]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 61.329092502593994\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5051810464683513e-07, 91)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [91, 86, 67, 27, 1, 75, 134, 71, 153, 121, 66, 33, 85, 95, 38, 87, 36, 122, 92, 138, 84, 79, 35, 78, 34, 152, 47, 26, 118, 72, 21, 130, 154, 101, 73, 46, 77, 74, 56, 70, 10, 50, 58, 146, 37, 32, 90, 117, 48, 60, 11, 12, 76, 13, 83, 64, 65, 151, 129, 0, 147, 49, 39, 116, 145, 93, 155, 88, 82, 80, 9, 40, 89, 136, 68, 20, 131, 63, 25, 28, 2, 59, 4, 104, 105, 144, 19, 81, 51, 137, 57, 94, 128, 133, 18, 150, 45, 119, 132, 31, 29, 103, 69, 111, 110, 135, 123, 41, 100, 126, 55, 127, 30, 107, 22, 102, 8, 156, 106, 139, 61, 43, 62, 3, 148, 120, 44, 14, 23, 157, 24, 149, 109, 42, 115, 108, 124, 158, 143, 125, 54, 17, 96, 142, 112, 114, 113, 99, 16, 5, 97, 7, 140, 6, 141, 98, 53] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4868],\n",
      "        [0.5357],\n",
      "        [0.6296],\n",
      "        [0.6903],\n",
      "        [0.8345],\n",
      "        [0.6147],\n",
      "        [0.1917],\n",
      "        [0.6188],\n",
      "        [0.0428],\n",
      "        [0.2675],\n",
      "        [0.5845],\n",
      "        [0.6797],\n",
      "        [0.5447],\n",
      "        [0.4363],\n",
      "        [0.6368],\n",
      "        [0.5293],\n",
      "        [0.6405],\n",
      "        [0.2570],\n",
      "        [0.4796],\n",
      "        [0.1561],\n",
      "        [0.5330],\n",
      "        [0.5975],\n",
      "        [0.6667],\n",
      "        [0.6003],\n",
      "        [0.6849],\n",
      "        [0.0428],\n",
      "        [0.6361],\n",
      "        [0.6940],\n",
      "        [0.3044],\n",
      "        [0.6434],\n",
      "        [0.6509],\n",
      "        [0.2234],\n",
      "        [0.0428],\n",
      "        [0.2795],\n",
      "        [0.6401],\n",
      "        [0.6292],\n",
      "        [0.6153],\n",
      "        [0.6213],\n",
      "        [0.4969],\n",
      "        [0.6278],\n",
      "        [0.7454],\n",
      "        [0.6043],\n",
      "        [0.5358],\n",
      "        [0.0921],\n",
      "        [0.6306],\n",
      "        [0.6779],\n",
      "        [0.4882],\n",
      "        [0.3363],\n",
      "        [0.6176],\n",
      "        [0.5662],\n",
      "        [0.7266],\n",
      "        [0.7384],\n",
      "        [0.6223],\n",
      "        [0.7436],\n",
      "        [0.5713],\n",
      "        [0.5869],\n",
      "        [0.5877],\n",
      "        [0.0667],\n",
      "        [0.2162],\n",
      "        [0.8494],\n",
      "        [0.0935],\n",
      "        [0.6104],\n",
      "        [0.6333],\n",
      "        [0.3386],\n",
      "        [0.0952],\n",
      "        [0.4438],\n",
      "        [0.0428],\n",
      "        [0.5041],\n",
      "        [0.5847],\n",
      "        [0.5887],\n",
      "        [0.7753],\n",
      "        [0.6289],\n",
      "        [0.4929],\n",
      "        [0.1617],\n",
      "        [0.6449],\n",
      "        [0.6554],\n",
      "        [0.2226],\n",
      "        [0.5856],\n",
      "        [0.6772],\n",
      "        [0.6709],\n",
      "        [0.8721],\n",
      "        [0.5723],\n",
      "        [0.8550],\n",
      "        [0.2689],\n",
      "        [0.2669],\n",
      "        [0.0954],\n",
      "        [0.6143],\n",
      "        [0.6026],\n",
      "        [0.6094],\n",
      "        [0.1369],\n",
      "        [0.4830],\n",
      "        [0.4205],\n",
      "        [0.2282],\n",
      "        [0.2038],\n",
      "        [0.6790],\n",
      "        [0.0779],\n",
      "        [0.6218],\n",
      "        [0.2610],\n",
      "        [0.2480],\n",
      "        [0.6528],\n",
      "        [0.6637],\n",
      "        [0.2661],\n",
      "        [0.6186],\n",
      "        [0.3373],\n",
      "        [0.3088],\n",
      "        [0.1630],\n",
      "        [0.2758],\n",
      "        [0.6237],\n",
      "        [0.3253],\n",
      "        [0.2468],\n",
      "        [0.5550],\n",
      "        [0.2419],\n",
      "        [0.6460],\n",
      "        [0.2961],\n",
      "        [0.6546],\n",
      "        [0.2394],\n",
      "        [0.8097],\n",
      "        [0.0428],\n",
      "        [0.2927],\n",
      "        [0.1856],\n",
      "        [0.5560],\n",
      "        [0.6218],\n",
      "        [0.5479],\n",
      "        [0.8632],\n",
      "        [0.0938],\n",
      "        [0.2552],\n",
      "        [0.6186],\n",
      "        [0.7377],\n",
      "        [0.6605],\n",
      "        [0.0428],\n",
      "        [0.6574],\n",
      "        [0.0981],\n",
      "        [0.2799],\n",
      "        [0.6176],\n",
      "        [0.3192],\n",
      "        [0.2767],\n",
      "        [0.2731],\n",
      "        [0.0428],\n",
      "        [0.1277],\n",
      "        [0.2582],\n",
      "        [0.5588],\n",
      "        [0.7202],\n",
      "        [0.4348],\n",
      "        [0.1364],\n",
      "        [0.3411],\n",
      "        [0.3206],\n",
      "        [0.3360],\n",
      "        [0.3904],\n",
      "        [0.7476],\n",
      "        [0.8374],\n",
      "        [0.4206],\n",
      "        [0.7939],\n",
      "        [0.1684],\n",
      "        [0.7947],\n",
      "        [0.1594],\n",
      "        [0.4028],\n",
      "        [0.6105]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0011],\n",
      "        [0.0018],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0052],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0078],\n",
      "        [0.0084],\n",
      "        [0.0093],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0143],\n",
      "        [0.0145],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.0171],\n",
      "        [0.0173],\n",
      "        [0.0180],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0216],\n",
      "        [0.0218],\n",
      "        [0.0233],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0256],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0282],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0301],\n",
      "        [0.0301],\n",
      "        [0.0304],\n",
      "        [0.0318],\n",
      "        [0.0318],\n",
      "        [0.0319],\n",
      "        [0.0331],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0366],\n",
      "        [0.0382],\n",
      "        [0.0383],\n",
      "        [0.0389],\n",
      "        [0.0402],\n",
      "        [0.0407],\n",
      "        [0.0410],\n",
      "        [0.0415],\n",
      "        [0.0425],\n",
      "        [0.0434],\n",
      "        [0.0435],\n",
      "        [0.0440],\n",
      "        [0.0441],\n",
      "        [0.0447],\n",
      "        [0.0456],\n",
      "        [0.0465],\n",
      "        [0.0502],\n",
      "        [0.0504],\n",
      "        [0.0505],\n",
      "        [0.0522],\n",
      "        [0.0524],\n",
      "        [0.0531],\n",
      "        [0.0531],\n",
      "        [0.0538],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0556],\n",
      "        [0.0558],\n",
      "        [0.0579],\n",
      "        [0.0591],\n",
      "        [0.0597],\n",
      "        [0.0610],\n",
      "        [0.0618],\n",
      "        [0.0619],\n",
      "        [0.0622],\n",
      "        [0.0633],\n",
      "        [0.0636],\n",
      "        [0.0640],\n",
      "        [0.0652],\n",
      "        [0.0675],\n",
      "        [0.0678],\n",
      "        [0.0689],\n",
      "        [0.0692],\n",
      "        [0.0696],\n",
      "        [0.0698],\n",
      "        [0.0700],\n",
      "        [0.0707],\n",
      "        [0.0747],\n",
      "        [0.0750],\n",
      "        [0.0764],\n",
      "        [0.0768],\n",
      "        [0.0772],\n",
      "        [0.0787],\n",
      "        [0.0790],\n",
      "        [0.0797],\n",
      "        [0.0820],\n",
      "        [0.0823],\n",
      "        [0.0877],\n",
      "        [0.0949],\n",
      "        [0.0954],\n",
      "        [0.1019],\n",
      "        [0.1058],\n",
      "        [0.1084],\n",
      "        [0.1094],\n",
      "        [0.1100],\n",
      "        [0.1159],\n",
      "        [0.1162],\n",
      "        [0.1254],\n",
      "        [0.1262],\n",
      "        [0.1335],\n",
      "        [0.1338],\n",
      "        [0.1354],\n",
      "        [0.1371],\n",
      "        [0.1454],\n",
      "        [0.1594],\n",
      "        [0.1754],\n",
      "        [0.1868]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0025],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0015],\n",
      "        [    0.0061],\n",
      "        [    0.0045],\n",
      "        [    0.0001],\n",
      "        [    0.0046],\n",
      "        [    0.0137],\n",
      "        [    0.0046],\n",
      "        [    0.0017],\n",
      "        [    0.0047],\n",
      "        [    0.0020],\n",
      "        [    0.0056],\n",
      "        [    0.0042],\n",
      "        [    0.0068],\n",
      "        [    0.0049],\n",
      "        [    0.0063],\n",
      "        [    0.0039],\n",
      "        [    0.0082],\n",
      "        [    0.0044],\n",
      "        [    0.0045],\n",
      "        [    0.0064],\n",
      "        [    0.0093],\n",
      "        [    0.0072],\n",
      "        [    0.0021],\n",
      "        [    0.0097],\n",
      "        [    0.0098],\n",
      "        [    0.0104],\n",
      "        [    0.0080],\n",
      "        [    0.0099],\n",
      "        [    0.0108],\n",
      "        [    0.0006],\n",
      "        [    0.0126],\n",
      "        [    0.0089],\n",
      "        [    0.0129],\n",
      "        [    0.0150],\n",
      "        [    0.0114],\n",
      "        [    0.0155],\n",
      "        [    0.0171],\n",
      "        [    0.0172],\n",
      "        [    0.0144],\n",
      "        [    0.0166],\n",
      "        [    0.0134],\n",
      "        [    0.0172],\n",
      "        [    0.0184],\n",
      "        [    0.0152],\n",
      "        [    0.0188],\n",
      "        [    0.0192],\n",
      "        [    0.0201],\n",
      "        [    0.0211],\n",
      "        [    0.0197],\n",
      "        [    0.0242],\n",
      "        [    0.0219],\n",
      "        [    0.0211],\n",
      "        [    0.0252],\n",
      "        [    0.0223],\n",
      "        [    0.0201],\n",
      "        [    0.0236],\n",
      "        [    0.0303],\n",
      "        [    0.0217],\n",
      "        [    0.0262],\n",
      "        [    0.0260],\n",
      "        [    0.0268],\n",
      "        [    0.0228],\n",
      "        [    0.0293],\n",
      "        [    0.0178],\n",
      "        [    0.0310],\n",
      "        [    0.0268],\n",
      "        [    0.0276],\n",
      "        [    0.0268],\n",
      "        [    0.0304],\n",
      "        [    0.0322],\n",
      "        [    0.0328],\n",
      "        [    0.0340],\n",
      "        [    0.0313],\n",
      "        [    0.0327],\n",
      "        [    0.0345],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0312],\n",
      "        [    0.0392],\n",
      "        [    0.0339],\n",
      "        [    0.0400],\n",
      "        [    0.0412],\n",
      "        [    0.0375],\n",
      "        [    0.0420],\n",
      "        [    0.0394],\n",
      "        [    0.0415],\n",
      "        [    0.0457],\n",
      "        [    0.0445],\n",
      "        [    0.0453],\n",
      "        [    0.0429],\n",
      "        [    0.0427],\n",
      "        [    0.0448],\n",
      "        [    0.0425],\n",
      "        [    0.0509],\n",
      "        [    0.0512],\n",
      "        [    0.0492],\n",
      "        [    0.0539],\n",
      "        [    0.0532],\n",
      "        [    0.0539],\n",
      "        [    0.0554],\n",
      "        [    0.0550],\n",
      "        [    0.0555],\n",
      "        [    0.0578],\n",
      "        [    0.0544],\n",
      "        [    0.0564],\n",
      "        [    0.0562],\n",
      "        [    0.0580],\n",
      "        [    0.0587],\n",
      "        [    0.0598],\n",
      "        [    0.0631],\n",
      "        [    0.0627],\n",
      "        [    0.0614],\n",
      "        [    0.0640],\n",
      "        [    0.0604],\n",
      "        [    0.0534],\n",
      "        [    0.0661],\n",
      "        [    0.0647],\n",
      "        [    0.0688],\n",
      "        [    0.0692],\n",
      "        [    0.0705],\n",
      "        [    0.0648],\n",
      "        [    0.0663],\n",
      "        [    0.0708],\n",
      "        [    0.0711],\n",
      "        [    0.0741],\n",
      "        [    0.0748],\n",
      "        [    0.0659],\n",
      "        [    0.0768],\n",
      "        [    0.0735],\n",
      "        [    0.0796],\n",
      "        [    0.0795],\n",
      "        [    0.0807],\n",
      "        [    0.0828],\n",
      "        [    0.0813],\n",
      "        [    0.0771],\n",
      "        [    0.0917],\n",
      "        [    0.0944],\n",
      "        [    0.1008],\n",
      "        [    0.1052],\n",
      "        [    0.1072],\n",
      "        [    0.1067],\n",
      "        [    0.1111],\n",
      "        [    0.1170],\n",
      "        [    0.1173],\n",
      "        [    0.1237],\n",
      "        [    0.1256],\n",
      "        [    0.1292],\n",
      "        [    0.1326],\n",
      "        [    0.1322],\n",
      "        [    0.1344],\n",
      "        [    0.1419],\n",
      "        [    0.1567],\n",
      "        [    0.1736],\n",
      "        [    0.1854]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 61.61604595184326\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.990895448500396e-08, 134)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [134, 154, 27, 66, 85, 152, 91, 86, 67, 92, 38, 84, 75, 79, 121, 71, 33, 36, 95, 1, 122, 35, 87, 34, 72, 138, 73, 78, 47, 26, 21, 118, 130, 74, 101, 46, 146, 153, 50, 77, 90, 56, 58, 70, 37, 10, 155, 32, 117, 48, 12, 151, 60, 83, 11, 147, 13, 65, 145, 129, 76, 64, 39, 49, 9, 116, 82, 80, 93, 0, 40, 88, 2, 20, 89, 131, 136, 4, 68, 25, 63, 28, 144, 59, 81, 104, 105, 51, 19, 150, 133, 128, 57, 18, 94, 137, 132, 45, 119, 29, 156, 31, 103, 123, 111, 69, 110, 100, 41, 135, 126, 55, 127, 8, 22, 107, 30, 102, 139, 3, 157, 106, 148, 61, 43, 62, 120, 44, 149, 14, 23, 24, 158, 42, 109, 115, 124, 108, 143, 125, 54, 17, 142, 96, 112, 114, 113, 99, 16, 5, 7, 97, 140, 6, 141, 98, 53, 52] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1890],\n",
      "        [0.0533],\n",
      "        [0.6899],\n",
      "        [0.5824],\n",
      "        [0.5426],\n",
      "        [0.0533],\n",
      "        [0.4848],\n",
      "        [0.5336],\n",
      "        [0.6274],\n",
      "        [0.4779],\n",
      "        [0.6369],\n",
      "        [0.5309],\n",
      "        [0.6121],\n",
      "        [0.5952],\n",
      "        [0.2664],\n",
      "        [0.6169],\n",
      "        [0.6789],\n",
      "        [0.6402],\n",
      "        [0.4349],\n",
      "        [0.8295],\n",
      "        [0.2559],\n",
      "        [0.6662],\n",
      "        [0.5272],\n",
      "        [0.6844],\n",
      "        [0.6413],\n",
      "        [0.1539],\n",
      "        [0.6375],\n",
      "        [0.5980],\n",
      "        [0.6357],\n",
      "        [0.6935],\n",
      "        [0.6511],\n",
      "        [0.3038],\n",
      "        [0.2228],\n",
      "        [0.6184],\n",
      "        [0.2784],\n",
      "        [0.6287],\n",
      "        [0.0889],\n",
      "        [0.0533],\n",
      "        [0.6032],\n",
      "        [0.6128],\n",
      "        [0.4861],\n",
      "        [0.4958],\n",
      "        [0.5349],\n",
      "        [0.6258],\n",
      "        [0.6303],\n",
      "        [0.7434],\n",
      "        [0.0533],\n",
      "        [0.6766],\n",
      "        [0.3355],\n",
      "        [0.6171],\n",
      "        [0.7389],\n",
      "        [0.0625],\n",
      "        [0.5650],\n",
      "        [0.5691],\n",
      "        [0.7252],\n",
      "        [0.0900],\n",
      "        [0.7437],\n",
      "        [0.5861],\n",
      "        [0.0919],\n",
      "        [0.2153],\n",
      "        [0.6198],\n",
      "        [0.5855],\n",
      "        [0.6331],\n",
      "        [0.6098],\n",
      "        [0.7723],\n",
      "        [0.3377],\n",
      "        [0.5826],\n",
      "        [0.5866],\n",
      "        [0.4426],\n",
      "        [0.8440],\n",
      "        [0.6286],\n",
      "        [0.5019],\n",
      "        [0.8666],\n",
      "        [0.6549],\n",
      "        [0.4908],\n",
      "        [0.2218],\n",
      "        [0.1593],\n",
      "        [0.8506],\n",
      "        [0.6427],\n",
      "        [0.6773],\n",
      "        [0.5842],\n",
      "        [0.6707],\n",
      "        [0.0922],\n",
      "        [0.5712],\n",
      "        [0.6005],\n",
      "        [0.2678],\n",
      "        [0.2659],\n",
      "        [0.6084],\n",
      "        [0.6133],\n",
      "        [0.0738],\n",
      "        [0.2017],\n",
      "        [0.2270],\n",
      "        [0.4820],\n",
      "        [0.6783],\n",
      "        [0.4193],\n",
      "        [0.1345],\n",
      "        [0.2467],\n",
      "        [0.6212],\n",
      "        [0.2603],\n",
      "        [0.6629],\n",
      "        [0.0533],\n",
      "        [0.6512],\n",
      "        [0.2654],\n",
      "        [0.2746],\n",
      "        [0.3361],\n",
      "        [0.6163],\n",
      "        [0.3080],\n",
      "        [0.3236],\n",
      "        [0.6230],\n",
      "        [0.1600],\n",
      "        [0.2456],\n",
      "        [0.5540],\n",
      "        [0.2407],\n",
      "        [0.8065],\n",
      "        [0.6554],\n",
      "        [0.2953],\n",
      "        [0.6447],\n",
      "        [0.2387],\n",
      "        [0.1828],\n",
      "        [0.8584],\n",
      "        [0.0533],\n",
      "        [0.2917],\n",
      "        [0.0902],\n",
      "        [0.5551],\n",
      "        [0.6215],\n",
      "        [0.5466],\n",
      "        [0.2545],\n",
      "        [0.6182],\n",
      "        [0.0944],\n",
      "        [0.7371],\n",
      "        [0.6607],\n",
      "        [0.6575],\n",
      "        [0.0533],\n",
      "        [0.6170],\n",
      "        [0.2789],\n",
      "        [0.3183],\n",
      "        [0.2722],\n",
      "        [0.2759],\n",
      "        [0.1246],\n",
      "        [0.2571],\n",
      "        [0.5577],\n",
      "        [0.7196],\n",
      "        [0.1337],\n",
      "        [0.4335],\n",
      "        [0.3399],\n",
      "        [0.3195],\n",
      "        [0.3349],\n",
      "        [0.3886],\n",
      "        [0.7470],\n",
      "        [0.8331],\n",
      "        [0.7906],\n",
      "        [0.4193],\n",
      "        [0.1657],\n",
      "        [0.7911],\n",
      "        [0.1567],\n",
      "        [0.4010],\n",
      "        [0.6091],\n",
      "        [0.6117]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0020],\n",
      "        [    0.0021],\n",
      "        [    0.0025],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0039],\n",
      "        [    0.0042],\n",
      "        [    0.0044],\n",
      "        [    0.0045],\n",
      "        [    0.0045],\n",
      "        [    0.0046],\n",
      "        [    0.0046],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0056],\n",
      "        [    0.0061],\n",
      "        [    0.0063],\n",
      "        [    0.0064],\n",
      "        [    0.0068],\n",
      "        [    0.0072],\n",
      "        [    0.0080],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0093],\n",
      "        [    0.0097],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0104],\n",
      "        [    0.0108],\n",
      "        [    0.0114],\n",
      "        [    0.0126],\n",
      "        [    0.0129],\n",
      "        [    0.0134],\n",
      "        [    0.0137],\n",
      "        [    0.0144],\n",
      "        [    0.0150],\n",
      "        [    0.0152],\n",
      "        [    0.0155],\n",
      "        [    0.0166],\n",
      "        [    0.0171],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0178],\n",
      "        [    0.0184],\n",
      "        [    0.0188],\n",
      "        [    0.0192],\n",
      "        [    0.0197],\n",
      "        [    0.0201],\n",
      "        [    0.0201],\n",
      "        [    0.0211],\n",
      "        [    0.0211],\n",
      "        [    0.0217],\n",
      "        [    0.0219],\n",
      "        [    0.0223],\n",
      "        [    0.0228],\n",
      "        [    0.0236],\n",
      "        [    0.0242],\n",
      "        [    0.0252],\n",
      "        [    0.0260],\n",
      "        [    0.0262],\n",
      "        [    0.0268],\n",
      "        [    0.0268],\n",
      "        [    0.0268],\n",
      "        [    0.0276],\n",
      "        [    0.0293],\n",
      "        [    0.0303],\n",
      "        [    0.0304],\n",
      "        [    0.0310],\n",
      "        [    0.0312],\n",
      "        [    0.0313],\n",
      "        [    0.0322],\n",
      "        [    0.0327],\n",
      "        [    0.0328],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0345],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0375],\n",
      "        [    0.0392],\n",
      "        [    0.0394],\n",
      "        [    0.0400],\n",
      "        [    0.0412],\n",
      "        [    0.0415],\n",
      "        [    0.0420],\n",
      "        [    0.0425],\n",
      "        [    0.0427],\n",
      "        [    0.0429],\n",
      "        [    0.0445],\n",
      "        [    0.0448],\n",
      "        [    0.0453],\n",
      "        [    0.0457],\n",
      "        [    0.0492],\n",
      "        [    0.0509],\n",
      "        [    0.0512],\n",
      "        [    0.0532],\n",
      "        [    0.0534],\n",
      "        [    0.0539],\n",
      "        [    0.0539],\n",
      "        [    0.0544],\n",
      "        [    0.0550],\n",
      "        [    0.0554],\n",
      "        [    0.0555],\n",
      "        [    0.0562],\n",
      "        [    0.0564],\n",
      "        [    0.0578],\n",
      "        [    0.0580],\n",
      "        [    0.0587],\n",
      "        [    0.0598],\n",
      "        [    0.0604],\n",
      "        [    0.0614],\n",
      "        [    0.0627],\n",
      "        [    0.0631],\n",
      "        [    0.0640],\n",
      "        [    0.0647],\n",
      "        [    0.0648],\n",
      "        [    0.0659],\n",
      "        [    0.0661],\n",
      "        [    0.0663],\n",
      "        [    0.0688],\n",
      "        [    0.0692],\n",
      "        [    0.0705],\n",
      "        [    0.0708],\n",
      "        [    0.0711],\n",
      "        [    0.0735],\n",
      "        [    0.0741],\n",
      "        [    0.0748],\n",
      "        [    0.0768],\n",
      "        [    0.0771],\n",
      "        [    0.0795],\n",
      "        [    0.0796],\n",
      "        [    0.0807],\n",
      "        [    0.0813],\n",
      "        [    0.0828],\n",
      "        [    0.0917],\n",
      "        [    0.0944],\n",
      "        [    0.1008],\n",
      "        [    0.1052],\n",
      "        [    0.1067],\n",
      "        [    0.1072],\n",
      "        [    0.1111],\n",
      "        [    0.1170],\n",
      "        [    0.1173],\n",
      "        [    0.1237],\n",
      "        [    0.1256],\n",
      "        [    0.1292],\n",
      "        [    0.1322],\n",
      "        [    0.1326],\n",
      "        [    0.1344],\n",
      "        [    0.1419],\n",
      "        [    0.1567],\n",
      "        [    0.1736],\n",
      "        [    0.1854],\n",
      "        [    0.1912]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0034],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0037],\n",
      "        [0.0027],\n",
      "        [0.0086],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0088],\n",
      "        [0.0004],\n",
      "        [0.0082],\n",
      "        [0.0003],\n",
      "        [0.0104],\n",
      "        [0.0009],\n",
      "        [0.0069],\n",
      "        [0.0099],\n",
      "        [0.0095],\n",
      "        [0.0006],\n",
      "        [0.0093],\n",
      "        [0.0150],\n",
      "        [0.0087],\n",
      "        [0.0019],\n",
      "        [0.0116],\n",
      "        [0.0026],\n",
      "        [0.0024],\n",
      "        [0.0110],\n",
      "        [0.0030],\n",
      "        [0.0146],\n",
      "        [0.0143],\n",
      "        [0.0143],\n",
      "        [0.0137],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0053],\n",
      "        [0.0154],\n",
      "        [0.0175],\n",
      "        [0.0101],\n",
      "        [0.0202],\n",
      "        [0.0093],\n",
      "        [0.0206],\n",
      "        [0.0104],\n",
      "        [0.0199],\n",
      "        [0.0208],\n",
      "        [0.0225],\n",
      "        [0.0214],\n",
      "        [0.0229],\n",
      "        [0.0113],\n",
      "        [0.0235],\n",
      "        [0.0210],\n",
      "        [0.0237],\n",
      "        [0.0235],\n",
      "        [0.0174],\n",
      "        [0.0246],\n",
      "        [0.0161],\n",
      "        [0.0264],\n",
      "        [0.0183],\n",
      "        [0.0179],\n",
      "        [0.0174],\n",
      "        [0.0193],\n",
      "        [0.0218],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0309],\n",
      "        [0.0200],\n",
      "        [0.0291],\n",
      "        [0.0217],\n",
      "        [0.0224],\n",
      "        [0.0329],\n",
      "        [0.0396],\n",
      "        [0.0346],\n",
      "        [0.0359],\n",
      "        [0.0218],\n",
      "        [0.0270],\n",
      "        [0.0370],\n",
      "        [0.0346],\n",
      "        [0.0357],\n",
      "        [0.0256],\n",
      "        [0.0396],\n",
      "        [0.0384],\n",
      "        [0.0392],\n",
      "        [0.0392],\n",
      "        [0.0340],\n",
      "        [0.0436],\n",
      "        [0.0342],\n",
      "        [0.0423],\n",
      "        [0.0432],\n",
      "        [0.0365],\n",
      "        [0.0464],\n",
      "        [0.0385],\n",
      "        [0.0399],\n",
      "        [0.0406],\n",
      "        [0.0487],\n",
      "        [0.0402],\n",
      "        [0.0489],\n",
      "        [0.0485],\n",
      "        [0.0468],\n",
      "        [0.0555],\n",
      "        [0.0531],\n",
      "        [0.0579],\n",
      "        [0.0470],\n",
      "        [0.0591],\n",
      "        [0.0560],\n",
      "        [0.0520],\n",
      "        [0.0573],\n",
      "        [0.0610],\n",
      "        [0.0576],\n",
      "        [0.0527],\n",
      "        [0.0610],\n",
      "        [0.0613],\n",
      "        [0.0557],\n",
      "        [0.0541],\n",
      "        [0.0575],\n",
      "        [0.0533],\n",
      "        [0.0645],\n",
      "        [0.0647],\n",
      "        [0.0681],\n",
      "        [0.0661],\n",
      "        [0.0614],\n",
      "        [0.0561],\n",
      "        [0.0594],\n",
      "        [0.0682],\n",
      "        [0.0628],\n",
      "        [0.0731],\n",
      "        [0.0734],\n",
      "        [0.0750],\n",
      "        [0.0727],\n",
      "        [0.0754],\n",
      "        [0.0697],\n",
      "        [0.0694],\n",
      "        [0.0786],\n",
      "        [0.0806],\n",
      "        [0.0707],\n",
      "        [0.0840],\n",
      "        [0.0817],\n",
      "        [0.0830],\n",
      "        [0.0792],\n",
      "        [0.0848],\n",
      "        [0.0882],\n",
      "        [0.0921],\n",
      "        [0.0960],\n",
      "        [0.1006],\n",
      "        [0.1035],\n",
      "        [0.1035],\n",
      "        [0.1136],\n",
      "        [0.1195],\n",
      "        [0.1197],\n",
      "        [0.1199],\n",
      "        [0.1209],\n",
      "        [0.1210],\n",
      "        [0.1251],\n",
      "        [0.1291],\n",
      "        [0.1311],\n",
      "        [0.1345],\n",
      "        [0.1533],\n",
      "        [0.1697],\n",
      "        [0.1801],\n",
      "        [0.1861]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 61.90266680717468\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1345164807607944e-07, 84)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [84, 92, 36, 79, 35, 72, 34, 85, 73, 134, 66, 74, 154, 27, 121, 91, 86, 38, 152, 122, 67, 50, 95, 33, 71, 146, 75, 90, 138, 155, 87, 118, 130, 21, 26, 47, 78, 1, 101, 83, 151, 65, 46, 13, 147, 145, 56, 9, 153, 77, 58, 117, 37, 82, 2, 129, 80, 70, 10, 12, 32, 48, 60, 4, 11, 20, 116, 76, 64, 39, 49, 93, 144, 81, 40, 131, 136, 88, 51, 89, 25, 150, 63, 28, 0, 68, 133, 18, 128, 104, 105, 59, 19, 132, 156, 137, 57, 94, 123, 100, 119, 8, 55, 45, 126, 103, 3, 111, 127, 110, 29, 31, 157, 69, 41, 135, 139, 148, 22, 107, 102, 30, 106, 14, 149, 158, 120, 61, 43, 62, 44, 23, 124, 24, 109, 115, 42, 108, 143, 125, 54, 17, 142, 96, 112, 114, 113, 99, 16, 5, 7, 97, 140, 6, 141, 98, 53, 52, 15] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5261],\n",
      "        [0.4735],\n",
      "        [0.6359],\n",
      "        [0.5898],\n",
      "        [0.6616],\n",
      "        [0.6358],\n",
      "        [0.6797],\n",
      "        [0.5379],\n",
      "        [0.6316],\n",
      "        [0.1857],\n",
      "        [0.5770],\n",
      "        [0.6123],\n",
      "        [0.0598],\n",
      "        [0.6855],\n",
      "        [0.2641],\n",
      "        [0.4800],\n",
      "        [0.5288],\n",
      "        [0.6330],\n",
      "        [0.0598],\n",
      "        [0.2535],\n",
      "        [0.6218],\n",
      "        [0.5981],\n",
      "        [0.4311],\n",
      "        [0.6741],\n",
      "        [0.6117],\n",
      "        [0.0856],\n",
      "        [0.6062],\n",
      "        [0.4814],\n",
      "        [0.1510],\n",
      "        [0.0598],\n",
      "        [0.5224],\n",
      "        [0.3019],\n",
      "        [0.2211],\n",
      "        [0.6474],\n",
      "        [0.6891],\n",
      "        [0.6311],\n",
      "        [0.5926],\n",
      "        [0.8206],\n",
      "        [0.2756],\n",
      "        [0.5640],\n",
      "        [0.0598],\n",
      "        [0.5812],\n",
      "        [0.6240],\n",
      "        [0.7397],\n",
      "        [0.0866],\n",
      "        [0.0885],\n",
      "        [0.4915],\n",
      "        [0.7656],\n",
      "        [0.0598],\n",
      "        [0.6072],\n",
      "        [0.5306],\n",
      "        [0.3333],\n",
      "        [0.6261],\n",
      "        [0.5776],\n",
      "        [0.8573],\n",
      "        [0.2135],\n",
      "        [0.5814],\n",
      "        [0.6205],\n",
      "        [0.7377],\n",
      "        [0.7351],\n",
      "        [0.6715],\n",
      "        [0.6126],\n",
      "        [0.5605],\n",
      "        [0.8422],\n",
      "        [0.7199],\n",
      "        [0.6506],\n",
      "        [0.3354],\n",
      "        [0.6140],\n",
      "        [0.5808],\n",
      "        [0.6289],\n",
      "        [0.6051],\n",
      "        [0.4390],\n",
      "        [0.0888],\n",
      "        [0.5952],\n",
      "        [0.6244],\n",
      "        [0.2200],\n",
      "        [0.1564],\n",
      "        [0.4971],\n",
      "        [0.6034],\n",
      "        [0.4861],\n",
      "        [0.6733],\n",
      "        [0.0699],\n",
      "        [0.5795],\n",
      "        [0.6664],\n",
      "        [0.8348],\n",
      "        [0.6371],\n",
      "        [0.1990],\n",
      "        [0.6737],\n",
      "        [0.2247],\n",
      "        [0.2655],\n",
      "        [0.2638],\n",
      "        [0.5669],\n",
      "        [0.6089],\n",
      "        [0.2444],\n",
      "        [0.0598],\n",
      "        [0.1317],\n",
      "        [0.4778],\n",
      "        [0.4156],\n",
      "        [0.2721],\n",
      "        [0.3201],\n",
      "        [0.2584],\n",
      "        [0.7994],\n",
      "        [0.5494],\n",
      "        [0.6165],\n",
      "        [0.2434],\n",
      "        [0.2632],\n",
      "        [0.8497],\n",
      "        [0.3338],\n",
      "        [0.2384],\n",
      "        [0.3059],\n",
      "        [0.6582],\n",
      "        [0.6459],\n",
      "        [0.0598],\n",
      "        [0.6108],\n",
      "        [0.6185],\n",
      "        [0.1566],\n",
      "        [0.1795],\n",
      "        [0.0867],\n",
      "        [0.6522],\n",
      "        [0.2933],\n",
      "        [0.2366],\n",
      "        [0.6396],\n",
      "        [0.2897],\n",
      "        [0.7324],\n",
      "        [0.0907],\n",
      "        [0.0598],\n",
      "        [0.2526],\n",
      "        [0.5508],\n",
      "        [0.6173],\n",
      "        [0.5420],\n",
      "        [0.6138],\n",
      "        [0.6569],\n",
      "        [0.2700],\n",
      "        [0.6536],\n",
      "        [0.2768],\n",
      "        [0.3159],\n",
      "        [0.6126],\n",
      "        [0.2739],\n",
      "        [0.1210],\n",
      "        [0.2549],\n",
      "        [0.5529],\n",
      "        [0.7149],\n",
      "        [0.1305],\n",
      "        [0.4299],\n",
      "        [0.3375],\n",
      "        [0.3170],\n",
      "        [0.3325],\n",
      "        [0.3848],\n",
      "        [0.7422],\n",
      "        [0.8249],\n",
      "        [0.7836],\n",
      "        [0.4159],\n",
      "        [0.1624],\n",
      "        [0.7837],\n",
      "        [0.1533],\n",
      "        [0.3971],\n",
      "        [0.6038],\n",
      "        [0.6066],\n",
      "        [0.7395]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0019],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0093],\n",
      "        [0.0093],\n",
      "        [0.0095],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0104],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0137],\n",
      "        [0.0143],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0161],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0179],\n",
      "        [0.0183],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0218],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0229],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0246],\n",
      "        [0.0256],\n",
      "        [0.0264],\n",
      "        [0.0270],\n",
      "        [0.0291],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0309],\n",
      "        [0.0329],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0357],\n",
      "        [0.0359],\n",
      "        [0.0365],\n",
      "        [0.0370],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0392],\n",
      "        [0.0392],\n",
      "        [0.0396],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0402],\n",
      "        [0.0406],\n",
      "        [0.0423],\n",
      "        [0.0432],\n",
      "        [0.0436],\n",
      "        [0.0464],\n",
      "        [0.0468],\n",
      "        [0.0470],\n",
      "        [0.0485],\n",
      "        [0.0487],\n",
      "        [0.0489],\n",
      "        [0.0520],\n",
      "        [0.0527],\n",
      "        [0.0531],\n",
      "        [0.0533],\n",
      "        [0.0541],\n",
      "        [0.0555],\n",
      "        [0.0557],\n",
      "        [0.0560],\n",
      "        [0.0561],\n",
      "        [0.0573],\n",
      "        [0.0575],\n",
      "        [0.0576],\n",
      "        [0.0579],\n",
      "        [0.0591],\n",
      "        [0.0594],\n",
      "        [0.0610],\n",
      "        [0.0610],\n",
      "        [0.0613],\n",
      "        [0.0614],\n",
      "        [0.0628],\n",
      "        [0.0645],\n",
      "        [0.0647],\n",
      "        [0.0661],\n",
      "        [0.0681],\n",
      "        [0.0682],\n",
      "        [0.0694],\n",
      "        [0.0697],\n",
      "        [0.0707],\n",
      "        [0.0727],\n",
      "        [0.0731],\n",
      "        [0.0734],\n",
      "        [0.0750],\n",
      "        [0.0754],\n",
      "        [0.0786],\n",
      "        [0.0792],\n",
      "        [0.0806],\n",
      "        [0.0817],\n",
      "        [0.0830],\n",
      "        [0.0840],\n",
      "        [0.0848],\n",
      "        [0.0882],\n",
      "        [0.0921],\n",
      "        [0.0960],\n",
      "        [0.1006],\n",
      "        [0.1035],\n",
      "        [0.1035],\n",
      "        [0.1136],\n",
      "        [0.1195],\n",
      "        [0.1197],\n",
      "        [0.1199],\n",
      "        [0.1209],\n",
      "        [0.1210],\n",
      "        [0.1251],\n",
      "        [0.1291],\n",
      "        [0.1311],\n",
      "        [0.1345],\n",
      "        [0.1533],\n",
      "        [0.1697],\n",
      "        [0.1801],\n",
      "        [0.1861],\n",
      "        [0.2783]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0008],\n",
      "        [    0.0005],\n",
      "        [    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0008],\n",
      "        [    0.0026],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0030],\n",
      "        [    0.0028],\n",
      "        [    0.0033],\n",
      "        [    0.0052],\n",
      "        [    0.0113],\n",
      "        [    0.0071],\n",
      "        [    0.0055],\n",
      "        [    0.0067],\n",
      "        [    0.0071],\n",
      "        [    0.0085],\n",
      "        [    0.0141],\n",
      "        [    0.0072],\n",
      "        [    0.0087],\n",
      "        [    0.0088],\n",
      "        [    0.0079],\n",
      "        [    0.0109],\n",
      "        [    0.0093],\n",
      "        [    0.0108],\n",
      "        [    0.0101],\n",
      "        [    0.0111],\n",
      "        [    0.0102],\n",
      "        [    0.0058],\n",
      "        [    0.0108],\n",
      "        [    0.0104],\n",
      "        [    0.0107],\n",
      "        [    0.0143],\n",
      "        [    0.0154],\n",
      "        [    0.0146],\n",
      "        [    0.0140],\n",
      "        [    0.0193],\n",
      "        [    0.0135],\n",
      "        [    0.0170],\n",
      "        [    0.0229],\n",
      "        [    0.0181],\n",
      "        [    0.0179],\n",
      "        [    0.0163],\n",
      "        [    0.0187],\n",
      "        [    0.0200],\n",
      "        [    0.0195],\n",
      "        [    0.0164],\n",
      "        [    0.0257],\n",
      "        [    0.0202],\n",
      "        [    0.0201],\n",
      "        [    0.0198],\n",
      "        [    0.0220],\n",
      "        [    0.0225],\n",
      "        [    0.0173],\n",
      "        [    0.0237],\n",
      "        [    0.0232],\n",
      "        [    0.0221],\n",
      "        [    0.0259],\n",
      "        [    0.0247],\n",
      "        [    0.0252],\n",
      "        [    0.0237],\n",
      "        [    0.0242],\n",
      "        [    0.0216],\n",
      "        [    0.0288],\n",
      "        [    0.0260],\n",
      "        [    0.0280],\n",
      "        [    0.0296],\n",
      "        [    0.0293],\n",
      "        [    0.0306],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0346],\n",
      "        [    0.0347],\n",
      "        [    0.0349],\n",
      "        [    0.0329],\n",
      "        [    0.0349],\n",
      "        [    0.0353],\n",
      "        [    0.0362],\n",
      "        [    0.0362],\n",
      "        [    0.0392],\n",
      "        [    0.0388],\n",
      "        [    0.0387],\n",
      "        [    0.0401],\n",
      "        [    0.0446],\n",
      "        [    0.0395],\n",
      "        [    0.0409],\n",
      "        [    0.0386],\n",
      "        [    0.0421],\n",
      "        [    0.0407],\n",
      "        [    0.0416],\n",
      "        [    0.0431],\n",
      "        [    0.0474],\n",
      "        [    0.0480],\n",
      "        [    0.0415],\n",
      "        [    0.0476],\n",
      "        [    0.0480],\n",
      "        [    0.0473],\n",
      "        [    0.0535],\n",
      "        [    0.0540],\n",
      "        [    0.0508],\n",
      "        [    0.0494],\n",
      "        [    0.0540],\n",
      "        [    0.0560],\n",
      "        [    0.0574],\n",
      "        [    0.0543],\n",
      "        [    0.0520],\n",
      "        [    0.0564],\n",
      "        [    0.0591],\n",
      "        [    0.0560],\n",
      "        [    0.0589],\n",
      "        [    0.0607],\n",
      "        [    0.0539],\n",
      "        [    0.0606],\n",
      "        [    0.0617],\n",
      "        [    0.0607],\n",
      "        [    0.0617],\n",
      "        [    0.0632],\n",
      "        [    0.0649],\n",
      "        [    0.0629],\n",
      "        [    0.0639],\n",
      "        [    0.0693],\n",
      "        [    0.0663],\n",
      "        [    0.0673],\n",
      "        [    0.0700],\n",
      "        [    0.0651],\n",
      "        [    0.0705],\n",
      "        [    0.0723],\n",
      "        [    0.0738],\n",
      "        [    0.0745],\n",
      "        [    0.0758],\n",
      "        [    0.0793],\n",
      "        [    0.0810],\n",
      "        [    0.0813],\n",
      "        [    0.0800],\n",
      "        [    0.0820],\n",
      "        [    0.0845],\n",
      "        [    0.0830],\n",
      "        [    0.0885],\n",
      "        [    0.0937],\n",
      "        [    0.0958],\n",
      "        [    0.0987],\n",
      "        [    0.1042],\n",
      "        [    0.1051],\n",
      "        [    0.1127],\n",
      "        [    0.1185],\n",
      "        [    0.1187],\n",
      "        [    0.1209],\n",
      "        [    0.1187],\n",
      "        [    0.1169],\n",
      "        [    0.1215],\n",
      "        [    0.1307],\n",
      "        [    0.1315],\n",
      "        [    0.1307],\n",
      "        [    0.1537],\n",
      "        [    0.1709],\n",
      "        [    0.1795],\n",
      "        [    0.1857],\n",
      "        [    0.2762]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 62.18968391418457\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 17 個區塊累積花費時間(s) 1.2129459381103516\n",
      "<<The performance of 17 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2129459381103516\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1202.96\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1620.22\n",
      "The accuracy(2000) for l = 1: 82.39%\n",
      "The accuracy(3000) for l = 1: 91.82%\n",
      "The maximum error: tensor(7050.5742)\n",
      "The minimum error: tensor(3.4805)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 792.0\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 863.0\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1157.2734375\n",
      "The minimum error: 437.2734375\n",
      "------------------------------------------------------------\n",
      "0.8238993710691824\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<18>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.8565756931820943e-08, 32)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [32, 75, 88, 80, 31, 30, 81, 68, 130, 69, 62, 70, 117, 151, 87, 82, 23, 118, 91, 34, 63, 46, 67, 71, 134, 114, 126, 83, 142, 29, 86, 150, 97, 74, 148, 17, 43, 22, 9, 5, 79, 157, 42, 61, 156, 143, 52, 113, 141, 54, 73, 0, 33, 66, 78, 147, 76, 125, 44, 56, 8, 28, 149, 6, 16, 112, 7, 60, 72, 35, 45, 89, 127, 140, 77, 132, 36, 84, 47, 85, 14, 59, 146, 21, 64, 24, 100, 129, 152, 101, 124, 55, 158, 155, 90, 15, 133, 128, 53, 4, 115, 119, 153, 96, 51, 99, 41, 106, 107, 122, 25, 123, 65, 131, 27, 135, 37, 103, 144, 98, 18, 154, 102, 10, 26, 145, 116, 57, 39, 58, 40, 19, 105, 120, 20, 111, 104, 38, 139, 121, 50, 13, 138, 92, 108, 1, 110, 12, 109, 95, 3, 93, 2, 136, 137]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0026],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0033],\n",
      "        [    0.0052],\n",
      "        [    0.0055],\n",
      "        [    0.0058],\n",
      "        [    0.0067],\n",
      "        [    0.0071],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0079],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0088],\n",
      "        [    0.0093],\n",
      "        [    0.0101],\n",
      "        [    0.0102],\n",
      "        [    0.0104],\n",
      "        [    0.0107],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0109],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0135],\n",
      "        [    0.0140],\n",
      "        [    0.0141],\n",
      "        [    0.0143],\n",
      "        [    0.0146],\n",
      "        [    0.0154],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0181],\n",
      "        [    0.0187],\n",
      "        [    0.0195],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0202],\n",
      "        [    0.0216],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0232],\n",
      "        [    0.0237],\n",
      "        [    0.0237],\n",
      "        [    0.0242],\n",
      "        [    0.0247],\n",
      "        [    0.0252],\n",
      "        [    0.0257],\n",
      "        [    0.0259],\n",
      "        [    0.0260],\n",
      "        [    0.0280],\n",
      "        [    0.0288],\n",
      "        [    0.0293],\n",
      "        [    0.0296],\n",
      "        [    0.0306],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0329],\n",
      "        [    0.0346],\n",
      "        [    0.0347],\n",
      "        [    0.0349],\n",
      "        [    0.0349],\n",
      "        [    0.0353],\n",
      "        [    0.0362],\n",
      "        [    0.0362],\n",
      "        [    0.0386],\n",
      "        [    0.0387],\n",
      "        [    0.0388],\n",
      "        [    0.0392],\n",
      "        [    0.0395],\n",
      "        [    0.0401],\n",
      "        [    0.0407],\n",
      "        [    0.0409],\n",
      "        [    0.0415],\n",
      "        [    0.0416],\n",
      "        [    0.0421],\n",
      "        [    0.0431],\n",
      "        [    0.0435],\n",
      "        [    0.0453],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0476],\n",
      "        [    0.0480],\n",
      "        [    0.0480],\n",
      "        [    0.0494],\n",
      "        [    0.0508],\n",
      "        [    0.0535],\n",
      "        [    0.0539],\n",
      "        [    0.0540],\n",
      "        [    0.0540],\n",
      "        [    0.0543],\n",
      "        [    0.0560],\n",
      "        [    0.0560],\n",
      "        [    0.0564],\n",
      "        [    0.0574],\n",
      "        [    0.0589],\n",
      "        [    0.0591],\n",
      "        [    0.0606],\n",
      "        [    0.0607],\n",
      "        [    0.0607],\n",
      "        [    0.0617],\n",
      "        [    0.0617],\n",
      "        [    0.0629],\n",
      "        [    0.0632],\n",
      "        [    0.0639],\n",
      "        [    0.0649],\n",
      "        [    0.0651],\n",
      "        [    0.0663],\n",
      "        [    0.0673],\n",
      "        [    0.0693],\n",
      "        [    0.0700],\n",
      "        [    0.0705],\n",
      "        [    0.0723],\n",
      "        [    0.0738],\n",
      "        [    0.0745],\n",
      "        [    0.0758],\n",
      "        [    0.0793],\n",
      "        [    0.0800],\n",
      "        [    0.0810],\n",
      "        [    0.0813],\n",
      "        [    0.0820],\n",
      "        [    0.0830],\n",
      "        [    0.0845],\n",
      "        [    0.0885],\n",
      "        [    0.0937],\n",
      "        [    0.0958],\n",
      "        [    0.0987],\n",
      "        [    0.1042],\n",
      "        [    0.1051],\n",
      "        [    0.1127],\n",
      "        [    0.1169],\n",
      "        [    0.1185],\n",
      "        [    0.1187],\n",
      "        [    0.1187],\n",
      "        [    0.1209],\n",
      "        [    0.1215],\n",
      "        [    0.1307],\n",
      "        [    0.1307],\n",
      "        [    0.1315],\n",
      "        [    0.1537]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.8565756931820943e-08, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [32, 75, 88, 80, 31, 30, 81, 68, 130, 69, 62, 70, 117, 151, 87, 82, 23, 118, 91, 34, 63, 46, 67, 71, 134, 114, 126, 83, 142, 29, 86, 150, 97, 74, 148, 17, 43, 22, 9, 5, 79, 157, 42, 61, 156, 143, 52, 113, 141, 54, 73, 0, 33, 66, 78, 147, 76, 125, 44, 56, 8, 28, 149, 6, 16, 112, 7, 60, 72, 35, 45, 89, 127, 140, 77, 132, 36, 84, 47, 85, 14, 59, 146, 21, 64, 24, 100, 129, 152, 101, 124, 55, 158, 155, 90, 15, 133, 128, 53, 4, 115, 119, 153, 96, 51, 99, 41, 106, 107, 122, 25, 123, 65, 131, 27, 135, 37, 103, 144, 98, 18, 154, 102, 10, 26, 145, 116, 57, 39, 58, 40, 19, 105, 120, 20, 111, 104, 38, 139, 121, 50, 13, 138, 92, 108, 1, 110, 12, 109, 95, 3, 93, 2, 136, 137, 94] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6352],\n",
      "        [0.5903],\n",
      "        [0.4745],\n",
      "        [0.5272],\n",
      "        [0.6605],\n",
      "        [0.6784],\n",
      "        [0.5388],\n",
      "        [0.6359],\n",
      "        [0.1863],\n",
      "        [0.6316],\n",
      "        [0.5773],\n",
      "        [0.6121],\n",
      "        [0.2655],\n",
      "        [0.0653],\n",
      "        [0.4805],\n",
      "        [0.5295],\n",
      "        [0.6843],\n",
      "        [0.2550],\n",
      "        [0.4325],\n",
      "        [0.6326],\n",
      "        [0.6219],\n",
      "        [0.5976],\n",
      "        [0.6123],\n",
      "        [0.6064],\n",
      "        [0.1519],\n",
      "        [0.3038],\n",
      "        [0.2230],\n",
      "        [0.5232],\n",
      "        [0.0863],\n",
      "        [0.6726],\n",
      "        [0.4820],\n",
      "        [0.0653],\n",
      "        [0.2774],\n",
      "        [0.5933],\n",
      "        [0.0653],\n",
      "        [0.6468],\n",
      "        [0.6308],\n",
      "        [0.6880],\n",
      "        [0.7381],\n",
      "        [0.7619],\n",
      "        [0.5650],\n",
      "        [0.0653],\n",
      "        [0.6236],\n",
      "        [0.5818],\n",
      "        [0.0653],\n",
      "        [0.0870],\n",
      "        [0.4919],\n",
      "        [0.3345],\n",
      "        [0.0891],\n",
      "        [0.5314],\n",
      "        [0.6075],\n",
      "        [0.8382],\n",
      "        [0.6255],\n",
      "        [0.6209],\n",
      "        [0.5783],\n",
      "        [0.0653],\n",
      "        [0.5821],\n",
      "        [0.2154],\n",
      "        [0.6127],\n",
      "        [0.5609],\n",
      "        [0.7339],\n",
      "        [0.6698],\n",
      "        [0.0653],\n",
      "        [0.7347],\n",
      "        [0.6496],\n",
      "        [0.3365],\n",
      "        [0.7175],\n",
      "        [0.5815],\n",
      "        [0.6143],\n",
      "        [0.6286],\n",
      "        [0.6050],\n",
      "        [0.4405],\n",
      "        [0.2217],\n",
      "        [0.0893],\n",
      "        [0.5958],\n",
      "        [0.1572],\n",
      "        [0.6241],\n",
      "        [0.4976],\n",
      "        [0.6031],\n",
      "        [0.4868],\n",
      "        [0.6721],\n",
      "        [0.5800],\n",
      "        [0.0701],\n",
      "        [0.6726],\n",
      "        [0.6372],\n",
      "        [0.6655],\n",
      "        [0.2671],\n",
      "        [0.1999],\n",
      "        [0.0653],\n",
      "        [0.2655],\n",
      "        [0.2262],\n",
      "        [0.5674],\n",
      "        [0.0653],\n",
      "        [0.0653],\n",
      "        [0.4172],\n",
      "        [0.6079],\n",
      "        [0.1327],\n",
      "        [0.2455],\n",
      "        [0.4785],\n",
      "        [0.7955],\n",
      "        [0.2606],\n",
      "        [0.2736],\n",
      "        [0.0653],\n",
      "        [0.3215],\n",
      "        [0.5494],\n",
      "        [0.2650],\n",
      "        [0.6160],\n",
      "        [0.3074],\n",
      "        [0.3348],\n",
      "        [0.2450],\n",
      "        [0.6572],\n",
      "        [0.2399],\n",
      "        [0.6111],\n",
      "        [0.1571],\n",
      "        [0.6443],\n",
      "        [0.1798],\n",
      "        [0.6177],\n",
      "        [0.2951],\n",
      "        [0.0871],\n",
      "        [0.2388],\n",
      "        [0.6518],\n",
      "        [0.0653],\n",
      "        [0.2916],\n",
      "        [0.7304],\n",
      "        [0.6384],\n",
      "        [0.0910],\n",
      "        [0.2548],\n",
      "        [0.5515],\n",
      "        [0.6169],\n",
      "        [0.5426],\n",
      "        [0.6134],\n",
      "        [0.6562],\n",
      "        [0.2785],\n",
      "        [0.2719],\n",
      "        [0.6529],\n",
      "        [0.3170],\n",
      "        [0.2757],\n",
      "        [0.6121],\n",
      "        [0.1214],\n",
      "        [0.2565],\n",
      "        [0.5527],\n",
      "        [0.7130],\n",
      "        [0.1313],\n",
      "        [0.4315],\n",
      "        [0.3384],\n",
      "        [0.8207],\n",
      "        [0.3180],\n",
      "        [0.7400],\n",
      "        [0.3335],\n",
      "        [0.3858],\n",
      "        [0.7800],\n",
      "        [0.4175],\n",
      "        [0.7800],\n",
      "        [0.1628],\n",
      "        [0.1537],\n",
      "        [0.3983]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0005],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0026],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0033],\n",
      "        [    0.0052],\n",
      "        [    0.0055],\n",
      "        [    0.0058],\n",
      "        [    0.0067],\n",
      "        [    0.0071],\n",
      "        [    0.0071],\n",
      "        [    0.0072],\n",
      "        [    0.0079],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0088],\n",
      "        [    0.0093],\n",
      "        [    0.0101],\n",
      "        [    0.0102],\n",
      "        [    0.0104],\n",
      "        [    0.0107],\n",
      "        [    0.0108],\n",
      "        [    0.0108],\n",
      "        [    0.0109],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0135],\n",
      "        [    0.0140],\n",
      "        [    0.0141],\n",
      "        [    0.0143],\n",
      "        [    0.0146],\n",
      "        [    0.0154],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0181],\n",
      "        [    0.0187],\n",
      "        [    0.0195],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0201],\n",
      "        [    0.0202],\n",
      "        [    0.0216],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0232],\n",
      "        [    0.0237],\n",
      "        [    0.0237],\n",
      "        [    0.0242],\n",
      "        [    0.0247],\n",
      "        [    0.0252],\n",
      "        [    0.0257],\n",
      "        [    0.0259],\n",
      "        [    0.0260],\n",
      "        [    0.0280],\n",
      "        [    0.0288],\n",
      "        [    0.0293],\n",
      "        [    0.0296],\n",
      "        [    0.0306],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0329],\n",
      "        [    0.0346],\n",
      "        [    0.0347],\n",
      "        [    0.0349],\n",
      "        [    0.0349],\n",
      "        [    0.0353],\n",
      "        [    0.0362],\n",
      "        [    0.0362],\n",
      "        [    0.0386],\n",
      "        [    0.0387],\n",
      "        [    0.0388],\n",
      "        [    0.0392],\n",
      "        [    0.0395],\n",
      "        [    0.0401],\n",
      "        [    0.0407],\n",
      "        [    0.0409],\n",
      "        [    0.0415],\n",
      "        [    0.0416],\n",
      "        [    0.0421],\n",
      "        [    0.0431],\n",
      "        [    0.0435],\n",
      "        [    0.0453],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0476],\n",
      "        [    0.0480],\n",
      "        [    0.0480],\n",
      "        [    0.0494],\n",
      "        [    0.0508],\n",
      "        [    0.0535],\n",
      "        [    0.0539],\n",
      "        [    0.0540],\n",
      "        [    0.0540],\n",
      "        [    0.0543],\n",
      "        [    0.0560],\n",
      "        [    0.0560],\n",
      "        [    0.0564],\n",
      "        [    0.0574],\n",
      "        [    0.0589],\n",
      "        [    0.0591],\n",
      "        [    0.0606],\n",
      "        [    0.0607],\n",
      "        [    0.0607],\n",
      "        [    0.0617],\n",
      "        [    0.0617],\n",
      "        [    0.0629],\n",
      "        [    0.0632],\n",
      "        [    0.0639],\n",
      "        [    0.0649],\n",
      "        [    0.0651],\n",
      "        [    0.0663],\n",
      "        [    0.0673],\n",
      "        [    0.0693],\n",
      "        [    0.0700],\n",
      "        [    0.0705],\n",
      "        [    0.0723],\n",
      "        [    0.0738],\n",
      "        [    0.0745],\n",
      "        [    0.0758],\n",
      "        [    0.0793],\n",
      "        [    0.0800],\n",
      "        [    0.0810],\n",
      "        [    0.0813],\n",
      "        [    0.0820],\n",
      "        [    0.0830],\n",
      "        [    0.0845],\n",
      "        [    0.0885],\n",
      "        [    0.0937],\n",
      "        [    0.0958],\n",
      "        [    0.0987],\n",
      "        [    0.1042],\n",
      "        [    0.1051],\n",
      "        [    0.1127],\n",
      "        [    0.1169],\n",
      "        [    0.1185],\n",
      "        [    0.1187],\n",
      "        [    0.1187],\n",
      "        [    0.1209],\n",
      "        [    0.1215],\n",
      "        [    0.1307],\n",
      "        [    0.1307],\n",
      "        [    0.1315],\n",
      "        [    0.1537],\n",
      "        [    0.1709]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0062],\n",
      "        [    0.0036],\n",
      "        [    0.0032],\n",
      "        [    0.0037],\n",
      "        [    0.0073],\n",
      "        [    0.0080],\n",
      "        [    0.0014],\n",
      "        [    0.0075],\n",
      "        [    0.0046],\n",
      "        [    0.0074],\n",
      "        [    0.0007],\n",
      "        [    0.0090],\n",
      "        [    0.0054],\n",
      "        [    0.0005],\n",
      "        [    0.0042],\n",
      "        [    0.0039],\n",
      "        [    0.0001],\n",
      "        [    0.0072],\n",
      "        [    0.0056],\n",
      "        [    0.0020],\n",
      "        [    0.0039],\n",
      "        [    0.0140],\n",
      "        [    0.0046],\n",
      "        [    0.0062],\n",
      "        [    0.0122],\n",
      "        [    0.0094],\n",
      "        [    0.0104],\n",
      "        [    0.0076],\n",
      "        [    0.0073],\n",
      "        [    0.0045],\n",
      "        [    0.0136],\n",
      "        [    0.0166],\n",
      "        [    0.0133],\n",
      "        [    0.0100],\n",
      "        [    0.0194],\n",
      "        [    0.0074],\n",
      "        [    0.0083],\n",
      "        [    0.0084],\n",
      "        [    0.0240],\n",
      "        [    0.0216],\n",
      "        [    0.0206],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0225],\n",
      "        [    0.0128],\n",
      "        [    0.0151],\n",
      "        [    0.0158],\n",
      "        [    0.0185],\n",
      "        [    0.0164],\n",
      "        [    0.0156],\n",
      "        [    0.0162],\n",
      "        [    0.0270],\n",
      "        [    0.0159],\n",
      "        [    0.0173],\n",
      "        [    0.0263],\n",
      "        [    0.0282],\n",
      "        [    0.0270],\n",
      "        [    0.0236],\n",
      "        [    0.0177],\n",
      "        [    0.0195],\n",
      "        [    0.0168],\n",
      "        [    0.0192],\n",
      "        [    0.0310],\n",
      "        [    0.0201],\n",
      "        [    0.0323],\n",
      "        [    0.0267],\n",
      "        [    0.0228],\n",
      "        [    0.0247],\n",
      "        [    0.0255],\n",
      "        [    0.0243],\n",
      "        [    0.0252],\n",
      "        [    0.0288],\n",
      "        [    0.0327],\n",
      "        [    0.0309],\n",
      "        [    0.0388],\n",
      "        [    0.0369],\n",
      "        [    0.0287],\n",
      "        [    0.0326],\n",
      "        [    0.0415],\n",
      "        [    0.0336],\n",
      "        [    0.0446],\n",
      "        [    0.0341],\n",
      "        [    0.0392],\n",
      "        [    0.0320],\n",
      "        [    0.0346],\n",
      "        [    0.0333],\n",
      "        [    0.0408],\n",
      "        [    0.0398],\n",
      "        [    0.0362],\n",
      "        [    0.0417],\n",
      "        [    0.0420],\n",
      "        [    0.0381],\n",
      "        [    0.0382],\n",
      "        [    0.0400],\n",
      "        [    0.0452],\n",
      "        [    0.0423],\n",
      "        [    0.0499],\n",
      "        [    0.0482],\n",
      "        [    0.0444],\n",
      "        [    0.0549],\n",
      "        [    0.0506],\n",
      "        [    0.0538],\n",
      "        [    0.0486],\n",
      "        [    0.0545],\n",
      "        [    0.0586],\n",
      "        [    0.0541],\n",
      "        [    0.0501],\n",
      "        [    0.0551],\n",
      "        [    0.0553],\n",
      "        [    0.0574],\n",
      "        [    0.0528],\n",
      "        [    0.0590],\n",
      "        [    0.0562],\n",
      "        [    0.0630],\n",
      "        [    0.0555],\n",
      "        [    0.0597],\n",
      "        [    0.0559],\n",
      "        [    0.0622],\n",
      "        [    0.0595],\n",
      "        [    0.0640],\n",
      "        [    0.0574],\n",
      "        [    0.0599],\n",
      "        [    0.0658],\n",
      "        [    0.0743],\n",
      "        [    0.0639],\n",
      "        [    0.0662],\n",
      "        [    0.0703],\n",
      "        [    0.0676],\n",
      "        [    0.0676],\n",
      "        [    0.0702],\n",
      "        [    0.0697],\n",
      "        [    0.0722],\n",
      "        [    0.0796],\n",
      "        [    0.0815],\n",
      "        [    0.0744],\n",
      "        [    0.0810],\n",
      "        [    0.0825],\n",
      "        [    0.0786],\n",
      "        [    0.0854],\n",
      "        [    0.0940],\n",
      "        [    0.1003],\n",
      "        [    0.1054],\n",
      "        [    0.1015],\n",
      "        [    0.1074],\n",
      "        [    0.1116],\n",
      "        [    0.1221],\n",
      "        [    0.1177],\n",
      "        [    0.1258],\n",
      "        [    0.1176],\n",
      "        [    0.1222],\n",
      "        [    0.1267],\n",
      "        [    0.1329],\n",
      "        [    0.1358],\n",
      "        [    0.1293],\n",
      "        [    0.1513],\n",
      "        [    0.1723]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 62.712157249450684\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.170343821854658e-08, 23)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [23, 151, 62, 81, 34, 88, 75, 80, 82, 63, 87, 29, 67, 130, 117, 91, 32, 71, 118, 142, 31, 17, 69, 68, 83, 30, 43, 22, 70, 114, 74, 126, 157, 42, 134, 156, 97, 86, 46, 143, 54, 52, 33, 73, 141, 150, 8, 66, 44, 113, 28, 148, 56, 6, 79, 5, 61, 7, 125, 9, 35, 60, 45, 72, 78, 112, 0, 76, 147, 36, 89, 140, 149, 21, 16, 84, 127, 24, 85, 59, 64, 152, 132, 55, 158, 77, 146, 129, 155, 100, 47, 101, 124, 15, 53, 14, 90, 128, 153, 133, 41, 115, 25, 119, 99, 96, 4, 106, 107, 27, 37, 65, 122, 18, 51, 123, 144, 135, 154, 103, 131, 26, 98, 102, 145, 57, 39, 40, 58, 116, 19, 10, 20, 38, 105, 111, 120, 104, 139, 121, 50, 138, 13, 92, 108, 109, 110, 1, 95, 12, 3, 136, 93, 2, 137, 94, 49] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6913],\n",
      "        [0.0706],\n",
      "        [0.5814],\n",
      "        [0.5420],\n",
      "        [0.6392],\n",
      "        [0.4771],\n",
      "        [0.5942],\n",
      "        [0.5302],\n",
      "        [0.5327],\n",
      "        [0.6266],\n",
      "        [0.4830],\n",
      "        [0.6790],\n",
      "        [0.6170],\n",
      "        [0.1846],\n",
      "        [0.2656],\n",
      "        [0.4348],\n",
      "        [0.6415],\n",
      "        [0.6104],\n",
      "        [0.2550],\n",
      "        [0.0828],\n",
      "        [0.6671],\n",
      "        [0.6537],\n",
      "        [0.6360],\n",
      "        [0.6408],\n",
      "        [0.5264],\n",
      "        [0.6851],\n",
      "        [0.6371],\n",
      "        [0.6949],\n",
      "        [0.6160],\n",
      "        [0.3048],\n",
      "        [0.5972],\n",
      "        [0.2232],\n",
      "        [0.0706],\n",
      "        [0.6297],\n",
      "        [0.1498],\n",
      "        [0.0706],\n",
      "        [0.2776],\n",
      "        [0.4846],\n",
      "        [0.6029],\n",
      "        [0.0833],\n",
      "        [0.5359],\n",
      "        [0.4955],\n",
      "        [0.6316],\n",
      "        [0.6116],\n",
      "        [0.0855],\n",
      "        [0.0706],\n",
      "        [0.7418],\n",
      "        [0.6257],\n",
      "        [0.6187],\n",
      "        [0.3358],\n",
      "        [0.6758],\n",
      "        [0.0706],\n",
      "        [0.5656],\n",
      "        [0.7405],\n",
      "        [0.5685],\n",
      "        [0.7671],\n",
      "        [0.5862],\n",
      "        [0.7235],\n",
      "        [0.2153],\n",
      "        [0.7457],\n",
      "        [0.6349],\n",
      "        [0.5860],\n",
      "        [0.6108],\n",
      "        [0.6184],\n",
      "        [0.5822],\n",
      "        [0.3377],\n",
      "        [0.8436],\n",
      "        [0.5860],\n",
      "        [0.0706],\n",
      "        [0.6303],\n",
      "        [0.4431],\n",
      "        [0.0856],\n",
      "        [0.0706],\n",
      "        [0.6797],\n",
      "        [0.6559],\n",
      "        [0.5003],\n",
      "        [0.2219],\n",
      "        [0.6723],\n",
      "        [0.4894],\n",
      "        [0.5846],\n",
      "        [0.6421],\n",
      "        [0.0706],\n",
      "        [0.1552],\n",
      "        [0.5724],\n",
      "        [0.0706],\n",
      "        [0.5998],\n",
      "        [0.0706],\n",
      "        [0.1989],\n",
      "        [0.0706],\n",
      "        [0.2670],\n",
      "        [0.6084],\n",
      "        [0.2654],\n",
      "        [0.2260],\n",
      "        [0.6130],\n",
      "        [0.4821],\n",
      "        [0.6781],\n",
      "        [0.4194],\n",
      "        [0.2458],\n",
      "        [0.0706],\n",
      "        [0.1303],\n",
      "        [0.6219],\n",
      "        [0.2609],\n",
      "        [0.6633],\n",
      "        [0.2739],\n",
      "        [0.2652],\n",
      "        [0.3219],\n",
      "        [0.8010],\n",
      "        [0.3083],\n",
      "        [0.3358],\n",
      "        [0.6495],\n",
      "        [0.6235],\n",
      "        [0.6155],\n",
      "        [0.2450],\n",
      "        [0.6593],\n",
      "        [0.5539],\n",
      "        [0.2398],\n",
      "        [0.0834],\n",
      "        [0.1778],\n",
      "        [0.0706],\n",
      "        [0.2958],\n",
      "        [0.1548],\n",
      "        [0.6438],\n",
      "        [0.2387],\n",
      "        [0.2921],\n",
      "        [0.0872],\n",
      "        [0.5563],\n",
      "        [0.6231],\n",
      "        [0.6195],\n",
      "        [0.5468],\n",
      "        [0.2549],\n",
      "        [0.6633],\n",
      "        [0.7373],\n",
      "        [0.6598],\n",
      "        [0.6180],\n",
      "        [0.2790],\n",
      "        [0.3179],\n",
      "        [0.2724],\n",
      "        [0.2762],\n",
      "        [0.1182],\n",
      "        [0.2567],\n",
      "        [0.5572],\n",
      "        [0.1285],\n",
      "        [0.7198],\n",
      "        [0.4338],\n",
      "        [0.3395],\n",
      "        [0.3346],\n",
      "        [0.3188],\n",
      "        [0.8259],\n",
      "        [0.3872],\n",
      "        [0.7471],\n",
      "        [0.7852],\n",
      "        [0.1606],\n",
      "        [0.4196],\n",
      "        [0.7850],\n",
      "        [0.1513],\n",
      "        [0.3997],\n",
      "        [0.6083]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0014],\n",
      "        [    0.0020],\n",
      "        [    0.0032],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0039],\n",
      "        [    0.0039],\n",
      "        [    0.0042],\n",
      "        [    0.0045],\n",
      "        [    0.0046],\n",
      "        [    0.0046],\n",
      "        [    0.0054],\n",
      "        [    0.0056],\n",
      "        [    0.0062],\n",
      "        [    0.0062],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0073],\n",
      "        [    0.0074],\n",
      "        [    0.0074],\n",
      "        [    0.0075],\n",
      "        [    0.0076],\n",
      "        [    0.0080],\n",
      "        [    0.0083],\n",
      "        [    0.0084],\n",
      "        [    0.0090],\n",
      "        [    0.0094],\n",
      "        [    0.0100],\n",
      "        [    0.0104],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0122],\n",
      "        [    0.0128],\n",
      "        [    0.0133],\n",
      "        [    0.0136],\n",
      "        [    0.0140],\n",
      "        [    0.0151],\n",
      "        [    0.0156],\n",
      "        [    0.0158],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0164],\n",
      "        [    0.0166],\n",
      "        [    0.0168],\n",
      "        [    0.0173],\n",
      "        [    0.0177],\n",
      "        [    0.0185],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0195],\n",
      "        [    0.0201],\n",
      "        [    0.0206],\n",
      "        [    0.0216],\n",
      "        [    0.0225],\n",
      "        [    0.0228],\n",
      "        [    0.0236],\n",
      "        [    0.0240],\n",
      "        [    0.0243],\n",
      "        [    0.0247],\n",
      "        [    0.0252],\n",
      "        [    0.0255],\n",
      "        [    0.0263],\n",
      "        [    0.0267],\n",
      "        [    0.0270],\n",
      "        [    0.0270],\n",
      "        [    0.0282],\n",
      "        [    0.0287],\n",
      "        [    0.0288],\n",
      "        [    0.0309],\n",
      "        [    0.0310],\n",
      "        [    0.0320],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0327],\n",
      "        [    0.0333],\n",
      "        [    0.0336],\n",
      "        [    0.0341],\n",
      "        [    0.0346],\n",
      "        [    0.0362],\n",
      "        [    0.0369],\n",
      "        [    0.0381],\n",
      "        [    0.0382],\n",
      "        [    0.0388],\n",
      "        [    0.0392],\n",
      "        [    0.0398],\n",
      "        [    0.0400],\n",
      "        [    0.0408],\n",
      "        [    0.0415],\n",
      "        [    0.0417],\n",
      "        [    0.0420],\n",
      "        [    0.0423],\n",
      "        [    0.0444],\n",
      "        [    0.0446],\n",
      "        [    0.0452],\n",
      "        [    0.0482],\n",
      "        [    0.0486],\n",
      "        [    0.0499],\n",
      "        [    0.0501],\n",
      "        [    0.0506],\n",
      "        [    0.0528],\n",
      "        [    0.0538],\n",
      "        [    0.0541],\n",
      "        [    0.0545],\n",
      "        [    0.0549],\n",
      "        [    0.0551],\n",
      "        [    0.0553],\n",
      "        [    0.0555],\n",
      "        [    0.0559],\n",
      "        [    0.0562],\n",
      "        [    0.0574],\n",
      "        [    0.0574],\n",
      "        [    0.0586],\n",
      "        [    0.0590],\n",
      "        [    0.0595],\n",
      "        [    0.0597],\n",
      "        [    0.0599],\n",
      "        [    0.0622],\n",
      "        [    0.0630],\n",
      "        [    0.0639],\n",
      "        [    0.0640],\n",
      "        [    0.0658],\n",
      "        [    0.0662],\n",
      "        [    0.0676],\n",
      "        [    0.0676],\n",
      "        [    0.0697],\n",
      "        [    0.0702],\n",
      "        [    0.0703],\n",
      "        [    0.0722],\n",
      "        [    0.0743],\n",
      "        [    0.0744],\n",
      "        [    0.0786],\n",
      "        [    0.0796],\n",
      "        [    0.0810],\n",
      "        [    0.0815],\n",
      "        [    0.0825],\n",
      "        [    0.0854],\n",
      "        [    0.0940],\n",
      "        [    0.1003],\n",
      "        [    0.1015],\n",
      "        [    0.1054],\n",
      "        [    0.1074],\n",
      "        [    0.1116],\n",
      "        [    0.1176],\n",
      "        [    0.1177],\n",
      "        [    0.1221],\n",
      "        [    0.1222],\n",
      "        [    0.1258],\n",
      "        [    0.1267],\n",
      "        [    0.1293],\n",
      "        [    0.1329],\n",
      "        [    0.1358],\n",
      "        [    0.1513],\n",
      "        [    0.1723],\n",
      "        [    0.1846]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0026],\n",
      "        [0.0014],\n",
      "        [0.0005],\n",
      "        [0.0028],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0018],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0071],\n",
      "        [0.0051],\n",
      "        [0.0088],\n",
      "        [0.0080],\n",
      "        [0.0054],\n",
      "        [0.0062],\n",
      "        [0.0078],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0096],\n",
      "        [0.0068],\n",
      "        [0.0096],\n",
      "        [0.0093],\n",
      "        [0.0063],\n",
      "        [0.0096],\n",
      "        [0.0122],\n",
      "        [0.0104],\n",
      "        [0.0087],\n",
      "        [0.0133],\n",
      "        [0.0136],\n",
      "        [0.0097],\n",
      "        [0.0147],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0131],\n",
      "        [0.0168],\n",
      "        [0.0176],\n",
      "        [0.0169],\n",
      "        [0.0185],\n",
      "        [0.0144],\n",
      "        [0.0198],\n",
      "        [0.0174],\n",
      "        [0.0192],\n",
      "        [0.0190],\n",
      "        [0.0189],\n",
      "        [0.0208],\n",
      "        [0.0225],\n",
      "        [0.0209],\n",
      "        [0.0227],\n",
      "        [0.0186],\n",
      "        [0.0183],\n",
      "        [0.0207],\n",
      "        [0.0249],\n",
      "        [0.0234],\n",
      "        [0.0231],\n",
      "        [0.0253],\n",
      "        [0.0264],\n",
      "        [0.0267],\n",
      "        [0.0279],\n",
      "        [0.0243],\n",
      "        [0.0272],\n",
      "        [0.0228],\n",
      "        [0.0250],\n",
      "        [0.0313],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0287],\n",
      "        [0.0341],\n",
      "        [0.0325],\n",
      "        [0.0313],\n",
      "        [0.0347],\n",
      "        [0.0329],\n",
      "        [0.0342],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0367],\n",
      "        [0.0330],\n",
      "        [0.0382],\n",
      "        [0.0393],\n",
      "        [0.0351],\n",
      "        [0.0366],\n",
      "        [0.0424],\n",
      "        [0.0388],\n",
      "        [0.0369],\n",
      "        [0.0418],\n",
      "        [0.0398],\n",
      "        [0.0425],\n",
      "        [0.0414],\n",
      "        [0.0437],\n",
      "        [0.0460],\n",
      "        [0.0431],\n",
      "        [0.0466],\n",
      "        [0.0478],\n",
      "        [0.0455],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0510],\n",
      "        [0.0541],\n",
      "        [0.0531],\n",
      "        [0.0551],\n",
      "        [0.0528],\n",
      "        [0.0514],\n",
      "        [0.0556],\n",
      "        [0.0558],\n",
      "        [0.0574],\n",
      "        [0.0573],\n",
      "        [0.0583],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0568],\n",
      "        [0.0584],\n",
      "        [0.0575],\n",
      "        [0.0580],\n",
      "        [0.0567],\n",
      "        [0.0626],\n",
      "        [0.0647],\n",
      "        [0.0656],\n",
      "        [0.0650],\n",
      "        [0.0663],\n",
      "        [0.0641],\n",
      "        [0.0688],\n",
      "        [0.0687],\n",
      "        [0.0709],\n",
      "        [0.0719],\n",
      "        [0.0708],\n",
      "        [0.0726],\n",
      "        [0.0728],\n",
      "        [0.0749],\n",
      "        [0.0798],\n",
      "        [0.0802],\n",
      "        [0.0816],\n",
      "        [0.0811],\n",
      "        [0.0830],\n",
      "        [0.0832],\n",
      "        [0.0934],\n",
      "        [0.0985],\n",
      "        [0.0996],\n",
      "        [0.1040],\n",
      "        [0.1061],\n",
      "        [0.1122],\n",
      "        [0.1183],\n",
      "        [0.1184],\n",
      "        [0.1179],\n",
      "        [0.1205],\n",
      "        [0.1243],\n",
      "        [0.1232],\n",
      "        [0.1276],\n",
      "        [0.1316],\n",
      "        [0.1319],\n",
      "        [0.1494],\n",
      "        [0.1705],\n",
      "        [0.1827]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 62.999919176101685\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1670167882348323e-07, 81)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [81, 23, 75, 88, 62, 80, 151, 34, 69, 32, 68, 142, 29, 82, 130, 63, 31, 117, 70, 87, 67, 30, 91, 17, 118, 157, 71, 22, 83, 114, 43, 156, 126, 86, 74, 46, 143, 42, 134, 141, 97, 54, 33, 8, 52, 5, 73, 79, 113, 44, 66, 150, 61, 28, 56, 148, 6, 0, 9, 125, 78, 7, 76, 35, 60, 45, 112, 72, 140, 36, 89, 16, 147, 21, 127, 152, 149, 24, 84, 158, 85, 59, 77, 64, 155, 132, 129, 55, 47, 124, 100, 146, 101, 14, 15, 153, 53, 90, 128, 115, 133, 4, 41, 96, 119, 25, 99, 106, 107, 154, 122, 51, 37, 18, 27, 144, 135, 65, 123, 103, 145, 131, 98, 26, 102, 39, 57, 116, 40, 58, 19, 10, 20, 38, 105, 120, 111, 104, 139, 121, 50, 138, 13, 92, 108, 1, 109, 110, 95, 3, 12, 136, 93, 2, 137, 94, 49, 48] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5401],\n",
      "        [0.6904],\n",
      "        [0.5920],\n",
      "        [0.4753],\n",
      "        [0.5793],\n",
      "        [0.5282],\n",
      "        [0.0737],\n",
      "        [0.6384],\n",
      "        [0.6336],\n",
      "        [0.6404],\n",
      "        [0.6387],\n",
      "        [0.0809],\n",
      "        [0.6777],\n",
      "        [0.5307],\n",
      "        [0.1831],\n",
      "        [0.6245],\n",
      "        [0.6659],\n",
      "        [0.2648],\n",
      "        [0.6132],\n",
      "        [0.4808],\n",
      "        [0.6151],\n",
      "        [0.6839],\n",
      "        [0.4333],\n",
      "        [0.6532],\n",
      "        [0.2542],\n",
      "        [0.0737],\n",
      "        [0.6078],\n",
      "        [0.6941],\n",
      "        [0.5244],\n",
      "        [0.3046],\n",
      "        [0.6357],\n",
      "        [0.0737],\n",
      "        [0.2232],\n",
      "        [0.4824],\n",
      "        [0.5951],\n",
      "        [0.6010],\n",
      "        [0.0814],\n",
      "        [0.6283],\n",
      "        [0.1484],\n",
      "        [0.0835],\n",
      "        [0.2763],\n",
      "        [0.5346],\n",
      "        [0.6305],\n",
      "        [0.7412],\n",
      "        [0.4938],\n",
      "        [0.7638],\n",
      "        [0.6093],\n",
      "        [0.5665],\n",
      "        [0.3354],\n",
      "        [0.6174],\n",
      "        [0.6237],\n",
      "        [0.0737],\n",
      "        [0.5844],\n",
      "        [0.6743],\n",
      "        [0.5642],\n",
      "        [0.0737],\n",
      "        [0.7379],\n",
      "        [0.8394],\n",
      "        [0.7449],\n",
      "        [0.2151],\n",
      "        [0.5802],\n",
      "        [0.7214],\n",
      "        [0.5840],\n",
      "        [0.6338],\n",
      "        [0.5844],\n",
      "        [0.6093],\n",
      "        [0.3373],\n",
      "        [0.6160],\n",
      "        [0.0834],\n",
      "        [0.6292],\n",
      "        [0.4418],\n",
      "        [0.6549],\n",
      "        [0.0737],\n",
      "        [0.6792],\n",
      "        [0.2217],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.6715],\n",
      "        [0.4982],\n",
      "        [0.0737],\n",
      "        [0.4873],\n",
      "        [0.5829],\n",
      "        [0.5977],\n",
      "        [0.6400],\n",
      "        [0.0737],\n",
      "        [0.1539],\n",
      "        [0.1979],\n",
      "        [0.5712],\n",
      "        [0.6067],\n",
      "        [0.2254],\n",
      "        [0.2660],\n",
      "        [0.0737],\n",
      "        [0.2646],\n",
      "        [0.6766],\n",
      "        [0.6116],\n",
      "        [0.0737],\n",
      "        [0.4805],\n",
      "        [0.4179],\n",
      "        [0.2453],\n",
      "        [0.2605],\n",
      "        [0.1288],\n",
      "        [0.7975],\n",
      "        [0.6205],\n",
      "        [0.3202],\n",
      "        [0.2732],\n",
      "        [0.6620],\n",
      "        [0.2642],\n",
      "        [0.3079],\n",
      "        [0.3353],\n",
      "        [0.0737],\n",
      "        [0.2444],\n",
      "        [0.5522],\n",
      "        [0.6222],\n",
      "        [0.6595],\n",
      "        [0.6476],\n",
      "        [0.0814],\n",
      "        [0.1761],\n",
      "        [0.6134],\n",
      "        [0.2392],\n",
      "        [0.2954],\n",
      "        [0.0850],\n",
      "        [0.1532],\n",
      "        [0.2376],\n",
      "        [0.6422],\n",
      "        [0.2916],\n",
      "        [0.6221],\n",
      "        [0.5551],\n",
      "        [0.2545],\n",
      "        [0.6184],\n",
      "        [0.5452],\n",
      "        [0.6629],\n",
      "        [0.7358],\n",
      "        [0.6593],\n",
      "        [0.6167],\n",
      "        [0.2784],\n",
      "        [0.2719],\n",
      "        [0.3173],\n",
      "        [0.2757],\n",
      "        [0.1160],\n",
      "        [0.2561],\n",
      "        [0.5554],\n",
      "        [0.1267],\n",
      "        [0.7183],\n",
      "        [0.4324],\n",
      "        [0.3389],\n",
      "        [0.8217],\n",
      "        [0.3339],\n",
      "        [0.3181],\n",
      "        [0.3854],\n",
      "        [0.7816],\n",
      "        [0.7456],\n",
      "        [0.1589],\n",
      "        [0.4184],\n",
      "        [0.7812],\n",
      "        [0.1494],\n",
      "        [0.3979],\n",
      "        [0.6064],\n",
      "        [0.6097]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0010],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0014],\n",
      "        [0.0018],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0096],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0104],\n",
      "        [0.0115],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0136],\n",
      "        [0.0144],\n",
      "        [0.0147],\n",
      "        [0.0168],\n",
      "        [0.0169],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0198],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0225],\n",
      "        [0.0227],\n",
      "        [0.0228],\n",
      "        [0.0231],\n",
      "        [0.0234],\n",
      "        [0.0243],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0253],\n",
      "        [0.0264],\n",
      "        [0.0267],\n",
      "        [0.0272],\n",
      "        [0.0279],\n",
      "        [0.0287],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0313],\n",
      "        [0.0313],\n",
      "        [0.0325],\n",
      "        [0.0329],\n",
      "        [0.0330],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0347],\n",
      "        [0.0351],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0369],\n",
      "        [0.0382],\n",
      "        [0.0388],\n",
      "        [0.0393],\n",
      "        [0.0398],\n",
      "        [0.0414],\n",
      "        [0.0418],\n",
      "        [0.0424],\n",
      "        [0.0425],\n",
      "        [0.0431],\n",
      "        [0.0437],\n",
      "        [0.0455],\n",
      "        [0.0460],\n",
      "        [0.0466],\n",
      "        [0.0478],\n",
      "        [0.0510],\n",
      "        [0.0514],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0528],\n",
      "        [0.0531],\n",
      "        [0.0541],\n",
      "        [0.0551],\n",
      "        [0.0556],\n",
      "        [0.0558],\n",
      "        [0.0567],\n",
      "        [0.0568],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0573],\n",
      "        [0.0574],\n",
      "        [0.0575],\n",
      "        [0.0580],\n",
      "        [0.0583],\n",
      "        [0.0584],\n",
      "        [0.0626],\n",
      "        [0.0641],\n",
      "        [0.0647],\n",
      "        [0.0650],\n",
      "        [0.0656],\n",
      "        [0.0663],\n",
      "        [0.0687],\n",
      "        [0.0688],\n",
      "        [0.0708],\n",
      "        [0.0709],\n",
      "        [0.0719],\n",
      "        [0.0726],\n",
      "        [0.0728],\n",
      "        [0.0749],\n",
      "        [0.0798],\n",
      "        [0.0802],\n",
      "        [0.0811],\n",
      "        [0.0816],\n",
      "        [0.0830],\n",
      "        [0.0832],\n",
      "        [0.0934],\n",
      "        [0.0985],\n",
      "        [0.0996],\n",
      "        [0.1040],\n",
      "        [0.1061],\n",
      "        [0.1122],\n",
      "        [0.1179],\n",
      "        [0.1183],\n",
      "        [0.1184],\n",
      "        [0.1205],\n",
      "        [0.1232],\n",
      "        [0.1243],\n",
      "        [0.1276],\n",
      "        [0.1316],\n",
      "        [0.1319],\n",
      "        [0.1494],\n",
      "        [0.1705],\n",
      "        [0.1827],\n",
      "        [0.1892]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0023],\n",
      "        [    0.0021],\n",
      "        [    0.0009],\n",
      "        [    0.0005],\n",
      "        [    0.0037],\n",
      "        [    0.0000],\n",
      "        [    0.0044],\n",
      "        [    0.0039],\n",
      "        [    0.0025],\n",
      "        [    0.0037],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0074],\n",
      "        [    0.0079],\n",
      "        [    0.0071],\n",
      "        [    0.0084],\n",
      "        [    0.0046],\n",
      "        [    0.0066],\n",
      "        [    0.0034],\n",
      "        [    0.0087],\n",
      "        [    0.0085],\n",
      "        [    0.0053],\n",
      "        [    0.0086],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0069],\n",
      "        [    0.0114],\n",
      "        [    0.0103],\n",
      "        [    0.0115],\n",
      "        [    0.0094],\n",
      "        [    0.0115],\n",
      "        [    0.0079],\n",
      "        [    0.0101],\n",
      "        [    0.0092],\n",
      "        [    0.0143],\n",
      "        [    0.0097],\n",
      "        [    0.0114],\n",
      "        [    0.0152],\n",
      "        [    0.0148],\n",
      "        [    0.0126],\n",
      "        [    0.0160],\n",
      "        [    0.0184],\n",
      "        [    0.0183],\n",
      "        [    0.0180],\n",
      "        [    0.0198],\n",
      "        [    0.0151],\n",
      "        [    0.0208],\n",
      "        [    0.0167],\n",
      "        [    0.0189],\n",
      "        [    0.0208],\n",
      "        [    0.0213],\n",
      "        [    0.0215],\n",
      "        [    0.0188],\n",
      "        [    0.0225],\n",
      "        [    0.0225],\n",
      "        [    0.0243],\n",
      "        [    0.0252],\n",
      "        [    0.0185],\n",
      "        [    0.0223],\n",
      "        [    0.0235],\n",
      "        [    0.0224],\n",
      "        [    0.0269],\n",
      "        [    0.0229],\n",
      "        [    0.0267],\n",
      "        [    0.0282],\n",
      "        [    0.0287],\n",
      "        [    0.0272],\n",
      "        [    0.0304],\n",
      "        [    0.0268],\n",
      "        [    0.0311],\n",
      "        [    0.0315],\n",
      "        [    0.0301],\n",
      "        [    0.0331],\n",
      "        [    0.0331],\n",
      "        [    0.0327],\n",
      "        [    0.0313],\n",
      "        [    0.0359],\n",
      "        [    0.0352],\n",
      "        [    0.0369],\n",
      "        [    0.0333],\n",
      "        [    0.0378],\n",
      "        [    0.0376],\n",
      "        [    0.0345],\n",
      "        [    0.0389],\n",
      "        [    0.0351],\n",
      "        [    0.0393],\n",
      "        [    0.0383],\n",
      "        [    0.0408],\n",
      "        [    0.0375],\n",
      "        [    0.0411],\n",
      "        [    0.0424],\n",
      "        [    0.0441],\n",
      "        [    0.0428],\n",
      "        [    0.0415],\n",
      "        [    0.0451],\n",
      "        [    0.0437],\n",
      "        [    0.0480],\n",
      "        [    0.0481],\n",
      "        [    0.0477],\n",
      "        [    0.0509],\n",
      "        [    0.0525],\n",
      "        [    0.0480],\n",
      "        [    0.0534],\n",
      "        [    0.0511],\n",
      "        [    0.0527],\n",
      "        [    0.0555],\n",
      "        [    0.0558],\n",
      "        [    0.0555],\n",
      "        [    0.0558],\n",
      "        [    0.0550],\n",
      "        [    0.0565],\n",
      "        [    0.0546],\n",
      "        [    0.0590],\n",
      "        [    0.0573],\n",
      "        [    0.0594],\n",
      "        [    0.0558],\n",
      "        [    0.0566],\n",
      "        [    0.0606],\n",
      "        [    0.0581],\n",
      "        [    0.0625],\n",
      "        [    0.0622],\n",
      "        [    0.0660],\n",
      "        [    0.0659],\n",
      "        [    0.0674],\n",
      "        [    0.0662],\n",
      "        [    0.0701],\n",
      "        [    0.0702],\n",
      "        [    0.0708],\n",
      "        [    0.0725],\n",
      "        [    0.0737],\n",
      "        [    0.0731],\n",
      "        [    0.0713],\n",
      "        [    0.0755],\n",
      "        [    0.0815],\n",
      "        [    0.0803],\n",
      "        [    0.0810],\n",
      "        [    0.0818],\n",
      "        [    0.0831],\n",
      "        [    0.0812],\n",
      "        [    0.0931],\n",
      "        [    0.0961],\n",
      "        [    0.0979],\n",
      "        [    0.1024],\n",
      "        [    0.1048],\n",
      "        [    0.1122],\n",
      "        [    0.1137],\n",
      "        [    0.1184],\n",
      "        [    0.1187],\n",
      "        [    0.1188],\n",
      "        [    0.1197],\n",
      "        [    0.1226],\n",
      "        [    0.1261],\n",
      "        [    0.1305],\n",
      "        [    0.1281],\n",
      "        [    0.1477],\n",
      "        [    0.1689],\n",
      "        [    0.1802],\n",
      "        [    0.1870]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 63.28649139404297\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4145520310648863e-09, 80)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [80, 88, 75, 23, 81, 69, 68, 70, 32, 62, 142, 34, 151, 31, 30, 117, 157, 130, 29, 82, 156, 63, 17, 67, 118, 91, 87, 86, 114, 46, 126, 22, 71, 143, 83, 43, 141, 74, 134, 5, 42, 97, 79, 8, 33, 54, 0, 61, 113, 52, 44, 73, 66, 150, 9, 78, 56, 28, 76, 125, 148, 6, 35, 140, 7, 112, 60, 45, 16, 72, 36, 152, 89, 127, 147, 21, 158, 77, 155, 24, 149, 84, 47, 59, 85, 129, 64, 132, 55, 124, 14, 100, 101, 153, 146, 15, 128, 53, 4, 90, 115, 96, 133, 119, 41, 51, 154, 106, 25, 99, 107, 144, 122, 135, 18, 123, 37, 27, 65, 145, 103, 98, 131, 102, 26, 39, 57, 116, 10, 40, 19, 58, 20, 105, 120, 139, 38, 111, 104, 121, 50, 138, 13, 92, 108, 1, 109, 110, 95, 3, 12, 136, 2, 93, 137, 94, 49, 48, 11] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5264],\n",
      "        [0.4734],\n",
      "        [0.5898],\n",
      "        [0.6893],\n",
      "        [0.5383],\n",
      "        [0.6311],\n",
      "        [0.6365],\n",
      "        [0.6104],\n",
      "        [0.6390],\n",
      "        [0.5770],\n",
      "        [0.0794],\n",
      "        [0.6372],\n",
      "        [0.0755],\n",
      "        [0.6644],\n",
      "        [0.6825],\n",
      "        [0.2643],\n",
      "        [0.0755],\n",
      "        [0.1821],\n",
      "        [0.6762],\n",
      "        [0.5287],\n",
      "        [0.0755],\n",
      "        [0.6222],\n",
      "        [0.6527],\n",
      "        [0.6131],\n",
      "        [0.2537],\n",
      "        [0.4319],\n",
      "        [0.4786],\n",
      "        [0.4802],\n",
      "        [0.3048],\n",
      "        [0.5986],\n",
      "        [0.2235],\n",
      "        [0.6931],\n",
      "        [0.6052],\n",
      "        [0.0797],\n",
      "        [0.5225],\n",
      "        [0.6339],\n",
      "        [0.0818],\n",
      "        [0.5929],\n",
      "        [0.1472],\n",
      "        [0.7606],\n",
      "        [0.6264],\n",
      "        [0.2750],\n",
      "        [0.5647],\n",
      "        [0.7406],\n",
      "        [0.6291],\n",
      "        [0.5330],\n",
      "        [0.8352],\n",
      "        [0.5826],\n",
      "        [0.3354],\n",
      "        [0.4916],\n",
      "        [0.6156],\n",
      "        [0.6070],\n",
      "        [0.6217],\n",
      "        [0.0755],\n",
      "        [0.7441],\n",
      "        [0.5782],\n",
      "        [0.5627],\n",
      "        [0.6726],\n",
      "        [0.5819],\n",
      "        [0.2152],\n",
      "        [0.0755],\n",
      "        [0.7354],\n",
      "        [0.6325],\n",
      "        [0.0815],\n",
      "        [0.7194],\n",
      "        [0.3373],\n",
      "        [0.5826],\n",
      "        [0.6073],\n",
      "        [0.6537],\n",
      "        [0.6136],\n",
      "        [0.6279],\n",
      "        [0.0755],\n",
      "        [0.4404],\n",
      "        [0.2219],\n",
      "        [0.0755],\n",
      "        [0.6786],\n",
      "        [0.0755],\n",
      "        [0.5956],\n",
      "        [0.0755],\n",
      "        [0.6704],\n",
      "        [0.0755],\n",
      "        [0.4960],\n",
      "        [0.6044],\n",
      "        [0.5811],\n",
      "        [0.4852],\n",
      "        [0.1973],\n",
      "        [0.6378],\n",
      "        [0.1528],\n",
      "        [0.5697],\n",
      "        [0.2251],\n",
      "        [0.6750],\n",
      "        [0.2654],\n",
      "        [0.2643],\n",
      "        [0.0755],\n",
      "        [0.0755],\n",
      "        [0.6102],\n",
      "        [0.2452],\n",
      "        [0.4785],\n",
      "        [0.7942],\n",
      "        [0.4165],\n",
      "        [0.2606],\n",
      "        [0.3185],\n",
      "        [0.1277],\n",
      "        [0.2729],\n",
      "        [0.6186],\n",
      "        [0.5499],\n",
      "        [0.0755],\n",
      "        [0.3080],\n",
      "        [0.6606],\n",
      "        [0.2635],\n",
      "        [0.3353],\n",
      "        [0.0798],\n",
      "        [0.2442],\n",
      "        [0.1747],\n",
      "        [0.6595],\n",
      "        [0.2389],\n",
      "        [0.6205],\n",
      "        [0.6456],\n",
      "        [0.6111],\n",
      "        [0.0831],\n",
      "        [0.2955],\n",
      "        [0.2368],\n",
      "        [0.1518],\n",
      "        [0.2917],\n",
      "        [0.6404],\n",
      "        [0.6206],\n",
      "        [0.5536],\n",
      "        [0.2545],\n",
      "        [0.7343],\n",
      "        [0.6168],\n",
      "        [0.6624],\n",
      "        [0.5433],\n",
      "        [0.6587],\n",
      "        [0.2783],\n",
      "        [0.2719],\n",
      "        [0.1141],\n",
      "        [0.6151],\n",
      "        [0.3171],\n",
      "        [0.2757],\n",
      "        [0.2559],\n",
      "        [0.5530],\n",
      "        [0.1250],\n",
      "        [0.7167],\n",
      "        [0.4312],\n",
      "        [0.3388],\n",
      "        [0.8175],\n",
      "        [0.3337],\n",
      "        [0.3178],\n",
      "        [0.3837],\n",
      "        [0.7782],\n",
      "        [0.7440],\n",
      "        [0.1574],\n",
      "        [0.7774],\n",
      "        [0.4172],\n",
      "        [0.1477],\n",
      "        [0.3963],\n",
      "        [0.6039],\n",
      "        [0.6074],\n",
      "        [0.7407]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0005],\n",
      "        [    0.0009],\n",
      "        [    0.0021],\n",
      "        [    0.0023],\n",
      "        [    0.0025],\n",
      "        [    0.0032],\n",
      "        [    0.0034],\n",
      "        [    0.0037],\n",
      "        [    0.0037],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0044],\n",
      "        [    0.0046],\n",
      "        [    0.0053],\n",
      "        [    0.0066],\n",
      "        [    0.0069],\n",
      "        [    0.0071],\n",
      "        [    0.0074],\n",
      "        [    0.0079],\n",
      "        [    0.0079],\n",
      "        [    0.0084],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0086],\n",
      "        [    0.0087],\n",
      "        [    0.0092],\n",
      "        [    0.0094],\n",
      "        [    0.0097],\n",
      "        [    0.0101],\n",
      "        [    0.0103],\n",
      "        [    0.0114],\n",
      "        [    0.0114],\n",
      "        [    0.0115],\n",
      "        [    0.0115],\n",
      "        [    0.0126],\n",
      "        [    0.0143],\n",
      "        [    0.0148],\n",
      "        [    0.0151],\n",
      "        [    0.0152],\n",
      "        [    0.0160],\n",
      "        [    0.0167],\n",
      "        [    0.0180],\n",
      "        [    0.0183],\n",
      "        [    0.0184],\n",
      "        [    0.0185],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0198],\n",
      "        [    0.0208],\n",
      "        [    0.0208],\n",
      "        [    0.0213],\n",
      "        [    0.0215],\n",
      "        [    0.0223],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0225],\n",
      "        [    0.0229],\n",
      "        [    0.0235],\n",
      "        [    0.0243],\n",
      "        [    0.0252],\n",
      "        [    0.0267],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0272],\n",
      "        [    0.0282],\n",
      "        [    0.0287],\n",
      "        [    0.0301],\n",
      "        [    0.0304],\n",
      "        [    0.0311],\n",
      "        [    0.0313],\n",
      "        [    0.0315],\n",
      "        [    0.0327],\n",
      "        [    0.0331],\n",
      "        [    0.0331],\n",
      "        [    0.0333],\n",
      "        [    0.0345],\n",
      "        [    0.0351],\n",
      "        [    0.0352],\n",
      "        [    0.0359],\n",
      "        [    0.0369],\n",
      "        [    0.0375],\n",
      "        [    0.0376],\n",
      "        [    0.0378],\n",
      "        [    0.0383],\n",
      "        [    0.0389],\n",
      "        [    0.0393],\n",
      "        [    0.0408],\n",
      "        [    0.0411],\n",
      "        [    0.0415],\n",
      "        [    0.0424],\n",
      "        [    0.0428],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0451],\n",
      "        [    0.0477],\n",
      "        [    0.0480],\n",
      "        [    0.0480],\n",
      "        [    0.0481],\n",
      "        [    0.0509],\n",
      "        [    0.0511],\n",
      "        [    0.0525],\n",
      "        [    0.0527],\n",
      "        [    0.0534],\n",
      "        [    0.0546],\n",
      "        [    0.0550],\n",
      "        [    0.0555],\n",
      "        [    0.0555],\n",
      "        [    0.0558],\n",
      "        [    0.0558],\n",
      "        [    0.0558],\n",
      "        [    0.0565],\n",
      "        [    0.0566],\n",
      "        [    0.0573],\n",
      "        [    0.0581],\n",
      "        [    0.0590],\n",
      "        [    0.0594],\n",
      "        [    0.0606],\n",
      "        [    0.0622],\n",
      "        [    0.0625],\n",
      "        [    0.0659],\n",
      "        [    0.0660],\n",
      "        [    0.0662],\n",
      "        [    0.0674],\n",
      "        [    0.0701],\n",
      "        [    0.0702],\n",
      "        [    0.0708],\n",
      "        [    0.0713],\n",
      "        [    0.0725],\n",
      "        [    0.0731],\n",
      "        [    0.0737],\n",
      "        [    0.0755],\n",
      "        [    0.0803],\n",
      "        [    0.0810],\n",
      "        [    0.0812],\n",
      "        [    0.0815],\n",
      "        [    0.0818],\n",
      "        [    0.0831],\n",
      "        [    0.0931],\n",
      "        [    0.0961],\n",
      "        [    0.0979],\n",
      "        [    0.1024],\n",
      "        [    0.1048],\n",
      "        [    0.1122],\n",
      "        [    0.1137],\n",
      "        [    0.1184],\n",
      "        [    0.1187],\n",
      "        [    0.1188],\n",
      "        [    0.1197],\n",
      "        [    0.1226],\n",
      "        [    0.1261],\n",
      "        [    0.1281],\n",
      "        [    0.1305],\n",
      "        [    0.1477],\n",
      "        [    0.1689],\n",
      "        [    0.1802],\n",
      "        [    0.1870],\n",
      "        [    0.2796]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0028],\n",
      "        [    0.0065],\n",
      "        [    0.0037],\n",
      "        [    0.0002],\n",
      "        [    0.0007],\n",
      "        [    0.0007],\n",
      "        [    0.0004],\n",
      "        [    0.0060],\n",
      "        [    0.0049],\n",
      "        [    0.0076],\n",
      "        [    0.0059],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0064],\n",
      "        [    0.0054],\n",
      "        [    0.0067],\n",
      "        [    0.0121],\n",
      "        [    0.0094],\n",
      "        [    0.0064],\n",
      "        [    0.0109],\n",
      "        [    0.0122],\n",
      "        [    0.0105],\n",
      "        [    0.0081],\n",
      "        [    0.0092],\n",
      "        [    0.0104],\n",
      "        [    0.0076],\n",
      "        [    0.0088],\n",
      "        [    0.0061],\n",
      "        [    0.0088],\n",
      "        [    0.0146],\n",
      "        [    0.0138],\n",
      "        [    0.0123],\n",
      "        [    0.0130],\n",
      "        [    0.0150],\n",
      "        [    0.0135],\n",
      "        [    0.0162],\n",
      "        [    0.0143],\n",
      "        [    0.0076],\n",
      "        [    0.0188],\n",
      "        [    0.0157],\n",
      "        [    0.0152],\n",
      "        [    0.0230],\n",
      "        [    0.0222],\n",
      "        [    0.0201],\n",
      "        [    0.0107],\n",
      "        [    0.0169],\n",
      "        [    0.0190],\n",
      "        [    0.0220],\n",
      "        [    0.0240],\n",
      "        [    0.0231],\n",
      "        [    0.0235],\n",
      "        [    0.0231],\n",
      "        [    0.0170],\n",
      "        [    0.0206],\n",
      "        [    0.0244],\n",
      "        [    0.0274],\n",
      "        [    0.0211],\n",
      "        [    0.0247],\n",
      "        [    0.0258],\n",
      "        [    0.0318],\n",
      "        [    0.0303],\n",
      "        [    0.0274],\n",
      "        [    0.0330],\n",
      "        [    0.0275],\n",
      "        [    0.0302],\n",
      "        [    0.0320],\n",
      "        [    0.0260],\n",
      "        [    0.0328],\n",
      "        [    0.0347],\n",
      "        [    0.0297],\n",
      "        [    0.0322],\n",
      "        [    0.0315],\n",
      "        [    0.0346],\n",
      "        [    0.0371],\n",
      "        [    0.0318],\n",
      "        [    0.0324],\n",
      "        [    0.0336],\n",
      "        [    0.0394],\n",
      "        [    0.0375],\n",
      "        [    0.0385],\n",
      "        [    0.0341],\n",
      "        [    0.0399],\n",
      "        [    0.0392],\n",
      "        [    0.0389],\n",
      "        [    0.0415],\n",
      "        [    0.0387],\n",
      "        [    0.0426],\n",
      "        [    0.0419],\n",
      "        [    0.0363],\n",
      "        [    0.0421],\n",
      "        [    0.0423],\n",
      "        [    0.0421],\n",
      "        [    0.0457],\n",
      "        [    0.0492],\n",
      "        [    0.0483],\n",
      "        [    0.0498],\n",
      "        [    0.0402],\n",
      "        [    0.0486],\n",
      "        [    0.0499],\n",
      "        [    0.0508],\n",
      "        [    0.0518],\n",
      "        [    0.0531],\n",
      "        [    0.0571],\n",
      "        [    0.0516],\n",
      "        [    0.0534],\n",
      "        [    0.0551],\n",
      "        [    0.0597],\n",
      "        [    0.0554],\n",
      "        [    0.0560],\n",
      "        [    0.0567],\n",
      "        [    0.0573],\n",
      "        [    0.0566],\n",
      "        [    0.0608],\n",
      "        [    0.0588],\n",
      "        [    0.0628],\n",
      "        [    0.0642],\n",
      "        [    0.0629],\n",
      "        [    0.0629],\n",
      "        [    0.0619],\n",
      "        [    0.0651],\n",
      "        [    0.0656],\n",
      "        [    0.0654],\n",
      "        [    0.0718],\n",
      "        [    0.0736],\n",
      "        [    0.0720],\n",
      "        [    0.0699],\n",
      "        [    0.0654],\n",
      "        [    0.0760],\n",
      "        [    0.0770],\n",
      "        [    0.0757],\n",
      "        [    0.0794],\n",
      "        [    0.0797],\n",
      "        [    0.0818],\n",
      "        [    0.0814],\n",
      "        [    0.0851],\n",
      "        [    0.0821],\n",
      "        [    0.0824],\n",
      "        [    0.0938],\n",
      "        [    0.0930],\n",
      "        [    0.0984],\n",
      "        [    0.0968],\n",
      "        [    0.1044],\n",
      "        [    0.1125],\n",
      "        [    0.1057],\n",
      "        [    0.1188],\n",
      "        [    0.1190],\n",
      "        [    0.1180],\n",
      "        [    0.1122],\n",
      "        [    0.1166],\n",
      "        [    0.1263],\n",
      "        [    0.1205],\n",
      "        [    0.1302],\n",
      "        [    0.1478],\n",
      "        [    0.1683],\n",
      "        [    0.1766],\n",
      "        [    0.1836],\n",
      "        [    0.2736]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 63.57353472709656\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 18 個區塊累積花費時間(s) 1.215681552886963\n",
      "<<The performance of 18 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.215681552886963\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1180.22\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1588.19\n",
      "The accuracy(2000) for l = 1: 83.02%\n",
      "The accuracy(3000) for l = 1: 93.08%\n",
      "The maximum error: tensor(6984.3281)\n",
      "The minimum error: tensor(5.3594)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 801.5\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 827.9\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1121.78125\n",
      "The minimum error: 590.21875\n",
      "------------------------------------------------------------\n",
      "0.8301886792452831\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<19>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.404455822282216e-08, 27)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [27, 65, 28, 26, 66, 64, 76, 84, 71, 77, 138, 153, 147, 58, 42, 152, 113, 19, 126, 30, 1, 82, 114, 110, 122, 87, 78, 83, 63, 59, 25, 13, 139, 79, 137, 67, 130, 18, 39, 75, 93, 70, 57, 5, 38, 109, 50, 74, 72, 48, 29, 4, 69, 146, 157, 62, 40, 52, 121, 156, 144, 12, 136, 24, 108, 148, 56, 31, 123, 154, 2, 41, 85, 73, 68, 3, 158, 151, 43, 143, 32, 10, 17, 145, 80, 128, 125, 81, 20, 55, 0, 60, 120, 96, 149, 97, 51, 155, 142, 124, 86, 11, 49, 111, 92, 47, 129, 115, 150, 102, 95, 103, 131, 140, 37, 118, 119, 21, 14, 99, 33, 61, 141, 23, 94, 98, 6, 127, 112, 22, 53, 35, 54, 36, 15, 16, 101, 135, 116, 107, 100, 34, 46, 117, 9, 134, 88, 104, 8, 91, 105, 106, 132, 89, 133]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0013],\n",
      "        [0.0018],\n",
      "        [0.0028],\n",
      "        [0.0037],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0081],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0109],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0152],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0235],\n",
      "        [0.0240],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0255],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0274],\n",
      "        [0.0274],\n",
      "        [0.0275],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0315],\n",
      "        [0.0318],\n",
      "        [0.0318],\n",
      "        [0.0320],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0328],\n",
      "        [0.0330],\n",
      "        [0.0331],\n",
      "        [0.0336],\n",
      "        [0.0341],\n",
      "        [0.0346],\n",
      "        [0.0347],\n",
      "        [0.0363],\n",
      "        [0.0371],\n",
      "        [0.0375],\n",
      "        [0.0385],\n",
      "        [0.0387],\n",
      "        [0.0389],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0399],\n",
      "        [0.0402],\n",
      "        [0.0415],\n",
      "        [0.0419],\n",
      "        [0.0421],\n",
      "        [0.0421],\n",
      "        [0.0423],\n",
      "        [0.0426],\n",
      "        [0.0439],\n",
      "        [0.0457],\n",
      "        [0.0483],\n",
      "        [0.0486],\n",
      "        [0.0492],\n",
      "        [0.0498],\n",
      "        [0.0499],\n",
      "        [0.0508],\n",
      "        [0.0516],\n",
      "        [0.0518],\n",
      "        [0.0531],\n",
      "        [0.0534],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0560],\n",
      "        [0.0566],\n",
      "        [0.0567],\n",
      "        [0.0571],\n",
      "        [0.0573],\n",
      "        [0.0588],\n",
      "        [0.0597],\n",
      "        [0.0608],\n",
      "        [0.0619],\n",
      "        [0.0628],\n",
      "        [0.0629],\n",
      "        [0.0629],\n",
      "        [0.0642],\n",
      "        [0.0651],\n",
      "        [0.0654],\n",
      "        [0.0654],\n",
      "        [0.0656],\n",
      "        [0.0699],\n",
      "        [0.0718],\n",
      "        [0.0720],\n",
      "        [0.0736],\n",
      "        [0.0757],\n",
      "        [0.0760],\n",
      "        [0.0770],\n",
      "        [0.0794],\n",
      "        [0.0797],\n",
      "        [0.0814],\n",
      "        [0.0818],\n",
      "        [0.0821],\n",
      "        [0.0824],\n",
      "        [0.0851],\n",
      "        [0.0930],\n",
      "        [0.0938],\n",
      "        [0.0968],\n",
      "        [0.0984],\n",
      "        [0.1044],\n",
      "        [0.1125],\n",
      "        [0.1166],\n",
      "        [0.1180],\n",
      "        [0.1188],\n",
      "        [0.1190],\n",
      "        [0.1263],\n",
      "        [0.1302],\n",
      "        [0.1478]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.404455822282216e-08, 27)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [27, 65, 28, 26, 66, 64, 76, 84, 71, 77, 138, 153, 147, 58, 42, 152, 113, 19, 126, 30, 1, 82, 114, 110, 122, 87, 78, 83, 63, 59, 25, 13, 139, 79, 137, 67, 130, 18, 39, 75, 93, 70, 57, 5, 38, 109, 50, 74, 72, 48, 29, 4, 69, 146, 157, 62, 40, 52, 121, 156, 144, 12, 136, 24, 108, 148, 56, 31, 123, 154, 2, 41, 85, 73, 68, 3, 158, 151, 43, 143, 32, 10, 17, 145, 80, 128, 125, 81, 20, 55, 0, 60, 120, 96, 149, 97, 51, 155, 142, 124, 86, 11, 49, 111, 92, 47, 129, 115, 150, 102, 95, 103, 131, 140, 37, 118, 119, 21, 14, 99, 33, 61, 141, 23, 94, 98, 6, 127, 112, 22, 53, 35, 54, 36, 15, 16, 101, 135, 116, 107, 100, 34, 46, 117, 9, 134, 88, 104, 8, 91, 105, 106, 132, 89, 133, 90] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6599],\n",
      "        [0.6284],\n",
      "        [0.6350],\n",
      "        [0.6778],\n",
      "        [0.6076],\n",
      "        [0.6340],\n",
      "        [0.5252],\n",
      "        [0.4722],\n",
      "        [0.5878],\n",
      "        [0.5369],\n",
      "        [0.0804],\n",
      "        [0.0771],\n",
      "        [0.0771],\n",
      "        [0.5747],\n",
      "        [0.5949],\n",
      "        [0.0771],\n",
      "        [0.2646],\n",
      "        [0.6849],\n",
      "        [0.1825],\n",
      "        [0.6336],\n",
      "        [0.7531],\n",
      "        [0.4786],\n",
      "        [0.2541],\n",
      "        [0.3054],\n",
      "        [0.2248],\n",
      "        [0.4313],\n",
      "        [0.5272],\n",
      "        [0.4769],\n",
      "        [0.6111],\n",
      "        [0.6196],\n",
      "        [0.6714],\n",
      "        [0.6489],\n",
      "        [0.0806],\n",
      "        [0.5210],\n",
      "        [0.0826],\n",
      "        [0.6027],\n",
      "        [0.1477],\n",
      "        [0.6887],\n",
      "        [0.6303],\n",
      "        [0.5632],\n",
      "        [0.2753],\n",
      "        [0.5910],\n",
      "        [0.5806],\n",
      "        [0.7387],\n",
      "        [0.6228],\n",
      "        [0.3353],\n",
      "        [0.5314],\n",
      "        [0.5764],\n",
      "        [0.5800],\n",
      "        [0.4893],\n",
      "        [0.6253],\n",
      "        [0.7356],\n",
      "        [0.6047],\n",
      "        [0.0771],\n",
      "        [0.0771],\n",
      "        [0.6195],\n",
      "        [0.6124],\n",
      "        [0.5607],\n",
      "        [0.2164],\n",
      "        [0.0771],\n",
      "        [0.0771],\n",
      "        [0.6496],\n",
      "        [0.0821],\n",
      "        [0.6676],\n",
      "        [0.3370],\n",
      "        [0.0771],\n",
      "        [0.5805],\n",
      "        [0.6288],\n",
      "        [0.2230],\n",
      "        [0.0771],\n",
      "        [0.7288],\n",
      "        [0.6040],\n",
      "        [0.4397],\n",
      "        [0.5935],\n",
      "        [0.6111],\n",
      "        [0.7133],\n",
      "        [0.0771],\n",
      "        [0.0771],\n",
      "        [0.6010],\n",
      "        [0.0771],\n",
      "        [0.6243],\n",
      "        [0.6698],\n",
      "        [0.6746],\n",
      "        [0.0771],\n",
      "        [0.4944],\n",
      "        [0.1533],\n",
      "        [0.1980],\n",
      "        [0.4838],\n",
      "        [0.6662],\n",
      "        [0.5788],\n",
      "        [0.7864],\n",
      "        [0.6353],\n",
      "        [0.2259],\n",
      "        [0.2657],\n",
      "        [0.0771],\n",
      "        [0.2648],\n",
      "        [0.5679],\n",
      "        [0.0771],\n",
      "        [0.0771],\n",
      "        [0.2459],\n",
      "        [0.4160],\n",
      "        [0.6062],\n",
      "        [0.4767],\n",
      "        [0.2616],\n",
      "        [0.3183],\n",
      "        [0.5470],\n",
      "        [0.1285],\n",
      "        [0.2733],\n",
      "        [0.0771],\n",
      "        [0.3083],\n",
      "        [0.2639],\n",
      "        [0.3351],\n",
      "        [0.1746],\n",
      "        [0.0807],\n",
      "        [0.6150],\n",
      "        [0.2449],\n",
      "        [0.2396],\n",
      "        [0.6563],\n",
      "        [0.6560],\n",
      "        [0.2961],\n",
      "        [0.6166],\n",
      "        [0.6088],\n",
      "        [0.0839],\n",
      "        [0.6408],\n",
      "        [0.2376],\n",
      "        [0.2925],\n",
      "        [0.7285],\n",
      "        [0.1523],\n",
      "        [0.2554],\n",
      "        [0.6360],\n",
      "        [0.5518],\n",
      "        [0.6171],\n",
      "        [0.5414],\n",
      "        [0.6133],\n",
      "        [0.6585],\n",
      "        [0.6549],\n",
      "        [0.2789],\n",
      "        [0.1143],\n",
      "        [0.2727],\n",
      "        [0.3168],\n",
      "        [0.2763],\n",
      "        [0.6115],\n",
      "        [0.5498],\n",
      "        [0.2565],\n",
      "        [0.7111],\n",
      "        [0.1254],\n",
      "        [0.4307],\n",
      "        [0.3385],\n",
      "        [0.7380],\n",
      "        [0.3830],\n",
      "        [0.3334],\n",
      "        [0.3175],\n",
      "        [0.1576],\n",
      "        [0.4169],\n",
      "        [0.1478],\n",
      "        [0.3956]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0013],\n",
      "        [0.0018],\n",
      "        [0.0028],\n",
      "        [0.0037],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0081],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0109],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0152],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0169],\n",
      "        [0.0170],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0231],\n",
      "        [0.0235],\n",
      "        [0.0240],\n",
      "        [0.0244],\n",
      "        [0.0247],\n",
      "        [0.0255],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0274],\n",
      "        [0.0274],\n",
      "        [0.0275],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0315],\n",
      "        [0.0318],\n",
      "        [0.0318],\n",
      "        [0.0320],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0328],\n",
      "        [0.0330],\n",
      "        [0.0331],\n",
      "        [0.0336],\n",
      "        [0.0341],\n",
      "        [0.0346],\n",
      "        [0.0347],\n",
      "        [0.0363],\n",
      "        [0.0371],\n",
      "        [0.0375],\n",
      "        [0.0385],\n",
      "        [0.0387],\n",
      "        [0.0389],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0399],\n",
      "        [0.0402],\n",
      "        [0.0415],\n",
      "        [0.0419],\n",
      "        [0.0421],\n",
      "        [0.0421],\n",
      "        [0.0423],\n",
      "        [0.0426],\n",
      "        [0.0439],\n",
      "        [0.0457],\n",
      "        [0.0483],\n",
      "        [0.0486],\n",
      "        [0.0492],\n",
      "        [0.0498],\n",
      "        [0.0499],\n",
      "        [0.0508],\n",
      "        [0.0516],\n",
      "        [0.0518],\n",
      "        [0.0531],\n",
      "        [0.0534],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0560],\n",
      "        [0.0566],\n",
      "        [0.0567],\n",
      "        [0.0571],\n",
      "        [0.0573],\n",
      "        [0.0588],\n",
      "        [0.0597],\n",
      "        [0.0608],\n",
      "        [0.0619],\n",
      "        [0.0628],\n",
      "        [0.0629],\n",
      "        [0.0629],\n",
      "        [0.0642],\n",
      "        [0.0651],\n",
      "        [0.0654],\n",
      "        [0.0654],\n",
      "        [0.0656],\n",
      "        [0.0699],\n",
      "        [0.0718],\n",
      "        [0.0720],\n",
      "        [0.0736],\n",
      "        [0.0757],\n",
      "        [0.0760],\n",
      "        [0.0770],\n",
      "        [0.0794],\n",
      "        [0.0797],\n",
      "        [0.0814],\n",
      "        [0.0818],\n",
      "        [0.0821],\n",
      "        [0.0824],\n",
      "        [0.0851],\n",
      "        [0.0930],\n",
      "        [0.0938],\n",
      "        [0.0968],\n",
      "        [0.0984],\n",
      "        [0.1044],\n",
      "        [0.1125],\n",
      "        [0.1166],\n",
      "        [0.1180],\n",
      "        [0.1188],\n",
      "        [0.1190],\n",
      "        [0.1263],\n",
      "        [0.1302],\n",
      "        [0.1478],\n",
      "        [0.1683]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0180],\n",
      "        [0.0137],\n",
      "        [0.0164],\n",
      "        [0.0191],\n",
      "        [0.0138],\n",
      "        [0.0149],\n",
      "        [0.0090],\n",
      "        [0.0063],\n",
      "        [0.0098],\n",
      "        [0.0069],\n",
      "        [0.0016],\n",
      "        [0.0085],\n",
      "        [0.0028],\n",
      "        [0.0059],\n",
      "        [0.0199],\n",
      "        [0.0095],\n",
      "        [0.0045],\n",
      "        [0.0120],\n",
      "        [0.0096],\n",
      "        [0.0092],\n",
      "        [0.0293],\n",
      "        [0.0157],\n",
      "        [0.0068],\n",
      "        [0.0049],\n",
      "        [0.0092],\n",
      "        [0.0025],\n",
      "        [0.0007],\n",
      "        [0.0023],\n",
      "        [0.0028],\n",
      "        [0.0026],\n",
      "        [0.0062],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0031],\n",
      "        [0.0058],\n",
      "        [0.0007],\n",
      "        [0.0186],\n",
      "        [0.0041],\n",
      "        [0.0005],\n",
      "        [0.0270],\n",
      "        [0.0148],\n",
      "        [0.0034],\n",
      "        [0.0293],\n",
      "        [0.0387],\n",
      "        [0.0036],\n",
      "        [0.0142],\n",
      "        [0.0089],\n",
      "        [0.0328],\n",
      "        [0.0334],\n",
      "        [0.0122],\n",
      "        [0.0057],\n",
      "        [0.0014],\n",
      "        [0.0098],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0098],\n",
      "        [0.0091],\n",
      "        [0.0125],\n",
      "        [0.0242],\n",
      "        [0.0286],\n",
      "        [0.0227],\n",
      "        [0.0433],\n",
      "        [0.0200],\n",
      "        [0.0091],\n",
      "        [0.0226],\n",
      "        [0.0328],\n",
      "        [0.0177],\n",
      "        [0.0139],\n",
      "        [0.0321],\n",
      "        [0.0349],\n",
      "        [0.0108],\n",
      "        [0.0176],\n",
      "        [0.0251],\n",
      "        [0.0452],\n",
      "        [0.0193],\n",
      "        [0.0125],\n",
      "        [0.0299],\n",
      "        [0.0367],\n",
      "        [0.0481],\n",
      "        [0.0315],\n",
      "        [0.0184],\n",
      "        [0.0548],\n",
      "        [0.0187],\n",
      "        [0.0343],\n",
      "        [0.0298],\n",
      "        [0.0427],\n",
      "        [0.0369],\n",
      "        [0.0309],\n",
      "        [0.0214],\n",
      "        [0.0274],\n",
      "        [0.0630],\n",
      "        [0.0272],\n",
      "        [0.0415],\n",
      "        [0.0411],\n",
      "        [0.0453],\n",
      "        [0.0411],\n",
      "        [0.0303],\n",
      "        [0.0471],\n",
      "        [0.0426],\n",
      "        [0.0484],\n",
      "        [0.0425],\n",
      "        [0.0334],\n",
      "        [0.0406],\n",
      "        [0.0478],\n",
      "        [0.0530],\n",
      "        [0.0636],\n",
      "        [0.0567],\n",
      "        [0.0550],\n",
      "        [0.0565],\n",
      "        [0.0518],\n",
      "        [0.0546],\n",
      "        [0.0516],\n",
      "        [0.0531],\n",
      "        [0.0500],\n",
      "        [0.0422],\n",
      "        [0.0578],\n",
      "        [0.0591],\n",
      "        [0.0420],\n",
      "        [0.0429],\n",
      "        [0.0592],\n",
      "        [0.0473],\n",
      "        [0.0497],\n",
      "        [0.0548],\n",
      "        [0.0473],\n",
      "        [0.0653],\n",
      "        [0.0630],\n",
      "        [0.0864],\n",
      "        [0.0699],\n",
      "        [0.0681],\n",
      "        [0.0550],\n",
      "        [0.0603],\n",
      "        [0.0581],\n",
      "        [0.0647],\n",
      "        [0.0608],\n",
      "        [0.0592],\n",
      "        [0.0617],\n",
      "        [0.0777],\n",
      "        [0.0753],\n",
      "        [0.0836],\n",
      "        [0.0781],\n",
      "        [0.0805],\n",
      "        [0.0698],\n",
      "        [0.1049],\n",
      "        [0.0948],\n",
      "        [0.1170],\n",
      "        [0.0929],\n",
      "        [0.1110],\n",
      "        [0.1080],\n",
      "        [0.1379],\n",
      "        [0.1226],\n",
      "        [0.1143],\n",
      "        [0.1151],\n",
      "        [0.1220],\n",
      "        [0.1364],\n",
      "        [0.1431],\n",
      "        [0.1735]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 64.09476017951965\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1399102934083203e-07, 39)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [39, 78, 67, 4, 138, 83, 87, 59, 147, 63, 79, 70, 38, 18, 113, 110, 13, 139, 29, 137, 58, 25, 84, 114, 77, 153, 50, 76, 40, 24, 122, 30, 152, 126, 71, 69, 62, 2, 19, 48, 52, 3, 65, 66, 31, 109, 93, 64, 82, 28, 41, 56, 27, 32, 130, 17, 26, 68, 42, 146, 136, 157, 20, 108, 144, 121, 85, 75, 60, 55, 156, 1, 57, 80, 158, 51, 81, 143, 123, 74, 148, 11, 72, 145, 154, 151, 125, 5, 49, 96, 97, 120, 21, 37, 86, 142, 128, 14, 12, 73, 149, 155, 23, 33, 111, 43, 124, 61, 140, 103, 102, 92, 131, 95, 10, 141, 115, 22, 150, 129, 118, 35, 119, 15, 99, 53, 36, 16, 0, 98, 47, 54, 94, 112, 34, 127, 135, 101, 107, 100, 116, 6, 134, 117, 46, 104, 88, 105, 106, 9, 132, 91, 89, 8, 133, 90, 45] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6458],\n",
      "        [0.5373],\n",
      "        [0.6158],\n",
      "        [0.7572],\n",
      "        [0.0739],\n",
      "        [0.4849],\n",
      "        [0.4379],\n",
      "        [0.6332],\n",
      "        [0.0739],\n",
      "        [0.6244],\n",
      "        [0.5309],\n",
      "        [0.6039],\n",
      "        [0.6379],\n",
      "        [0.7075],\n",
      "        [0.2665],\n",
      "        [0.3092],\n",
      "        [0.6664],\n",
      "        [0.0739],\n",
      "        [0.6417],\n",
      "        [0.0749],\n",
      "        [0.5866],\n",
      "        [0.6898],\n",
      "        [0.4803],\n",
      "        [0.2554],\n",
      "        [0.5475],\n",
      "        [0.0739],\n",
      "        [0.5425],\n",
      "        [0.5354],\n",
      "        [0.6273],\n",
      "        [0.6859],\n",
      "        [0.2244],\n",
      "        [0.6504],\n",
      "        [0.0739],\n",
      "        [0.1795],\n",
      "        [0.6005],\n",
      "        [0.6180],\n",
      "        [0.6332],\n",
      "        [0.7498],\n",
      "        [0.7034],\n",
      "        [0.4991],\n",
      "        [0.5726],\n",
      "        [0.7338],\n",
      "        [0.6423],\n",
      "        [0.6208],\n",
      "        [0.6453],\n",
      "        [0.3401],\n",
      "        [0.2761],\n",
      "        [0.6482],\n",
      "        [0.4867],\n",
      "        [0.6518],\n",
      "        [0.6184],\n",
      "        [0.5930],\n",
      "        [0.6777],\n",
      "        [0.6406],\n",
      "        [0.1435],\n",
      "        [0.6930],\n",
      "        [0.6963],\n",
      "        [0.6247],\n",
      "        [0.6087],\n",
      "        [0.0739],\n",
      "        [0.0747],\n",
      "        [0.0739],\n",
      "        [0.6842],\n",
      "        [0.3419],\n",
      "        [0.0739],\n",
      "        [0.2159],\n",
      "        [0.4468],\n",
      "        [0.5749],\n",
      "        [0.6495],\n",
      "        [0.5913],\n",
      "        [0.0739],\n",
      "        [0.7748],\n",
      "        [0.5930],\n",
      "        [0.5032],\n",
      "        [0.0739],\n",
      "        [0.5802],\n",
      "        [0.4922],\n",
      "        [0.0739],\n",
      "        [0.2224],\n",
      "        [0.5886],\n",
      "        [0.0739],\n",
      "        [0.6219],\n",
      "        [0.5924],\n",
      "        [0.0739],\n",
      "        [0.0739],\n",
      "        [0.0739],\n",
      "        [0.1960],\n",
      "        [0.7605],\n",
      "        [0.4860],\n",
      "        [0.2667],\n",
      "        [0.2660],\n",
      "        [0.2255],\n",
      "        [0.6740],\n",
      "        [0.6298],\n",
      "        [0.4221],\n",
      "        [0.0739],\n",
      "        [0.1494],\n",
      "        [0.6739],\n",
      "        [0.6669],\n",
      "        [0.6063],\n",
      "        [0.0739],\n",
      "        [0.0739],\n",
      "        [0.6578],\n",
      "        [0.6322],\n",
      "        [0.2636],\n",
      "        [0.6150],\n",
      "        [0.2459],\n",
      "        [0.6220],\n",
      "        [0.0739],\n",
      "        [0.3395],\n",
      "        [0.3117],\n",
      "        [0.3204],\n",
      "        [0.1712],\n",
      "        [0.2646],\n",
      "        [0.6883],\n",
      "        [0.0758],\n",
      "        [0.2751],\n",
      "        [0.6527],\n",
      "        [0.0739],\n",
      "        [0.1235],\n",
      "        [0.2454],\n",
      "        [0.6326],\n",
      "        [0.2399],\n",
      "        [0.6763],\n",
      "        [0.2988],\n",
      "        [0.5636],\n",
      "        [0.6285],\n",
      "        [0.6725],\n",
      "        [0.8092],\n",
      "        [0.2949],\n",
      "        [0.5590],\n",
      "        [0.5524],\n",
      "        [0.2374],\n",
      "        [0.2571],\n",
      "        [0.6268],\n",
      "        [0.1479],\n",
      "        [0.1082],\n",
      "        [0.2808],\n",
      "        [0.3208],\n",
      "        [0.2782],\n",
      "        [0.2745],\n",
      "        [0.7495],\n",
      "        [0.1200],\n",
      "        [0.2576],\n",
      "        [0.5618],\n",
      "        [0.3430],\n",
      "        [0.4374],\n",
      "        [0.3379],\n",
      "        [0.3214],\n",
      "        [0.7313],\n",
      "        [0.1533],\n",
      "        [0.3876],\n",
      "        [0.4232],\n",
      "        [0.7592],\n",
      "        [0.1431],\n",
      "        [0.4009],\n",
      "        [0.6139]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0028],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0036],\n",
      "        [0.0041],\n",
      "        [0.0045],\n",
      "        [0.0049],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0108],\n",
      "        [0.0120],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0125],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0157],\n",
      "        [0.0164],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0187],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0214],\n",
      "        [0.0226],\n",
      "        [0.0227],\n",
      "        [0.0242],\n",
      "        [0.0251],\n",
      "        [0.0270],\n",
      "        [0.0272],\n",
      "        [0.0274],\n",
      "        [0.0286],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0303],\n",
      "        [0.0309],\n",
      "        [0.0315],\n",
      "        [0.0321],\n",
      "        [0.0328],\n",
      "        [0.0328],\n",
      "        [0.0334],\n",
      "        [0.0334],\n",
      "        [0.0343],\n",
      "        [0.0349],\n",
      "        [0.0367],\n",
      "        [0.0369],\n",
      "        [0.0387],\n",
      "        [0.0406],\n",
      "        [0.0411],\n",
      "        [0.0411],\n",
      "        [0.0415],\n",
      "        [0.0420],\n",
      "        [0.0422],\n",
      "        [0.0425],\n",
      "        [0.0426],\n",
      "        [0.0427],\n",
      "        [0.0429],\n",
      "        [0.0433],\n",
      "        [0.0452],\n",
      "        [0.0453],\n",
      "        [0.0471],\n",
      "        [0.0473],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0481],\n",
      "        [0.0484],\n",
      "        [0.0497],\n",
      "        [0.0500],\n",
      "        [0.0516],\n",
      "        [0.0518],\n",
      "        [0.0530],\n",
      "        [0.0531],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0548],\n",
      "        [0.0550],\n",
      "        [0.0550],\n",
      "        [0.0565],\n",
      "        [0.0567],\n",
      "        [0.0578],\n",
      "        [0.0581],\n",
      "        [0.0591],\n",
      "        [0.0592],\n",
      "        [0.0592],\n",
      "        [0.0603],\n",
      "        [0.0608],\n",
      "        [0.0617],\n",
      "        [0.0630],\n",
      "        [0.0630],\n",
      "        [0.0636],\n",
      "        [0.0647],\n",
      "        [0.0653],\n",
      "        [0.0681],\n",
      "        [0.0698],\n",
      "        [0.0699],\n",
      "        [0.0753],\n",
      "        [0.0777],\n",
      "        [0.0781],\n",
      "        [0.0805],\n",
      "        [0.0836],\n",
      "        [0.0864],\n",
      "        [0.0929],\n",
      "        [0.0948],\n",
      "        [0.1049],\n",
      "        [0.1080],\n",
      "        [0.1110],\n",
      "        [0.1143],\n",
      "        [0.1151],\n",
      "        [0.1170],\n",
      "        [0.1220],\n",
      "        [0.1226],\n",
      "        [0.1364],\n",
      "        [0.1379],\n",
      "        [0.1431],\n",
      "        [0.1735],\n",
      "        [0.1902]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0036],\n",
      "        [0.0031],\n",
      "        [0.0054],\n",
      "        [0.0034],\n",
      "        [0.0016],\n",
      "        [0.0062],\n",
      "        [0.0056],\n",
      "        [0.0021],\n",
      "        [0.0028],\n",
      "        [0.0016],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0013],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0029],\n",
      "        [0.0057],\n",
      "        [0.0086],\n",
      "        [0.0048],\n",
      "        [0.0013],\n",
      "        [0.0035],\n",
      "        [0.0027],\n",
      "        [0.0071],\n",
      "        [0.0033],\n",
      "        [0.0085],\n",
      "        [0.0124],\n",
      "        [0.0054],\n",
      "        [0.0131],\n",
      "        [0.0117],\n",
      "        [0.0092],\n",
      "        [0.0063],\n",
      "        [0.0095],\n",
      "        [0.0106],\n",
      "        [0.0056],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0133],\n",
      "        [0.0090],\n",
      "        [0.0160],\n",
      "        [0.0161],\n",
      "        [0.0148],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0169],\n",
      "        [0.0141],\n",
      "        [0.0171],\n",
      "        [0.0104],\n",
      "        [0.0118],\n",
      "        [0.0134],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0149],\n",
      "        [0.0214],\n",
      "        [0.0197],\n",
      "        [0.0213],\n",
      "        [0.0162],\n",
      "        [0.0237],\n",
      "        [0.0155],\n",
      "        [0.0199],\n",
      "        [0.0192],\n",
      "        [0.0200],\n",
      "        [0.0243],\n",
      "        [0.0224],\n",
      "        [0.0227],\n",
      "        [0.0242],\n",
      "        [0.0282],\n",
      "        [0.0232],\n",
      "        [0.0318],\n",
      "        [0.0316],\n",
      "        [0.0286],\n",
      "        [0.0263],\n",
      "        [0.0250],\n",
      "        [0.0336],\n",
      "        [0.0299],\n",
      "        [0.0340],\n",
      "        [0.0347],\n",
      "        [0.0315],\n",
      "        [0.0324],\n",
      "        [0.0288],\n",
      "        [0.0329],\n",
      "        [0.0360],\n",
      "        [0.0294],\n",
      "        [0.0343],\n",
      "        [0.0349],\n",
      "        [0.0367],\n",
      "        [0.0363],\n",
      "        [0.0368],\n",
      "        [0.0441],\n",
      "        [0.0418],\n",
      "        [0.0416],\n",
      "        [0.0410],\n",
      "        [0.0449],\n",
      "        [0.0462],\n",
      "        [0.0457],\n",
      "        [0.0425],\n",
      "        [0.0436],\n",
      "        [0.0449],\n",
      "        [0.0406],\n",
      "        [0.0411],\n",
      "        [0.0453],\n",
      "        [0.0471],\n",
      "        [0.0502],\n",
      "        [0.0505],\n",
      "        [0.0479],\n",
      "        [0.0439],\n",
      "        [0.0480],\n",
      "        [0.0543],\n",
      "        [0.0500],\n",
      "        [0.0515],\n",
      "        [0.0519],\n",
      "        [0.0502],\n",
      "        [0.0519],\n",
      "        [0.0558],\n",
      "        [0.0522],\n",
      "        [0.0534],\n",
      "        [0.0546],\n",
      "        [0.0581],\n",
      "        [0.0566],\n",
      "        [0.0577],\n",
      "        [0.0573],\n",
      "        [0.0614],\n",
      "        [0.0586],\n",
      "        [0.0617],\n",
      "        [0.0594],\n",
      "        [0.0640],\n",
      "        [0.0644],\n",
      "        [0.0643],\n",
      "        [0.0599],\n",
      "        [0.0633],\n",
      "        [0.0597],\n",
      "        [0.0688],\n",
      "        [0.0667],\n",
      "        [0.0683],\n",
      "        [0.0731],\n",
      "        [0.0712],\n",
      "        [0.0738],\n",
      "        [0.0781],\n",
      "        [0.0781],\n",
      "        [0.0810],\n",
      "        [0.0832],\n",
      "        [0.0839],\n",
      "        [0.0915],\n",
      "        [0.0943],\n",
      "        [0.1008],\n",
      "        [0.1081],\n",
      "        [0.1080],\n",
      "        [0.1144],\n",
      "        [0.1152],\n",
      "        [0.1143],\n",
      "        [0.1207],\n",
      "        [0.1197],\n",
      "        [0.1338],\n",
      "        [0.1352],\n",
      "        [0.1417],\n",
      "        [0.1707],\n",
      "        [0.1857]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 64.38155794143677\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.663391344663978e-06, 58)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [58, 18, 138, 63, 59, 84, 147, 13, 78, 77, 4, 25, 39, 137, 113, 110, 76, 67, 87, 71, 139, 83, 30, 79, 114, 70, 38, 153, 29, 65, 19, 66, 122, 152, 64, 126, 24, 82, 50, 40, 2, 28, 69, 109, 62, 3, 27, 42, 48, 52, 26, 31, 93, 136, 130, 146, 157, 17, 32, 41, 56, 108, 144, 75, 68, 121, 20, 57, 1, 85, 156, 74, 72, 158, 143, 55, 60, 123, 148, 80, 51, 145, 81, 154, 11, 125, 151, 5, 12, 120, 73, 97, 96, 142, 128, 43, 49, 21, 14, 149, 86, 37, 155, 111, 124, 140, 92, 23, 33, 103, 131, 102, 10, 141, 61, 115, 95, 150, 118, 129, 22, 119, 99, 47, 0, 35, 15, 98, 53, 16, 36, 94, 112, 54, 127, 34, 135, 107, 101, 100, 116, 6, 134, 117, 46, 88, 104, 9, 105, 106, 91, 132, 89, 8, 133, 90, 45, 44] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5820],\n",
      "        [0.7047],\n",
      "        [0.0739],\n",
      "        [0.6200],\n",
      "        [0.6284],\n",
      "        [0.4767],\n",
      "        [0.0739],\n",
      "        [0.6640],\n",
      "        [0.5335],\n",
      "        [0.5439],\n",
      "        [0.7552],\n",
      "        [0.6870],\n",
      "        [0.6418],\n",
      "        [0.0739],\n",
      "        [0.2661],\n",
      "        [0.3093],\n",
      "        [0.5318],\n",
      "        [0.6112],\n",
      "        [0.4348],\n",
      "        [0.5963],\n",
      "        [0.0739],\n",
      "        [0.4811],\n",
      "        [0.6475],\n",
      "        [0.5271],\n",
      "        [0.2551],\n",
      "        [0.5997],\n",
      "        [0.6339],\n",
      "        [0.0739],\n",
      "        [0.6389],\n",
      "        [0.6377],\n",
      "        [0.7004],\n",
      "        [0.6161],\n",
      "        [0.2244],\n",
      "        [0.0739],\n",
      "        [0.6438],\n",
      "        [0.1786],\n",
      "        [0.6833],\n",
      "        [0.4827],\n",
      "        [0.5390],\n",
      "        [0.6233],\n",
      "        [0.7473],\n",
      "        [0.6487],\n",
      "        [0.6137],\n",
      "        [0.3402],\n",
      "        [0.6287],\n",
      "        [0.7315],\n",
      "        [0.6747],\n",
      "        [0.6044],\n",
      "        [0.4954],\n",
      "        [0.5690],\n",
      "        [0.6934],\n",
      "        [0.6422],\n",
      "        [0.2739],\n",
      "        [0.0739],\n",
      "        [0.1424],\n",
      "        [0.0739],\n",
      "        [0.0739],\n",
      "        [0.6904],\n",
      "        [0.6375],\n",
      "        [0.6143],\n",
      "        [0.5888],\n",
      "        [0.3421],\n",
      "        [0.0739],\n",
      "        [0.5711],\n",
      "        [0.6202],\n",
      "        [0.2159],\n",
      "        [0.6813],\n",
      "        [0.5888],\n",
      "        [0.7718],\n",
      "        [0.4437],\n",
      "        [0.0739],\n",
      "        [0.5847],\n",
      "        [0.5884],\n",
      "        [0.0739],\n",
      "        [0.0739],\n",
      "        [0.5871],\n",
      "        [0.6450],\n",
      "        [0.2222],\n",
      "        [0.0739],\n",
      "        [0.4994],\n",
      "        [0.5765],\n",
      "        [0.0739],\n",
      "        [0.4884],\n",
      "        [0.0739],\n",
      "        [0.6193],\n",
      "        [0.1954],\n",
      "        [0.0739],\n",
      "        [0.7585],\n",
      "        [0.6642],\n",
      "        [0.2250],\n",
      "        [0.6022],\n",
      "        [0.2655],\n",
      "        [0.2660],\n",
      "        [0.0739],\n",
      "        [0.1485],\n",
      "        [0.6108],\n",
      "        [0.4824],\n",
      "        [0.6712],\n",
      "        [0.6718],\n",
      "        [0.0739],\n",
      "        [0.4189],\n",
      "        [0.6258],\n",
      "        [0.0739],\n",
      "        [0.2635],\n",
      "        [0.2456],\n",
      "        [0.0739],\n",
      "        [0.3177],\n",
      "        [0.6548],\n",
      "        [0.6289],\n",
      "        [0.3396],\n",
      "        [0.1700],\n",
      "        [0.3116],\n",
      "        [0.6857],\n",
      "        [0.0743],\n",
      "        [0.6174],\n",
      "        [0.2747],\n",
      "        [0.2634],\n",
      "        [0.0739],\n",
      "        [0.2450],\n",
      "        [0.1225],\n",
      "        [0.6497],\n",
      "        [0.2394],\n",
      "        [0.2986],\n",
      "        [0.5550],\n",
      "        [0.8060],\n",
      "        [0.6293],\n",
      "        [0.6738],\n",
      "        [0.2946],\n",
      "        [0.5599],\n",
      "        [0.6700],\n",
      "        [0.6249],\n",
      "        [0.2360],\n",
      "        [0.2570],\n",
      "        [0.5482],\n",
      "        [0.1466],\n",
      "        [0.6235],\n",
      "        [0.1066],\n",
      "        [0.3208],\n",
      "        [0.2805],\n",
      "        [0.2777],\n",
      "        [0.2740],\n",
      "        [0.7470],\n",
      "        [0.1186],\n",
      "        [0.2570],\n",
      "        [0.5577],\n",
      "        [0.4344],\n",
      "        [0.3430],\n",
      "        [0.7287],\n",
      "        [0.3378],\n",
      "        [0.3213],\n",
      "        [0.3847],\n",
      "        [0.1520],\n",
      "        [0.4206],\n",
      "        [0.7566],\n",
      "        [0.1417],\n",
      "        [0.3980],\n",
      "        [0.6094],\n",
      "        [0.6139]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0013],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0048],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0077],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0141],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.0160],\n",
      "        [0.0161],\n",
      "        [0.0162],\n",
      "        [0.0169],\n",
      "        [0.0171],\n",
      "        [0.0192],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0224],\n",
      "        [0.0227],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0250],\n",
      "        [0.0263],\n",
      "        [0.0282],\n",
      "        [0.0286],\n",
      "        [0.0288],\n",
      "        [0.0294],\n",
      "        [0.0299],\n",
      "        [0.0315],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0336],\n",
      "        [0.0340],\n",
      "        [0.0343],\n",
      "        [0.0347],\n",
      "        [0.0349],\n",
      "        [0.0360],\n",
      "        [0.0363],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0406],\n",
      "        [0.0410],\n",
      "        [0.0411],\n",
      "        [0.0416],\n",
      "        [0.0418],\n",
      "        [0.0425],\n",
      "        [0.0436],\n",
      "        [0.0439],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0453],\n",
      "        [0.0457],\n",
      "        [0.0462],\n",
      "        [0.0471],\n",
      "        [0.0479],\n",
      "        [0.0480],\n",
      "        [0.0500],\n",
      "        [0.0502],\n",
      "        [0.0502],\n",
      "        [0.0505],\n",
      "        [0.0515],\n",
      "        [0.0519],\n",
      "        [0.0519],\n",
      "        [0.0522],\n",
      "        [0.0534],\n",
      "        [0.0543],\n",
      "        [0.0546],\n",
      "        [0.0558],\n",
      "        [0.0566],\n",
      "        [0.0573],\n",
      "        [0.0577],\n",
      "        [0.0581],\n",
      "        [0.0586],\n",
      "        [0.0594],\n",
      "        [0.0597],\n",
      "        [0.0599],\n",
      "        [0.0614],\n",
      "        [0.0617],\n",
      "        [0.0633],\n",
      "        [0.0640],\n",
      "        [0.0643],\n",
      "        [0.0644],\n",
      "        [0.0667],\n",
      "        [0.0683],\n",
      "        [0.0688],\n",
      "        [0.0712],\n",
      "        [0.0731],\n",
      "        [0.0738],\n",
      "        [0.0781],\n",
      "        [0.0781],\n",
      "        [0.0810],\n",
      "        [0.0832],\n",
      "        [0.0839],\n",
      "        [0.0915],\n",
      "        [0.0943],\n",
      "        [0.1008],\n",
      "        [0.1080],\n",
      "        [0.1081],\n",
      "        [0.1143],\n",
      "        [0.1144],\n",
      "        [0.1152],\n",
      "        [0.1197],\n",
      "        [0.1207],\n",
      "        [0.1338],\n",
      "        [0.1352],\n",
      "        [0.1417],\n",
      "        [0.1707],\n",
      "        [0.1857],\n",
      "        [0.1934]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0031],\n",
      "        [0.0040],\n",
      "        [0.0016],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0042],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0032],\n",
      "        [0.0053],\n",
      "        [0.0049],\n",
      "        [0.0034],\n",
      "        [0.0028],\n",
      "        [0.0046],\n",
      "        [0.0070],\n",
      "        [0.0064],\n",
      "        [0.0044],\n",
      "        [0.0058],\n",
      "        [0.0076],\n",
      "        [0.0058],\n",
      "        [0.0080],\n",
      "        [0.0057],\n",
      "        [0.0087],\n",
      "        [0.0094],\n",
      "        [0.0084],\n",
      "        [0.0091],\n",
      "        [0.0074],\n",
      "        [0.0086],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0094],\n",
      "        [0.0089],\n",
      "        [0.0099],\n",
      "        [0.0118],\n",
      "        [0.0103],\n",
      "        [0.0135],\n",
      "        [0.0148],\n",
      "        [0.0129],\n",
      "        [0.0127],\n",
      "        [0.0154],\n",
      "        [0.0121],\n",
      "        [0.0157],\n",
      "        [0.0144],\n",
      "        [0.0143],\n",
      "        [0.0134],\n",
      "        [0.0175],\n",
      "        [0.0172],\n",
      "        [0.0158],\n",
      "        [0.0176],\n",
      "        [0.0175],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0213],\n",
      "        [0.0220],\n",
      "        [0.0236],\n",
      "        [0.0233],\n",
      "        [0.0203],\n",
      "        [0.0228],\n",
      "        [0.0223],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0247],\n",
      "        [0.0237],\n",
      "        [0.0264],\n",
      "        [0.0291],\n",
      "        [0.0284],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0301],\n",
      "        [0.0316],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0310],\n",
      "        [0.0327],\n",
      "        [0.0349],\n",
      "        [0.0350],\n",
      "        [0.0345],\n",
      "        [0.0359],\n",
      "        [0.0348],\n",
      "        [0.0360],\n",
      "        [0.0373],\n",
      "        [0.0366],\n",
      "        [0.0376],\n",
      "        [0.0405],\n",
      "        [0.0421],\n",
      "        [0.0399],\n",
      "        [0.0400],\n",
      "        [0.0406],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0420],\n",
      "        [0.0454],\n",
      "        [0.0451],\n",
      "        [0.0445],\n",
      "        [0.0451],\n",
      "        [0.0465],\n",
      "        [0.0479],\n",
      "        [0.0469],\n",
      "        [0.0461],\n",
      "        [0.0492],\n",
      "        [0.0501],\n",
      "        [0.0494],\n",
      "        [0.0506],\n",
      "        [0.0514],\n",
      "        [0.0494],\n",
      "        [0.0522],\n",
      "        [0.0500],\n",
      "        [0.0523],\n",
      "        [0.0532],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0552],\n",
      "        [0.0564],\n",
      "        [0.0586],\n",
      "        [0.0572],\n",
      "        [0.0585],\n",
      "        [0.0599],\n",
      "        [0.0576],\n",
      "        [0.0580],\n",
      "        [0.0598],\n",
      "        [0.0624],\n",
      "        [0.0616],\n",
      "        [0.0615],\n",
      "        [0.0651],\n",
      "        [0.0642],\n",
      "        [0.0656],\n",
      "        [0.0663],\n",
      "        [0.0666],\n",
      "        [0.0703],\n",
      "        [0.0710],\n",
      "        [0.0741],\n",
      "        [0.0736],\n",
      "        [0.0763],\n",
      "        [0.0766],\n",
      "        [0.0795],\n",
      "        [0.0846],\n",
      "        [0.0842],\n",
      "        [0.0915],\n",
      "        [0.0955],\n",
      "        [0.0989],\n",
      "        [0.1075],\n",
      "        [0.1061],\n",
      "        [0.1144],\n",
      "        [0.1125],\n",
      "        [0.1134],\n",
      "        [0.1190],\n",
      "        [0.1209],\n",
      "        [0.1336],\n",
      "        [0.1353],\n",
      "        [0.1416],\n",
      "        [0.1701],\n",
      "        [0.1836],\n",
      "        [0.1915]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 64.66719341278076\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.7412261260906234e-07, 58)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [58, 18, 138, 84, 77, 4, 110, 147, 13, 63, 25, 113, 59, 78, 71, 76, 137, 39, 114, 139, 30, 87, 67, 66, 65, 83, 122, 79, 153, 19, 70, 64, 29, 152, 38, 126, 82, 24, 109, 28, 2, 42, 50, 27, 3, 40, 69, 62, 26, 52, 93, 48, 31, 136, 130, 146, 157, 108, 17, 32, 75, 144, 56, 41, 57, 20, 68, 121, 1, 74, 72, 156, 85, 158, 123, 143, 148, 55, 60, 145, 154, 80, 51, 81, 11, 151, 125, 5, 73, 97, 12, 96, 43, 120, 142, 128, 14, 21, 149, 49, 111, 86, 155, 37, 124, 103, 92, 102, 140, 23, 33, 131, 10, 141, 95, 61, 115, 150, 129, 99, 47, 22, 118, 0, 119, 98, 15, 35, 16, 53, 36, 94, 112, 54, 127, 135, 34, 107, 101, 100, 6, 116, 134, 117, 46, 104, 88, 105, 106, 9, 91, 132, 89, 8, 133, 90, 45, 44, 7] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5802],\n",
      "        [0.7046],\n",
      "        [0.0741],\n",
      "        [0.4755],\n",
      "        [0.5430],\n",
      "        [0.7560],\n",
      "        [0.3114],\n",
      "        [0.0741],\n",
      "        [0.6641],\n",
      "        [0.6185],\n",
      "        [0.6867],\n",
      "        [0.2676],\n",
      "        [0.6266],\n",
      "        [0.5324],\n",
      "        [0.5951],\n",
      "        [0.5310],\n",
      "        [0.0741],\n",
      "        [0.6401],\n",
      "        [0.2565],\n",
      "        [0.0741],\n",
      "        [0.6470],\n",
      "        [0.4340],\n",
      "        [0.6096],\n",
      "        [0.6143],\n",
      "        [0.6360],\n",
      "        [0.4796],\n",
      "        [0.2260],\n",
      "        [0.5260],\n",
      "        [0.0741],\n",
      "        [0.7001],\n",
      "        [0.5986],\n",
      "        [0.6423],\n",
      "        [0.6383],\n",
      "        [0.0741],\n",
      "        [0.6321],\n",
      "        [0.1792],\n",
      "        [0.4812],\n",
      "        [0.6832],\n",
      "        [0.3422],\n",
      "        [0.6481],\n",
      "        [0.7477],\n",
      "        [0.6023],\n",
      "        [0.5379],\n",
      "        [0.6740],\n",
      "        [0.7319],\n",
      "        [0.6216],\n",
      "        [0.6124],\n",
      "        [0.6272],\n",
      "        [0.6930],\n",
      "        [0.5679],\n",
      "        [0.2734],\n",
      "        [0.4938],\n",
      "        [0.6416],\n",
      "        [0.0741],\n",
      "        [0.1427],\n",
      "        [0.0741],\n",
      "        [0.0741],\n",
      "        [0.3442],\n",
      "        [0.6904],\n",
      "        [0.6370],\n",
      "        [0.5702],\n",
      "        [0.0741],\n",
      "        [0.5875],\n",
      "        [0.6124],\n",
      "        [0.5874],\n",
      "        [0.6810],\n",
      "        [0.6188],\n",
      "        [0.2175],\n",
      "        [0.7719],\n",
      "        [0.5836],\n",
      "        [0.5873],\n",
      "        [0.0741],\n",
      "        [0.4429],\n",
      "        [0.0741],\n",
      "        [0.2235],\n",
      "        [0.0741],\n",
      "        [0.0741],\n",
      "        [0.5857],\n",
      "        [0.6434],\n",
      "        [0.0741],\n",
      "        [0.0741],\n",
      "        [0.4981],\n",
      "        [0.5755],\n",
      "        [0.4871],\n",
      "        [0.6193],\n",
      "        [0.0741],\n",
      "        [0.1964],\n",
      "        [0.7594],\n",
      "        [0.6010],\n",
      "        [0.2671],\n",
      "        [0.6641],\n",
      "        [0.2671],\n",
      "        [0.6089],\n",
      "        [0.2262],\n",
      "        [0.0741],\n",
      "        [0.1490],\n",
      "        [0.6722],\n",
      "        [0.6710],\n",
      "        [0.0741],\n",
      "        [0.4811],\n",
      "        [0.2654],\n",
      "        [0.4180],\n",
      "        [0.0741],\n",
      "        [0.6242],\n",
      "        [0.2468],\n",
      "        [0.3417],\n",
      "        [0.3169],\n",
      "        [0.3134],\n",
      "        [0.0741],\n",
      "        [0.6544],\n",
      "        [0.6280],\n",
      "        [0.1703],\n",
      "        [0.6857],\n",
      "        [0.0741],\n",
      "        [0.2641],\n",
      "        [0.6158],\n",
      "        [0.2761],\n",
      "        [0.0741],\n",
      "        [0.1230],\n",
      "        [0.3004],\n",
      "        [0.5533],\n",
      "        [0.6493],\n",
      "        [0.2462],\n",
      "        [0.8060],\n",
      "        [0.2407],\n",
      "        [0.2964],\n",
      "        [0.6738],\n",
      "        [0.6283],\n",
      "        [0.6700],\n",
      "        [0.5588],\n",
      "        [0.6236],\n",
      "        [0.2364],\n",
      "        [0.2586],\n",
      "        [0.5467],\n",
      "        [0.1468],\n",
      "        [0.1064],\n",
      "        [0.6225],\n",
      "        [0.3227],\n",
      "        [0.2820],\n",
      "        [0.2792],\n",
      "        [0.7472],\n",
      "        [0.2754],\n",
      "        [0.1185],\n",
      "        [0.2583],\n",
      "        [0.5558],\n",
      "        [0.3449],\n",
      "        [0.4338],\n",
      "        [0.3396],\n",
      "        [0.3231],\n",
      "        [0.7287],\n",
      "        [0.3839],\n",
      "        [0.1522],\n",
      "        [0.4203],\n",
      "        [0.7566],\n",
      "        [0.1416],\n",
      "        [0.3975],\n",
      "        [0.6073],\n",
      "        [0.6120],\n",
      "        [0.7531]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0012],\n",
      "        [0.0015],\n",
      "        [0.0016],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0044],\n",
      "        [0.0046],\n",
      "        [0.0049],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0058],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0074],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0084],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0089],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0094],\n",
      "        [0.0099],\n",
      "        [0.0103],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0127],\n",
      "        [0.0129],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0157],\n",
      "        [0.0158],\n",
      "        [0.0172],\n",
      "        [0.0175],\n",
      "        [0.0175],\n",
      "        [0.0176],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0213],\n",
      "        [0.0220],\n",
      "        [0.0223],\n",
      "        [0.0228],\n",
      "        [0.0233],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0247],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0264],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0284],\n",
      "        [0.0291],\n",
      "        [0.0301],\n",
      "        [0.0310],\n",
      "        [0.0316],\n",
      "        [0.0327],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0349],\n",
      "        [0.0350],\n",
      "        [0.0359],\n",
      "        [0.0360],\n",
      "        [0.0366],\n",
      "        [0.0373],\n",
      "        [0.0376],\n",
      "        [0.0399],\n",
      "        [0.0400],\n",
      "        [0.0405],\n",
      "        [0.0406],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0445],\n",
      "        [0.0451],\n",
      "        [0.0451],\n",
      "        [0.0454],\n",
      "        [0.0461],\n",
      "        [0.0465],\n",
      "        [0.0469],\n",
      "        [0.0479],\n",
      "        [0.0492],\n",
      "        [0.0494],\n",
      "        [0.0494],\n",
      "        [0.0500],\n",
      "        [0.0501],\n",
      "        [0.0506],\n",
      "        [0.0514],\n",
      "        [0.0522],\n",
      "        [0.0523],\n",
      "        [0.0532],\n",
      "        [0.0552],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0564],\n",
      "        [0.0572],\n",
      "        [0.0576],\n",
      "        [0.0580],\n",
      "        [0.0585],\n",
      "        [0.0586],\n",
      "        [0.0598],\n",
      "        [0.0599],\n",
      "        [0.0615],\n",
      "        [0.0616],\n",
      "        [0.0624],\n",
      "        [0.0642],\n",
      "        [0.0651],\n",
      "        [0.0656],\n",
      "        [0.0663],\n",
      "        [0.0666],\n",
      "        [0.0703],\n",
      "        [0.0710],\n",
      "        [0.0736],\n",
      "        [0.0741],\n",
      "        [0.0763],\n",
      "        [0.0766],\n",
      "        [0.0795],\n",
      "        [0.0842],\n",
      "        [0.0846],\n",
      "        [0.0915],\n",
      "        [0.0955],\n",
      "        [0.0989],\n",
      "        [0.1061],\n",
      "        [0.1075],\n",
      "        [0.1125],\n",
      "        [0.1134],\n",
      "        [0.1144],\n",
      "        [0.1190],\n",
      "        [0.1209],\n",
      "        [0.1336],\n",
      "        [0.1353],\n",
      "        [0.1416],\n",
      "        [0.1701],\n",
      "        [0.1836],\n",
      "        [0.1915],\n",
      "        [0.2920]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0036],\n",
      "        [    0.0044],\n",
      "        [    0.0006],\n",
      "        [    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0085],\n",
      "        [    0.0019],\n",
      "        [    0.0038],\n",
      "        [    0.0020],\n",
      "        [    0.0060],\n",
      "        [    0.0024],\n",
      "        [    0.0026],\n",
      "        [    0.0074],\n",
      "        [    0.0062],\n",
      "        [    0.0020],\n",
      "        [    0.0030],\n",
      "        [    0.0062],\n",
      "        [    0.0102],\n",
      "        [    0.0048],\n",
      "        [    0.0066],\n",
      "        [    0.0010],\n",
      "        [    0.0074],\n",
      "        [    0.0099],\n",
      "        [    0.0042],\n",
      "        [    0.0041],\n",
      "        [    0.0097],\n",
      "        [    0.0062],\n",
      "        [    0.0100],\n",
      "        [    0.0076],\n",
      "        [    0.0029],\n",
      "        [    0.0110],\n",
      "        [    0.0057],\n",
      "        [    0.0139],\n",
      "        [    0.0085],\n",
      "        [    0.0144],\n",
      "        [    0.0089],\n",
      "        [    0.0083],\n",
      "        [    0.0174],\n",
      "        [    0.0118],\n",
      "        [    0.0076],\n",
      "        [    0.0192],\n",
      "        [    0.0087],\n",
      "        [    0.0160],\n",
      "        [    0.0088],\n",
      "        [    0.0203],\n",
      "        [    0.0193],\n",
      "        [    0.0181],\n",
      "        [    0.0187],\n",
      "        [    0.0101],\n",
      "        [    0.0200],\n",
      "        [    0.0171],\n",
      "        [    0.0204],\n",
      "        [    0.0223],\n",
      "        [    0.0203],\n",
      "        [    0.0183],\n",
      "        [    0.0209],\n",
      "        [    0.0210],\n",
      "        [    0.0200],\n",
      "        [    0.0266],\n",
      "        [    0.0266],\n",
      "        [    0.0203],\n",
      "        [    0.0237],\n",
      "        [    0.0262],\n",
      "        [    0.0282],\n",
      "        [    0.0209],\n",
      "        [    0.0301],\n",
      "        [    0.0280],\n",
      "        [    0.0274],\n",
      "        [    0.0196],\n",
      "        [    0.0254],\n",
      "        [    0.0259],\n",
      "        [    0.0276],\n",
      "        [    0.0302],\n",
      "        [    0.0309],\n",
      "        [    0.0297],\n",
      "        [    0.0325],\n",
      "        [    0.0319],\n",
      "        [    0.0362],\n",
      "        [    0.0367],\n",
      "        [    0.0353],\n",
      "        [    0.0340],\n",
      "        [    0.0368],\n",
      "        [    0.0378],\n",
      "        [    0.0377],\n",
      "        [    0.0406],\n",
      "        [    0.0358],\n",
      "        [    0.0385],\n",
      "        [    0.0316],\n",
      "        [    0.0373],\n",
      "        [    0.0388],\n",
      "        [    0.0354],\n",
      "        [    0.0396],\n",
      "        [    0.0375],\n",
      "        [    0.0433],\n",
      "        [    0.0435],\n",
      "        [    0.0420],\n",
      "        [    0.0495],\n",
      "        [    0.0502],\n",
      "        [    0.0443],\n",
      "        [    0.0478],\n",
      "        [    0.0447],\n",
      "        [    0.0475],\n",
      "        [    0.0461],\n",
      "        [    0.0528],\n",
      "        [    0.0501],\n",
      "        [    0.0490],\n",
      "        [    0.0492],\n",
      "        [    0.0492],\n",
      "        [    0.0510],\n",
      "        [    0.0559],\n",
      "        [    0.0563],\n",
      "        [    0.0528],\n",
      "        [    0.0465],\n",
      "        [    0.0550],\n",
      "        [    0.0543],\n",
      "        [    0.0589],\n",
      "        [    0.0568],\n",
      "        [    0.0556],\n",
      "        [    0.0559],\n",
      "        [    0.0566],\n",
      "        [    0.0542],\n",
      "        [    0.0635],\n",
      "        [    0.0598],\n",
      "        [    0.0526],\n",
      "        [    0.0610],\n",
      "        [    0.0603],\n",
      "        [    0.0669],\n",
      "        [    0.0671],\n",
      "        [    0.0694],\n",
      "        [    0.0678],\n",
      "        [    0.0704],\n",
      "        [    0.0652],\n",
      "        [    0.0653],\n",
      "        [    0.0732],\n",
      "        [    0.0700],\n",
      "        [    0.0748],\n",
      "        [    0.0788],\n",
      "        [    0.0760],\n",
      "        [    0.0756],\n",
      "        [    0.0785],\n",
      "        [    0.0778],\n",
      "        [    0.0857],\n",
      "        [    0.0927],\n",
      "        [    0.0965],\n",
      "        [    0.0949],\n",
      "        [    0.1059],\n",
      "        [    0.1066],\n",
      "        [    0.1123],\n",
      "        [    0.1132],\n",
      "        [    0.1081],\n",
      "        [    0.1181],\n",
      "        [    0.1217],\n",
      "        [    0.1330],\n",
      "        [    0.1286],\n",
      "        [    0.1425],\n",
      "        [    0.1694],\n",
      "        [    0.1789],\n",
      "        [    0.1870],\n",
      "        [    0.2854]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 64.95342326164246\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 19 個區塊累積花費時間(s) 1.2123863697052002\n",
      "<<The performance of 19 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2123863697052002\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1099.46\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1523.64\n",
      "The accuracy(2000) for l = 1: 87.42%\n",
      "The accuracy(3000) for l = 1: 94.34%\n",
      "The maximum error: tensor(7287.1250)\n",
      "The minimum error: tensor(4.7891)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 955.4\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 999.2\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1455.02734375\n",
      "The minimum error: 729.1484375\n",
      "------------------------------------------------------------\n",
      "0.8742138364779874\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<20>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.52070514963998e-08, 80)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [80, 73, 134, 26, 106, 67, 9, 21, 109, 15, 72, 54, 143, 61, 62, 14, 110, 60, 59, 118, 74, 133, 135, 83, 55, 149, 24, 78, 0, 148, 38, 23, 122, 79, 63, 75, 22, 35, 66, 105, 25, 34, 46, 89, 20, 65, 126, 58, 36, 48, 104, 132, 71, 44, 53, 142, 153, 27, 140, 70, 68, 52, 28, 13, 117, 152, 64, 37, 158, 119, 16, 81, 155, 154, 1, 144, 139, 157, 150, 141, 8, 147, 51, 56, 76, 69, 39, 77, 47, 121, 93, 92, 7, 124, 116, 138, 145, 107, 151, 6, 82, 45, 99, 88, 98, 10, 120, 17, 136, 33, 127, 43, 91, 137, 146, 125, 19, 29, 95, 111, 156, 57, 114, 94, 115, 18, 90, 108, 11, 31, 49, 12, 123, 32, 50, 131, 97, 103, 2, 96, 30, 112, 130, 42, 113, 100, 84, 5, 101, 102, 87, 128, 4, 85, 129]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0010],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0020],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0044],\n",
      "        [0.0048],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0110],\n",
      "        [0.0118],\n",
      "        [0.0139],\n",
      "        [0.0144],\n",
      "        [0.0160],\n",
      "        [0.0171],\n",
      "        [0.0174],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0193],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0223],\n",
      "        [0.0237],\n",
      "        [0.0254],\n",
      "        [0.0259],\n",
      "        [0.0262],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0274],\n",
      "        [0.0276],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0286],\n",
      "        [0.0297],\n",
      "        [0.0301],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0325],\n",
      "        [0.0339],\n",
      "        [0.0340],\n",
      "        [0.0353],\n",
      "        [0.0354],\n",
      "        [0.0358],\n",
      "        [0.0362],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0378],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.0396],\n",
      "        [0.0406],\n",
      "        [0.0420],\n",
      "        [0.0433],\n",
      "        [0.0435],\n",
      "        [0.0443],\n",
      "        [0.0447],\n",
      "        [0.0461],\n",
      "        [0.0465],\n",
      "        [0.0475],\n",
      "        [0.0478],\n",
      "        [0.0490],\n",
      "        [0.0492],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0501],\n",
      "        [0.0502],\n",
      "        [0.0510],\n",
      "        [0.0528],\n",
      "        [0.0528],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0550],\n",
      "        [0.0556],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0563],\n",
      "        [0.0566],\n",
      "        [0.0568],\n",
      "        [0.0570],\n",
      "        [0.0589],\n",
      "        [0.0598],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0635],\n",
      "        [0.0652],\n",
      "        [0.0653],\n",
      "        [0.0669],\n",
      "        [0.0671],\n",
      "        [0.0678],\n",
      "        [0.0694],\n",
      "        [0.0700],\n",
      "        [0.0704],\n",
      "        [0.0732],\n",
      "        [0.0748],\n",
      "        [0.0756],\n",
      "        [0.0760],\n",
      "        [0.0778],\n",
      "        [0.0785],\n",
      "        [0.0788],\n",
      "        [0.0857],\n",
      "        [0.0927],\n",
      "        [0.0949],\n",
      "        [0.0965],\n",
      "        [0.1059],\n",
      "        [0.1066],\n",
      "        [0.1081],\n",
      "        [0.1123],\n",
      "        [0.1132],\n",
      "        [0.1181],\n",
      "        [0.1217],\n",
      "        [0.1286],\n",
      "        [0.1330],\n",
      "        [0.1425]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.52070514963998e-08, 80)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [80, 73, 134, 26, 106, 67, 9, 21, 109, 15, 72, 54, 143, 61, 62, 14, 110, 60, 59, 118, 74, 133, 135, 83, 55, 149, 24, 78, 0, 148, 38, 23, 122, 79, 63, 75, 22, 35, 66, 105, 25, 34, 46, 89, 20, 65, 126, 58, 36, 48, 104, 132, 71, 44, 53, 142, 153, 27, 140, 70, 68, 52, 28, 13, 117, 152, 64, 37, 158, 119, 16, 81, 155, 154, 1, 144, 139, 157, 150, 141, 8, 147, 51, 56, 76, 69, 39, 77, 47, 121, 93, 92, 7, 124, 116, 138, 145, 107, 151, 6, 82, 45, 99, 88, 98, 10, 120, 17, 136, 33, 127, 43, 91, 137, 146, 125, 19, 29, 95, 111, 156, 57, 114, 94, 115, 18, 90, 108, 11, 31, 49, 12, 123, 32, 50, 131, 97, 103, 2, 96, 30, 112, 130, 42, 113, 100, 84, 5, 101, 102, 87, 128, 4, 85, 129, 86] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4738],\n",
      "        [0.5412],\n",
      "        [0.0749],\n",
      "        [0.6422],\n",
      "        [0.3123],\n",
      "        [0.5926],\n",
      "        [0.6591],\n",
      "        [0.6811],\n",
      "        [0.2684],\n",
      "        [0.6944],\n",
      "        [0.5294],\n",
      "        [0.5771],\n",
      "        [0.0749],\n",
      "        [0.6327],\n",
      "        [0.6111],\n",
      "        [0.6990],\n",
      "        [0.2574],\n",
      "        [0.6390],\n",
      "        [0.6156],\n",
      "        [0.2275],\n",
      "        [0.5305],\n",
      "        [0.0754],\n",
      "        [0.0749],\n",
      "        [0.4330],\n",
      "        [0.6231],\n",
      "        [0.0749],\n",
      "        [0.6429],\n",
      "        [0.4792],\n",
      "        [0.7501],\n",
      "        [0.0749],\n",
      "        [0.5975],\n",
      "        [0.6686],\n",
      "        [0.1803],\n",
      "        [0.4775],\n",
      "        [0.6067],\n",
      "        [0.5240],\n",
      "        [0.6873],\n",
      "        [0.6352],\n",
      "        [0.5962],\n",
      "        [0.3425],\n",
      "        [0.6335],\n",
      "        [0.6272],\n",
      "        [0.5355],\n",
      "        [0.2738],\n",
      "        [0.6776],\n",
      "        [0.6097],\n",
      "        [0.1437],\n",
      "        [0.6243],\n",
      "        [0.6170],\n",
      "        [0.5652],\n",
      "        [0.3444],\n",
      "        [0.0750],\n",
      "        [0.5683],\n",
      "        [0.4910],\n",
      "        [0.5846],\n",
      "        [0.0749],\n",
      "        [0.0749],\n",
      "        [0.6368],\n",
      "        [0.0749],\n",
      "        [0.5813],\n",
      "        [0.5849],\n",
      "        [0.5846],\n",
      "        [0.6324],\n",
      "        [0.6851],\n",
      "        [0.2191],\n",
      "        [0.0749],\n",
      "        [0.6160],\n",
      "        [0.6078],\n",
      "        [0.0749],\n",
      "        [0.2249],\n",
      "        [0.6755],\n",
      "        [0.4417],\n",
      "        [0.0749],\n",
      "        [0.0749],\n",
      "        [0.7534],\n",
      "        [0.0749],\n",
      "        [0.0749],\n",
      "        [0.0902],\n",
      "        [0.0749],\n",
      "        [0.0749],\n",
      "        [0.6590],\n",
      "        [0.0749],\n",
      "        [0.5826],\n",
      "        [0.6400],\n",
      "        [0.4961],\n",
      "        [0.5984],\n",
      "        [0.6044],\n",
      "        [0.4853],\n",
      "        [0.5727],\n",
      "        [0.1976],\n",
      "        [0.2683],\n",
      "        [0.2682],\n",
      "        [0.6147],\n",
      "        [0.1501],\n",
      "        [0.2273],\n",
      "        [0.0749],\n",
      "        [0.0749],\n",
      "        [0.2668],\n",
      "        [0.0749],\n",
      "        [0.6800],\n",
      "        [0.4170],\n",
      "        [0.4787],\n",
      "        [0.3421],\n",
      "        [0.3166],\n",
      "        [0.3142],\n",
      "        [0.6673],\n",
      "        [0.2476],\n",
      "        [0.6659],\n",
      "        [0.0749],\n",
      "        [0.6193],\n",
      "        [0.1709],\n",
      "        [0.5495],\n",
      "        [0.2649],\n",
      "        [0.0760],\n",
      "        [0.0749],\n",
      "        [0.1244],\n",
      "        [0.6491],\n",
      "        [0.6232],\n",
      "        [0.3014],\n",
      "        [0.2769],\n",
      "        [0.1049],\n",
      "        [0.6128],\n",
      "        [0.2474],\n",
      "        [0.2976],\n",
      "        [0.2418],\n",
      "        [0.6442],\n",
      "        [0.2375],\n",
      "        [0.2600],\n",
      "        [0.6686],\n",
      "        [0.6236],\n",
      "        [0.5561],\n",
      "        [0.6648],\n",
      "        [0.1478],\n",
      "        [0.6189],\n",
      "        [0.5439],\n",
      "        [0.1076],\n",
      "        [0.2830],\n",
      "        [0.3230],\n",
      "        [0.7408],\n",
      "        [0.2803],\n",
      "        [0.6178],\n",
      "        [0.2765],\n",
      "        [0.1198],\n",
      "        [0.5517],\n",
      "        [0.2593],\n",
      "        [0.3451],\n",
      "        [0.4330],\n",
      "        [0.7224],\n",
      "        [0.3398],\n",
      "        [0.3233],\n",
      "        [0.3830],\n",
      "        [0.1530],\n",
      "        [0.7500],\n",
      "        [0.4197],\n",
      "        [0.1425],\n",
      "        [0.3967]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0010],\n",
      "        [0.0019],\n",
      "        [0.0020],\n",
      "        [0.0020],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0030],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0044],\n",
      "        [0.0048],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0066],\n",
      "        [0.0074],\n",
      "        [0.0074],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0110],\n",
      "        [0.0118],\n",
      "        [0.0139],\n",
      "        [0.0144],\n",
      "        [0.0160],\n",
      "        [0.0171],\n",
      "        [0.0174],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0193],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0223],\n",
      "        [0.0237],\n",
      "        [0.0254],\n",
      "        [0.0259],\n",
      "        [0.0262],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0274],\n",
      "        [0.0276],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0286],\n",
      "        [0.0297],\n",
      "        [0.0301],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0325],\n",
      "        [0.0339],\n",
      "        [0.0340],\n",
      "        [0.0353],\n",
      "        [0.0354],\n",
      "        [0.0358],\n",
      "        [0.0362],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0378],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.0396],\n",
      "        [0.0406],\n",
      "        [0.0420],\n",
      "        [0.0433],\n",
      "        [0.0435],\n",
      "        [0.0443],\n",
      "        [0.0447],\n",
      "        [0.0461],\n",
      "        [0.0465],\n",
      "        [0.0475],\n",
      "        [0.0478],\n",
      "        [0.0490],\n",
      "        [0.0492],\n",
      "        [0.0492],\n",
      "        [0.0495],\n",
      "        [0.0501],\n",
      "        [0.0502],\n",
      "        [0.0510],\n",
      "        [0.0528],\n",
      "        [0.0528],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0550],\n",
      "        [0.0556],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0563],\n",
      "        [0.0566],\n",
      "        [0.0568],\n",
      "        [0.0570],\n",
      "        [0.0589],\n",
      "        [0.0598],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0635],\n",
      "        [0.0652],\n",
      "        [0.0653],\n",
      "        [0.0669],\n",
      "        [0.0671],\n",
      "        [0.0678],\n",
      "        [0.0694],\n",
      "        [0.0700],\n",
      "        [0.0704],\n",
      "        [0.0732],\n",
      "        [0.0748],\n",
      "        [0.0756],\n",
      "        [0.0760],\n",
      "        [0.0778],\n",
      "        [0.0785],\n",
      "        [0.0788],\n",
      "        [0.0857],\n",
      "        [0.0927],\n",
      "        [0.0949],\n",
      "        [0.0965],\n",
      "        [0.1059],\n",
      "        [0.1066],\n",
      "        [0.1081],\n",
      "        [0.1123],\n",
      "        [0.1132],\n",
      "        [0.1181],\n",
      "        [0.1217],\n",
      "        [0.1286],\n",
      "        [0.1330],\n",
      "        [0.1425],\n",
      "        [0.1694]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0027],\n",
      "        [    0.0055],\n",
      "        [    0.0065],\n",
      "        [    0.0104],\n",
      "        [    0.0014],\n",
      "        [    0.0082],\n",
      "        [    0.0080],\n",
      "        [    0.0082],\n",
      "        [    0.0036],\n",
      "        [    0.0136],\n",
      "        [    0.0076],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0113],\n",
      "        [    0.0107],\n",
      "        [    0.0065],\n",
      "        [    0.0062],\n",
      "        [    0.0132],\n",
      "        [    0.0008],\n",
      "        [    0.0087],\n",
      "        [    0.0016],\n",
      "        [    0.0002],\n",
      "        [    0.0007],\n",
      "        [    0.0055],\n",
      "        [    0.0006],\n",
      "        [    0.0135],\n",
      "        [    0.0169],\n",
      "        [    0.0111],\n",
      "        [    0.0047],\n",
      "        [    0.0145],\n",
      "        [    0.0155],\n",
      "        [    0.0188],\n",
      "        [    0.0137],\n",
      "        [    0.0069],\n",
      "        [    0.0034],\n",
      "        [    0.0057],\n",
      "        [    0.0208],\n",
      "        [    0.0020],\n",
      "        [    0.0047],\n",
      "        [    0.0105],\n",
      "        [    0.0049],\n",
      "        [    0.0064],\n",
      "        [    0.0108],\n",
      "        [    0.0195],\n",
      "        [    0.0068],\n",
      "        [    0.0113],\n",
      "        [    0.0242],\n",
      "        [    0.0116],\n",
      "        [    0.0116],\n",
      "        [    0.0141],\n",
      "        [    0.0186],\n",
      "        [    0.0143],\n",
      "        [    0.0260],\n",
      "        [    0.0164],\n",
      "        [    0.0269],\n",
      "        [    0.0150],\n",
      "        [    0.0150],\n",
      "        [    0.0132],\n",
      "        [    0.0178],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0201],\n",
      "        [    0.0177],\n",
      "        [    0.0160],\n",
      "        [    0.0246],\n",
      "        [    0.0335],\n",
      "        [    0.0211],\n",
      "        [    0.0209],\n",
      "        [    0.0226],\n",
      "        [    0.0325],\n",
      "        [    0.0199],\n",
      "        [    0.0281],\n",
      "        [    0.0244],\n",
      "        [    0.0250],\n",
      "        [    0.0450],\n",
      "        [    0.0378],\n",
      "        [    0.0266],\n",
      "        [    0.0250],\n",
      "        [    0.0399],\n",
      "        [    0.0294],\n",
      "        [    0.0452],\n",
      "        [    0.0417],\n",
      "        [    0.0301],\n",
      "        [    0.0292],\n",
      "        [    0.0334],\n",
      "        [    0.0437],\n",
      "        [    0.0446],\n",
      "        [    0.0347],\n",
      "        [    0.0316],\n",
      "        [    0.0345],\n",
      "        [    0.0403],\n",
      "        [    0.0414],\n",
      "        [    0.0321],\n",
      "        [    0.0477],\n",
      "        [    0.0405],\n",
      "        [    0.0376],\n",
      "        [    0.0502],\n",
      "        [    0.0456],\n",
      "        [    0.0520],\n",
      "        [    0.0571],\n",
      "        [    0.0461],\n",
      "        [    0.0442],\n",
      "        [    0.0479],\n",
      "        [    0.0477],\n",
      "        [    0.0490],\n",
      "        [    0.0391],\n",
      "        [    0.0478],\n",
      "        [    0.0401],\n",
      "        [    0.0450],\n",
      "        [    0.0451],\n",
      "        [    0.0476],\n",
      "        [    0.0598],\n",
      "        [    0.0564],\n",
      "        [    0.0481],\n",
      "        [    0.0615],\n",
      "        [    0.0623],\n",
      "        [    0.0465],\n",
      "        [    0.0479],\n",
      "        [    0.0569],\n",
      "        [    0.0558],\n",
      "        [    0.0484],\n",
      "        [    0.0522],\n",
      "        [    0.0577],\n",
      "        [    0.0608],\n",
      "        [    0.0587],\n",
      "        [    0.0543],\n",
      "        [    0.0681],\n",
      "        [    0.0665],\n",
      "        [    0.0567],\n",
      "        [    0.0588],\n",
      "        [    0.0621],\n",
      "        [    0.0594],\n",
      "        [    0.0761],\n",
      "        [    0.0623],\n",
      "        [    0.0683],\n",
      "        [    0.0674],\n",
      "        [    0.0765],\n",
      "        [    0.0753],\n",
      "        [    0.0904],\n",
      "        [    0.0795],\n",
      "        [    0.0707],\n",
      "        [    0.0846],\n",
      "        [    0.0858],\n",
      "        [    0.1005],\n",
      "        [    0.0948],\n",
      "        [    0.1047],\n",
      "        [    0.1085],\n",
      "        [    0.1201],\n",
      "        [    0.1112],\n",
      "        [    0.1126],\n",
      "        [    0.1185],\n",
      "        [    0.1158],\n",
      "        [    0.1414],\n",
      "        [    0.1348],\n",
      "        [    0.1362],\n",
      "        [    0.1703]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 65.47320556640625\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.4275678356389108e-08, 133)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [133, 55, 135, 59, 106, 74, 54, 35, 143, 80, 63, 109, 66, 0, 25, 83, 73, 75, 110, 34, 14, 134, 20, 79, 72, 9, 21, 67, 118, 26, 105, 62, 46, 78, 61, 65, 36, 58, 60, 27, 149, 15, 122, 48, 132, 148, 142, 153, 38, 13, 44, 24, 28, 140, 104, 23, 89, 16, 52, 22, 37, 64, 158, 126, 155, 117, 154, 157, 71, 139, 53, 81, 56, 141, 51, 70, 47, 68, 7, 119, 76, 152, 121, 77, 138, 144, 10, 150, 17, 93, 116, 92, 147, 69, 45, 39, 1, 136, 33, 8, 107, 82, 19, 127, 124, 88, 120, 99, 29, 137, 156, 98, 145, 151, 57, 18, 111, 91, 11, 95, 6, 114, 115, 31, 12, 43, 94, 146, 49, 125, 32, 108, 131, 90, 50, 30, 103, 123, 97, 96, 112, 130, 2, 113, 42, 100, 84, 101, 102, 128, 87, 5, 85, 129, 4, 86, 41] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.0690],\n",
      "        [0.6300],\n",
      "        [0.0690],\n",
      "        [0.6224],\n",
      "        [0.3128],\n",
      "        [0.5350],\n",
      "        [0.5827],\n",
      "        [0.6434],\n",
      "        [0.0690],\n",
      "        [0.4766],\n",
      "        [0.6132],\n",
      "        [0.2674],\n",
      "        [0.6026],\n",
      "        [0.7633],\n",
      "        [0.6426],\n",
      "        [0.4349],\n",
      "        [0.5461],\n",
      "        [0.5283],\n",
      "        [0.2560],\n",
      "        [0.6351],\n",
      "        [0.7099],\n",
      "        [0.0690],\n",
      "        [0.6882],\n",
      "        [0.4803],\n",
      "        [0.5340],\n",
      "        [0.6690],\n",
      "        [0.6917],\n",
      "        [0.5989],\n",
      "        [0.2249],\n",
      "        [0.6515],\n",
      "        [0.3438],\n",
      "        [0.6177],\n",
      "        [0.5407],\n",
      "        [0.4820],\n",
      "        [0.6399],\n",
      "        [0.6164],\n",
      "        [0.6248],\n",
      "        [0.6313],\n",
      "        [0.6465],\n",
      "        [0.6459],\n",
      "        [0.0690],\n",
      "        [0.7050],\n",
      "        [0.1755],\n",
      "        [0.5710],\n",
      "        [0.0690],\n",
      "        [0.0690],\n",
      "        [0.0690],\n",
      "        [0.0690],\n",
      "        [0.6043],\n",
      "        [0.6958],\n",
      "        [0.4950],\n",
      "        [0.6522],\n",
      "        [0.6413],\n",
      "        [0.0690],\n",
      "        [0.3459],\n",
      "        [0.6786],\n",
      "        [0.2715],\n",
      "        [0.6858],\n",
      "        [0.5907],\n",
      "        [0.6980],\n",
      "        [0.6151],\n",
      "        [0.6229],\n",
      "        [0.0690],\n",
      "        [0.1378],\n",
      "        [0.0690],\n",
      "        [0.2163],\n",
      "        [0.0690],\n",
      "        [0.0814],\n",
      "        [0.5740],\n",
      "        [0.0690],\n",
      "        [0.5907],\n",
      "        [0.4439],\n",
      "        [0.6476],\n",
      "        [0.0690],\n",
      "        [0.5886],\n",
      "        [0.5873],\n",
      "        [0.5789],\n",
      "        [0.5911],\n",
      "        [0.6232],\n",
      "        [0.2221],\n",
      "        [0.4995],\n",
      "        [0.0690],\n",
      "        [0.1936],\n",
      "        [0.4884],\n",
      "        [0.0690],\n",
      "        [0.0690],\n",
      "        [0.6777],\n",
      "        [0.0690],\n",
      "        [0.6759],\n",
      "        [0.2668],\n",
      "        [0.2246],\n",
      "        [0.2664],\n",
      "        [0.0690],\n",
      "        [0.6048],\n",
      "        [0.4823],\n",
      "        [0.6115],\n",
      "        [0.7667],\n",
      "        [0.0690],\n",
      "        [0.6270],\n",
      "        [0.6688],\n",
      "        [0.2658],\n",
      "        [0.4184],\n",
      "        [0.6586],\n",
      "        [0.1657],\n",
      "        [0.1444],\n",
      "        [0.3152],\n",
      "        [0.2453],\n",
      "        [0.3432],\n",
      "        [0.6316],\n",
      "        [0.0690],\n",
      "        [0.0963],\n",
      "        [0.3145],\n",
      "        [0.0690],\n",
      "        [0.0690],\n",
      "        [0.6195],\n",
      "        [0.6535],\n",
      "        [0.2759],\n",
      "        [0.2629],\n",
      "        [0.6788],\n",
      "        [0.3011],\n",
      "        [0.6906],\n",
      "        [0.2453],\n",
      "        [0.2395],\n",
      "        [0.6319],\n",
      "        [0.6749],\n",
      "        [0.5552],\n",
      "        [0.2971],\n",
      "        [0.0690],\n",
      "        [0.5617],\n",
      "        [0.1179],\n",
      "        [0.6269],\n",
      "        [0.2588],\n",
      "        [0.1002],\n",
      "        [0.2346],\n",
      "        [0.5488],\n",
      "        [0.6259],\n",
      "        [0.3237],\n",
      "        [0.1418],\n",
      "        [0.2821],\n",
      "        [0.2792],\n",
      "        [0.2755],\n",
      "        [0.1129],\n",
      "        [0.7534],\n",
      "        [0.2576],\n",
      "        [0.5574],\n",
      "        [0.3463],\n",
      "        [0.4349],\n",
      "        [0.3409],\n",
      "        [0.3239],\n",
      "        [0.1471],\n",
      "        [0.3835],\n",
      "        [0.7344],\n",
      "        [0.4215],\n",
      "        [0.1362],\n",
      "        [0.7628],\n",
      "        [0.3977],\n",
      "        [0.6094]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0006],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0027],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0047],\n",
      "        [    0.0047],\n",
      "        [    0.0049],\n",
      "        [    0.0055],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0062],\n",
      "        [    0.0064],\n",
      "        [    0.0065],\n",
      "        [    0.0065],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0076],\n",
      "        [    0.0080],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0087],\n",
      "        [    0.0104],\n",
      "        [    0.0105],\n",
      "        [    0.0107],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0113],\n",
      "        [    0.0113],\n",
      "        [    0.0116],\n",
      "        [    0.0116],\n",
      "        [    0.0132],\n",
      "        [    0.0132],\n",
      "        [    0.0135],\n",
      "        [    0.0136],\n",
      "        [    0.0137],\n",
      "        [    0.0141],\n",
      "        [    0.0143],\n",
      "        [    0.0145],\n",
      "        [    0.0150],\n",
      "        [    0.0150],\n",
      "        [    0.0155],\n",
      "        [    0.0160],\n",
      "        [    0.0164],\n",
      "        [    0.0169],\n",
      "        [    0.0177],\n",
      "        [    0.0178],\n",
      "        [    0.0186],\n",
      "        [    0.0188],\n",
      "        [    0.0195],\n",
      "        [    0.0199],\n",
      "        [    0.0201],\n",
      "        [    0.0208],\n",
      "        [    0.0209],\n",
      "        [    0.0211],\n",
      "        [    0.0226],\n",
      "        [    0.0242],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0250],\n",
      "        [    0.0250],\n",
      "        [    0.0260],\n",
      "        [    0.0266],\n",
      "        [    0.0269],\n",
      "        [    0.0281],\n",
      "        [    0.0292],\n",
      "        [    0.0294],\n",
      "        [    0.0301],\n",
      "        [    0.0315],\n",
      "        [    0.0316],\n",
      "        [    0.0321],\n",
      "        [    0.0321],\n",
      "        [    0.0325],\n",
      "        [    0.0334],\n",
      "        [    0.0335],\n",
      "        [    0.0345],\n",
      "        [    0.0347],\n",
      "        [    0.0376],\n",
      "        [    0.0378],\n",
      "        [    0.0391],\n",
      "        [    0.0399],\n",
      "        [    0.0401],\n",
      "        [    0.0403],\n",
      "        [    0.0405],\n",
      "        [    0.0414],\n",
      "        [    0.0417],\n",
      "        [    0.0437],\n",
      "        [    0.0442],\n",
      "        [    0.0446],\n",
      "        [    0.0450],\n",
      "        [    0.0450],\n",
      "        [    0.0451],\n",
      "        [    0.0452],\n",
      "        [    0.0456],\n",
      "        [    0.0461],\n",
      "        [    0.0465],\n",
      "        [    0.0476],\n",
      "        [    0.0477],\n",
      "        [    0.0477],\n",
      "        [    0.0478],\n",
      "        [    0.0479],\n",
      "        [    0.0479],\n",
      "        [    0.0481],\n",
      "        [    0.0484],\n",
      "        [    0.0490],\n",
      "        [    0.0502],\n",
      "        [    0.0520],\n",
      "        [    0.0522],\n",
      "        [    0.0543],\n",
      "        [    0.0558],\n",
      "        [    0.0564],\n",
      "        [    0.0567],\n",
      "        [    0.0569],\n",
      "        [    0.0571],\n",
      "        [    0.0577],\n",
      "        [    0.0587],\n",
      "        [    0.0588],\n",
      "        [    0.0594],\n",
      "        [    0.0598],\n",
      "        [    0.0608],\n",
      "        [    0.0615],\n",
      "        [    0.0621],\n",
      "        [    0.0623],\n",
      "        [    0.0623],\n",
      "        [    0.0665],\n",
      "        [    0.0674],\n",
      "        [    0.0681],\n",
      "        [    0.0683],\n",
      "        [    0.0707],\n",
      "        [    0.0753],\n",
      "        [    0.0761],\n",
      "        [    0.0765],\n",
      "        [    0.0795],\n",
      "        [    0.0846],\n",
      "        [    0.0858],\n",
      "        [    0.0904],\n",
      "        [    0.0948],\n",
      "        [    0.1005],\n",
      "        [    0.1047],\n",
      "        [    0.1085],\n",
      "        [    0.1112],\n",
      "        [    0.1126],\n",
      "        [    0.1158],\n",
      "        [    0.1185],\n",
      "        [    0.1201],\n",
      "        [    0.1348],\n",
      "        [    0.1362],\n",
      "        [    0.1414],\n",
      "        [    0.1703],\n",
      "        [    0.1857]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0019],\n",
      "        [0.0015],\n",
      "        [0.0002],\n",
      "        [0.0007],\n",
      "        [0.0023],\n",
      "        [0.0005],\n",
      "        [0.0030],\n",
      "        [0.0014],\n",
      "        [0.0018],\n",
      "        [0.0046],\n",
      "        [0.0021],\n",
      "        [0.0054],\n",
      "        [0.0061],\n",
      "        [0.0049],\n",
      "        [0.0059],\n",
      "        [0.0051],\n",
      "        [0.0064],\n",
      "        [0.0048],\n",
      "        [0.0075],\n",
      "        [0.0068],\n",
      "        [0.0058],\n",
      "        [0.0064],\n",
      "        [0.0080],\n",
      "        [0.0071],\n",
      "        [0.0084],\n",
      "        [0.0084],\n",
      "        [0.0074],\n",
      "        [0.0072],\n",
      "        [0.0104],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0114],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0122],\n",
      "        [0.0126],\n",
      "        [0.0126],\n",
      "        [0.0122],\n",
      "        [0.0134],\n",
      "        [0.0127],\n",
      "        [0.0137],\n",
      "        [0.0132],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0137],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0140],\n",
      "        [0.0156],\n",
      "        [0.0175],\n",
      "        [0.0168],\n",
      "        [0.0177],\n",
      "        [0.0185],\n",
      "        [0.0164],\n",
      "        [0.0188],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0211],\n",
      "        [0.0210],\n",
      "        [0.0221],\n",
      "        [0.0220],\n",
      "        [0.0234],\n",
      "        [0.0241],\n",
      "        [0.0251],\n",
      "        [0.0261],\n",
      "        [0.0257],\n",
      "        [0.0239],\n",
      "        [0.0255],\n",
      "        [0.0273],\n",
      "        [0.0259],\n",
      "        [0.0286],\n",
      "        [0.0302],\n",
      "        [0.0301],\n",
      "        [0.0312],\n",
      "        [0.0308],\n",
      "        [0.0322],\n",
      "        [0.0314],\n",
      "        [0.0320],\n",
      "        [0.0312],\n",
      "        [0.0343],\n",
      "        [0.0328],\n",
      "        [0.0353],\n",
      "        [0.0356],\n",
      "        [0.0384],\n",
      "        [0.0370],\n",
      "        [0.0383],\n",
      "        [0.0391],\n",
      "        [0.0399],\n",
      "        [0.0387],\n",
      "        [0.0416],\n",
      "        [0.0401],\n",
      "        [0.0409],\n",
      "        [0.0430],\n",
      "        [0.0451],\n",
      "        [0.0434],\n",
      "        [0.0464],\n",
      "        [0.0458],\n",
      "        [0.0461],\n",
      "        [0.0454],\n",
      "        [0.0439],\n",
      "        [0.0467],\n",
      "        [0.0465],\n",
      "        [0.0477],\n",
      "        [0.0474],\n",
      "        [0.0471],\n",
      "        [0.0490],\n",
      "        [0.0456],\n",
      "        [0.0482],\n",
      "        [0.0488],\n",
      "        [0.0471],\n",
      "        [0.0471],\n",
      "        [0.0495],\n",
      "        [0.0513],\n",
      "        [0.0534],\n",
      "        [0.0543],\n",
      "        [0.0572],\n",
      "        [0.0556],\n",
      "        [0.0563],\n",
      "        [0.0551],\n",
      "        [0.0576],\n",
      "        [0.0589],\n",
      "        [0.0600],\n",
      "        [0.0592],\n",
      "        [0.0590],\n",
      "        [0.0588],\n",
      "        [0.0591],\n",
      "        [0.0607],\n",
      "        [0.0629],\n",
      "        [0.0621],\n",
      "        [0.0630],\n",
      "        [0.0649],\n",
      "        [0.0670],\n",
      "        [0.0676],\n",
      "        [0.0694],\n",
      "        [0.0711],\n",
      "        [0.0733],\n",
      "        [0.0761],\n",
      "        [0.0750],\n",
      "        [0.0781],\n",
      "        [0.0860],\n",
      "        [0.0856],\n",
      "        [0.0912],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1026],\n",
      "        [0.1082],\n",
      "        [0.1092],\n",
      "        [0.1106],\n",
      "        [0.1158],\n",
      "        [0.1181],\n",
      "        [0.1207],\n",
      "        [0.1348],\n",
      "        [0.1360],\n",
      "        [0.1421],\n",
      "        [0.1700],\n",
      "        [0.1843]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 65.76984286308289\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.8618740205720314e-08, 59)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [59, 54, 133, 106, 143, 135, 80, 55, 109, 74, 35, 63, 110, 25, 73, 66, 134, 83, 0, 75, 20, 14, 72, 118, 67, 34, 79, 105, 9, 21, 62, 78, 61, 26, 46, 65, 60, 36, 58, 149, 122, 27, 15, 148, 38, 48, 132, 13, 142, 153, 104, 24, 44, 28, 140, 23, 16, 89, 22, 52, 64, 37, 158, 157, 126, 155, 71, 154, 53, 117, 139, 81, 141, 56, 70, 51, 119, 68, 7, 47, 152, 76, 121, 77, 144, 10, 138, 93, 150, 17, 92, 147, 116, 69, 39, 107, 45, 8, 99, 136, 33, 1, 19, 82, 98, 88, 156, 124, 127, 29, 137, 120, 145, 151, 57, 18, 95, 91, 11, 111, 6, 43, 114, 12, 94, 31, 115, 146, 125, 49, 32, 108, 131, 90, 50, 30, 103, 97, 123, 96, 130, 112, 2, 113, 42, 100, 84, 101, 102, 128, 87, 5, 85, 129, 4, 86, 41, 40] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6214],\n",
      "        [0.5812],\n",
      "        [0.0697],\n",
      "        [0.3149],\n",
      "        [0.0697],\n",
      "        [0.0697],\n",
      "        [0.4758],\n",
      "        [0.6286],\n",
      "        [0.2688],\n",
      "        [0.5343],\n",
      "        [0.6424],\n",
      "        [0.6120],\n",
      "        [0.2574],\n",
      "        [0.6426],\n",
      "        [0.5457],\n",
      "        [0.6018],\n",
      "        [0.0697],\n",
      "        [0.4345],\n",
      "        [0.7647],\n",
      "        [0.5276],\n",
      "        [0.6886],\n",
      "        [0.7102],\n",
      "        [0.5336],\n",
      "        [0.2264],\n",
      "        [0.5981],\n",
      "        [0.6340],\n",
      "        [0.4792],\n",
      "        [0.3460],\n",
      "        [0.6695],\n",
      "        [0.6920],\n",
      "        [0.6163],\n",
      "        [0.4809],\n",
      "        [0.6388],\n",
      "        [0.6516],\n",
      "        [0.5400],\n",
      "        [0.6156],\n",
      "        [0.6455],\n",
      "        [0.6238],\n",
      "        [0.6304],\n",
      "        [0.0697],\n",
      "        [0.1759],\n",
      "        [0.6458],\n",
      "        [0.7051],\n",
      "        [0.0697],\n",
      "        [0.6029],\n",
      "        [0.5704],\n",
      "        [0.0697],\n",
      "        [0.6961],\n",
      "        [0.0697],\n",
      "        [0.0697],\n",
      "        [0.3481],\n",
      "        [0.6521],\n",
      "        [0.4939],\n",
      "        [0.6412],\n",
      "        [0.0697],\n",
      "        [0.6785],\n",
      "        [0.6859],\n",
      "        [0.2712],\n",
      "        [0.6981],\n",
      "        [0.5897],\n",
      "        [0.6219],\n",
      "        [0.6139],\n",
      "        [0.0697],\n",
      "        [0.0803],\n",
      "        [0.1379],\n",
      "        [0.0697],\n",
      "        [0.5734],\n",
      "        [0.0697],\n",
      "        [0.5897],\n",
      "        [0.2178],\n",
      "        [0.0697],\n",
      "        [0.4433],\n",
      "        [0.0697],\n",
      "        [0.6465],\n",
      "        [0.5866],\n",
      "        [0.5876],\n",
      "        [0.2234],\n",
      "        [0.5904],\n",
      "        [0.6233],\n",
      "        [0.5783],\n",
      "        [0.0697],\n",
      "        [0.4986],\n",
      "        [0.1944],\n",
      "        [0.4875],\n",
      "        [0.0697],\n",
      "        [0.6784],\n",
      "        [0.0697],\n",
      "        [0.2683],\n",
      "        [0.0697],\n",
      "        [0.6762],\n",
      "        [0.2677],\n",
      "        [0.0697],\n",
      "        [0.2257],\n",
      "        [0.6041],\n",
      "        [0.6103],\n",
      "        [0.2676],\n",
      "        [0.4814],\n",
      "        [0.6690],\n",
      "        [0.3455],\n",
      "        [0.0697],\n",
      "        [0.6260],\n",
      "        [0.7681],\n",
      "        [0.6586],\n",
      "        [0.4178],\n",
      "        [0.3164],\n",
      "        [0.3146],\n",
      "        [0.0950],\n",
      "        [0.1447],\n",
      "        [0.1658],\n",
      "        [0.6312],\n",
      "        [0.0697],\n",
      "        [0.2465],\n",
      "        [0.0697],\n",
      "        [0.0697],\n",
      "        [0.6183],\n",
      "        [0.6534],\n",
      "        [0.3029],\n",
      "        [0.2637],\n",
      "        [0.6792],\n",
      "        [0.2773],\n",
      "        [0.6911],\n",
      "        [0.5541],\n",
      "        [0.2466],\n",
      "        [0.6752],\n",
      "        [0.2988],\n",
      "        [0.6316],\n",
      "        [0.2408],\n",
      "        [0.0697],\n",
      "        [0.1181],\n",
      "        [0.5610],\n",
      "        [0.6263],\n",
      "        [0.2604],\n",
      "        [0.0999],\n",
      "        [0.2351],\n",
      "        [0.5477],\n",
      "        [0.6255],\n",
      "        [0.3256],\n",
      "        [0.2836],\n",
      "        [0.1417],\n",
      "        [0.2807],\n",
      "        [0.1126],\n",
      "        [0.2769],\n",
      "        [0.7542],\n",
      "        [0.2588],\n",
      "        [0.5562],\n",
      "        [0.3484],\n",
      "        [0.4346],\n",
      "        [0.3429],\n",
      "        [0.3259],\n",
      "        [0.1471],\n",
      "        [0.3830],\n",
      "        [0.7350],\n",
      "        [0.4216],\n",
      "        [0.1360],\n",
      "        [0.7635],\n",
      "        [0.3974],\n",
      "        [0.6080],\n",
      "        [0.6135]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0030],\n",
      "        [0.0046],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0064],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0074],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0084],\n",
      "        [0.0094],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0114],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0126],\n",
      "        [0.0126],\n",
      "        [0.0127],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0137],\n",
      "        [0.0137],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0150],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0164],\n",
      "        [0.0168],\n",
      "        [0.0175],\n",
      "        [0.0177],\n",
      "        [0.0185],\n",
      "        [0.0188],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0210],\n",
      "        [0.0211],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0234],\n",
      "        [0.0239],\n",
      "        [0.0241],\n",
      "        [0.0251],\n",
      "        [0.0255],\n",
      "        [0.0257],\n",
      "        [0.0259],\n",
      "        [0.0261],\n",
      "        [0.0273],\n",
      "        [0.0286],\n",
      "        [0.0301],\n",
      "        [0.0302],\n",
      "        [0.0308],\n",
      "        [0.0312],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0320],\n",
      "        [0.0322],\n",
      "        [0.0328],\n",
      "        [0.0343],\n",
      "        [0.0353],\n",
      "        [0.0356],\n",
      "        [0.0370],\n",
      "        [0.0383],\n",
      "        [0.0384],\n",
      "        [0.0387],\n",
      "        [0.0391],\n",
      "        [0.0399],\n",
      "        [0.0401],\n",
      "        [0.0409],\n",
      "        [0.0416],\n",
      "        [0.0430],\n",
      "        [0.0434],\n",
      "        [0.0439],\n",
      "        [0.0451],\n",
      "        [0.0454],\n",
      "        [0.0456],\n",
      "        [0.0458],\n",
      "        [0.0461],\n",
      "        [0.0464],\n",
      "        [0.0465],\n",
      "        [0.0467],\n",
      "        [0.0471],\n",
      "        [0.0471],\n",
      "        [0.0471],\n",
      "        [0.0474],\n",
      "        [0.0477],\n",
      "        [0.0482],\n",
      "        [0.0488],\n",
      "        [0.0490],\n",
      "        [0.0495],\n",
      "        [0.0513],\n",
      "        [0.0534],\n",
      "        [0.0543],\n",
      "        [0.0551],\n",
      "        [0.0556],\n",
      "        [0.0563],\n",
      "        [0.0572],\n",
      "        [0.0576],\n",
      "        [0.0588],\n",
      "        [0.0589],\n",
      "        [0.0590],\n",
      "        [0.0591],\n",
      "        [0.0592],\n",
      "        [0.0600],\n",
      "        [0.0607],\n",
      "        [0.0621],\n",
      "        [0.0629],\n",
      "        [0.0630],\n",
      "        [0.0649],\n",
      "        [0.0670],\n",
      "        [0.0676],\n",
      "        [0.0694],\n",
      "        [0.0711],\n",
      "        [0.0733],\n",
      "        [0.0750],\n",
      "        [0.0761],\n",
      "        [0.0781],\n",
      "        [0.0856],\n",
      "        [0.0860],\n",
      "        [0.0912],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1026],\n",
      "        [0.1082],\n",
      "        [0.1092],\n",
      "        [0.1106],\n",
      "        [0.1158],\n",
      "        [0.1181],\n",
      "        [0.1207],\n",
      "        [0.1348],\n",
      "        [0.1360],\n",
      "        [0.1421],\n",
      "        [0.1700],\n",
      "        [0.1843],\n",
      "        [0.1930]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0028],\n",
      "        [0.0025],\n",
      "        [0.0007],\n",
      "        [0.0017],\n",
      "        [0.0013],\n",
      "        [0.0015],\n",
      "        [0.0005],\n",
      "        [0.0050],\n",
      "        [0.0017],\n",
      "        [0.0044],\n",
      "        [0.0062],\n",
      "        [0.0074],\n",
      "        [0.0043],\n",
      "        [0.0070],\n",
      "        [0.0033],\n",
      "        [0.0077],\n",
      "        [0.0057],\n",
      "        [0.0077],\n",
      "        [0.0054],\n",
      "        [0.0085],\n",
      "        [0.0082],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0065],\n",
      "        [0.0051],\n",
      "        [0.0108],\n",
      "        [0.0106],\n",
      "        [0.0072],\n",
      "        [0.0069],\n",
      "        [0.0065],\n",
      "        [0.0064],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0083],\n",
      "        [0.0138],\n",
      "        [0.0146],\n",
      "        [0.0096],\n",
      "        [0.0158],\n",
      "        [0.0152],\n",
      "        [0.0126],\n",
      "        [0.0136],\n",
      "        [0.0156],\n",
      "        [0.0116],\n",
      "        [0.0136],\n",
      "        [0.0104],\n",
      "        [0.0169],\n",
      "        [0.0151],\n",
      "        [0.0173],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0152],\n",
      "        [0.0144],\n",
      "        [0.0204],\n",
      "        [0.0199],\n",
      "        [0.0186],\n",
      "        [0.0165],\n",
      "        [0.0217],\n",
      "        [0.0211],\n",
      "        [0.0189],\n",
      "        [0.0236],\n",
      "        [0.0246],\n",
      "        [0.0255],\n",
      "        [0.0235],\n",
      "        [0.0222],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0235],\n",
      "        [0.0258],\n",
      "        [0.0234],\n",
      "        [0.0268],\n",
      "        [0.0274],\n",
      "        [0.0305],\n",
      "        [0.0302],\n",
      "        [0.0329],\n",
      "        [0.0286],\n",
      "        [0.0338],\n",
      "        [0.0307],\n",
      "        [0.0292],\n",
      "        [0.0337],\n",
      "        [0.0344],\n",
      "        [0.0327],\n",
      "        [0.0366],\n",
      "        [0.0354],\n",
      "        [0.0379],\n",
      "        [0.0370],\n",
      "        [0.0395],\n",
      "        [0.0384],\n",
      "        [0.0378],\n",
      "        [0.0390],\n",
      "        [0.0418],\n",
      "        [0.0396],\n",
      "        [0.0408],\n",
      "        [0.0418],\n",
      "        [0.0407],\n",
      "        [0.0400],\n",
      "        [0.0431],\n",
      "        [0.0477],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0459],\n",
      "        [0.0492],\n",
      "        [0.0458],\n",
      "        [0.0485],\n",
      "        [0.0487],\n",
      "        [0.0460],\n",
      "        [0.0453],\n",
      "        [0.0452],\n",
      "        [0.0479],\n",
      "        [0.0471],\n",
      "        [0.0507],\n",
      "        [0.0489],\n",
      "        [0.0493],\n",
      "        [0.0494],\n",
      "        [0.0512],\n",
      "        [0.0562],\n",
      "        [0.0565],\n",
      "        [0.0541],\n",
      "        [0.0556],\n",
      "        [0.0580],\n",
      "        [0.0577],\n",
      "        [0.0561],\n",
      "        [0.0557],\n",
      "        [0.0593],\n",
      "        [0.0607],\n",
      "        [0.0580],\n",
      "        [0.0617],\n",
      "        [0.0602],\n",
      "        [0.0607],\n",
      "        [0.0627],\n",
      "        [0.0652],\n",
      "        [0.0658],\n",
      "        [0.0642],\n",
      "        [0.0659],\n",
      "        [0.0680],\n",
      "        [0.0722],\n",
      "        [0.0736],\n",
      "        [0.0723],\n",
      "        [0.0742],\n",
      "        [0.0770],\n",
      "        [0.0774],\n",
      "        [0.0846],\n",
      "        [0.0865],\n",
      "        [0.0900],\n",
      "        [0.0964],\n",
      "        [0.0960],\n",
      "        [0.1014],\n",
      "        [0.1067],\n",
      "        [0.1082],\n",
      "        [0.1097],\n",
      "        [0.1150],\n",
      "        [0.1164],\n",
      "        [0.1192],\n",
      "        [0.1337],\n",
      "        [0.1350],\n",
      "        [0.1407],\n",
      "        [0.1686],\n",
      "        [0.1806],\n",
      "        [0.1896]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 66.05937838554382\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1487426238309126e-07, 80)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [80, 133, 143, 135, 109, 106, 54, 59, 73, 110, 74, 14, 55, 67, 72, 0, 134, 35, 62, 118, 21, 9, 25, 105, 61, 78, 63, 83, 66, 20, 26, 75, 60, 38, 79, 34, 15, 149, 148, 122, 46, 24, 65, 132, 104, 58, 27, 36, 142, 153, 23, 48, 13, 140, 22, 28, 44, 89, 16, 157, 53, 158, 71, 52, 64, 126, 155, 37, 154, 117, 139, 70, 68, 141, 81, 119, 152, 56, 7, 51, 47, 121, 76, 144, 93, 77, 138, 150, 10, 92, 39, 69, 147, 17, 116, 107, 8, 99, 156, 88, 1, 136, 98, 127, 45, 124, 19, 82, 137, 33, 120, 145, 29, 151, 95, 91, 43, 6, 57, 18, 111, 11, 94, 114, 115, 146, 12, 31, 125, 108, 49, 32, 131, 90, 50, 103, 30, 97, 123, 96, 130, 112, 2, 42, 113, 100, 84, 101, 102, 128, 87, 5, 85, 129, 4, 86, 41, 40, 3] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4735],\n",
      "        [0.0698],\n",
      "        [0.0698],\n",
      "        [0.0698],\n",
      "        [0.2693],\n",
      "        [0.3159],\n",
      "        [0.5782],\n",
      "        [0.6188],\n",
      "        [0.5438],\n",
      "        [0.2579],\n",
      "        [0.5322],\n",
      "        [0.7083],\n",
      "        [0.6256],\n",
      "        [0.5957],\n",
      "        [0.5318],\n",
      "        [0.7640],\n",
      "        [0.0698],\n",
      "        [0.6392],\n",
      "        [0.6134],\n",
      "        [0.2271],\n",
      "        [0.6901],\n",
      "        [0.6680],\n",
      "        [0.6404],\n",
      "        [0.3470],\n",
      "        [0.6360],\n",
      "        [0.4783],\n",
      "        [0.6092],\n",
      "        [0.4327],\n",
      "        [0.5995],\n",
      "        [0.6868],\n",
      "        [0.6494],\n",
      "        [0.5255],\n",
      "        [0.6429],\n",
      "        [0.5992],\n",
      "        [0.4766],\n",
      "        [0.6308],\n",
      "        [0.7030],\n",
      "        [0.0698],\n",
      "        [0.0698],\n",
      "        [0.1755],\n",
      "        [0.5376],\n",
      "        [0.6498],\n",
      "        [0.6132],\n",
      "        [0.0698],\n",
      "        [0.3493],\n",
      "        [0.6278],\n",
      "        [0.6435],\n",
      "        [0.6205],\n",
      "        [0.0698],\n",
      "        [0.0698],\n",
      "        [0.6762],\n",
      "        [0.5682],\n",
      "        [0.6945],\n",
      "        [0.0698],\n",
      "        [0.6961],\n",
      "        [0.6391],\n",
      "        [0.4910],\n",
      "        [0.2699],\n",
      "        [0.6839],\n",
      "        [0.0786],\n",
      "        [0.5871],\n",
      "        [0.0698],\n",
      "        [0.5715],\n",
      "        [0.5872],\n",
      "        [0.6194],\n",
      "        [0.1373],\n",
      "        [0.0698],\n",
      "        [0.6105],\n",
      "        [0.0698],\n",
      "        [0.2185],\n",
      "        [0.0698],\n",
      "        [0.5845],\n",
      "        [0.5882],\n",
      "        [0.0698],\n",
      "        [0.4415],\n",
      "        [0.2239],\n",
      "        [0.0698],\n",
      "        [0.6438],\n",
      "        [0.6216],\n",
      "        [0.5849],\n",
      "        [0.5761],\n",
      "        [0.1944],\n",
      "        [0.4963],\n",
      "        [0.0698],\n",
      "        [0.2693],\n",
      "        [0.4851],\n",
      "        [0.0698],\n",
      "        [0.0698],\n",
      "        [0.6773],\n",
      "        [0.2682],\n",
      "        [0.6069],\n",
      "        [0.6018],\n",
      "        [0.0698],\n",
      "        [0.6743],\n",
      "        [0.2258],\n",
      "        [0.2684],\n",
      "        [0.6673],\n",
      "        [0.3469],\n",
      "        [0.0931],\n",
      "        [0.3128],\n",
      "        [0.7675],\n",
      "        [0.0698],\n",
      "        [0.3175],\n",
      "        [0.1652],\n",
      "        [0.4788],\n",
      "        [0.1442],\n",
      "        [0.6565],\n",
      "        [0.4159],\n",
      "        [0.0698],\n",
      "        [0.6228],\n",
      "        [0.2469],\n",
      "        [0.0698],\n",
      "        [0.6288],\n",
      "        [0.0698],\n",
      "        [0.3039],\n",
      "        [0.2636],\n",
      "        [0.5511],\n",
      "        [0.6896],\n",
      "        [0.6155],\n",
      "        [0.6513],\n",
      "        [0.2778],\n",
      "        [0.6775],\n",
      "        [0.2999],\n",
      "        [0.2469],\n",
      "        [0.2410],\n",
      "        [0.0698],\n",
      "        [0.6735],\n",
      "        [0.6290],\n",
      "        [0.1175],\n",
      "        [0.2610],\n",
      "        [0.5587],\n",
      "        [0.6235],\n",
      "        [0.0988],\n",
      "        [0.2347],\n",
      "        [0.5449],\n",
      "        [0.3266],\n",
      "        [0.6230],\n",
      "        [0.2843],\n",
      "        [0.1408],\n",
      "        [0.2813],\n",
      "        [0.1116],\n",
      "        [0.2774],\n",
      "        [0.7530],\n",
      "        [0.5529],\n",
      "        [0.2591],\n",
      "        [0.3496],\n",
      "        [0.4330],\n",
      "        [0.3440],\n",
      "        [0.3268],\n",
      "        [0.1463],\n",
      "        [0.3814],\n",
      "        [0.7335],\n",
      "        [0.4204],\n",
      "        [0.1350],\n",
      "        [0.7620],\n",
      "        [0.3959],\n",
      "        [0.6043],\n",
      "        [0.6101],\n",
      "        [0.7583]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0007],\n",
      "        [0.0013],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0025],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0096],\n",
      "        [0.0104],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0116],\n",
      "        [0.0126],\n",
      "        [0.0136],\n",
      "        [0.0136],\n",
      "        [0.0138],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0151],\n",
      "        [0.0152],\n",
      "        [0.0152],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0173],\n",
      "        [0.0186],\n",
      "        [0.0189],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0217],\n",
      "        [0.0222],\n",
      "        [0.0234],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0246],\n",
      "        [0.0248],\n",
      "        [0.0252],\n",
      "        [0.0255],\n",
      "        [0.0258],\n",
      "        [0.0268],\n",
      "        [0.0274],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0302],\n",
      "        [0.0305],\n",
      "        [0.0307],\n",
      "        [0.0327],\n",
      "        [0.0329],\n",
      "        [0.0337],\n",
      "        [0.0338],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0366],\n",
      "        [0.0370],\n",
      "        [0.0378],\n",
      "        [0.0379],\n",
      "        [0.0384],\n",
      "        [0.0390],\n",
      "        [0.0395],\n",
      "        [0.0396],\n",
      "        [0.0400],\n",
      "        [0.0407],\n",
      "        [0.0408],\n",
      "        [0.0418],\n",
      "        [0.0418],\n",
      "        [0.0431],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0452],\n",
      "        [0.0453],\n",
      "        [0.0458],\n",
      "        [0.0459],\n",
      "        [0.0460],\n",
      "        [0.0471],\n",
      "        [0.0477],\n",
      "        [0.0479],\n",
      "        [0.0485],\n",
      "        [0.0487],\n",
      "        [0.0489],\n",
      "        [0.0492],\n",
      "        [0.0493],\n",
      "        [0.0494],\n",
      "        [0.0507],\n",
      "        [0.0512],\n",
      "        [0.0541],\n",
      "        [0.0556],\n",
      "        [0.0557],\n",
      "        [0.0561],\n",
      "        [0.0562],\n",
      "        [0.0565],\n",
      "        [0.0577],\n",
      "        [0.0580],\n",
      "        [0.0580],\n",
      "        [0.0593],\n",
      "        [0.0602],\n",
      "        [0.0607],\n",
      "        [0.0607],\n",
      "        [0.0617],\n",
      "        [0.0627],\n",
      "        [0.0642],\n",
      "        [0.0652],\n",
      "        [0.0658],\n",
      "        [0.0659],\n",
      "        [0.0680],\n",
      "        [0.0722],\n",
      "        [0.0723],\n",
      "        [0.0736],\n",
      "        [0.0742],\n",
      "        [0.0770],\n",
      "        [0.0774],\n",
      "        [0.0846],\n",
      "        [0.0865],\n",
      "        [0.0900],\n",
      "        [0.0960],\n",
      "        [0.0964],\n",
      "        [0.1014],\n",
      "        [0.1067],\n",
      "        [0.1082],\n",
      "        [0.1097],\n",
      "        [0.1150],\n",
      "        [0.1164],\n",
      "        [0.1192],\n",
      "        [0.1337],\n",
      "        [0.1350],\n",
      "        [0.1407],\n",
      "        [0.1686],\n",
      "        [0.1806],\n",
      "        [0.1896],\n",
      "        [0.2972]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0036],\n",
      "        [    0.0007],\n",
      "        [    0.0013],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0015],\n",
      "        [    0.0073],\n",
      "        [    0.0074],\n",
      "        [    0.0001],\n",
      "        [    0.0043],\n",
      "        [    0.0078],\n",
      "        [    0.0036],\n",
      "        [    0.0102],\n",
      "        [    0.0009],\n",
      "        [    0.0023],\n",
      "        [    0.0037],\n",
      "        [    0.0057],\n",
      "        [    0.0135],\n",
      "        [    0.0015],\n",
      "        [    0.0056],\n",
      "        [    0.0020],\n",
      "        [    0.0009],\n",
      "        [    0.0145],\n",
      "        [    0.0082],\n",
      "        [    0.0022],\n",
      "        [    0.0040],\n",
      "        [    0.0120],\n",
      "        [    0.0099],\n",
      "        [    0.0118],\n",
      "        [    0.0167],\n",
      "        [    0.0008],\n",
      "        [    0.0119],\n",
      "        [    0.0045],\n",
      "        [    0.0034],\n",
      "        [    0.0141],\n",
      "        [    0.0180],\n",
      "        [    0.0030],\n",
      "        [    0.0126],\n",
      "        [    0.0136],\n",
      "        [    0.0129],\n",
      "        [    0.0180],\n",
      "        [    0.0066],\n",
      "        [    0.0191],\n",
      "        [    0.0151],\n",
      "        [    0.0162],\n",
      "        [    0.0200],\n",
      "        [    0.0229],\n",
      "        [    0.0226],\n",
      "        [    0.0159],\n",
      "        [    0.0159],\n",
      "        [    0.0082],\n",
      "        [    0.0214],\n",
      "        [    0.0255],\n",
      "        [    0.0186],\n",
      "        [    0.0103],\n",
      "        [    0.0270],\n",
      "        [    0.0249],\n",
      "        [    0.0213],\n",
      "        [    0.0301],\n",
      "        [    0.0242],\n",
      "        [    0.0188],\n",
      "        [    0.0235],\n",
      "        [    0.0200],\n",
      "        [    0.0282],\n",
      "        [    0.0292],\n",
      "        [    0.0240],\n",
      "        [    0.0252],\n",
      "        [    0.0323],\n",
      "        [    0.0259],\n",
      "        [    0.0278],\n",
      "        [    0.0274],\n",
      "        [    0.0247],\n",
      "        [    0.0252],\n",
      "        [    0.0303],\n",
      "        [    0.0329],\n",
      "        [    0.0300],\n",
      "        [    0.0326],\n",
      "        [    0.0381],\n",
      "        [    0.0411],\n",
      "        [    0.0387],\n",
      "        [    0.0389],\n",
      "        [    0.0361],\n",
      "        [    0.0400],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0410],\n",
      "        [    0.0385],\n",
      "        [    0.0390],\n",
      "        [    0.0473],\n",
      "        [    0.0393],\n",
      "        [    0.0334],\n",
      "        [    0.0364],\n",
      "        [    0.0408],\n",
      "        [    0.0498],\n",
      "        [    0.0423],\n",
      "        [    0.0426],\n",
      "        [    0.0358],\n",
      "        [    0.0449],\n",
      "        [    0.0468],\n",
      "        [    0.0443],\n",
      "        [    0.0364],\n",
      "        [    0.0459],\n",
      "        [    0.0461],\n",
      "        [    0.0474],\n",
      "        [    0.0517],\n",
      "        [    0.0471],\n",
      "        [    0.0566],\n",
      "        [    0.0508],\n",
      "        [    0.0489],\n",
      "        [    0.0565],\n",
      "        [    0.0495],\n",
      "        [    0.0493],\n",
      "        [    0.0580],\n",
      "        [    0.0511],\n",
      "        [    0.0539],\n",
      "        [    0.0554],\n",
      "        [    0.0501],\n",
      "        [    0.0473],\n",
      "        [    0.0609],\n",
      "        [    0.0642],\n",
      "        [    0.0577],\n",
      "        [    0.0661],\n",
      "        [    0.0576],\n",
      "        [    0.0597],\n",
      "        [    0.0607],\n",
      "        [    0.0606],\n",
      "        [    0.0687],\n",
      "        [    0.0688],\n",
      "        [    0.0616],\n",
      "        [    0.0638],\n",
      "        [    0.0696],\n",
      "        [    0.0729],\n",
      "        [    0.0671],\n",
      "        [    0.0674],\n",
      "        [    0.0767],\n",
      "        [    0.0732],\n",
      "        [    0.0808],\n",
      "        [    0.0741],\n",
      "        [    0.0763],\n",
      "        [    0.0772],\n",
      "        [    0.0857],\n",
      "        [    0.0868],\n",
      "        [    0.0803],\n",
      "        [    0.0900],\n",
      "        [    0.0966],\n",
      "        [    0.1024],\n",
      "        [    0.1047],\n",
      "        [    0.1092],\n",
      "        [    0.1106],\n",
      "        [    0.1156],\n",
      "        [    0.1145],\n",
      "        [    0.1097],\n",
      "        [    0.1319],\n",
      "        [    0.1357],\n",
      "        [    0.1307],\n",
      "        [    0.1668],\n",
      "        [    0.1739],\n",
      "        [    0.1831],\n",
      "        [    0.2873]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 66.34953808784485\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 20 個區塊累積花費時間(s) 1.229712724685669\n",
      "<<The performance of 20 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.229712724685669\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1097.64\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1512.76\n",
      "The accuracy(2000) for l = 1: 87.42%\n",
      "The accuracy(3000) for l = 1: 95.60%\n",
      "The maximum error: tensor(7334.9375)\n",
      "The minimum error: tensor(1.4375)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 968.2\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1101.0\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1573.41015625\n",
      "The minimum error: 154.58984375\n",
      "------------------------------------------------------------\n",
      "0.8742138364779874\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<21>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.179380314577429e-09, 69)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [69, 129, 22, 5, 63, 139, 58, 102, 131, 105, 17, 57, 68, 11, 34, 10, 76, 74, 106, 56, 114, 130, 155, 20, 50, 55, 70, 19, 101, 79, 51, 18, 62, 71, 59, 145, 118, 31, 144, 75, 21, 128, 138, 149, 100, 16, 42, 30, 136, 49, 61, 67, 54, 85, 44, 32, 23, 154, 122, 153, 66, 40, 64, 151, 9, 150, 24, 135, 113, 48, 60, 115, 12, 137, 33, 148, 77, 35, 4, 156, 117, 65, 140, 89, 52, 134, 47, 43, 146, 88, 72, 143, 73, 3, 112, 103, 84, 95, 132, 94, 152, 120, 2, 6, 123, 157, 133, 141, 116, 13, 39, 78, 147, 41, 91, 87, 29, 15, 90, 107, 25, 110, 142, 111, 53, 121, 158, 104, 14, 7, 127, 86, 8, 27, 45, 28, 99, 93, 119, 46, 92, 26, 126, 108, 38, 109, 96, 80, 97, 1, 98, 83, 124, 0, 81]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0009],\n",
      "        [    0.0013],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0023],\n",
      "        [    0.0030],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0043],\n",
      "        [    0.0045],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0061],\n",
      "        [    0.0066],\n",
      "        [    0.0073],\n",
      "        [    0.0074],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0099],\n",
      "        [    0.0102],\n",
      "        [    0.0103],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0120],\n",
      "        [    0.0126],\n",
      "        [    0.0129],\n",
      "        [    0.0135],\n",
      "        [    0.0136],\n",
      "        [    0.0141],\n",
      "        [    0.0145],\n",
      "        [    0.0151],\n",
      "        [    0.0159],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0167],\n",
      "        [    0.0180],\n",
      "        [    0.0180],\n",
      "        [    0.0186],\n",
      "        [    0.0188],\n",
      "        [    0.0191],\n",
      "        [    0.0200],\n",
      "        [    0.0200],\n",
      "        [    0.0213],\n",
      "        [    0.0214],\n",
      "        [    0.0226],\n",
      "        [    0.0229],\n",
      "        [    0.0235],\n",
      "        [    0.0240],\n",
      "        [    0.0242],\n",
      "        [    0.0247],\n",
      "        [    0.0249],\n",
      "        [    0.0252],\n",
      "        [    0.0252],\n",
      "        [    0.0255],\n",
      "        [    0.0259],\n",
      "        [    0.0270],\n",
      "        [    0.0274],\n",
      "        [    0.0278],\n",
      "        [    0.0282],\n",
      "        [    0.0292],\n",
      "        [    0.0300],\n",
      "        [    0.0301],\n",
      "        [    0.0303],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0329],\n",
      "        [    0.0334],\n",
      "        [    0.0358],\n",
      "        [    0.0359],\n",
      "        [    0.0361],\n",
      "        [    0.0364],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0381],\n",
      "        [    0.0385],\n",
      "        [    0.0387],\n",
      "        [    0.0389],\n",
      "        [    0.0390],\n",
      "        [    0.0393],\n",
      "        [    0.0400],\n",
      "        [    0.0408],\n",
      "        [    0.0410],\n",
      "        [    0.0411],\n",
      "        [    0.0423],\n",
      "        [    0.0426],\n",
      "        [    0.0443],\n",
      "        [    0.0449],\n",
      "        [    0.0459],\n",
      "        [    0.0461],\n",
      "        [    0.0468],\n",
      "        [    0.0471],\n",
      "        [    0.0473],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0482],\n",
      "        [    0.0489],\n",
      "        [    0.0493],\n",
      "        [    0.0495],\n",
      "        [    0.0498],\n",
      "        [    0.0501],\n",
      "        [    0.0508],\n",
      "        [    0.0511],\n",
      "        [    0.0517],\n",
      "        [    0.0539],\n",
      "        [    0.0554],\n",
      "        [    0.0565],\n",
      "        [    0.0566],\n",
      "        [    0.0576],\n",
      "        [    0.0577],\n",
      "        [    0.0580],\n",
      "        [    0.0597],\n",
      "        [    0.0606],\n",
      "        [    0.0607],\n",
      "        [    0.0609],\n",
      "        [    0.0616],\n",
      "        [    0.0616],\n",
      "        [    0.0638],\n",
      "        [    0.0642],\n",
      "        [    0.0661],\n",
      "        [    0.0671],\n",
      "        [    0.0674],\n",
      "        [    0.0687],\n",
      "        [    0.0688],\n",
      "        [    0.0696],\n",
      "        [    0.0729],\n",
      "        [    0.0732],\n",
      "        [    0.0741],\n",
      "        [    0.0763],\n",
      "        [    0.0767],\n",
      "        [    0.0772],\n",
      "        [    0.0808],\n",
      "        [    0.0857],\n",
      "        [    0.0868],\n",
      "        [    0.0900],\n",
      "        [    0.0966],\n",
      "        [    0.1024],\n",
      "        [    0.1047],\n",
      "        [    0.1092],\n",
      "        [    0.1097],\n",
      "        [    0.1106],\n",
      "        [    0.1145],\n",
      "        [    0.1156],\n",
      "        [    0.1307],\n",
      "        [    0.1319]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.179380314577429e-09, 69)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [69, 129, 22, 5, 63, 139, 58, 102, 131, 105, 17, 57, 68, 11, 34, 10, 76, 74, 106, 56, 114, 130, 155, 20, 50, 55, 70, 19, 101, 79, 51, 18, 62, 71, 59, 145, 118, 31, 144, 75, 21, 128, 138, 149, 100, 16, 42, 30, 136, 49, 61, 67, 54, 85, 44, 32, 23, 154, 122, 153, 66, 40, 64, 151, 9, 150, 24, 135, 113, 48, 60, 115, 12, 137, 33, 148, 77, 35, 4, 156, 117, 65, 140, 89, 52, 134, 47, 43, 146, 88, 72, 143, 73, 3, 112, 103, 84, 95, 132, 94, 152, 120, 2, 6, 123, 157, 133, 141, 116, 13, 39, 78, 147, 41, 91, 87, 29, 15, 90, 107, 25, 110, 142, 111, 53, 121, 158, 104, 14, 7, 127, 86, 8, 27, 45, 28, 99, 93, 119, 46, 92, 26, 126, 108, 38, 109, 96, 80, 97, 1, 98, 83, 124, 0, 81, 125] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5405],\n",
      "        [0.0699],\n",
      "        [0.6419],\n",
      "        [0.6602],\n",
      "        [0.5916],\n",
      "        [0.0699],\n",
      "        [0.6084],\n",
      "        [0.3157],\n",
      "        [0.0699],\n",
      "        [0.2692],\n",
      "        [0.6815],\n",
      "        [0.6309],\n",
      "        [0.5287],\n",
      "        [0.6944],\n",
      "        [0.5923],\n",
      "        [0.6998],\n",
      "        [0.4704],\n",
      "        [0.4749],\n",
      "        [0.2579],\n",
      "        [0.6378],\n",
      "        [0.2280],\n",
      "        [0.0699],\n",
      "        [0.0699],\n",
      "        [0.6419],\n",
      "        [0.5734],\n",
      "        [0.6142],\n",
      "        [0.5288],\n",
      "        [0.6679],\n",
      "        [0.3461],\n",
      "        [0.4305],\n",
      "        [0.6203],\n",
      "        [0.6874],\n",
      "        [0.5954],\n",
      "        [0.5221],\n",
      "        [0.6045],\n",
      "        [0.0699],\n",
      "        [0.1762],\n",
      "        [0.6319],\n",
      "        [0.0699],\n",
      "        [0.4732],\n",
      "        [0.6329],\n",
      "        [0.0699],\n",
      "        [0.0699],\n",
      "        [0.0699],\n",
      "        [0.3483],\n",
      "        [0.6783],\n",
      "        [0.5335],\n",
      "        [0.6235],\n",
      "        [0.0699],\n",
      "        [0.5826],\n",
      "        [0.6087],\n",
      "        [0.5679],\n",
      "        [0.6230],\n",
      "        [0.2696],\n",
      "        [0.5637],\n",
      "        [0.6137],\n",
      "        [0.6362],\n",
      "        [0.0699],\n",
      "        [0.1380],\n",
      "        [0.0806],\n",
      "        [0.5805],\n",
      "        [0.4864],\n",
      "        [0.5841],\n",
      "        [0.0699],\n",
      "        [0.6862],\n",
      "        [0.0699],\n",
      "        [0.6319],\n",
      "        [0.0699],\n",
      "        [0.2195],\n",
      "        [0.5825],\n",
      "        [0.6147],\n",
      "        [0.2246],\n",
      "        [0.6756],\n",
      "        [0.0699],\n",
      "        [0.6037],\n",
      "        [0.0699],\n",
      "        [0.4390],\n",
      "        [0.6003],\n",
      "        [0.6593],\n",
      "        [0.0699],\n",
      "        [0.1952],\n",
      "        [0.5975],\n",
      "        [0.0699],\n",
      "        [0.2698],\n",
      "        [0.6387],\n",
      "        [0.0699],\n",
      "        [0.5800],\n",
      "        [0.5716],\n",
      "        [0.0699],\n",
      "        [0.2685],\n",
      "        [0.4930],\n",
      "        [0.0699],\n",
      "        [0.4820],\n",
      "        [0.6143],\n",
      "        [0.2263],\n",
      "        [0.2689],\n",
      "        [0.3118],\n",
      "        [0.3462],\n",
      "        [0.0699],\n",
      "        [0.3173],\n",
      "        [0.0947],\n",
      "        [0.1450],\n",
      "        [0.6808],\n",
      "        [0.6694],\n",
      "        [0.1655],\n",
      "        [0.0699],\n",
      "        [0.0699],\n",
      "        [0.0699],\n",
      "        [0.2471],\n",
      "        [0.6663],\n",
      "        [0.5454],\n",
      "        [0.4137],\n",
      "        [0.0699],\n",
      "        [0.4748],\n",
      "        [0.3041],\n",
      "        [0.2638],\n",
      "        [0.6155],\n",
      "        [0.6484],\n",
      "        [0.3003],\n",
      "        [0.2778],\n",
      "        [0.6215],\n",
      "        [0.2474],\n",
      "        [0.0699],\n",
      "        [0.2415],\n",
      "        [0.6108],\n",
      "        [0.1187],\n",
      "        [0.0699],\n",
      "        [0.2615],\n",
      "        [0.6435],\n",
      "        [0.6694],\n",
      "        [0.0999],\n",
      "        [0.2353],\n",
      "        [0.6655],\n",
      "        [0.6219],\n",
      "        [0.5542],\n",
      "        [0.6164],\n",
      "        [0.3257],\n",
      "        [0.2845],\n",
      "        [0.1415],\n",
      "        [0.5404],\n",
      "        [0.2815],\n",
      "        [0.6158],\n",
      "        [0.1127],\n",
      "        [0.2777],\n",
      "        [0.5469],\n",
      "        [0.2594],\n",
      "        [0.3487],\n",
      "        [0.4310],\n",
      "        [0.3430],\n",
      "        [0.7240],\n",
      "        [0.3259],\n",
      "        [0.3795],\n",
      "        [0.1469],\n",
      "        [0.7521],\n",
      "        [0.4187],\n",
      "        [0.1357]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0009],\n",
      "        [    0.0013],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0018],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0023],\n",
      "        [    0.0030],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0043],\n",
      "        [    0.0045],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0061],\n",
      "        [    0.0066],\n",
      "        [    0.0073],\n",
      "        [    0.0074],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0099],\n",
      "        [    0.0102],\n",
      "        [    0.0103],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0120],\n",
      "        [    0.0126],\n",
      "        [    0.0129],\n",
      "        [    0.0135],\n",
      "        [    0.0136],\n",
      "        [    0.0141],\n",
      "        [    0.0145],\n",
      "        [    0.0151],\n",
      "        [    0.0159],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0167],\n",
      "        [    0.0180],\n",
      "        [    0.0180],\n",
      "        [    0.0186],\n",
      "        [    0.0188],\n",
      "        [    0.0191],\n",
      "        [    0.0200],\n",
      "        [    0.0200],\n",
      "        [    0.0213],\n",
      "        [    0.0214],\n",
      "        [    0.0226],\n",
      "        [    0.0229],\n",
      "        [    0.0235],\n",
      "        [    0.0240],\n",
      "        [    0.0242],\n",
      "        [    0.0247],\n",
      "        [    0.0249],\n",
      "        [    0.0252],\n",
      "        [    0.0252],\n",
      "        [    0.0255],\n",
      "        [    0.0259],\n",
      "        [    0.0270],\n",
      "        [    0.0274],\n",
      "        [    0.0278],\n",
      "        [    0.0282],\n",
      "        [    0.0292],\n",
      "        [    0.0300],\n",
      "        [    0.0301],\n",
      "        [    0.0303],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0329],\n",
      "        [    0.0334],\n",
      "        [    0.0358],\n",
      "        [    0.0359],\n",
      "        [    0.0361],\n",
      "        [    0.0364],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0381],\n",
      "        [    0.0385],\n",
      "        [    0.0387],\n",
      "        [    0.0389],\n",
      "        [    0.0390],\n",
      "        [    0.0393],\n",
      "        [    0.0400],\n",
      "        [    0.0408],\n",
      "        [    0.0410],\n",
      "        [    0.0411],\n",
      "        [    0.0423],\n",
      "        [    0.0426],\n",
      "        [    0.0443],\n",
      "        [    0.0449],\n",
      "        [    0.0459],\n",
      "        [    0.0461],\n",
      "        [    0.0468],\n",
      "        [    0.0471],\n",
      "        [    0.0473],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0482],\n",
      "        [    0.0489],\n",
      "        [    0.0493],\n",
      "        [    0.0495],\n",
      "        [    0.0498],\n",
      "        [    0.0501],\n",
      "        [    0.0508],\n",
      "        [    0.0511],\n",
      "        [    0.0517],\n",
      "        [    0.0539],\n",
      "        [    0.0554],\n",
      "        [    0.0565],\n",
      "        [    0.0566],\n",
      "        [    0.0576],\n",
      "        [    0.0577],\n",
      "        [    0.0580],\n",
      "        [    0.0597],\n",
      "        [    0.0606],\n",
      "        [    0.0607],\n",
      "        [    0.0609],\n",
      "        [    0.0616],\n",
      "        [    0.0616],\n",
      "        [    0.0638],\n",
      "        [    0.0642],\n",
      "        [    0.0661],\n",
      "        [    0.0671],\n",
      "        [    0.0674],\n",
      "        [    0.0687],\n",
      "        [    0.0688],\n",
      "        [    0.0696],\n",
      "        [    0.0729],\n",
      "        [    0.0732],\n",
      "        [    0.0741],\n",
      "        [    0.0763],\n",
      "        [    0.0767],\n",
      "        [    0.0772],\n",
      "        [    0.0808],\n",
      "        [    0.0857],\n",
      "        [    0.0868],\n",
      "        [    0.0900],\n",
      "        [    0.0966],\n",
      "        [    0.1024],\n",
      "        [    0.1047],\n",
      "        [    0.1092],\n",
      "        [    0.1097],\n",
      "        [    0.1106],\n",
      "        [    0.1145],\n",
      "        [    0.1156],\n",
      "        [    0.1307],\n",
      "        [    0.1319],\n",
      "        [    0.1357]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0089],\n",
      "        [0.0033],\n",
      "        [0.0143],\n",
      "        [0.0131],\n",
      "        [0.0111],\n",
      "        [0.0013],\n",
      "        [0.0118],\n",
      "        [0.0059],\n",
      "        [0.0042],\n",
      "        [0.0009],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0109],\n",
      "        [0.0176],\n",
      "        [0.0141],\n",
      "        [0.0112],\n",
      "        [0.0033],\n",
      "        [0.0108],\n",
      "        [0.0020],\n",
      "        [0.0160],\n",
      "        [0.0045],\n",
      "        [0.0031],\n",
      "        [0.0086],\n",
      "        [0.0200],\n",
      "        [0.0021],\n",
      "        [0.0034],\n",
      "        [0.0007],\n",
      "        [0.0223],\n",
      "        [0.0028],\n",
      "        [0.0039],\n",
      "        [0.0006],\n",
      "        [0.0252],\n",
      "        [0.0015],\n",
      "        [0.0037],\n",
      "        [0.0017],\n",
      "        [0.0100],\n",
      "        [0.0142],\n",
      "        [0.0012],\n",
      "        [0.0110],\n",
      "        [0.0072],\n",
      "        [0.0013],\n",
      "        [0.0177],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0106],\n",
      "        [0.0022],\n",
      "        [0.0089],\n",
      "        [0.0061],\n",
      "        [0.0212],\n",
      "        [0.0288],\n",
      "        [0.0084],\n",
      "        [0.0296],\n",
      "        [0.0089],\n",
      "        [0.0194],\n",
      "        [0.0115],\n",
      "        [0.0110],\n",
      "        [0.0096],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0188],\n",
      "        [0.0347],\n",
      "        [0.0171],\n",
      "        [0.0354],\n",
      "        [0.0278],\n",
      "        [0.0110],\n",
      "        [0.0284],\n",
      "        [0.0139],\n",
      "        [0.0300],\n",
      "        [0.0287],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0290],\n",
      "        [0.0159],\n",
      "        [0.0328],\n",
      "        [0.0212],\n",
      "        [0.0301],\n",
      "        [0.0266],\n",
      "        [0.0444],\n",
      "        [0.0496],\n",
      "        [0.0333],\n",
      "        [0.0357],\n",
      "        [0.0470],\n",
      "        [0.0343],\n",
      "        [0.0341],\n",
      "        [0.0266],\n",
      "        [0.0411],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0364],\n",
      "        [0.0363],\n",
      "        [0.0327],\n",
      "        [0.0382],\n",
      "        [0.0340],\n",
      "        [0.0287],\n",
      "        [0.0431],\n",
      "        [0.0399],\n",
      "        [0.0470],\n",
      "        [0.0396],\n",
      "        [0.0485],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0492],\n",
      "        [0.0622],\n",
      "        [0.0328],\n",
      "        [0.0459],\n",
      "        [0.0456],\n",
      "        [0.0515],\n",
      "        [0.0468],\n",
      "        [0.0510],\n",
      "        [0.0358],\n",
      "        [0.0596],\n",
      "        [0.0454],\n",
      "        [0.0486],\n",
      "        [0.0443],\n",
      "        [0.0498],\n",
      "        [0.0528],\n",
      "        [0.0448],\n",
      "        [0.0434],\n",
      "        [0.0536],\n",
      "        [0.0604],\n",
      "        [0.0454],\n",
      "        [0.0614],\n",
      "        [0.0580],\n",
      "        [0.0620],\n",
      "        [0.0503],\n",
      "        [0.0645],\n",
      "        [0.0591],\n",
      "        [0.0613],\n",
      "        [0.0512],\n",
      "        [0.0520],\n",
      "        [0.0633],\n",
      "        [0.0658],\n",
      "        [0.0549],\n",
      "        [0.0564],\n",
      "        [0.0599],\n",
      "        [0.0608],\n",
      "        [0.0684],\n",
      "        [0.0708],\n",
      "        [0.0790],\n",
      "        [0.0678],\n",
      "        [0.0740],\n",
      "        [0.0685],\n",
      "        [0.0824],\n",
      "        [0.0895],\n",
      "        [0.0996],\n",
      "        [0.0987],\n",
      "        [0.0969],\n",
      "        [0.1108],\n",
      "        [0.1039],\n",
      "        [0.1260],\n",
      "        [0.1059],\n",
      "        [0.1193],\n",
      "        [0.1134],\n",
      "        [0.1480],\n",
      "        [0.1381],\n",
      "        [0.1331]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 66.87691926956177\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.178035967721371e-07, 51)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [51, 70, 105, 31, 139, 21, 62, 59, 106, 50, 16, 101, 130, 129, 76, 55, 71, 79, 131, 114, 102, 30, 75, 61, 155, 42, 54, 69, 23, 145, 100, 74, 68, 144, 32, 9, 63, 10, 44, 58, 17, 5, 57, 24, 34, 118, 22, 12, 56, 40, 11, 128, 48, 60, 138, 149, 153, 85, 20, 33, 136, 19, 18, 154, 122, 52, 77, 151, 150, 47, 43, 113, 3, 49, 115, 67, 135, 148, 72, 6, 137, 156, 73, 89, 140, 66, 64, 117, 13, 88, 146, 143, 95, 103, 134, 152, 94, 112, 15, 41, 35, 29, 78, 25, 157, 123, 141, 65, 84, 132, 147, 120, 4, 91, 53, 116, 14, 133, 7, 87, 90, 8, 27, 142, 158, 39, 45, 107, 28, 104, 110, 111, 2, 127, 121, 86, 46, 99, 26, 93, 92, 119, 126, 108, 96, 109, 38, 97, 98, 80, 124, 83, 1, 125, 81, 0, 82] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6311],\n",
      "        [0.5373],\n",
      "        [0.2719],\n",
      "        [0.6442],\n",
      "        [0.0724],\n",
      "        [0.6461],\n",
      "        [0.6058],\n",
      "        [0.6149],\n",
      "        [0.2602],\n",
      "        [0.5828],\n",
      "        [0.6929],\n",
      "        [0.3515],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.4773],\n",
      "        [0.6250],\n",
      "        [0.5303],\n",
      "        [0.4365],\n",
      "        [0.0724],\n",
      "        [0.2291],\n",
      "        [0.3201],\n",
      "        [0.6354],\n",
      "        [0.4800],\n",
      "        [0.6194],\n",
      "        [0.0724],\n",
      "        [0.5425],\n",
      "        [0.6340],\n",
      "        [0.5495],\n",
      "        [0.6495],\n",
      "        [0.0724],\n",
      "        [0.3539],\n",
      "        [0.4817],\n",
      "        [0.5373],\n",
      "        [0.0724],\n",
      "        [0.6253],\n",
      "        [0.7007],\n",
      "        [0.6018],\n",
      "        [0.7146],\n",
      "        [0.5737],\n",
      "        [0.6188],\n",
      "        [0.6962],\n",
      "        [0.6742],\n",
      "        [0.6420],\n",
      "        [0.6451],\n",
      "        [0.6029],\n",
      "        [0.1749],\n",
      "        [0.6555],\n",
      "        [0.6898],\n",
      "        [0.6493],\n",
      "        [0.4942],\n",
      "        [0.7090],\n",
      "        [0.0724],\n",
      "        [0.5926],\n",
      "        [0.6256],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.0752],\n",
      "        [0.2715],\n",
      "        [0.6553],\n",
      "        [0.6148],\n",
      "        [0.0724],\n",
      "        [0.6821],\n",
      "        [0.7023],\n",
      "        [0.0724],\n",
      "        [0.1358],\n",
      "        [0.6502],\n",
      "        [0.4453],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.5901],\n",
      "        [0.5818],\n",
      "        [0.2204],\n",
      "        [0.6266],\n",
      "        [0.5925],\n",
      "        [0.2255],\n",
      "        [0.5775],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.5002],\n",
      "        [0.6840],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.4890],\n",
      "        [0.2729],\n",
      "        [0.0724],\n",
      "        [0.5906],\n",
      "        [0.5943],\n",
      "        [0.1948],\n",
      "        [0.6802],\n",
      "        [0.2715],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.3515],\n",
      "        [0.2716],\n",
      "        [0.0724],\n",
      "        [0.0896],\n",
      "        [0.3217],\n",
      "        [0.2272],\n",
      "        [0.6616],\n",
      "        [0.4823],\n",
      "        [0.6113],\n",
      "        [0.6272],\n",
      "        [0.4192],\n",
      "        [0.6340],\n",
      "        [0.0724],\n",
      "        [0.1640],\n",
      "        [0.0724],\n",
      "        [0.6081],\n",
      "        [0.3145],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.1429],\n",
      "        [0.6731],\n",
      "        [0.3082],\n",
      "        [0.6214],\n",
      "        [0.2485],\n",
      "        [0.6566],\n",
      "        [0.0724],\n",
      "        [0.6835],\n",
      "        [0.2665],\n",
      "        [0.3043],\n",
      "        [0.6794],\n",
      "        [0.6343],\n",
      "        [0.0724],\n",
      "        [0.0724],\n",
      "        [0.5550],\n",
      "        [0.5639],\n",
      "        [0.2805],\n",
      "        [0.6284],\n",
      "        [0.2640],\n",
      "        [0.2490],\n",
      "        [0.2429],\n",
      "        [0.6957],\n",
      "        [0.0962],\n",
      "        [0.1157],\n",
      "        [0.2369],\n",
      "        [0.5493],\n",
      "        [0.3306],\n",
      "        [0.6281],\n",
      "        [0.2878],\n",
      "        [0.2847],\n",
      "        [0.1389],\n",
      "        [0.1095],\n",
      "        [0.2803],\n",
      "        [0.3541],\n",
      "        [0.2614],\n",
      "        [0.5564],\n",
      "        [0.3483],\n",
      "        [0.3306],\n",
      "        [0.4371],\n",
      "        [0.1447],\n",
      "        [0.3843],\n",
      "        [0.7403],\n",
      "        [0.1331],\n",
      "        [0.4249],\n",
      "        [0.7694],\n",
      "        [0.3994]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0007],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0013],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0028],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0045],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0072],\n",
      "        [0.0084],\n",
      "        [0.0086],\n",
      "        [0.0089],\n",
      "        [0.0089],\n",
      "        [0.0089],\n",
      "        [0.0096],\n",
      "        [0.0100],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0110],\n",
      "        [0.0111],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0131],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0141],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0171],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0188],\n",
      "        [0.0194],\n",
      "        [0.0200],\n",
      "        [0.0212],\n",
      "        [0.0212],\n",
      "        [0.0223],\n",
      "        [0.0252],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0278],\n",
      "        [0.0284],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0288],\n",
      "        [0.0290],\n",
      "        [0.0296],\n",
      "        [0.0300],\n",
      "        [0.0301],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0347],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0363],\n",
      "        [0.0364],\n",
      "        [0.0382],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0431],\n",
      "        [0.0434],\n",
      "        [0.0443],\n",
      "        [0.0444],\n",
      "        [0.0448],\n",
      "        [0.0454],\n",
      "        [0.0454],\n",
      "        [0.0456],\n",
      "        [0.0459],\n",
      "        [0.0468],\n",
      "        [0.0470],\n",
      "        [0.0470],\n",
      "        [0.0485],\n",
      "        [0.0486],\n",
      "        [0.0492],\n",
      "        [0.0496],\n",
      "        [0.0498],\n",
      "        [0.0503],\n",
      "        [0.0510],\n",
      "        [0.0512],\n",
      "        [0.0515],\n",
      "        [0.0520],\n",
      "        [0.0528],\n",
      "        [0.0536],\n",
      "        [0.0549],\n",
      "        [0.0564],\n",
      "        [0.0580],\n",
      "        [0.0591],\n",
      "        [0.0596],\n",
      "        [0.0599],\n",
      "        [0.0604],\n",
      "        [0.0608],\n",
      "        [0.0613],\n",
      "        [0.0614],\n",
      "        [0.0620],\n",
      "        [0.0622],\n",
      "        [0.0633],\n",
      "        [0.0645],\n",
      "        [0.0658],\n",
      "        [0.0678],\n",
      "        [0.0684],\n",
      "        [0.0685],\n",
      "        [0.0708],\n",
      "        [0.0740],\n",
      "        [0.0790],\n",
      "        [0.0824],\n",
      "        [0.0895],\n",
      "        [0.0969],\n",
      "        [0.0987],\n",
      "        [0.0996],\n",
      "        [0.1039],\n",
      "        [0.1059],\n",
      "        [0.1108],\n",
      "        [0.1134],\n",
      "        [0.1193],\n",
      "        [0.1260],\n",
      "        [0.1331],\n",
      "        [0.1381],\n",
      "        [0.1480],\n",
      "        [0.1721]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0025],\n",
      "        [    0.0008],\n",
      "        [    0.0021],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0012],\n",
      "        [    0.0009],\n",
      "        [    0.0021],\n",
      "        [    0.0019],\n",
      "        [    0.0045],\n",
      "        [    0.0017],\n",
      "        [    0.0032],\n",
      "        [    0.0047],\n",
      "        [    0.0057],\n",
      "        [    0.0054],\n",
      "        [    0.0050],\n",
      "        [    0.0059],\n",
      "        [    0.0052],\n",
      "        [    0.0089],\n",
      "        [    0.0088],\n",
      "        [    0.0099],\n",
      "        [    0.0087],\n",
      "        [    0.0090],\n",
      "        [    0.0080],\n",
      "        [    0.0076],\n",
      "        [    0.0088],\n",
      "        [    0.0097],\n",
      "        [    0.0091],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0104],\n",
      "        [    0.0079],\n",
      "        [    0.0105],\n",
      "        [    0.0143],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0157],\n",
      "        [    0.0162],\n",
      "        [    0.0131],\n",
      "        [    0.0119],\n",
      "        [    0.0142],\n",
      "        [    0.0161],\n",
      "        [    0.0166],\n",
      "        [    0.0132],\n",
      "        [    0.0162],\n",
      "        [    0.0179],\n",
      "        [    0.0205],\n",
      "        [    0.0189],\n",
      "        [    0.0184],\n",
      "        [    0.0188],\n",
      "        [    0.0197],\n",
      "        [    0.0197],\n",
      "        [    0.0173],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0208],\n",
      "        [    0.0225],\n",
      "        [    0.0249],\n",
      "        [    0.0282],\n",
      "        [    0.0273],\n",
      "        [    0.0288],\n",
      "        [    0.0265],\n",
      "        [    0.0283],\n",
      "        [    0.0291],\n",
      "        [    0.0297],\n",
      "        [    0.0287],\n",
      "        [    0.0279],\n",
      "        [    0.0279],\n",
      "        [    0.0271],\n",
      "        [    0.0283],\n",
      "        [    0.0297],\n",
      "        [    0.0287],\n",
      "        [    0.0312],\n",
      "        [    0.0288],\n",
      "        [    0.0341],\n",
      "        [    0.0291],\n",
      "        [    0.0341],\n",
      "        [    0.0321],\n",
      "        [    0.0356],\n",
      "        [    0.0346],\n",
      "        [    0.0331],\n",
      "        [    0.0340],\n",
      "        [    0.0347],\n",
      "        [    0.0343],\n",
      "        [    0.0335],\n",
      "        [    0.0371],\n",
      "        [    0.0352],\n",
      "        [    0.0370],\n",
      "        [    0.0386],\n",
      "        [    0.0410],\n",
      "        [    0.0423],\n",
      "        [    0.0374],\n",
      "        [    0.0414],\n",
      "        [    0.0421],\n",
      "        [    0.0413],\n",
      "        [    0.0449],\n",
      "        [    0.0447],\n",
      "        [    0.0438],\n",
      "        [    0.0474],\n",
      "        [    0.0437],\n",
      "        [    0.0444],\n",
      "        [    0.0438],\n",
      "        [    0.0455],\n",
      "        [    0.0465],\n",
      "        [    0.0444],\n",
      "        [    0.0497],\n",
      "        [    0.0473],\n",
      "        [    0.0516],\n",
      "        [    0.0522],\n",
      "        [    0.0498],\n",
      "        [    0.0507],\n",
      "        [    0.0507],\n",
      "        [    0.0494],\n",
      "        [    0.0527],\n",
      "        [    0.0490],\n",
      "        [    0.0540],\n",
      "        [    0.0538],\n",
      "        [    0.0520],\n",
      "        [    0.0547],\n",
      "        [    0.0568],\n",
      "        [    0.0578],\n",
      "        [    0.0597],\n",
      "        [    0.0595],\n",
      "        [    0.0597],\n",
      "        [    0.0594],\n",
      "        [    0.0624],\n",
      "        [    0.0604],\n",
      "        [    0.0611],\n",
      "        [    0.0652],\n",
      "        [    0.0600],\n",
      "        [    0.0674],\n",
      "        [    0.0678],\n",
      "        [    0.0683],\n",
      "        [    0.0678],\n",
      "        [    0.0669],\n",
      "        [    0.0712],\n",
      "        [    0.0744],\n",
      "        [    0.0817],\n",
      "        [    0.0792],\n",
      "        [    0.0888],\n",
      "        [    0.0961],\n",
      "        [    0.0978],\n",
      "        [    0.0995],\n",
      "        [    0.1032],\n",
      "        [    0.1054],\n",
      "        [    0.1091],\n",
      "        [    0.1110],\n",
      "        [    0.1175],\n",
      "        [    0.1296],\n",
      "        [    0.1303],\n",
      "        [    0.1368],\n",
      "        [    0.1522],\n",
      "        [    0.1703]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 67.16853499412537\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.994852614321644e-09, 105)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [105, 31, 51, 70, 21, 16, 50, 76, 130, 101, 62, 59, 139, 106, 55, 129, 71, 114, 30, 131, 79, 102, 23, 9, 69, 42, 145, 61, 75, 54, 74, 68, 100, 144, 155, 32, 63, 44, 58, 24, 57, 12, 34, 10, 17, 118, 5, 56, 22, 153, 40, 48, 60, 128, 138, 149, 11, 33, 85, 20, 136, 19, 52, 3, 154, 43, 113, 18, 77, 49, 47, 67, 122, 148, 151, 6, 150, 115, 135, 156, 140, 13, 66, 137, 72, 117, 89, 64, 146, 73, 143, 88, 152, 95, 103, 15, 94, 112, 134, 25, 29, 123, 157, 84, 35, 41, 141, 65, 147, 78, 7, 14, 132, 91, 116, 53, 120, 8, 4, 133, 90, 87, 27, 142, 158, 28, 45, 39, 107, 127, 110, 111, 104, 2, 26, 121, 99, 86, 46, 93, 92, 126, 119, 108, 96, 109, 38, 97, 98, 80, 124, 83, 1, 125, 81, 0, 82, 37] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2711],\n",
      "        [0.6452],\n",
      "        [0.6309],\n",
      "        [0.5362],\n",
      "        [0.6482],\n",
      "        [0.6959],\n",
      "        [0.5819],\n",
      "        [0.4757],\n",
      "        [0.0737],\n",
      "        [0.3522],\n",
      "        [0.6051],\n",
      "        [0.6141],\n",
      "        [0.0737],\n",
      "        [0.2594],\n",
      "        [0.6248],\n",
      "        [0.0737],\n",
      "        [0.5292],\n",
      "        [0.2286],\n",
      "        [0.6363],\n",
      "        [0.0737],\n",
      "        [0.4348],\n",
      "        [0.3201],\n",
      "        [0.6516],\n",
      "        [0.7038],\n",
      "        [0.5486],\n",
      "        [0.5427],\n",
      "        [0.0737],\n",
      "        [0.6189],\n",
      "        [0.4783],\n",
      "        [0.6340],\n",
      "        [0.4800],\n",
      "        [0.5360],\n",
      "        [0.3548],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.6260],\n",
      "        [0.6012],\n",
      "        [0.5743],\n",
      "        [0.6181],\n",
      "        [0.6471],\n",
      "        [0.6418],\n",
      "        [0.6924],\n",
      "        [0.6031],\n",
      "        [0.7176],\n",
      "        [0.6992],\n",
      "        [0.1730],\n",
      "        [0.6772],\n",
      "        [0.6495],\n",
      "        [0.6578],\n",
      "        [0.0737],\n",
      "        [0.4935],\n",
      "        [0.5924],\n",
      "        [0.6251],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.7119],\n",
      "        [0.6152],\n",
      "        [0.2689],\n",
      "        [0.6575],\n",
      "        [0.0737],\n",
      "        [0.6846],\n",
      "        [0.6502],\n",
      "        [0.6282],\n",
      "        [0.0737],\n",
      "        [0.5826],\n",
      "        [0.2196],\n",
      "        [0.7054],\n",
      "        [0.4436],\n",
      "        [0.5921],\n",
      "        [0.5900],\n",
      "        [0.5766],\n",
      "        [0.1332],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.6876],\n",
      "        [0.0737],\n",
      "        [0.2249],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.6826],\n",
      "        [0.5899],\n",
      "        [0.0737],\n",
      "        [0.4988],\n",
      "        [0.1934],\n",
      "        [0.2725],\n",
      "        [0.5937],\n",
      "        [0.0737],\n",
      "        [0.4875],\n",
      "        [0.0737],\n",
      "        [0.2707],\n",
      "        [0.0853],\n",
      "        [0.3525],\n",
      "        [0.2705],\n",
      "        [0.6637],\n",
      "        [0.3220],\n",
      "        [0.2261],\n",
      "        [0.0737],\n",
      "        [0.6357],\n",
      "        [0.6282],\n",
      "        [0.1619],\n",
      "        [0.0737],\n",
      "        [0.3118],\n",
      "        [0.6116],\n",
      "        [0.4816],\n",
      "        [0.0737],\n",
      "        [0.6076],\n",
      "        [0.0737],\n",
      "        [0.4171],\n",
      "        [0.6865],\n",
      "        [0.6584],\n",
      "        [0.0737],\n",
      "        [0.3082],\n",
      "        [0.2482],\n",
      "        [0.6210],\n",
      "        [0.1405],\n",
      "        [0.6822],\n",
      "        [0.6758],\n",
      "        [0.0737],\n",
      "        [0.3041],\n",
      "        [0.2653],\n",
      "        [0.6361],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.6299],\n",
      "        [0.5643],\n",
      "        [0.5550],\n",
      "        [0.2799],\n",
      "        [0.0928],\n",
      "        [0.2480],\n",
      "        [0.2419],\n",
      "        [0.2628],\n",
      "        [0.6987],\n",
      "        [0.6297],\n",
      "        [0.1129],\n",
      "        [0.3311],\n",
      "        [0.2348],\n",
      "        [0.5488],\n",
      "        [0.2873],\n",
      "        [0.2843],\n",
      "        [0.1063],\n",
      "        [0.1361],\n",
      "        [0.2797],\n",
      "        [0.3549],\n",
      "        [0.2606],\n",
      "        [0.5564],\n",
      "        [0.3489],\n",
      "        [0.3311],\n",
      "        [0.4355],\n",
      "        [0.1423],\n",
      "        [0.3824],\n",
      "        [0.7440],\n",
      "        [0.1303],\n",
      "        [0.4236],\n",
      "        [0.7735],\n",
      "        [0.3977],\n",
      "        [0.6085]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0004],\n",
      "        [    0.0008],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0025],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0032],\n",
      "        [    0.0045],\n",
      "        [    0.0047],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0054],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0076],\n",
      "        [    0.0079],\n",
      "        [    0.0080],\n",
      "        [    0.0087],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0089],\n",
      "        [    0.0090],\n",
      "        [    0.0091],\n",
      "        [    0.0096],\n",
      "        [    0.0097],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0104],\n",
      "        [    0.0105],\n",
      "        [    0.0108],\n",
      "        [    0.0111],\n",
      "        [    0.0119],\n",
      "        [    0.0131],\n",
      "        [    0.0132],\n",
      "        [    0.0142],\n",
      "        [    0.0143],\n",
      "        [    0.0157],\n",
      "        [    0.0161],\n",
      "        [    0.0162],\n",
      "        [    0.0162],\n",
      "        [    0.0166],\n",
      "        [    0.0173],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0197],\n",
      "        [    0.0197],\n",
      "        [    0.0205],\n",
      "        [    0.0208],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0225],\n",
      "        [    0.0249],\n",
      "        [    0.0265],\n",
      "        [    0.0271],\n",
      "        [    0.0273],\n",
      "        [    0.0279],\n",
      "        [    0.0279],\n",
      "        [    0.0282],\n",
      "        [    0.0283],\n",
      "        [    0.0283],\n",
      "        [    0.0287],\n",
      "        [    0.0287],\n",
      "        [    0.0288],\n",
      "        [    0.0288],\n",
      "        [    0.0291],\n",
      "        [    0.0291],\n",
      "        [    0.0297],\n",
      "        [    0.0297],\n",
      "        [    0.0312],\n",
      "        [    0.0321],\n",
      "        [    0.0331],\n",
      "        [    0.0335],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0346],\n",
      "        [    0.0347],\n",
      "        [    0.0352],\n",
      "        [    0.0356],\n",
      "        [    0.0370],\n",
      "        [    0.0371],\n",
      "        [    0.0374],\n",
      "        [    0.0386],\n",
      "        [    0.0410],\n",
      "        [    0.0413],\n",
      "        [    0.0414],\n",
      "        [    0.0421],\n",
      "        [    0.0423],\n",
      "        [    0.0437],\n",
      "        [    0.0438],\n",
      "        [    0.0438],\n",
      "        [    0.0444],\n",
      "        [    0.0444],\n",
      "        [    0.0447],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0465],\n",
      "        [    0.0473],\n",
      "        [    0.0474],\n",
      "        [    0.0490],\n",
      "        [    0.0494],\n",
      "        [    0.0497],\n",
      "        [    0.0498],\n",
      "        [    0.0507],\n",
      "        [    0.0507],\n",
      "        [    0.0516],\n",
      "        [    0.0520],\n",
      "        [    0.0522],\n",
      "        [    0.0527],\n",
      "        [    0.0538],\n",
      "        [    0.0540],\n",
      "        [    0.0547],\n",
      "        [    0.0568],\n",
      "        [    0.0578],\n",
      "        [    0.0594],\n",
      "        [    0.0595],\n",
      "        [    0.0597],\n",
      "        [    0.0597],\n",
      "        [    0.0600],\n",
      "        [    0.0604],\n",
      "        [    0.0611],\n",
      "        [    0.0624],\n",
      "        [    0.0652],\n",
      "        [    0.0669],\n",
      "        [    0.0674],\n",
      "        [    0.0678],\n",
      "        [    0.0678],\n",
      "        [    0.0683],\n",
      "        [    0.0712],\n",
      "        [    0.0744],\n",
      "        [    0.0792],\n",
      "        [    0.0817],\n",
      "        [    0.0888],\n",
      "        [    0.0961],\n",
      "        [    0.0978],\n",
      "        [    0.0995],\n",
      "        [    0.1032],\n",
      "        [    0.1054],\n",
      "        [    0.1091],\n",
      "        [    0.1110],\n",
      "        [    0.1175],\n",
      "        [    0.1296],\n",
      "        [    0.1303],\n",
      "        [    0.1368],\n",
      "        [    0.1522],\n",
      "        [    0.1703],\n",
      "        [    0.1848]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0045],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0024],\n",
      "        [0.0019],\n",
      "        [0.0033],\n",
      "        [0.0022],\n",
      "        [0.0018],\n",
      "        [0.0025],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0026],\n",
      "        [0.0038],\n",
      "        [0.0009],\n",
      "        [0.0046],\n",
      "        [0.0086],\n",
      "        [0.0055],\n",
      "        [0.0096],\n",
      "        [0.0054],\n",
      "        [0.0091],\n",
      "        [0.0053],\n",
      "        [0.0109],\n",
      "        [0.0108],\n",
      "        [0.0043],\n",
      "        [0.0122],\n",
      "        [0.0087],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0050],\n",
      "        [0.0058],\n",
      "        [0.0099],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0148],\n",
      "        [0.0064],\n",
      "        [0.0141],\n",
      "        [0.0066],\n",
      "        [0.0152],\n",
      "        [0.0088],\n",
      "        [0.0164],\n",
      "        [0.0095],\n",
      "        [0.0112],\n",
      "        [0.0127],\n",
      "        [0.0174],\n",
      "        [0.0136],\n",
      "        [0.0121],\n",
      "        [0.0134],\n",
      "        [0.0173],\n",
      "        [0.0219],\n",
      "        [0.0223],\n",
      "        [0.0231],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0173],\n",
      "        [0.0254],\n",
      "        [0.0247],\n",
      "        [0.0187],\n",
      "        [0.0225],\n",
      "        [0.0216],\n",
      "        [0.0307],\n",
      "        [0.0302],\n",
      "        [0.0274],\n",
      "        [0.0311],\n",
      "        [0.0274],\n",
      "        [0.0251],\n",
      "        [0.0319],\n",
      "        [0.0243],\n",
      "        [0.0326],\n",
      "        [0.0248],\n",
      "        [0.0304],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0314],\n",
      "        [0.0297],\n",
      "        [0.0303],\n",
      "        [0.0313],\n",
      "        [0.0320],\n",
      "        [0.0331],\n",
      "        [0.0367],\n",
      "        [0.0301],\n",
      "        [0.0341],\n",
      "        [0.0381],\n",
      "        [0.0334],\n",
      "        [0.0350],\n",
      "        [0.0308],\n",
      "        [0.0351],\n",
      "        [0.0395],\n",
      "        [0.0369],\n",
      "        [0.0378],\n",
      "        [0.0349],\n",
      "        [0.0387],\n",
      "        [0.0419],\n",
      "        [0.0446],\n",
      "        [0.0417],\n",
      "        [0.0411],\n",
      "        [0.0423],\n",
      "        [0.0472],\n",
      "        [0.0480],\n",
      "        [0.0424],\n",
      "        [0.0443],\n",
      "        [0.0412],\n",
      "        [0.0402],\n",
      "        [0.0486],\n",
      "        [0.0455],\n",
      "        [0.0426],\n",
      "        [0.0473],\n",
      "        [0.0511],\n",
      "        [0.0518],\n",
      "        [0.0528],\n",
      "        [0.0498],\n",
      "        [0.0502],\n",
      "        [0.0500],\n",
      "        [0.0550],\n",
      "        [0.0530],\n",
      "        [0.0549],\n",
      "        [0.0493],\n",
      "        [0.0528],\n",
      "        [0.0541],\n",
      "        [0.0552],\n",
      "        [0.0582],\n",
      "        [0.0568],\n",
      "        [0.0578],\n",
      "        [0.0632],\n",
      "        [0.0630],\n",
      "        [0.0555],\n",
      "        [0.0588],\n",
      "        [0.0581],\n",
      "        [0.0594],\n",
      "        [0.0601],\n",
      "        [0.0634],\n",
      "        [0.0623],\n",
      "        [0.0705],\n",
      "        [0.0689],\n",
      "        [0.0682],\n",
      "        [0.0696],\n",
      "        [0.0723],\n",
      "        [0.0719],\n",
      "        [0.0751],\n",
      "        [0.0773],\n",
      "        [0.0834],\n",
      "        [0.0879],\n",
      "        [0.0963],\n",
      "        [0.0968],\n",
      "        [0.0952],\n",
      "        [0.1036],\n",
      "        [0.1058],\n",
      "        [0.1059],\n",
      "        [0.1094],\n",
      "        [0.1144],\n",
      "        [0.1268],\n",
      "        [0.1284],\n",
      "        [0.1341],\n",
      "        [0.1495],\n",
      "        [0.1673],\n",
      "        [0.1801]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 67.4594931602478\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.676393008499872e-07, 105)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [105, 55, 130, 16, 76, 21, 101, 139, 50, 106, 51, 70, 69, 31, 129, 74, 102, 131, 114, 68, 62, 63, 58, 59, 71, 145, 57, 79, 34, 30, 144, 155, 100, 9, 23, 10, 56, 42, 17, 61, 75, 54, 22, 5, 44, 32, 24, 12, 11, 153, 118, 20, 128, 138, 149, 19, 40, 48, 136, 60, 49, 85, 67, 18, 33, 113, 154, 148, 151, 150, 66, 3, 115, 122, 52, 64, 43, 135, 6, 77, 156, 47, 140, 117, 137, 152, 89, 146, 13, 143, 88, 72, 95, 73, 35, 112, 84, 94, 103, 134, 123, 65, 157, 15, 141, 25, 147, 29, 41, 4, 132, 116, 91, 78, 7, 133, 14, 120, 90, 8, 53, 87, 39, 142, 158, 127, 27, 107, 110, 111, 2, 45, 28, 104, 99, 121, 86, 26, 93, 46, 92, 126, 119, 108, 38, 96, 109, 97, 98, 80, 124, 83, 1, 125, 81, 0, 82, 37, 36] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2700],\n",
      "        [0.6206],\n",
      "        [0.0737],\n",
      "        [0.6931],\n",
      "        [0.4718],\n",
      "        [0.6450],\n",
      "        [0.3518],\n",
      "        [0.0737],\n",
      "        [0.5774],\n",
      "        [0.2584],\n",
      "        [0.6264],\n",
      "        [0.5324],\n",
      "        [0.5449],\n",
      "        [0.6409],\n",
      "        [0.0737],\n",
      "        [0.4759],\n",
      "        [0.3195],\n",
      "        [0.0737],\n",
      "        [0.2282],\n",
      "        [0.5323],\n",
      "        [0.6011],\n",
      "        [0.5971],\n",
      "        [0.6135],\n",
      "        [0.6096],\n",
      "        [0.5254],\n",
      "        [0.0737],\n",
      "        [0.6374],\n",
      "        [0.4313],\n",
      "        [0.5983],\n",
      "        [0.6320],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.3546],\n",
      "        [0.7009],\n",
      "        [0.6482],\n",
      "        [0.7146],\n",
      "        [0.6454],\n",
      "        [0.5392],\n",
      "        [0.6963],\n",
      "        [0.6148],\n",
      "        [0.4743],\n",
      "        [0.6298],\n",
      "        [0.6545],\n",
      "        [0.6746],\n",
      "        [0.5710],\n",
      "        [0.6216],\n",
      "        [0.6438],\n",
      "        [0.6892],\n",
      "        [0.7087],\n",
      "        [0.0737],\n",
      "        [0.1718],\n",
      "        [0.6541],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.6813],\n",
      "        [0.4894],\n",
      "        [0.5884],\n",
      "        [0.0737],\n",
      "        [0.6209],\n",
      "        [0.5880],\n",
      "        [0.2662],\n",
      "        [0.5728],\n",
      "        [0.7022],\n",
      "        [0.6106],\n",
      "        [0.2191],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.5859],\n",
      "        [0.6251],\n",
      "        [0.2243],\n",
      "        [0.1316],\n",
      "        [0.6460],\n",
      "        [0.5898],\n",
      "        [0.5794],\n",
      "        [0.0737],\n",
      "        [0.6854],\n",
      "        [0.4400],\n",
      "        [0.0737],\n",
      "        [0.5861],\n",
      "        [0.0737],\n",
      "        [0.1925],\n",
      "        [0.0737],\n",
      "        [0.0828],\n",
      "        [0.2721],\n",
      "        [0.0737],\n",
      "        [0.6794],\n",
      "        [0.0737],\n",
      "        [0.2700],\n",
      "        [0.4949],\n",
      "        [0.3524],\n",
      "        [0.4835],\n",
      "        [0.6071],\n",
      "        [0.2252],\n",
      "        [0.3087],\n",
      "        [0.3217],\n",
      "        [0.2695],\n",
      "        [0.0737],\n",
      "        [0.1605],\n",
      "        [0.6036],\n",
      "        [0.0737],\n",
      "        [0.6604],\n",
      "        [0.0737],\n",
      "        [0.6322],\n",
      "        [0.0737],\n",
      "        [0.6240],\n",
      "        [0.4779],\n",
      "        [0.6728],\n",
      "        [0.0737],\n",
      "        [0.2476],\n",
      "        [0.3079],\n",
      "        [0.4135],\n",
      "        [0.6837],\n",
      "        [0.0737],\n",
      "        [0.6549],\n",
      "        [0.1391],\n",
      "        [0.3038],\n",
      "        [0.6794],\n",
      "        [0.6167],\n",
      "        [0.2640],\n",
      "        [0.5509],\n",
      "        [0.0737],\n",
      "        [0.0737],\n",
      "        [0.0910],\n",
      "        [0.6325],\n",
      "        [0.2789],\n",
      "        [0.2471],\n",
      "        [0.2409],\n",
      "        [0.6958],\n",
      "        [0.5609],\n",
      "        [0.6261],\n",
      "        [0.2618],\n",
      "        [0.3307],\n",
      "        [0.1113],\n",
      "        [0.2331],\n",
      "        [0.6261],\n",
      "        [0.2867],\n",
      "        [0.5448],\n",
      "        [0.2836],\n",
      "        [0.1044],\n",
      "        [0.1344],\n",
      "        [0.2788],\n",
      "        [0.5521],\n",
      "        [0.3547],\n",
      "        [0.2596],\n",
      "        [0.3485],\n",
      "        [0.3307],\n",
      "        [0.4322],\n",
      "        [0.1407],\n",
      "        [0.3793],\n",
      "        [0.7412],\n",
      "        [0.1284],\n",
      "        [0.4208],\n",
      "        [0.7708],\n",
      "        [0.3947],\n",
      "        [0.6038],\n",
      "        [0.6104]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0009],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0033],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0046],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0121],\n",
      "        [0.0122],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0136],\n",
      "        [0.0141],\n",
      "        [0.0148],\n",
      "        [0.0152],\n",
      "        [0.0164],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0174],\n",
      "        [0.0187],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0216],\n",
      "        [0.0219],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0231],\n",
      "        [0.0243],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0274],\n",
      "        [0.0274],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0301],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0304],\n",
      "        [0.0307],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.0313],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0320],\n",
      "        [0.0326],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0341],\n",
      "        [0.0349],\n",
      "        [0.0350],\n",
      "        [0.0351],\n",
      "        [0.0367],\n",
      "        [0.0369],\n",
      "        [0.0378],\n",
      "        [0.0381],\n",
      "        [0.0387],\n",
      "        [0.0395],\n",
      "        [0.0402],\n",
      "        [0.0411],\n",
      "        [0.0412],\n",
      "        [0.0417],\n",
      "        [0.0419],\n",
      "        [0.0423],\n",
      "        [0.0424],\n",
      "        [0.0426],\n",
      "        [0.0443],\n",
      "        [0.0446],\n",
      "        [0.0455],\n",
      "        [0.0472],\n",
      "        [0.0473],\n",
      "        [0.0480],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0498],\n",
      "        [0.0500],\n",
      "        [0.0502],\n",
      "        [0.0511],\n",
      "        [0.0518],\n",
      "        [0.0528],\n",
      "        [0.0528],\n",
      "        [0.0530],\n",
      "        [0.0541],\n",
      "        [0.0549],\n",
      "        [0.0550],\n",
      "        [0.0552],\n",
      "        [0.0555],\n",
      "        [0.0568],\n",
      "        [0.0578],\n",
      "        [0.0581],\n",
      "        [0.0582],\n",
      "        [0.0588],\n",
      "        [0.0594],\n",
      "        [0.0601],\n",
      "        [0.0623],\n",
      "        [0.0630],\n",
      "        [0.0632],\n",
      "        [0.0634],\n",
      "        [0.0682],\n",
      "        [0.0689],\n",
      "        [0.0696],\n",
      "        [0.0705],\n",
      "        [0.0719],\n",
      "        [0.0723],\n",
      "        [0.0751],\n",
      "        [0.0773],\n",
      "        [0.0834],\n",
      "        [0.0879],\n",
      "        [0.0952],\n",
      "        [0.0963],\n",
      "        [0.0968],\n",
      "        [0.1036],\n",
      "        [0.1058],\n",
      "        [0.1059],\n",
      "        [0.1094],\n",
      "        [0.1144],\n",
      "        [0.1268],\n",
      "        [0.1284],\n",
      "        [0.1341],\n",
      "        [0.1495],\n",
      "        [0.1673],\n",
      "        [0.1801],\n",
      "        [0.1899]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0036],\n",
      "        [0.0018],\n",
      "        [0.0039],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0018],\n",
      "        [0.0026],\n",
      "        [0.0064],\n",
      "        [0.0037],\n",
      "        [0.0072],\n",
      "        [0.0066],\n",
      "        [0.0022],\n",
      "        [0.0081],\n",
      "        [0.0046],\n",
      "        [0.0021],\n",
      "        [0.0059],\n",
      "        [0.0055],\n",
      "        [0.0049],\n",
      "        [0.0037],\n",
      "        [0.0086],\n",
      "        [0.0038],\n",
      "        [0.0035],\n",
      "        [0.0099],\n",
      "        [0.0110],\n",
      "        [0.0087],\n",
      "        [0.0060],\n",
      "        [0.0113],\n",
      "        [0.0054],\n",
      "        [0.0132],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0091],\n",
      "        [0.0127],\n",
      "        [0.0134],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0146],\n",
      "        [0.0106],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0109],\n",
      "        [0.0119],\n",
      "        [0.0161],\n",
      "        [0.0184],\n",
      "        [0.0176],\n",
      "        [0.0188],\n",
      "        [0.0150],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0161],\n",
      "        [0.0190],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0190],\n",
      "        [0.0251],\n",
      "        [0.0248],\n",
      "        [0.0225],\n",
      "        [0.0258],\n",
      "        [0.0217],\n",
      "        [0.0264],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0292],\n",
      "        [0.0279],\n",
      "        [0.0274],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0298],\n",
      "        [0.0276],\n",
      "        [0.0322],\n",
      "        [0.0299],\n",
      "        [0.0311],\n",
      "        [0.0335],\n",
      "        [0.0284],\n",
      "        [0.0331],\n",
      "        [0.0313],\n",
      "        [0.0327],\n",
      "        [0.0342],\n",
      "        [0.0320],\n",
      "        [0.0352],\n",
      "        [0.0330],\n",
      "        [0.0334],\n",
      "        [0.0342],\n",
      "        [0.0333],\n",
      "        [0.0340],\n",
      "        [0.0351],\n",
      "        [0.0389],\n",
      "        [0.0369],\n",
      "        [0.0373],\n",
      "        [0.0407],\n",
      "        [0.0375],\n",
      "        [0.0422],\n",
      "        [0.0365],\n",
      "        [0.0411],\n",
      "        [0.0392],\n",
      "        [0.0408],\n",
      "        [0.0417],\n",
      "        [0.0424],\n",
      "        [0.0418],\n",
      "        [0.0401],\n",
      "        [0.0443],\n",
      "        [0.0469],\n",
      "        [0.0454],\n",
      "        [0.0499],\n",
      "        [0.0472],\n",
      "        [0.0515],\n",
      "        [0.0514],\n",
      "        [0.0474],\n",
      "        [0.0498],\n",
      "        [0.0503],\n",
      "        [0.0492],\n",
      "        [0.0535],\n",
      "        [0.0537],\n",
      "        [0.0528],\n",
      "        [0.0553],\n",
      "        [0.0536],\n",
      "        [0.0531],\n",
      "        [0.0568],\n",
      "        [0.0578],\n",
      "        [0.0553],\n",
      "        [0.0522],\n",
      "        [0.0567],\n",
      "        [0.0577],\n",
      "        [0.0571],\n",
      "        [0.0610],\n",
      "        [0.0589],\n",
      "        [0.0595],\n",
      "        [0.0601],\n",
      "        [0.0605],\n",
      "        [0.0652],\n",
      "        [0.0663],\n",
      "        [0.0633],\n",
      "        [0.0676],\n",
      "        [0.0696],\n",
      "        [0.0702],\n",
      "        [0.0732],\n",
      "        [0.0713],\n",
      "        [0.0750],\n",
      "        [0.0746],\n",
      "        [0.0764],\n",
      "        [0.0843],\n",
      "        [0.0881],\n",
      "        [0.0916],\n",
      "        [0.0954],\n",
      "        [0.0969],\n",
      "        [0.1029],\n",
      "        [0.1052],\n",
      "        [0.1040],\n",
      "        [0.1087],\n",
      "        [0.1124],\n",
      "        [0.1251],\n",
      "        [0.1275],\n",
      "        [0.1326],\n",
      "        [0.1478],\n",
      "        [0.1656],\n",
      "        [0.1762],\n",
      "        [0.1862]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 67.74973821640015\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 21 個區塊累積花費時間(s) 1.2314679622650146\n",
      "<<The performance of 21 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2314679622650146\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1032.00\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1371.69\n",
      "The accuracy(2000) for l = 1: 89.31%\n",
      "The accuracy(3000) for l = 1: 95.60%\n",
      "The maximum error: tensor(4754.6523)\n",
      "The minimum error: tensor(22.6875)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1556.1\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1582.3\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1956.07421875\n",
      "The minimum error: 1148.07421875\n",
      "------------------------------------------------------------\n",
      "0.8930817610062893\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<22>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.897440355009167e-07, 101)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [101, 126, 97, 70, 65, 135, 54, 51, 64, 102, 59, 12, 125, 72, 17, 110, 30, 127, 98, 53, 46, 66, 47, 27, 58, 141, 96, 6, 52, 140, 55, 151, 13, 18, 67, 75, 1, 5, 26, 19, 38, 7, 57, 71, 50, 40, 16, 149, 20, 114, 28, 8, 15, 124, 134, 145, 45, 132, 63, 14, 44, 36, 56, 81, 150, 62, 109, 60, 144, 147, 29, 146, 111, 118, 131, 152, 2, 136, 39, 148, 113, 48, 85, 133, 73, 142, 43, 31, 139, 84, 91, 9, 80, 61, 68, 90, 108, 99, 119, 69, 130, 153, 158, 137, 11, 143, 0, 87, 128, 21, 112, 37, 25, 35, 129, 86, 74, 116, 3, 10, 83, 138, 4, 123, 154, 49, 103, 106, 157, 107, 23, 156, 100, 41, 24, 95, 117, 82, 89, 22, 88, 42, 122, 155, 115, 104, 34, 92, 105, 93, 76, 94, 120, 79, 121]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0018],\n",
      "        [0.0018],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0026],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0046],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0072],\n",
      "        [0.0081],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0127],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0176],\n",
      "        [0.0184],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0217],\n",
      "        [0.0225],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0248],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0264],\n",
      "        [0.0274],\n",
      "        [0.0276],\n",
      "        [0.0279],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0311],\n",
      "        [0.0313],\n",
      "        [0.0320],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0331],\n",
      "        [0.0333],\n",
      "        [0.0334],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0351],\n",
      "        [0.0352],\n",
      "        [0.0365],\n",
      "        [0.0369],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0392],\n",
      "        [0.0401],\n",
      "        [0.0407],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0418],\n",
      "        [0.0422],\n",
      "        [0.0424],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0469],\n",
      "        [0.0472],\n",
      "        [0.0474],\n",
      "        [0.0492],\n",
      "        [0.0498],\n",
      "        [0.0499],\n",
      "        [0.0503],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0528],\n",
      "        [0.0531],\n",
      "        [0.0535],\n",
      "        [0.0536],\n",
      "        [0.0537],\n",
      "        [0.0553],\n",
      "        [0.0553],\n",
      "        [0.0567],\n",
      "        [0.0568],\n",
      "        [0.0571],\n",
      "        [0.0577],\n",
      "        [0.0578],\n",
      "        [0.0589],\n",
      "        [0.0595],\n",
      "        [0.0596],\n",
      "        [0.0601],\n",
      "        [0.0610],\n",
      "        [0.0626],\n",
      "        [0.0633],\n",
      "        [0.0652],\n",
      "        [0.0663],\n",
      "        [0.0676],\n",
      "        [0.0696],\n",
      "        [0.0702],\n",
      "        [0.0713],\n",
      "        [0.0732],\n",
      "        [0.0746],\n",
      "        [0.0750],\n",
      "        [0.0764],\n",
      "        [0.0766],\n",
      "        [0.0843],\n",
      "        [0.0881],\n",
      "        [0.0916],\n",
      "        [0.0954],\n",
      "        [0.0969],\n",
      "        [0.1029],\n",
      "        [0.1040],\n",
      "        [0.1052],\n",
      "        [0.1087],\n",
      "        [0.1124],\n",
      "        [0.1275]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.897440355009167e-07, 101)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [101, 126, 97, 70, 65, 135, 54, 51, 64, 102, 59, 12, 125, 72, 17, 110, 30, 127, 98, 53, 46, 66, 47, 27, 58, 141, 96, 6, 52, 140, 55, 151, 13, 18, 67, 75, 1, 5, 26, 19, 38, 7, 57, 71, 50, 40, 16, 149, 20, 114, 28, 8, 15, 124, 134, 145, 45, 132, 63, 14, 44, 36, 56, 81, 150, 62, 109, 60, 144, 147, 29, 146, 111, 118, 131, 152, 2, 136, 39, 148, 113, 48, 85, 133, 73, 142, 43, 31, 139, 84, 91, 9, 80, 61, 68, 90, 108, 99, 119, 69, 130, 153, 158, 137, 11, 143, 0, 87, 128, 21, 112, 37, 25, 35, 129, 86, 74, 116, 3, 10, 83, 138, 4, 123, 154, 49, 103, 106, 157, 107, 23, 156, 100, 41, 24, 95, 117, 82, 89, 22, 88, 42, 122, 155, 115, 104, 34, 92, 105, 93, 76, 94, 120, 79, 121, 77] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2701],\n",
      "        [0.0738],\n",
      "        [0.3525],\n",
      "        [0.4730],\n",
      "        [0.5427],\n",
      "        [0.0738],\n",
      "        [0.6105],\n",
      "        [0.6180],\n",
      "        [0.5301],\n",
      "        [0.2585],\n",
      "        [0.5945],\n",
      "        [0.6911],\n",
      "        [0.0738],\n",
      "        [0.4691],\n",
      "        [0.6426],\n",
      "        [0.2287],\n",
      "        [0.5943],\n",
      "        [0.0738],\n",
      "        [0.3201],\n",
      "        [0.6346],\n",
      "        [0.5743],\n",
      "        [0.5300],\n",
      "        [0.6234],\n",
      "        [0.6373],\n",
      "        [0.5986],\n",
      "        [0.0738],\n",
      "        [0.3554],\n",
      "        [0.7126],\n",
      "        [0.6428],\n",
      "        [0.0738],\n",
      "        [0.6067],\n",
      "        [0.0738],\n",
      "        [0.6941],\n",
      "        [0.6521],\n",
      "        [0.5230],\n",
      "        [0.4292],\n",
      "        [0.6730],\n",
      "        [0.6990],\n",
      "        [0.6284],\n",
      "        [0.6458],\n",
      "        [0.5368],\n",
      "        [0.7064],\n",
      "        [0.6123],\n",
      "        [0.4714],\n",
      "        [0.6271],\n",
      "        [0.5690],\n",
      "        [0.6515],\n",
      "        [0.0738],\n",
      "        [0.6414],\n",
      "        [0.1715],\n",
      "        [0.6180],\n",
      "        [0.6869],\n",
      "        [0.6788],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.5855],\n",
      "        [0.0738],\n",
      "        [0.5705],\n",
      "        [0.7000],\n",
      "        [0.5859],\n",
      "        [0.4863],\n",
      "        [0.6182],\n",
      "        [0.2646],\n",
      "        [0.0738],\n",
      "        [0.5835],\n",
      "        [0.2195],\n",
      "        [0.5874],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.6068],\n",
      "        [0.0738],\n",
      "        [0.2246],\n",
      "        [0.1309],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.6841],\n",
      "        [0.0738],\n",
      "        [0.5774],\n",
      "        [0.0812],\n",
      "        [0.1925],\n",
      "        [0.6433],\n",
      "        [0.2731],\n",
      "        [0.0738],\n",
      "        [0.4377],\n",
      "        [0.0738],\n",
      "        [0.5835],\n",
      "        [0.6034],\n",
      "        [0.0738],\n",
      "        [0.2705],\n",
      "        [0.3536],\n",
      "        [0.6772],\n",
      "        [0.3067],\n",
      "        [0.6012],\n",
      "        [0.4922],\n",
      "        [0.3226],\n",
      "        [0.2252],\n",
      "        [0.2697],\n",
      "        [0.1599],\n",
      "        [0.4809],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.0738],\n",
      "        [0.6581],\n",
      "        [0.0738],\n",
      "        [0.6709],\n",
      "        [0.3088],\n",
      "        [0.0738],\n",
      "        [0.6295],\n",
      "        [0.2478],\n",
      "        [0.4751],\n",
      "        [0.6205],\n",
      "        [0.5475],\n",
      "        [0.0738],\n",
      "        [0.3048],\n",
      "        [0.4111],\n",
      "        [0.1385],\n",
      "        [0.6817],\n",
      "        [0.6525],\n",
      "        [0.2640],\n",
      "        [0.0738],\n",
      "        [0.6774],\n",
      "        [0.0899],\n",
      "        [0.0738],\n",
      "        [0.6139],\n",
      "        [0.2791],\n",
      "        [0.2472],\n",
      "        [0.0738],\n",
      "        [0.2409],\n",
      "        [0.6297],\n",
      "        [0.0738],\n",
      "        [0.2620],\n",
      "        [0.5586],\n",
      "        [0.6230],\n",
      "        [0.3314],\n",
      "        [0.1106],\n",
      "        [0.2325],\n",
      "        [0.2873],\n",
      "        [0.6234],\n",
      "        [0.2842],\n",
      "        [0.5421],\n",
      "        [0.1034],\n",
      "        [0.0738],\n",
      "        [0.1335],\n",
      "        [0.2790],\n",
      "        [0.5485],\n",
      "        [0.3557],\n",
      "        [0.2597],\n",
      "        [0.3493],\n",
      "        [0.4304],\n",
      "        [0.3313],\n",
      "        [0.1400],\n",
      "        [0.3774],\n",
      "        [0.1275],\n",
      "        [0.4194]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0018],\n",
      "        [0.0018],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0026],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0039],\n",
      "        [0.0046],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0055],\n",
      "        [0.0059],\n",
      "        [0.0060],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0072],\n",
      "        [0.0081],\n",
      "        [0.0086],\n",
      "        [0.0087],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0099],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0110],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0127],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0146],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0161],\n",
      "        [0.0161],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0176],\n",
      "        [0.0184],\n",
      "        [0.0188],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0198],\n",
      "        [0.0198],\n",
      "        [0.0217],\n",
      "        [0.0225],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0248],\n",
      "        [0.0251],\n",
      "        [0.0258],\n",
      "        [0.0264],\n",
      "        [0.0274],\n",
      "        [0.0276],\n",
      "        [0.0279],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0291],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0311],\n",
      "        [0.0313],\n",
      "        [0.0320],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0331],\n",
      "        [0.0333],\n",
      "        [0.0334],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0351],\n",
      "        [0.0352],\n",
      "        [0.0365],\n",
      "        [0.0369],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0392],\n",
      "        [0.0401],\n",
      "        [0.0407],\n",
      "        [0.0408],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0418],\n",
      "        [0.0422],\n",
      "        [0.0424],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0469],\n",
      "        [0.0472],\n",
      "        [0.0474],\n",
      "        [0.0492],\n",
      "        [0.0498],\n",
      "        [0.0499],\n",
      "        [0.0503],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0528],\n",
      "        [0.0531],\n",
      "        [0.0535],\n",
      "        [0.0536],\n",
      "        [0.0537],\n",
      "        [0.0553],\n",
      "        [0.0553],\n",
      "        [0.0567],\n",
      "        [0.0568],\n",
      "        [0.0571],\n",
      "        [0.0577],\n",
      "        [0.0578],\n",
      "        [0.0589],\n",
      "        [0.0595],\n",
      "        [0.0596],\n",
      "        [0.0601],\n",
      "        [0.0610],\n",
      "        [0.0626],\n",
      "        [0.0633],\n",
      "        [0.0652],\n",
      "        [0.0663],\n",
      "        [0.0676],\n",
      "        [0.0696],\n",
      "        [0.0702],\n",
      "        [0.0713],\n",
      "        [0.0732],\n",
      "        [0.0746],\n",
      "        [0.0750],\n",
      "        [0.0764],\n",
      "        [0.0766],\n",
      "        [0.0843],\n",
      "        [0.0881],\n",
      "        [0.0916],\n",
      "        [0.0954],\n",
      "        [0.0969],\n",
      "        [0.1029],\n",
      "        [0.1040],\n",
      "        [0.1052],\n",
      "        [0.1087],\n",
      "        [0.1124],\n",
      "        [0.1275],\n",
      "        [0.1326]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0026],\n",
      "        [0.0037],\n",
      "        [0.0051],\n",
      "        [0.0078],\n",
      "        [0.0100],\n",
      "        [0.0081],\n",
      "        [0.0123],\n",
      "        [0.0058],\n",
      "        [0.0109],\n",
      "        [0.0005],\n",
      "        [0.0125],\n",
      "        [0.0118],\n",
      "        [0.0101],\n",
      "        [0.0011],\n",
      "        [0.0089],\n",
      "        [0.0029],\n",
      "        [0.0155],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0156],\n",
      "        [0.0015],\n",
      "        [0.0007],\n",
      "        [0.0022],\n",
      "        [0.0038],\n",
      "        [0.0002],\n",
      "        [0.0032],\n",
      "        [0.0019],\n",
      "        [0.0251],\n",
      "        [0.0197],\n",
      "        [0.0042],\n",
      "        [0.0012],\n",
      "        [0.0154],\n",
      "        [0.0264],\n",
      "        [0.0250],\n",
      "        [0.0039],\n",
      "        [0.0062],\n",
      "        [0.0271],\n",
      "        [0.0029],\n",
      "        [0.0016],\n",
      "        [0.0002],\n",
      "        [0.0067],\n",
      "        [0.0305],\n",
      "        [0.0063],\n",
      "        [0.0099],\n",
      "        [0.0063],\n",
      "        [0.0070],\n",
      "        [0.0301],\n",
      "        [0.0229],\n",
      "        [0.0042],\n",
      "        [0.0182],\n",
      "        [0.0074],\n",
      "        [0.0036],\n",
      "        [0.0340],\n",
      "        [0.0245],\n",
      "        [0.0252],\n",
      "        [0.0253],\n",
      "        [0.0303],\n",
      "        [0.0280],\n",
      "        [0.0307],\n",
      "        [0.0387],\n",
      "        [0.0159],\n",
      "        [0.0182],\n",
      "        [0.0164],\n",
      "        [0.0249],\n",
      "        [0.0329],\n",
      "        [0.0363],\n",
      "        [0.0297],\n",
      "        [0.0371],\n",
      "        [0.0233],\n",
      "        [0.0346],\n",
      "        [0.0187],\n",
      "        [0.0352],\n",
      "        [0.0281],\n",
      "        [0.0326],\n",
      "        [0.0368],\n",
      "        [0.0265],\n",
      "        [0.0167],\n",
      "        [0.0276],\n",
      "        [0.0238],\n",
      "        [0.0313],\n",
      "        [0.0338],\n",
      "        [0.0233],\n",
      "        [0.0298],\n",
      "        [0.0396],\n",
      "        [0.0288],\n",
      "        [0.0296],\n",
      "        [0.0262],\n",
      "        [0.0468],\n",
      "        [0.0314],\n",
      "        [0.0334],\n",
      "        [0.0304],\n",
      "        [0.0242],\n",
      "        [0.0413],\n",
      "        [0.0494],\n",
      "        [0.0345],\n",
      "        [0.0352],\n",
      "        [0.0427],\n",
      "        [0.0384],\n",
      "        [0.0413],\n",
      "        [0.0363],\n",
      "        [0.0478],\n",
      "        [0.0388],\n",
      "        [0.0395],\n",
      "        [0.0400],\n",
      "        [0.0329],\n",
      "        [0.0418],\n",
      "        [0.0622],\n",
      "        [0.0441],\n",
      "        [0.0553],\n",
      "        [0.0370],\n",
      "        [0.0528],\n",
      "        [0.0449],\n",
      "        [0.0400],\n",
      "        [0.0611],\n",
      "        [0.0583],\n",
      "        [0.0483],\n",
      "        [0.0490],\n",
      "        [0.0549],\n",
      "        [0.0384],\n",
      "        [0.0416],\n",
      "        [0.0521],\n",
      "        [0.0513],\n",
      "        [0.0418],\n",
      "        [0.0538],\n",
      "        [0.0523],\n",
      "        [0.0488],\n",
      "        [0.0624],\n",
      "        [0.0618],\n",
      "        [0.0542],\n",
      "        [0.0622],\n",
      "        [0.0484],\n",
      "        [0.0571],\n",
      "        [0.0603],\n",
      "        [0.0565],\n",
      "        [0.0542],\n",
      "        [0.0612],\n",
      "        [0.0719],\n",
      "        [0.0684],\n",
      "        [0.0671],\n",
      "        [0.0607],\n",
      "        [0.0705],\n",
      "        [0.0673],\n",
      "        [0.0735],\n",
      "        [0.0712],\n",
      "        [0.0865],\n",
      "        [0.0913],\n",
      "        [0.1006],\n",
      "        [0.0882],\n",
      "        [0.0995],\n",
      "        [0.0961],\n",
      "        [0.1093],\n",
      "        [0.0989],\n",
      "        [0.1073],\n",
      "        [0.1168],\n",
      "        [0.1256],\n",
      "        [0.1382]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 68.27537965774536\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.561231227105054e-08, 58)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [58, 19, 102, 66, 72, 55, 46, 26, 96, 47, 101, 110, 5, 141, 8, 126, 27, 67, 20, 140, 97, 51, 75, 50, 57, 38, 40, 28, 70, 135, 17, 71, 65, 125, 64, 127, 98, 12, 54, 59, 151, 30, 53, 44, 56, 2, 114, 36, 29, 52, 149, 144, 48, 39, 9, 124, 81, 18, 6, 134, 145, 43, 13, 152, 1, 136, 132, 111, 73, 142, 109, 85, 16, 45, 91, 7, 63, 148, 139, 118, 150, 11, 84, 113, 15, 68, 147, 146, 90, 69, 62, 131, 21, 60, 3, 99, 14, 153, 158, 133, 137, 25, 119, 80, 10, 4, 143, 108, 87, 37, 31, 130, 86, 23, 49, 74, 61, 138, 83, 154, 112, 123, 157, 24, 116, 128, 41, 156, 129, 100, 22, 35, 95, 106, 107, 0, 103, 89, 42, 82, 88, 155, 117, 122, 115, 92, 104, 93, 94, 105, 34, 120, 76, 79, 121, 77, 78] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6074],\n",
      "        [0.6593],\n",
      "        [0.2617],\n",
      "        [0.5373],\n",
      "        [0.4750],\n",
      "        [0.6154],\n",
      "        [0.5822],\n",
      "        [0.6400],\n",
      "        [0.3626],\n",
      "        [0.6327],\n",
      "        [0.2736],\n",
      "        [0.2307],\n",
      "        [0.7146],\n",
      "        [0.0792],\n",
      "        [0.7020],\n",
      "        [0.0792],\n",
      "        [0.6492],\n",
      "        [0.5301],\n",
      "        [0.6548],\n",
      "        [0.0792],\n",
      "        [0.3594],\n",
      "        [0.6274],\n",
      "        [0.4342],\n",
      "        [0.6367],\n",
      "        [0.6215],\n",
      "        [0.5448],\n",
      "        [0.5781],\n",
      "        [0.6290],\n",
      "        [0.4788],\n",
      "        [0.0792],\n",
      "        [0.6564],\n",
      "        [0.4773],\n",
      "        [0.5506],\n",
      "        [0.0792],\n",
      "        [0.5373],\n",
      "        [0.0792],\n",
      "        [0.3254],\n",
      "        [0.7069],\n",
      "        [0.6193],\n",
      "        [0.6032],\n",
      "        [0.0792],\n",
      "        [0.6043],\n",
      "        [0.6443],\n",
      "        [0.5948],\n",
      "        [0.6275],\n",
      "        [0.7001],\n",
      "        [0.1709],\n",
      "        [0.4931],\n",
      "        [0.6173],\n",
      "        [0.6531],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.6534],\n",
      "        [0.5867],\n",
      "        [0.6919],\n",
      "        [0.0792],\n",
      "        [0.2661],\n",
      "        [0.6661],\n",
      "        [0.7284],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.5925],\n",
      "        [0.7099],\n",
      "        [0.0792],\n",
      "        [0.6881],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.2264],\n",
      "        [0.4431],\n",
      "        [0.0792],\n",
      "        [0.2213],\n",
      "        [0.2773],\n",
      "        [0.6654],\n",
      "        [0.5941],\n",
      "        [0.3607],\n",
      "        [0.7220],\n",
      "        [0.5787],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.1294],\n",
      "        [0.0792],\n",
      "        [0.6721],\n",
      "        [0.2744],\n",
      "        [0.1929],\n",
      "        [0.6937],\n",
      "        [0.4985],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.3282],\n",
      "        [0.4867],\n",
      "        [0.5921],\n",
      "        [0.0792],\n",
      "        [0.6424],\n",
      "        [0.5961],\n",
      "        [0.6971],\n",
      "        [0.2730],\n",
      "        [0.7159],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.6320],\n",
      "        [0.1593],\n",
      "        [0.3088],\n",
      "        [0.6661],\n",
      "        [0.6925],\n",
      "        [0.0792],\n",
      "        [0.2267],\n",
      "        [0.3139],\n",
      "        [0.4817],\n",
      "        [0.6137],\n",
      "        [0.0792],\n",
      "        [0.3096],\n",
      "        [0.6423],\n",
      "        [0.6229],\n",
      "        [0.4155],\n",
      "        [0.6105],\n",
      "        [0.0792],\n",
      "        [0.2672],\n",
      "        [0.0792],\n",
      "        [0.2504],\n",
      "        [0.0867],\n",
      "        [0.0792],\n",
      "        [0.6351],\n",
      "        [0.1372],\n",
      "        [0.0792],\n",
      "        [0.5674],\n",
      "        [0.0792],\n",
      "        [0.0792],\n",
      "        [0.2650],\n",
      "        [0.6359],\n",
      "        [0.5565],\n",
      "        [0.3377],\n",
      "        [0.2494],\n",
      "        [0.2430],\n",
      "        [0.6858],\n",
      "        [0.2826],\n",
      "        [0.2914],\n",
      "        [0.5498],\n",
      "        [0.2342],\n",
      "        [0.2882],\n",
      "        [0.0792],\n",
      "        [0.1083],\n",
      "        [0.1006],\n",
      "        [0.1313],\n",
      "        [0.3628],\n",
      "        [0.2822],\n",
      "        [0.3561],\n",
      "        [0.3376],\n",
      "        [0.2623],\n",
      "        [0.5575],\n",
      "        [0.1385],\n",
      "        [0.4356],\n",
      "        [0.3817],\n",
      "        [0.1256],\n",
      "        [0.4250],\n",
      "        [0.3977]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0011],\n",
      "        [    0.0012],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0019],\n",
      "        [    0.0022],\n",
      "        [    0.0026],\n",
      "        [    0.0029],\n",
      "        [    0.0029],\n",
      "        [    0.0032],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0042],\n",
      "        [    0.0042],\n",
      "        [    0.0051],\n",
      "        [    0.0058],\n",
      "        [    0.0062],\n",
      "        [    0.0063],\n",
      "        [    0.0063],\n",
      "        [    0.0067],\n",
      "        [    0.0070],\n",
      "        [    0.0074],\n",
      "        [    0.0078],\n",
      "        [    0.0081],\n",
      "        [    0.0089],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0109],\n",
      "        [    0.0109],\n",
      "        [    0.0112],\n",
      "        [    0.0118],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0154],\n",
      "        [    0.0155],\n",
      "        [    0.0156],\n",
      "        [    0.0159],\n",
      "        [    0.0164],\n",
      "        [    0.0167],\n",
      "        [    0.0182],\n",
      "        [    0.0182],\n",
      "        [    0.0187],\n",
      "        [    0.0197],\n",
      "        [    0.0229],\n",
      "        [    0.0233],\n",
      "        [    0.0233],\n",
      "        [    0.0238],\n",
      "        [    0.0242],\n",
      "        [    0.0245],\n",
      "        [    0.0249],\n",
      "        [    0.0250],\n",
      "        [    0.0251],\n",
      "        [    0.0252],\n",
      "        [    0.0253],\n",
      "        [    0.0262],\n",
      "        [    0.0264],\n",
      "        [    0.0265],\n",
      "        [    0.0271],\n",
      "        [    0.0276],\n",
      "        [    0.0280],\n",
      "        [    0.0281],\n",
      "        [    0.0288],\n",
      "        [    0.0296],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0301],\n",
      "        [    0.0303],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0307],\n",
      "        [    0.0313],\n",
      "        [    0.0314],\n",
      "        [    0.0326],\n",
      "        [    0.0329],\n",
      "        [    0.0329],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0345],\n",
      "        [    0.0346],\n",
      "        [    0.0352],\n",
      "        [    0.0352],\n",
      "        [    0.0363],\n",
      "        [    0.0363],\n",
      "        [    0.0368],\n",
      "        [    0.0370],\n",
      "        [    0.0371],\n",
      "        [    0.0384],\n",
      "        [    0.0384],\n",
      "        [    0.0387],\n",
      "        [    0.0388],\n",
      "        [    0.0395],\n",
      "        [    0.0396],\n",
      "        [    0.0400],\n",
      "        [    0.0400],\n",
      "        [    0.0413],\n",
      "        [    0.0413],\n",
      "        [    0.0416],\n",
      "        [    0.0418],\n",
      "        [    0.0418],\n",
      "        [    0.0427],\n",
      "        [    0.0441],\n",
      "        [    0.0449],\n",
      "        [    0.0468],\n",
      "        [    0.0478],\n",
      "        [    0.0483],\n",
      "        [    0.0484],\n",
      "        [    0.0488],\n",
      "        [    0.0490],\n",
      "        [    0.0494],\n",
      "        [    0.0513],\n",
      "        [    0.0521],\n",
      "        [    0.0523],\n",
      "        [    0.0528],\n",
      "        [    0.0538],\n",
      "        [    0.0542],\n",
      "        [    0.0542],\n",
      "        [    0.0549],\n",
      "        [    0.0553],\n",
      "        [    0.0565],\n",
      "        [    0.0571],\n",
      "        [    0.0583],\n",
      "        [    0.0603],\n",
      "        [    0.0607],\n",
      "        [    0.0611],\n",
      "        [    0.0612],\n",
      "        [    0.0618],\n",
      "        [    0.0622],\n",
      "        [    0.0622],\n",
      "        [    0.0624],\n",
      "        [    0.0671],\n",
      "        [    0.0673],\n",
      "        [    0.0684],\n",
      "        [    0.0705],\n",
      "        [    0.0712],\n",
      "        [    0.0719],\n",
      "        [    0.0735],\n",
      "        [    0.0865],\n",
      "        [    0.0882],\n",
      "        [    0.0913],\n",
      "        [    0.0961],\n",
      "        [    0.0989],\n",
      "        [    0.0995],\n",
      "        [    0.1006],\n",
      "        [    0.1073],\n",
      "        [    0.1093],\n",
      "        [    0.1168],\n",
      "        [    0.1256],\n",
      "        [    0.1382],\n",
      "        [    0.1703]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0028],\n",
      "        [0.0013],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0044],\n",
      "        [0.0017],\n",
      "        [0.0023],\n",
      "        [0.0010],\n",
      "        [0.0003],\n",
      "        [0.0010],\n",
      "        [0.0043],\n",
      "        [0.0060],\n",
      "        [0.0025],\n",
      "        [0.0012],\n",
      "        [0.0044],\n",
      "        [0.0033],\n",
      "        [0.0070],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0056],\n",
      "        [0.0034],\n",
      "        [0.0101],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0088],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0039],\n",
      "        [0.0088],\n",
      "        [0.0104],\n",
      "        [0.0137],\n",
      "        [0.0070],\n",
      "        [0.0108],\n",
      "        [0.0074],\n",
      "        [0.0117],\n",
      "        [0.0105],\n",
      "        [0.0149],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0162],\n",
      "        [0.0138],\n",
      "        [0.0133],\n",
      "        [0.0182],\n",
      "        [0.0191],\n",
      "        [0.0128],\n",
      "        [0.0213],\n",
      "        [0.0211],\n",
      "        [0.0202],\n",
      "        [0.0179],\n",
      "        [0.0236],\n",
      "        [0.0225],\n",
      "        [0.0253],\n",
      "        [0.0250],\n",
      "        [0.0221],\n",
      "        [0.0252],\n",
      "        [0.0295],\n",
      "        [0.0264],\n",
      "        [0.0281],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0283],\n",
      "        [0.0293],\n",
      "        [0.0258],\n",
      "        [0.0300],\n",
      "        [0.0268],\n",
      "        [0.0288],\n",
      "        [0.0298],\n",
      "        [0.0326],\n",
      "        [0.0289],\n",
      "        [0.0279],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0278],\n",
      "        [0.0294],\n",
      "        [0.0333],\n",
      "        [0.0276],\n",
      "        [0.0321],\n",
      "        [0.0307],\n",
      "        [0.0365],\n",
      "        [0.0336],\n",
      "        [0.0312],\n",
      "        [0.0350],\n",
      "        [0.0313],\n",
      "        [0.0360],\n",
      "        [0.0379],\n",
      "        [0.0354],\n",
      "        [0.0360],\n",
      "        [0.0354],\n",
      "        [0.0400],\n",
      "        [0.0334],\n",
      "        [0.0375],\n",
      "        [0.0363],\n",
      "        [0.0343],\n",
      "        [0.0354],\n",
      "        [0.0406],\n",
      "        [0.0416],\n",
      "        [0.0381],\n",
      "        [0.0388],\n",
      "        [0.0404],\n",
      "        [0.0392],\n",
      "        [0.0404],\n",
      "        [0.0381],\n",
      "        [0.0366],\n",
      "        [0.0403],\n",
      "        [0.0389],\n",
      "        [0.0410],\n",
      "        [0.0405],\n",
      "        [0.0448],\n",
      "        [0.0477],\n",
      "        [0.0453],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0478],\n",
      "        [0.0515],\n",
      "        [0.0534],\n",
      "        [0.0468],\n",
      "        [0.0505],\n",
      "        [0.0544],\n",
      "        [0.0515],\n",
      "        [0.0517],\n",
      "        [0.0490],\n",
      "        [0.0534],\n",
      "        [0.0540],\n",
      "        [0.0585],\n",
      "        [0.0560],\n",
      "        [0.0581],\n",
      "        [0.0564],\n",
      "        [0.0590],\n",
      "        [0.0625],\n",
      "        [0.0601],\n",
      "        [0.0593],\n",
      "        [0.0608],\n",
      "        [0.0598],\n",
      "        [0.0602],\n",
      "        [0.0647],\n",
      "        [0.0609],\n",
      "        [0.0683],\n",
      "        [0.0699],\n",
      "        [0.0721],\n",
      "        [0.0718],\n",
      "        [0.0704],\n",
      "        [0.0761],\n",
      "        [0.0687],\n",
      "        [0.0907],\n",
      "        [0.0873],\n",
      "        [0.0896],\n",
      "        [0.0956],\n",
      "        [0.0986],\n",
      "        [0.0976],\n",
      "        [0.0988],\n",
      "        [0.1036],\n",
      "        [0.1056],\n",
      "        [0.1131],\n",
      "        [0.1214],\n",
      "        [0.1351],\n",
      "        [0.1667]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 68.56323575973511\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.987105897835136e-08, 47)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [47, 96, 101, 8, 19, 46, 102, 26, 141, 66, 72, 58, 20, 27, 51, 140, 70, 110, 55, 126, 97, 5, 65, 67, 64, 40, 28, 50, 38, 135, 57, 54, 59, 75, 17, 98, 125, 127, 2, 53, 71, 30, 12, 151, 52, 44, 56, 29, 36, 114, 9, 144, 149, 39, 124, 48, 152, 134, 145, 18, 136, 63, 45, 109, 6, 43, 132, 142, 13, 91, 81, 111, 1, 139, 85, 11, 113, 16, 148, 73, 7, 62, 150, 60, 84, 147, 90, 3, 146, 15, 21, 118, 80, 131, 68, 153, 119, 158, 4, 137, 69, 10, 133, 25, 108, 99, 143, 14, 87, 31, 61, 37, 23, 130, 123, 86, 138, 49, 154, 112, 74, 157, 24, 83, 128, 156, 41, 116, 129, 35, 106, 22, 107, 95, 103, 100, 0, 89, 122, 42, 155, 88, 82, 117, 92, 104, 115, 93, 105, 94, 34, 120, 76, 79, 121, 77, 78, 33] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6302],\n",
      "        [0.3635],\n",
      "        [0.2720],\n",
      "        [0.7044],\n",
      "        [0.6604],\n",
      "        [0.5789],\n",
      "        [0.2601],\n",
      "        [0.6393],\n",
      "        [0.0800],\n",
      "        [0.5341],\n",
      "        [0.4713],\n",
      "        [0.6045],\n",
      "        [0.6558],\n",
      "        [0.6487],\n",
      "        [0.6250],\n",
      "        [0.0800],\n",
      "        [0.4748],\n",
      "        [0.2293],\n",
      "        [0.6122],\n",
      "        [0.0800],\n",
      "        [0.3599],\n",
      "        [0.7177],\n",
      "        [0.5476],\n",
      "        [0.5270],\n",
      "        [0.5338],\n",
      "        [0.5768],\n",
      "        [0.6280],\n",
      "        [0.6344],\n",
      "        [0.5427],\n",
      "        [0.0800],\n",
      "        [0.6188],\n",
      "        [0.6163],\n",
      "        [0.6003],\n",
      "        [0.4304],\n",
      "        [0.6578],\n",
      "        [0.3247],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.7039],\n",
      "        [0.6419],\n",
      "        [0.4735],\n",
      "        [0.6026],\n",
      "        [0.7100],\n",
      "        [0.0800],\n",
      "        [0.6513],\n",
      "        [0.5925],\n",
      "        [0.6248],\n",
      "        [0.6158],\n",
      "        [0.4902],\n",
      "        [0.1679],\n",
      "        [0.6940],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.5855],\n",
      "        [0.0800],\n",
      "        [0.6514],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.6676],\n",
      "        [0.0800],\n",
      "        [0.5755],\n",
      "        [0.5915],\n",
      "        [0.2196],\n",
      "        [0.7314],\n",
      "        [0.5905],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.7129],\n",
      "        [0.3617],\n",
      "        [0.2615],\n",
      "        [0.2248],\n",
      "        [0.6910],\n",
      "        [0.0800],\n",
      "        [0.2762],\n",
      "        [0.6738],\n",
      "        [0.1904],\n",
      "        [0.6669],\n",
      "        [0.0800],\n",
      "        [0.4393],\n",
      "        [0.7247],\n",
      "        [0.5892],\n",
      "        [0.0800],\n",
      "        [0.5933],\n",
      "        [0.2728],\n",
      "        [0.0800],\n",
      "        [0.3281],\n",
      "        [0.7001],\n",
      "        [0.0800],\n",
      "        [0.6958],\n",
      "        [0.6432],\n",
      "        [0.1255],\n",
      "        [0.3041],\n",
      "        [0.0800],\n",
      "        [0.4950],\n",
      "        [0.0800],\n",
      "        [0.1562],\n",
      "        [0.0800],\n",
      "        [0.6953],\n",
      "        [0.0800],\n",
      "        [0.4830],\n",
      "        [0.6674],\n",
      "        [0.0800],\n",
      "        [0.6317],\n",
      "        [0.2246],\n",
      "        [0.2709],\n",
      "        [0.0800],\n",
      "        [0.7187],\n",
      "        [0.3132],\n",
      "        [0.6122],\n",
      "        [0.6079],\n",
      "        [0.4788],\n",
      "        [0.6429],\n",
      "        [0.0800],\n",
      "        [0.0819],\n",
      "        [0.3086],\n",
      "        [0.0800],\n",
      "        [0.6203],\n",
      "        [0.0800],\n",
      "        [0.2492],\n",
      "        [0.4112],\n",
      "        [0.0800],\n",
      "        [0.6353],\n",
      "        [0.2648],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.5657],\n",
      "        [0.1336],\n",
      "        [0.0800],\n",
      "        [0.5547],\n",
      "        [0.2474],\n",
      "        [0.6365],\n",
      "        [0.2410],\n",
      "        [0.3381],\n",
      "        [0.2811],\n",
      "        [0.2628],\n",
      "        [0.6882],\n",
      "        [0.2902],\n",
      "        [0.0958],\n",
      "        [0.5472],\n",
      "        [0.0800],\n",
      "        [0.2869],\n",
      "        [0.2306],\n",
      "        [0.1041],\n",
      "        [0.3637],\n",
      "        [0.2804],\n",
      "        [0.1271],\n",
      "        [0.3566],\n",
      "        [0.2604],\n",
      "        [0.3379],\n",
      "        [0.5557],\n",
      "        [0.1349],\n",
      "        [0.4319],\n",
      "        [0.3780],\n",
      "        [0.1214],\n",
      "        [0.4219],\n",
      "        [0.3941],\n",
      "        [0.6083]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0010],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0017],\n",
      "        [0.0021],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0031],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0044],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0070],\n",
      "        [0.0070],\n",
      "        [0.0074],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0117],\n",
      "        [0.0128],\n",
      "        [0.0133],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0149],\n",
      "        [0.0162],\n",
      "        [0.0179],\n",
      "        [0.0182],\n",
      "        [0.0191],\n",
      "        [0.0202],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0236],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0253],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0264],\n",
      "        [0.0268],\n",
      "        [0.0276],\n",
      "        [0.0278],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0293],\n",
      "        [0.0294],\n",
      "        [0.0295],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0307],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0321],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0334],\n",
      "        [0.0336],\n",
      "        [0.0343],\n",
      "        [0.0350],\n",
      "        [0.0354],\n",
      "        [0.0354],\n",
      "        [0.0354],\n",
      "        [0.0360],\n",
      "        [0.0360],\n",
      "        [0.0363],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0375],\n",
      "        [0.0379],\n",
      "        [0.0381],\n",
      "        [0.0381],\n",
      "        [0.0388],\n",
      "        [0.0389],\n",
      "        [0.0392],\n",
      "        [0.0400],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0404],\n",
      "        [0.0405],\n",
      "        [0.0406],\n",
      "        [0.0410],\n",
      "        [0.0416],\n",
      "        [0.0448],\n",
      "        [0.0453],\n",
      "        [0.0468],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0486],\n",
      "        [0.0490],\n",
      "        [0.0493],\n",
      "        [0.0505],\n",
      "        [0.0515],\n",
      "        [0.0515],\n",
      "        [0.0517],\n",
      "        [0.0534],\n",
      "        [0.0534],\n",
      "        [0.0540],\n",
      "        [0.0544],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0581],\n",
      "        [0.0585],\n",
      "        [0.0590],\n",
      "        [0.0593],\n",
      "        [0.0598],\n",
      "        [0.0601],\n",
      "        [0.0602],\n",
      "        [0.0608],\n",
      "        [0.0609],\n",
      "        [0.0625],\n",
      "        [0.0647],\n",
      "        [0.0683],\n",
      "        [0.0687],\n",
      "        [0.0699],\n",
      "        [0.0704],\n",
      "        [0.0718],\n",
      "        [0.0721],\n",
      "        [0.0761],\n",
      "        [0.0873],\n",
      "        [0.0896],\n",
      "        [0.0907],\n",
      "        [0.0956],\n",
      "        [0.0976],\n",
      "        [0.0986],\n",
      "        [0.0988],\n",
      "        [0.1036],\n",
      "        [0.1056],\n",
      "        [0.1131],\n",
      "        [0.1214],\n",
      "        [0.1351],\n",
      "        [0.1667],\n",
      "        [0.1846]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0023],\n",
      "        [0.0017],\n",
      "        [0.0022],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0040],\n",
      "        [0.0010],\n",
      "        [0.0041],\n",
      "        [0.0017],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0043],\n",
      "        [0.0035],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0027],\n",
      "        [0.0018],\n",
      "        [0.0032],\n",
      "        [0.0064],\n",
      "        [0.0052],\n",
      "        [0.0080],\n",
      "        [0.0067],\n",
      "        [0.0059],\n",
      "        [0.0085],\n",
      "        [0.0061],\n",
      "        [0.0093],\n",
      "        [0.0104],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0096],\n",
      "        [0.0104],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0115],\n",
      "        [0.0102],\n",
      "        [0.0123],\n",
      "        [0.0116],\n",
      "        [0.0124],\n",
      "        [0.0114],\n",
      "        [0.0116],\n",
      "        [0.0157],\n",
      "        [0.0113],\n",
      "        [0.0158],\n",
      "        [0.0169],\n",
      "        [0.0165],\n",
      "        [0.0197],\n",
      "        [0.0208],\n",
      "        [0.0225],\n",
      "        [0.0234],\n",
      "        [0.0212],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0244],\n",
      "        [0.0261],\n",
      "        [0.0260],\n",
      "        [0.0269],\n",
      "        [0.0250],\n",
      "        [0.0267],\n",
      "        [0.0268],\n",
      "        [0.0262],\n",
      "        [0.0261],\n",
      "        [0.0262],\n",
      "        [0.0261],\n",
      "        [0.0290],\n",
      "        [0.0288],\n",
      "        [0.0298],\n",
      "        [0.0295],\n",
      "        [0.0281],\n",
      "        [0.0300],\n",
      "        [0.0264],\n",
      "        [0.0307],\n",
      "        [0.0288],\n",
      "        [0.0309],\n",
      "        [0.0299],\n",
      "        [0.0288],\n",
      "        [0.0309],\n",
      "        [0.0318],\n",
      "        [0.0313],\n",
      "        [0.0328],\n",
      "        [0.0342],\n",
      "        [0.0337],\n",
      "        [0.0319],\n",
      "        [0.0344],\n",
      "        [0.0329],\n",
      "        [0.0334],\n",
      "        [0.0361],\n",
      "        [0.0331],\n",
      "        [0.0346],\n",
      "        [0.0367],\n",
      "        [0.0361],\n",
      "        [0.0368],\n",
      "        [0.0370],\n",
      "        [0.0350],\n",
      "        [0.0383],\n",
      "        [0.0397],\n",
      "        [0.0373],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0382],\n",
      "        [0.0385],\n",
      "        [0.0419],\n",
      "        [0.0404],\n",
      "        [0.0411],\n",
      "        [0.0420],\n",
      "        [0.0411],\n",
      "        [0.0394],\n",
      "        [0.0403],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0432],\n",
      "        [0.0455],\n",
      "        [0.0497],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0480],\n",
      "        [0.0472],\n",
      "        [0.0498],\n",
      "        [0.0533],\n",
      "        [0.0508],\n",
      "        [0.0527],\n",
      "        [0.0552],\n",
      "        [0.0527],\n",
      "        [0.0552],\n",
      "        [0.0537],\n",
      "        [0.0568],\n",
      "        [0.0556],\n",
      "        [0.0595],\n",
      "        [0.0588],\n",
      "        [0.0598],\n",
      "        [0.0573],\n",
      "        [0.0606],\n",
      "        [0.0609],\n",
      "        [0.0609],\n",
      "        [0.0585],\n",
      "        [0.0621],\n",
      "        [0.0615],\n",
      "        [0.0653],\n",
      "        [0.0667],\n",
      "        [0.0678],\n",
      "        [0.0718],\n",
      "        [0.0697],\n",
      "        [0.0703],\n",
      "        [0.0721],\n",
      "        [0.0767],\n",
      "        [0.0845],\n",
      "        [0.0906],\n",
      "        [0.0916],\n",
      "        [0.0931],\n",
      "        [0.0985],\n",
      "        [0.0963],\n",
      "        [0.0967],\n",
      "        [0.1032],\n",
      "        [0.1044],\n",
      "        [0.1120],\n",
      "        [0.1207],\n",
      "        [0.1345],\n",
      "        [0.1657],\n",
      "        [0.1822]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 68.85043430328369\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.830312824808061e-07, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [8, 19, 102, 27, 96, 141, 51, 70, 101, 47, 140, 110, 20, 66, 46, 26, 58, 72, 126, 65, 64, 55, 5, 54, 59, 97, 67, 40, 135, 50, 17, 38, 57, 28, 30, 2, 75, 125, 53, 98, 127, 71, 12, 52, 151, 44, 56, 114, 9, 144, 29, 36, 149, 152, 124, 136, 39, 45, 63, 18, 91, 134, 145, 48, 142, 85, 6, 111, 109, 132, 43, 139, 13, 81, 1, 11, 16, 113, 62, 148, 60, 90, 84, 7, 73, 150, 3, 80, 15, 147, 146, 21, 118, 153, 158, 119, 4, 131, 137, 99, 68, 143, 10, 133, 108, 69, 25, 14, 87, 31, 61, 86, 123, 23, 130, 37, 138, 154, 157, 112, 49, 83, 74, 24, 156, 128, 35, 95, 116, 41, 129, 106, 107, 22, 100, 103, 0, 89, 122, 155, 88, 42, 82, 117, 92, 104, 115, 93, 94, 34, 105, 120, 76, 79, 121, 77, 78, 33, 32] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7048],\n",
      "        [0.6601],\n",
      "        [0.2612],\n",
      "        [0.6469],\n",
      "        [0.3662],\n",
      "        [0.0807],\n",
      "        [0.6233],\n",
      "        [0.4727],\n",
      "        [0.2732],\n",
      "        [0.6282],\n",
      "        [0.0807],\n",
      "        [0.2304],\n",
      "        [0.6555],\n",
      "        [0.5327],\n",
      "        [0.5767],\n",
      "        [0.6374],\n",
      "        [0.6029],\n",
      "        [0.4695],\n",
      "        [0.0807],\n",
      "        [0.5465],\n",
      "        [0.5326],\n",
      "        [0.6102],\n",
      "        [0.7185],\n",
      "        [0.6143],\n",
      "        [0.5987],\n",
      "        [0.3623],\n",
      "        [0.5255],\n",
      "        [0.5758],\n",
      "        [0.0807],\n",
      "        [0.6328],\n",
      "        [0.6577],\n",
      "        [0.5411],\n",
      "        [0.6174],\n",
      "        [0.6259],\n",
      "        [0.6001],\n",
      "        [0.7054],\n",
      "        [0.4289],\n",
      "        [0.0807],\n",
      "        [0.6402],\n",
      "        [0.3265],\n",
      "        [0.0807],\n",
      "        [0.4715],\n",
      "        [0.7109],\n",
      "        [0.6499],\n",
      "        [0.0807],\n",
      "        [0.5910],\n",
      "        [0.6232],\n",
      "        [0.1679],\n",
      "        [0.6944],\n",
      "        [0.0807],\n",
      "        [0.6135],\n",
      "        [0.4880],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.5844],\n",
      "        [0.5899],\n",
      "        [0.5741],\n",
      "        [0.6674],\n",
      "        [0.3648],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.6499],\n",
      "        [0.0807],\n",
      "        [0.2783],\n",
      "        [0.7322],\n",
      "        [0.2257],\n",
      "        [0.2207],\n",
      "        [0.0807],\n",
      "        [0.5889],\n",
      "        [0.0807],\n",
      "        [0.7135],\n",
      "        [0.2602],\n",
      "        [0.6919],\n",
      "        [0.6741],\n",
      "        [0.6666],\n",
      "        [0.1909],\n",
      "        [0.5878],\n",
      "        [0.0807],\n",
      "        [0.5919],\n",
      "        [0.3303],\n",
      "        [0.2744],\n",
      "        [0.7251],\n",
      "        [0.4377],\n",
      "        [0.0807],\n",
      "        [0.7008],\n",
      "        [0.3025],\n",
      "        [0.6958],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.6426],\n",
      "        [0.1250],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.1561],\n",
      "        [0.6960],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.2720],\n",
      "        [0.4933],\n",
      "        [0.0807],\n",
      "        [0.6674],\n",
      "        [0.0807],\n",
      "        [0.2252],\n",
      "        [0.4812],\n",
      "        [0.6300],\n",
      "        [0.7192],\n",
      "        [0.3153],\n",
      "        [0.6101],\n",
      "        [0.6066],\n",
      "        [0.3107],\n",
      "        [0.0809],\n",
      "        [0.6421],\n",
      "        [0.0807],\n",
      "        [0.4768],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.2502],\n",
      "        [0.6184],\n",
      "        [0.2656],\n",
      "        [0.4094],\n",
      "        [0.6340],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.5527],\n",
      "        [0.3404],\n",
      "        [0.1333],\n",
      "        [0.5644],\n",
      "        [0.0807],\n",
      "        [0.2482],\n",
      "        [0.2417],\n",
      "        [0.6357],\n",
      "        [0.2638],\n",
      "        [0.2822],\n",
      "        [0.6889],\n",
      "        [0.2918],\n",
      "        [0.0948],\n",
      "        [0.0807],\n",
      "        [0.2884],\n",
      "        [0.5453],\n",
      "        [0.2306],\n",
      "        [0.1035],\n",
      "        [0.3665],\n",
      "        [0.2814],\n",
      "        [0.1263],\n",
      "        [0.3591],\n",
      "        [0.3402],\n",
      "        [0.5535],\n",
      "        [0.2612],\n",
      "        [0.1345],\n",
      "        [0.4307],\n",
      "        [0.3769],\n",
      "        [0.1207],\n",
      "        [0.4212],\n",
      "        [0.3931],\n",
      "        [0.6059],\n",
      "        [0.6134]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0015],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0043],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0073],\n",
      "        [0.0080],\n",
      "        [0.0080],\n",
      "        [0.0085],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0101],\n",
      "        [0.0102],\n",
      "        [0.0104],\n",
      "        [0.0104],\n",
      "        [0.0104],\n",
      "        [0.0113],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0157],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0197],\n",
      "        [0.0208],\n",
      "        [0.0212],\n",
      "        [0.0217],\n",
      "        [0.0218],\n",
      "        [0.0225],\n",
      "        [0.0234],\n",
      "        [0.0244],\n",
      "        [0.0250],\n",
      "        [0.0260],\n",
      "        [0.0261],\n",
      "        [0.0261],\n",
      "        [0.0261],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0264],\n",
      "        [0.0267],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0281],\n",
      "        [0.0288],\n",
      "        [0.0288],\n",
      "        [0.0288],\n",
      "        [0.0290],\n",
      "        [0.0295],\n",
      "        [0.0298],\n",
      "        [0.0299],\n",
      "        [0.0300],\n",
      "        [0.0307],\n",
      "        [0.0309],\n",
      "        [0.0309],\n",
      "        [0.0313],\n",
      "        [0.0318],\n",
      "        [0.0319],\n",
      "        [0.0328],\n",
      "        [0.0329],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0337],\n",
      "        [0.0342],\n",
      "        [0.0344],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0361],\n",
      "        [0.0361],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0382],\n",
      "        [0.0383],\n",
      "        [0.0385],\n",
      "        [0.0394],\n",
      "        [0.0397],\n",
      "        [0.0403],\n",
      "        [0.0404],\n",
      "        [0.0411],\n",
      "        [0.0411],\n",
      "        [0.0419],\n",
      "        [0.0420],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0432],\n",
      "        [0.0455],\n",
      "        [0.0472],\n",
      "        [0.0480],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0497],\n",
      "        [0.0498],\n",
      "        [0.0508],\n",
      "        [0.0527],\n",
      "        [0.0527],\n",
      "        [0.0533],\n",
      "        [0.0537],\n",
      "        [0.0552],\n",
      "        [0.0552],\n",
      "        [0.0556],\n",
      "        [0.0568],\n",
      "        [0.0573],\n",
      "        [0.0585],\n",
      "        [0.0588],\n",
      "        [0.0595],\n",
      "        [0.0598],\n",
      "        [0.0606],\n",
      "        [0.0609],\n",
      "        [0.0609],\n",
      "        [0.0615],\n",
      "        [0.0621],\n",
      "        [0.0653],\n",
      "        [0.0667],\n",
      "        [0.0678],\n",
      "        [0.0697],\n",
      "        [0.0703],\n",
      "        [0.0718],\n",
      "        [0.0721],\n",
      "        [0.0767],\n",
      "        [0.0845],\n",
      "        [0.0906],\n",
      "        [0.0916],\n",
      "        [0.0931],\n",
      "        [0.0963],\n",
      "        [0.0967],\n",
      "        [0.0985],\n",
      "        [0.1032],\n",
      "        [0.1044],\n",
      "        [0.1120],\n",
      "        [0.1207],\n",
      "        [0.1345],\n",
      "        [0.1657],\n",
      "        [0.1822],\n",
      "        [0.1929]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0047],\n",
      "        [0.0033],\n",
      "        [0.0014],\n",
      "        [0.0037],\n",
      "        [0.0020],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0018],\n",
      "        [0.0070],\n",
      "        [0.0035],\n",
      "        [0.0033],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0086],\n",
      "        [0.0094],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0044],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0108],\n",
      "        [0.0033],\n",
      "        [0.0028],\n",
      "        [0.0039],\n",
      "        [0.0082],\n",
      "        [0.0122],\n",
      "        [0.0130],\n",
      "        [0.0088],\n",
      "        [0.0144],\n",
      "        [0.0062],\n",
      "        [0.0145],\n",
      "        [0.0145],\n",
      "        [0.0158],\n",
      "        [0.0056],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0108],\n",
      "        [0.0072],\n",
      "        [0.0123],\n",
      "        [0.0117],\n",
      "        [0.0197],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0162],\n",
      "        [0.0237],\n",
      "        [0.0250],\n",
      "        [0.0219],\n",
      "        [0.0253],\n",
      "        [0.0225],\n",
      "        [0.0281],\n",
      "        [0.0280],\n",
      "        [0.0236],\n",
      "        [0.0258],\n",
      "        [0.0252],\n",
      "        [0.0268],\n",
      "        [0.0299],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0221],\n",
      "        [0.0256],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0312],\n",
      "        [0.0289],\n",
      "        [0.0281],\n",
      "        [0.0252],\n",
      "        [0.0291],\n",
      "        [0.0289],\n",
      "        [0.0288],\n",
      "        [0.0339],\n",
      "        [0.0307],\n",
      "        [0.0263],\n",
      "        [0.0330],\n",
      "        [0.0277],\n",
      "        [0.0346],\n",
      "        [0.0271],\n",
      "        [0.0314],\n",
      "        [0.0281],\n",
      "        [0.0321],\n",
      "        [0.0290],\n",
      "        [0.0327],\n",
      "        [0.0332],\n",
      "        [0.0299],\n",
      "        [0.0376],\n",
      "        [0.0336],\n",
      "        [0.0381],\n",
      "        [0.0323],\n",
      "        [0.0319],\n",
      "        [0.0354],\n",
      "        [0.0360],\n",
      "        [0.0411],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0388],\n",
      "        [0.0373],\n",
      "        [0.0417],\n",
      "        [0.0375],\n",
      "        [0.0392],\n",
      "        [0.0397],\n",
      "        [0.0434],\n",
      "        [0.0410],\n",
      "        [0.0442],\n",
      "        [0.0404],\n",
      "        [0.0405],\n",
      "        [0.0457],\n",
      "        [0.0471],\n",
      "        [0.0382],\n",
      "        [0.0423],\n",
      "        [0.0378],\n",
      "        [0.0416],\n",
      "        [0.0467],\n",
      "        [0.0471],\n",
      "        [0.0531],\n",
      "        [0.0486],\n",
      "        [0.0539],\n",
      "        [0.0505],\n",
      "        [0.0515],\n",
      "        [0.0534],\n",
      "        [0.0523],\n",
      "        [0.0577],\n",
      "        [0.0542],\n",
      "        [0.0586],\n",
      "        [0.0600],\n",
      "        [0.0564],\n",
      "        [0.0560],\n",
      "        [0.0525],\n",
      "        [0.0584],\n",
      "        [0.0596],\n",
      "        [0.0634],\n",
      "        [0.0591],\n",
      "        [0.0601],\n",
      "        [0.0603],\n",
      "        [0.0653],\n",
      "        [0.0619],\n",
      "        [0.0616],\n",
      "        [0.0620],\n",
      "        [0.0667],\n",
      "        [0.0666],\n",
      "        [0.0704],\n",
      "        [0.0703],\n",
      "        [0.0761],\n",
      "        [0.0731],\n",
      "        [0.0776],\n",
      "        [0.0839],\n",
      "        [0.0901],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0962],\n",
      "        [0.0916],\n",
      "        [0.0979],\n",
      "        [0.1023],\n",
      "        [0.1014],\n",
      "        [0.1093],\n",
      "        [0.1196],\n",
      "        [0.1321],\n",
      "        [0.1632],\n",
      "        [0.1767],\n",
      "        [0.1876]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 69.13678097724915\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 22 個區塊累積花費時間(s) 1.2178618907928467\n",
      "<<The performance of 22 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2178618907928467\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 996.11\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1300.09\n",
      "The accuracy(2000) for l = 1: 90.57%\n",
      "The accuracy(3000) for l = 1: 96.86%\n",
      "The maximum error: tensor(4788.9336)\n",
      "The minimum error: tensor(35.1523)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 325.4\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 413.8\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 731.41015625\n",
      "The minimum error: 23.41015625\n",
      "------------------------------------------------------------\n",
      "0.9056603773584906\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<23>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.407570817325904e-07, 157)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [157, 98, 97, 92, 66, 47, 137, 61, 60, 50, 106, 15, 1, 136, 23, 55, 122, 4, 26, 13, 43, 49, 16, 62, 93, 54, 68, 42, 131, 22, 158, 156, 121, 51, 123, 63, 94, 48, 8, 36, 46, 53, 34, 71, 24, 147, 67, 110, 41, 14, 59, 140, 145, 40, 52, 2, 120, 5, 87, 148, 130, 141, 9, 132, 12, 32, 81, 58, 25, 155, 128, 138, 105, 56, 107, 3, 35, 135, 44, 109, 11, 144, 76, 86, 77, 80, 146, 39, 7, 143, 142, 115, 127, 69, 27, 114, 149, 10, 154, 133, 95, 129, 104, 139, 17, 57, 0, 83, 64, 6, 65, 82, 119, 21, 126, 134, 150, 108, 31, 19, 153, 33, 79, 124, 152, 45, 91, 70, 125, 112, 20, 102, 103, 99, 96, 37, 18, 118, 85, 84, 151, 78, 38, 113, 88, 100, 30, 111, 89, 90, 101, 72, 116, 75, 117]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0014],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0044],\n",
      "        [0.0047],\n",
      "        [0.0056],\n",
      "        [0.0062],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0086],\n",
      "        [0.0088],\n",
      "        [0.0094],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0117],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0130],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0145],\n",
      "        [0.0147],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0197],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0225],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0252],\n",
      "        [0.0253],\n",
      "        [0.0256],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0271],\n",
      "        [0.0280],\n",
      "        [0.0281],\n",
      "        [0.0281],\n",
      "        [0.0281],\n",
      "        [0.0286],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0291],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0307],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0321],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0332],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0346],\n",
      "        [0.0354],\n",
      "        [0.0360],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0378],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0382],\n",
      "        [0.0388],\n",
      "        [0.0392],\n",
      "        [0.0397],\n",
      "        [0.0404],\n",
      "        [0.0405],\n",
      "        [0.0410],\n",
      "        [0.0411],\n",
      "        [0.0416],\n",
      "        [0.0417],\n",
      "        [0.0423],\n",
      "        [0.0434],\n",
      "        [0.0442],\n",
      "        [0.0457],\n",
      "        [0.0467],\n",
      "        [0.0471],\n",
      "        [0.0471],\n",
      "        [0.0486],\n",
      "        [0.0505],\n",
      "        [0.0515],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0531],\n",
      "        [0.0534],\n",
      "        [0.0539],\n",
      "        [0.0542],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0577],\n",
      "        [0.0584],\n",
      "        [0.0586],\n",
      "        [0.0591],\n",
      "        [0.0596],\n",
      "        [0.0600],\n",
      "        [0.0601],\n",
      "        [0.0603],\n",
      "        [0.0616],\n",
      "        [0.0619],\n",
      "        [0.0634],\n",
      "        [0.0653],\n",
      "        [0.0666],\n",
      "        [0.0667],\n",
      "        [0.0703],\n",
      "        [0.0704],\n",
      "        [0.0731],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0839],\n",
      "        [0.0901],\n",
      "        [0.0916],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0962],\n",
      "        [0.0979],\n",
      "        [0.1014],\n",
      "        [0.1023],\n",
      "        [0.1093],\n",
      "        [0.1196]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.407570817325904e-07, 157)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [157, 98, 97, 92, 66, 47, 137, 61, 60, 50, 106, 15, 1, 136, 23, 55, 122, 4, 26, 13, 43, 49, 16, 62, 93, 54, 68, 42, 131, 22, 158, 156, 121, 51, 123, 63, 94, 48, 8, 36, 46, 53, 34, 71, 24, 147, 67, 110, 41, 14, 59, 140, 145, 40, 52, 2, 120, 5, 87, 148, 130, 141, 9, 132, 12, 32, 81, 58, 25, 155, 128, 138, 105, 56, 107, 3, 35, 135, 44, 109, 11, 144, 76, 86, 77, 80, 146, 39, 7, 143, 142, 115, 127, 69, 27, 114, 149, 10, 154, 133, 95, 129, 104, 139, 17, 57, 0, 83, 64, 6, 65, 82, 119, 21, 126, 134, 150, 108, 31, 19, 153, 33, 79, 124, 152, 45, 91, 70, 125, 112, 20, 102, 103, 99, 96, 37, 18, 118, 85, 84, 151, 78, 38, 113, 88, 100, 30, 111, 89, 90, 101, 72, 116, 75, 117, 73] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.0800],\n",
      "        [0.2608],\n",
      "        [0.2727],\n",
      "        [0.3665],\n",
      "        [0.4687],\n",
      "        [0.6191],\n",
      "        [0.0800],\n",
      "        [0.5431],\n",
      "        [0.5292],\n",
      "        [0.6098],\n",
      "        [0.2303],\n",
      "        [0.6559],\n",
      "        [0.7150],\n",
      "        [0.0800],\n",
      "        [0.6417],\n",
      "        [0.5946],\n",
      "        [0.0800],\n",
      "        [0.7009],\n",
      "        [0.5944],\n",
      "        [0.6536],\n",
      "        [0.6236],\n",
      "        [0.6359],\n",
      "        [0.6515],\n",
      "        [0.5291],\n",
      "        [0.3625],\n",
      "        [0.5990],\n",
      "        [0.4657],\n",
      "        [0.5721],\n",
      "        [0.0800],\n",
      "        [0.6321],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.6057],\n",
      "        [0.0800],\n",
      "        [0.5218],\n",
      "        [0.3265],\n",
      "        [0.6457],\n",
      "        [0.7074],\n",
      "        [0.5721],\n",
      "        [0.6286],\n",
      "        [0.6133],\n",
      "        [0.5369],\n",
      "        [0.4257],\n",
      "        [0.6206],\n",
      "        [0.0800],\n",
      "        [0.4676],\n",
      "        [0.1673],\n",
      "        [0.5859],\n",
      "        [0.6633],\n",
      "        [0.5704],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.5870],\n",
      "        [0.6190],\n",
      "        [0.7286],\n",
      "        [0.0800],\n",
      "        [0.6907],\n",
      "        [0.3655],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.7098],\n",
      "        [0.0800],\n",
      "        [0.6624],\n",
      "        [0.4834],\n",
      "        [0.2790],\n",
      "        [0.5839],\n",
      "        [0.6079],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.2206],\n",
      "        [0.5880],\n",
      "        [0.2255],\n",
      "        [0.7213],\n",
      "        [0.5806],\n",
      "        [0.0800],\n",
      "        [0.6455],\n",
      "        [0.1904],\n",
      "        [0.6916],\n",
      "        [0.0800],\n",
      "        [0.2998],\n",
      "        [0.3307],\n",
      "        [0.2580],\n",
      "        [0.2746],\n",
      "        [0.0800],\n",
      "        [0.5848],\n",
      "        [0.6704],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.1554],\n",
      "        [0.0800],\n",
      "        [0.4343],\n",
      "        [0.6047],\n",
      "        [0.1241],\n",
      "        [0.0800],\n",
      "        [0.7154],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.2717],\n",
      "        [0.0800],\n",
      "        [0.2246],\n",
      "        [0.0800],\n",
      "        [0.6384],\n",
      "        [0.6027],\n",
      "        [0.6926],\n",
      "        [0.3157],\n",
      "        [0.4895],\n",
      "        [0.6636],\n",
      "        [0.4774],\n",
      "        [0.3112],\n",
      "        [0.0800],\n",
      "        [0.6249],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.2498],\n",
      "        [0.5478],\n",
      "        [0.6376],\n",
      "        [0.0800],\n",
      "        [0.4726],\n",
      "        [0.2651],\n",
      "        [0.0800],\n",
      "        [0.0800],\n",
      "        [0.6140],\n",
      "        [0.3406],\n",
      "        [0.4059],\n",
      "        [0.0800],\n",
      "        [0.1325],\n",
      "        [0.6292],\n",
      "        [0.2477],\n",
      "        [0.2411],\n",
      "        [0.2818],\n",
      "        [0.2633],\n",
      "        [0.5605],\n",
      "        [0.6313],\n",
      "        [0.0937],\n",
      "        [0.2919],\n",
      "        [0.2884],\n",
      "        [0.0800],\n",
      "        [0.2295],\n",
      "        [0.5410],\n",
      "        [0.1026],\n",
      "        [0.3671],\n",
      "        [0.2810],\n",
      "        [0.5485],\n",
      "        [0.1250],\n",
      "        [0.3594],\n",
      "        [0.3403],\n",
      "        [0.2607],\n",
      "        [0.4278],\n",
      "        [0.1336],\n",
      "        [0.3742],\n",
      "        [0.1196],\n",
      "        [0.4188]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0014],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0033],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0044],\n",
      "        [0.0047],\n",
      "        [0.0056],\n",
      "        [0.0062],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0086],\n",
      "        [0.0088],\n",
      "        [0.0094],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0117],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0130],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0145],\n",
      "        [0.0147],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0197],\n",
      "        [0.0219],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0225],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0250],\n",
      "        [0.0252],\n",
      "        [0.0252],\n",
      "        [0.0253],\n",
      "        [0.0256],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0271],\n",
      "        [0.0280],\n",
      "        [0.0281],\n",
      "        [0.0281],\n",
      "        [0.0281],\n",
      "        [0.0286],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0289],\n",
      "        [0.0290],\n",
      "        [0.0291],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0307],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0319],\n",
      "        [0.0321],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0332],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0346],\n",
      "        [0.0354],\n",
      "        [0.0360],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0378],\n",
      "        [0.0380],\n",
      "        [0.0380],\n",
      "        [0.0382],\n",
      "        [0.0388],\n",
      "        [0.0392],\n",
      "        [0.0397],\n",
      "        [0.0404],\n",
      "        [0.0405],\n",
      "        [0.0410],\n",
      "        [0.0411],\n",
      "        [0.0416],\n",
      "        [0.0417],\n",
      "        [0.0423],\n",
      "        [0.0434],\n",
      "        [0.0442],\n",
      "        [0.0457],\n",
      "        [0.0467],\n",
      "        [0.0471],\n",
      "        [0.0471],\n",
      "        [0.0486],\n",
      "        [0.0505],\n",
      "        [0.0515],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0531],\n",
      "        [0.0534],\n",
      "        [0.0539],\n",
      "        [0.0542],\n",
      "        [0.0560],\n",
      "        [0.0564],\n",
      "        [0.0577],\n",
      "        [0.0584],\n",
      "        [0.0586],\n",
      "        [0.0591],\n",
      "        [0.0596],\n",
      "        [0.0600],\n",
      "        [0.0601],\n",
      "        [0.0603],\n",
      "        [0.0616],\n",
      "        [0.0619],\n",
      "        [0.0634],\n",
      "        [0.0653],\n",
      "        [0.0666],\n",
      "        [0.0667],\n",
      "        [0.0703],\n",
      "        [0.0704],\n",
      "        [0.0731],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0839],\n",
      "        [0.0901],\n",
      "        [0.0916],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0962],\n",
      "        [0.0979],\n",
      "        [0.1014],\n",
      "        [0.1023],\n",
      "        [0.1093],\n",
      "        [0.1196],\n",
      "        [0.1321]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0022],\n",
      "        [    0.0056],\n",
      "        [    0.0073],\n",
      "        [    0.0028],\n",
      "        [    0.0046],\n",
      "        [    0.0017],\n",
      "        [    0.0091],\n",
      "        [    0.0092],\n",
      "        [    0.0097],\n",
      "        [    0.0006],\n",
      "        [    0.0046],\n",
      "        [    0.0115],\n",
      "        [    0.0027],\n",
      "        [    0.0031],\n",
      "        [    0.0110],\n",
      "        [    0.0052],\n",
      "        [    0.0033],\n",
      "        [    0.0116],\n",
      "        [    0.0139],\n",
      "        [    0.0001],\n",
      "        [    0.0144],\n",
      "        [    0.0003],\n",
      "        [    0.0014],\n",
      "        [    0.0133],\n",
      "        [    0.0011],\n",
      "        [    0.0032],\n",
      "        [    0.0024],\n",
      "        [    0.0096],\n",
      "        [    0.0027],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0116],\n",
      "        [    0.0038],\n",
      "        [    0.0124],\n",
      "        [    0.0063],\n",
      "        [    0.0170],\n",
      "        [    0.0197],\n",
      "        [    0.0210],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0072],\n",
      "        [    0.0084],\n",
      "        [    0.0098],\n",
      "        [    0.0093],\n",
      "        [    0.0169],\n",
      "        [    0.0146],\n",
      "        [    0.0204],\n",
      "        [    0.0288],\n",
      "        [    0.0300],\n",
      "        [    0.0294],\n",
      "        [    0.0218],\n",
      "        [    0.0244],\n",
      "        [    0.0170],\n",
      "        [    0.0176],\n",
      "        [    0.0337],\n",
      "        [    0.0260],\n",
      "        [    0.0171],\n",
      "        [    0.0202],\n",
      "        [    0.0250],\n",
      "        [    0.0267],\n",
      "        [    0.0268],\n",
      "        [    0.0348],\n",
      "        [    0.0261],\n",
      "        [    0.0348],\n",
      "        [    0.0227],\n",
      "        [    0.0234],\n",
      "        [    0.0351],\n",
      "        [    0.0219],\n",
      "        [    0.0279],\n",
      "        [    0.0295],\n",
      "        [    0.0281],\n",
      "        [    0.0316],\n",
      "        [    0.0361],\n",
      "        [    0.0266],\n",
      "        [    0.0380],\n",
      "        [    0.0233],\n",
      "        [    0.0299],\n",
      "        [    0.0237],\n",
      "        [    0.0333],\n",
      "        [    0.0400],\n",
      "        [    0.0328],\n",
      "        [    0.0358],\n",
      "        [    0.0279],\n",
      "        [    0.0297],\n",
      "        [    0.0287],\n",
      "        [    0.0344],\n",
      "        [    0.0273],\n",
      "        [    0.0266],\n",
      "        [    0.0361],\n",
      "        [    0.0367],\n",
      "        [    0.0386],\n",
      "        [    0.0383],\n",
      "        [    0.0328],\n",
      "        [    0.0442],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0467],\n",
      "        [    0.0380],\n",
      "        [    0.0385],\n",
      "        [    0.0357],\n",
      "        [    0.0411],\n",
      "        [    0.0431],\n",
      "        [    0.0403],\n",
      "        [    0.0337],\n",
      "        [    0.0488],\n",
      "        [    0.0338],\n",
      "        [    0.0374],\n",
      "        [    0.0381],\n",
      "        [    0.0363],\n",
      "        [    0.0404],\n",
      "        [    0.0417],\n",
      "        [    0.0479],\n",
      "        [    0.0406],\n",
      "        [    0.0493],\n",
      "        [    0.0498],\n",
      "        [    0.0508],\n",
      "        [    0.0550],\n",
      "        [    0.0584],\n",
      "        [    0.0460],\n",
      "        [    0.0527],\n",
      "        [    0.0486],\n",
      "        [    0.0500],\n",
      "        [    0.0568],\n",
      "        [    0.0556],\n",
      "        [    0.0508],\n",
      "        [    0.0535],\n",
      "        [    0.0541],\n",
      "        [    0.0598],\n",
      "        [    0.0585],\n",
      "        [    0.0532],\n",
      "        [    0.0632],\n",
      "        [    0.0632],\n",
      "        [    0.0654],\n",
      "        [    0.0580],\n",
      "        [    0.0571],\n",
      "        [    0.0582],\n",
      "        [    0.0673],\n",
      "        [    0.0623],\n",
      "        [    0.0660],\n",
      "        [    0.0697],\n",
      "        [    0.0696],\n",
      "        [    0.0702],\n",
      "        [    0.0768],\n",
      "        [    0.0786],\n",
      "        [    0.0938],\n",
      "        [    0.0973],\n",
      "        [    0.0922],\n",
      "        [    0.0875],\n",
      "        [    0.0913],\n",
      "        [    0.1012],\n",
      "        [    0.1065],\n",
      "        [    0.1033],\n",
      "        [    0.1138],\n",
      "        [    0.1205],\n",
      "        [    0.1374]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 69.6535291671753\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.52118367586263e-09, 43)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [43, 157, 16, 106, 54, 62, 137, 98, 42, 22, 136, 66, 23, 68, 4, 51, 47, 15, 122, 97, 63, 36, 53, 46, 92, 34, 61, 60, 24, 131, 50, 71, 158, 156, 55, 1, 121, 26, 123, 93, 13, 49, 67, 147, 94, 40, 5, 52, 48, 87, 110, 8, 140, 25, 32, 35, 81, 44, 145, 148, 120, 132, 107, 7, 130, 141, 39, 86, 155, 138, 80, 41, 59, 128, 77, 135, 14, 105, 69, 144, 109, 17, 2, 0, 146, 9, 12, 58, 95, 76, 143, 56, 6, 142, 114, 149, 83, 154, 3, 64, 127, 133, 115, 11, 139, 65, 21, 129, 82, 104, 27, 19, 10, 119, 33, 57, 126, 134, 79, 45, 150, 153, 20, 91, 70, 108, 152, 124, 37, 96, 18, 31, 112, 125, 85, 102, 103, 99, 84, 118, 78, 151, 38, 113, 88, 89, 90, 111, 100, 30, 101, 116, 72, 75, 117, 73, 74] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6305],\n",
      "        [0.0807],\n",
      "        [0.6593],\n",
      "        [0.2330],\n",
      "        [0.6061],\n",
      "        [0.5352],\n",
      "        [0.0807],\n",
      "        [0.2644],\n",
      "        [0.5783],\n",
      "        [0.6388],\n",
      "        [0.0807],\n",
      "        [0.4737],\n",
      "        [0.6485],\n",
      "        [0.4708],\n",
      "        [0.7090],\n",
      "        [0.6127],\n",
      "        [0.6262],\n",
      "        [0.6637],\n",
      "        [0.0807],\n",
      "        [0.2766],\n",
      "        [0.5277],\n",
      "        [0.5787],\n",
      "        [0.6206],\n",
      "        [0.6358],\n",
      "        [0.3717],\n",
      "        [0.5430],\n",
      "        [0.5497],\n",
      "        [0.5357],\n",
      "        [0.6271],\n",
      "        [0.0807],\n",
      "        [0.6166],\n",
      "        [0.4306],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.6017],\n",
      "        [0.7233],\n",
      "        [0.0807],\n",
      "        [0.6004],\n",
      "        [0.0807],\n",
      "        [0.3676],\n",
      "        [0.6614],\n",
      "        [0.6430],\n",
      "        [0.4726],\n",
      "        [0.0807],\n",
      "        [0.3312],\n",
      "        [0.5937],\n",
      "        [0.6990],\n",
      "        [0.6263],\n",
      "        [0.6530],\n",
      "        [0.3709],\n",
      "        [0.1687],\n",
      "        [0.7160],\n",
      "        [0.0807],\n",
      "        [0.6141],\n",
      "        [0.4886],\n",
      "        [0.5872],\n",
      "        [0.2837],\n",
      "        [0.6530],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.2280],\n",
      "        [0.6784],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.5914],\n",
      "        [0.3356],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.2791],\n",
      "        [0.5925],\n",
      "        [0.5773],\n",
      "        [0.0807],\n",
      "        [0.2613],\n",
      "        [0.0807],\n",
      "        [0.6711],\n",
      "        [0.2233],\n",
      "        [0.4391],\n",
      "        [0.0807],\n",
      "        [0.1924],\n",
      "        [0.6458],\n",
      "        [0.7370],\n",
      "        [0.7004],\n",
      "        [0.0807],\n",
      "        [0.7183],\n",
      "        [0.6701],\n",
      "        [0.5909],\n",
      "        [0.2757],\n",
      "        [0.3032],\n",
      "        [0.0807],\n",
      "        [0.5951],\n",
      "        [0.6715],\n",
      "        [0.0807],\n",
      "        [0.1251],\n",
      "        [0.0807],\n",
      "        [0.3206],\n",
      "        [0.0807],\n",
      "        [0.7294],\n",
      "        [0.4948],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.1567],\n",
      "        [0.6998],\n",
      "        [0.0807],\n",
      "        [0.4826],\n",
      "        [0.6314],\n",
      "        [0.0807],\n",
      "        [0.3162],\n",
      "        [0.2271],\n",
      "        [0.6111],\n",
      "        [0.6447],\n",
      "        [0.7239],\n",
      "        [0.0807],\n",
      "        [0.4779],\n",
      "        [0.6099],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.2692],\n",
      "        [0.6210],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.6360],\n",
      "        [0.3455],\n",
      "        [0.4104],\n",
      "        [0.2525],\n",
      "        [0.0807],\n",
      "        [0.0807],\n",
      "        [0.5668],\n",
      "        [0.2672],\n",
      "        [0.6384],\n",
      "        [0.5537],\n",
      "        [0.1335],\n",
      "        [0.0807],\n",
      "        [0.2962],\n",
      "        [0.2508],\n",
      "        [0.2440],\n",
      "        [0.2855],\n",
      "        [0.2927],\n",
      "        [0.0943],\n",
      "        [0.2331],\n",
      "        [0.0807],\n",
      "        [0.5469],\n",
      "        [0.1034],\n",
      "        [0.3724],\n",
      "        [0.3647],\n",
      "        [0.3452],\n",
      "        [0.1257],\n",
      "        [0.2847],\n",
      "        [0.5542],\n",
      "        [0.2639],\n",
      "        [0.1346],\n",
      "        [0.4329],\n",
      "        [0.3787],\n",
      "        [0.1205],\n",
      "        [0.4241],\n",
      "        [0.3954]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0003],\n",
      "        [    0.0006],\n",
      "        [    0.0011],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0022],\n",
      "        [    0.0024],\n",
      "        [    0.0027],\n",
      "        [    0.0027],\n",
      "        [    0.0028],\n",
      "        [    0.0031],\n",
      "        [    0.0032],\n",
      "        [    0.0033],\n",
      "        [    0.0038],\n",
      "        [    0.0046],\n",
      "        [    0.0046],\n",
      "        [    0.0052],\n",
      "        [    0.0056],\n",
      "        [    0.0063],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0084],\n",
      "        [    0.0091],\n",
      "        [    0.0092],\n",
      "        [    0.0093],\n",
      "        [    0.0096],\n",
      "        [    0.0097],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0116],\n",
      "        [    0.0116],\n",
      "        [    0.0124],\n",
      "        [    0.0133],\n",
      "        [    0.0139],\n",
      "        [    0.0144],\n",
      "        [    0.0146],\n",
      "        [    0.0169],\n",
      "        [    0.0170],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0176],\n",
      "        [    0.0197],\n",
      "        [    0.0202],\n",
      "        [    0.0204],\n",
      "        [    0.0210],\n",
      "        [    0.0218],\n",
      "        [    0.0219],\n",
      "        [    0.0227],\n",
      "        [    0.0233],\n",
      "        [    0.0234],\n",
      "        [    0.0237],\n",
      "        [    0.0244],\n",
      "        [    0.0250],\n",
      "        [    0.0260],\n",
      "        [    0.0261],\n",
      "        [    0.0266],\n",
      "        [    0.0266],\n",
      "        [    0.0267],\n",
      "        [    0.0268],\n",
      "        [    0.0273],\n",
      "        [    0.0279],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0287],\n",
      "        [    0.0288],\n",
      "        [    0.0294],\n",
      "        [    0.0295],\n",
      "        [    0.0297],\n",
      "        [    0.0299],\n",
      "        [    0.0300],\n",
      "        [    0.0316],\n",
      "        [    0.0328],\n",
      "        [    0.0328],\n",
      "        [    0.0333],\n",
      "        [    0.0337],\n",
      "        [    0.0337],\n",
      "        [    0.0338],\n",
      "        [    0.0344],\n",
      "        [    0.0348],\n",
      "        [    0.0348],\n",
      "        [    0.0351],\n",
      "        [    0.0357],\n",
      "        [    0.0358],\n",
      "        [    0.0361],\n",
      "        [    0.0361],\n",
      "        [    0.0363],\n",
      "        [    0.0367],\n",
      "        [    0.0369],\n",
      "        [    0.0373],\n",
      "        [    0.0374],\n",
      "        [    0.0380],\n",
      "        [    0.0380],\n",
      "        [    0.0381],\n",
      "        [    0.0383],\n",
      "        [    0.0385],\n",
      "        [    0.0386],\n",
      "        [    0.0400],\n",
      "        [    0.0403],\n",
      "        [    0.0404],\n",
      "        [    0.0406],\n",
      "        [    0.0411],\n",
      "        [    0.0417],\n",
      "        [    0.0431],\n",
      "        [    0.0442],\n",
      "        [    0.0460],\n",
      "        [    0.0467],\n",
      "        [    0.0479],\n",
      "        [    0.0486],\n",
      "        [    0.0488],\n",
      "        [    0.0493],\n",
      "        [    0.0498],\n",
      "        [    0.0500],\n",
      "        [    0.0508],\n",
      "        [    0.0508],\n",
      "        [    0.0527],\n",
      "        [    0.0532],\n",
      "        [    0.0535],\n",
      "        [    0.0541],\n",
      "        [    0.0550],\n",
      "        [    0.0556],\n",
      "        [    0.0568],\n",
      "        [    0.0571],\n",
      "        [    0.0580],\n",
      "        [    0.0582],\n",
      "        [    0.0584],\n",
      "        [    0.0585],\n",
      "        [    0.0598],\n",
      "        [    0.0623],\n",
      "        [    0.0632],\n",
      "        [    0.0632],\n",
      "        [    0.0654],\n",
      "        [    0.0660],\n",
      "        [    0.0673],\n",
      "        [    0.0696],\n",
      "        [    0.0697],\n",
      "        [    0.0702],\n",
      "        [    0.0768],\n",
      "        [    0.0786],\n",
      "        [    0.0875],\n",
      "        [    0.0913],\n",
      "        [    0.0922],\n",
      "        [    0.0938],\n",
      "        [    0.0973],\n",
      "        [    0.1012],\n",
      "        [    0.1033],\n",
      "        [    0.1065],\n",
      "        [    0.1138],\n",
      "        [    0.1205],\n",
      "        [    0.1374],\n",
      "        [    0.1681]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0009],\n",
      "        [    0.0001],\n",
      "        [    0.0012],\n",
      "        [    0.0012],\n",
      "        [    0.0022],\n",
      "        [    0.0030],\n",
      "        [    0.0017],\n",
      "        [    0.0013],\n",
      "        [    0.0035],\n",
      "        [    0.0031],\n",
      "        [    0.0026],\n",
      "        [    0.0006],\n",
      "        [    0.0028],\n",
      "        [    0.0054],\n",
      "        [    0.0046],\n",
      "        [    0.0050],\n",
      "        [    0.0039],\n",
      "        [    0.0055],\n",
      "        [    0.0053],\n",
      "        [    0.0048],\n",
      "        [    0.0079],\n",
      "        [    0.0063],\n",
      "        [    0.0081],\n",
      "        [    0.0078],\n",
      "        [    0.0076],\n",
      "        [    0.0086],\n",
      "        [    0.0078],\n",
      "        [    0.0076],\n",
      "        [    0.0100],\n",
      "        [    0.0097],\n",
      "        [    0.0085],\n",
      "        [    0.0120],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0098],\n",
      "        [    0.0134],\n",
      "        [    0.0117],\n",
      "        [    0.0105],\n",
      "        [    0.0125],\n",
      "        [    0.0133],\n",
      "        [    0.0148],\n",
      "        [    0.0135],\n",
      "        [    0.0168],\n",
      "        [    0.0170],\n",
      "        [    0.0165],\n",
      "        [    0.0176],\n",
      "        [    0.0158],\n",
      "        [    0.0185],\n",
      "        [    0.0191],\n",
      "        [    0.0196],\n",
      "        [    0.0219],\n",
      "        [    0.0229],\n",
      "        [    0.0217],\n",
      "        [    0.0228],\n",
      "        [    0.0238],\n",
      "        [    0.0230],\n",
      "        [    0.0235],\n",
      "        [    0.0242],\n",
      "        [    0.0244],\n",
      "        [    0.0249],\n",
      "        [    0.0261],\n",
      "        [    0.0260],\n",
      "        [    0.0274],\n",
      "        [    0.0254],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0278],\n",
      "        [    0.0277],\n",
      "        [    0.0278],\n",
      "        [    0.0281],\n",
      "        [    0.0290],\n",
      "        [    0.0280],\n",
      "        [    0.0279],\n",
      "        [    0.0296],\n",
      "        [    0.0320],\n",
      "        [    0.0299],\n",
      "        [    0.0309],\n",
      "        [    0.0309],\n",
      "        [    0.0351],\n",
      "        [    0.0329],\n",
      "        [    0.0321],\n",
      "        [    0.0331],\n",
      "        [    0.0355],\n",
      "        [    0.0321],\n",
      "        [    0.0345],\n",
      "        [    0.0366],\n",
      "        [    0.0357],\n",
      "        [    0.0337],\n",
      "        [    0.0370],\n",
      "        [    0.0332],\n",
      "        [    0.0362],\n",
      "        [    0.0349],\n",
      "        [    0.0354],\n",
      "        [    0.0368],\n",
      "        [    0.0389],\n",
      "        [    0.0372],\n",
      "        [    0.0374],\n",
      "        [    0.0379],\n",
      "        [    0.0396],\n",
      "        [    0.0400],\n",
      "        [    0.0384],\n",
      "        [    0.0384],\n",
      "        [    0.0370],\n",
      "        [    0.0413],\n",
      "        [    0.0402],\n",
      "        [    0.0424],\n",
      "        [    0.0409],\n",
      "        [    0.0412],\n",
      "        [    0.0418],\n",
      "        [    0.0420],\n",
      "        [    0.0433],\n",
      "        [    0.0456],\n",
      "        [    0.0484],\n",
      "        [    0.0479],\n",
      "        [    0.0494],\n",
      "        [    0.0477],\n",
      "        [    0.0494],\n",
      "        [    0.0497],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0507],\n",
      "        [    0.0526],\n",
      "        [    0.0531],\n",
      "        [    0.0533],\n",
      "        [    0.0567],\n",
      "        [    0.0543],\n",
      "        [    0.0555],\n",
      "        [    0.0569],\n",
      "        [    0.0572],\n",
      "        [    0.0592],\n",
      "        [    0.0578],\n",
      "        [    0.0577],\n",
      "        [    0.0606],\n",
      "        [    0.0599],\n",
      "        [    0.0627],\n",
      "        [    0.0622],\n",
      "        [    0.0622],\n",
      "        [    0.0646],\n",
      "        [    0.0664],\n",
      "        [    0.0649],\n",
      "        [    0.0711],\n",
      "        [    0.0696],\n",
      "        [    0.0709],\n",
      "        [    0.0791],\n",
      "        [    0.0781],\n",
      "        [    0.0872],\n",
      "        [    0.0912],\n",
      "        [    0.0945],\n",
      "        [    0.0930],\n",
      "        [    0.0964],\n",
      "        [    0.1003],\n",
      "        [    0.1013],\n",
      "        [    0.1045],\n",
      "        [    0.1117],\n",
      "        [    0.1183],\n",
      "        [    0.1357],\n",
      "        [    0.1661]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 69.93824529647827\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.968293497777722e-09, 157)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [157, 66, 43, 16, 106, 98, 137, 54, 136, 23, 62, 22, 42, 47, 4, 97, 51, 122, 68, 15, 36, 92, 60, 61, 46, 63, 53, 50, 34, 131, 158, 55, 156, 24, 26, 121, 71, 123, 93, 1, 49, 13, 5, 94, 67, 147, 40, 52, 48, 87, 140, 110, 25, 8, 35, 81, 32, 44, 145, 148, 7, 132, 120, 130, 141, 107, 86, 39, 155, 59, 41, 138, 80, 128, 135, 105, 14, 77, 109, 0, 144, 17, 76, 58, 146, 56, 69, 6, 2, 12, 143, 9, 142, 95, 115, 149, 83, 154, 127, 133, 114, 3, 64, 139, 21, 129, 11, 82, 104, 65, 27, 19, 57, 119, 10, 33, 126, 134, 150, 79, 45, 153, 20, 91, 108, 152, 70, 124, 37, 31, 18, 96, 125, 112, 102, 103, 85, 99, 118, 84, 151, 38, 78, 88, 113, 89, 90, 100, 111, 30, 101, 116, 72, 75, 117, 73, 74, 29] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.0808],\n",
      "        [0.4716],\n",
      "        [0.6297],\n",
      "        [0.6601],\n",
      "        [0.2324],\n",
      "        [0.2635],\n",
      "        [0.0808],\n",
      "        [0.6050],\n",
      "        [0.0808],\n",
      "        [0.6481],\n",
      "        [0.5337],\n",
      "        [0.6385],\n",
      "        [0.5772],\n",
      "        [0.6254],\n",
      "        [0.7103],\n",
      "        [0.2757],\n",
      "        [0.6116],\n",
      "        [0.0808],\n",
      "        [0.4686],\n",
      "        [0.6646],\n",
      "        [0.5788],\n",
      "        [0.3721],\n",
      "        [0.5340],\n",
      "        [0.5483],\n",
      "        [0.6352],\n",
      "        [0.5261],\n",
      "        [0.6197],\n",
      "        [0.6155],\n",
      "        [0.5428],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.6005],\n",
      "        [0.0808],\n",
      "        [0.6264],\n",
      "        [0.5994],\n",
      "        [0.0808],\n",
      "        [0.4284],\n",
      "        [0.0808],\n",
      "        [0.3676],\n",
      "        [0.7252],\n",
      "        [0.6422],\n",
      "        [0.6622],\n",
      "        [0.7003],\n",
      "        [0.3307],\n",
      "        [0.4704],\n",
      "        [0.0808],\n",
      "        [0.5931],\n",
      "        [0.6255],\n",
      "        [0.6524],\n",
      "        [0.3715],\n",
      "        [0.0808],\n",
      "        [0.1672],\n",
      "        [0.6132],\n",
      "        [0.7179],\n",
      "        [0.5875],\n",
      "        [0.2836],\n",
      "        [0.4876],\n",
      "        [0.6525],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.6796],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.2272],\n",
      "        [0.3358],\n",
      "        [0.5909],\n",
      "        [0.0808],\n",
      "        [0.5759],\n",
      "        [0.5918],\n",
      "        [0.0808],\n",
      "        [0.2788],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.2226],\n",
      "        [0.6721],\n",
      "        [0.2589],\n",
      "        [0.1912],\n",
      "        [0.7021],\n",
      "        [0.0808],\n",
      "        [0.6464],\n",
      "        [0.3007],\n",
      "        [0.5896],\n",
      "        [0.0808],\n",
      "        [0.5939],\n",
      "        [0.4368],\n",
      "        [0.6724],\n",
      "        [0.7389],\n",
      "        [0.6710],\n",
      "        [0.0808],\n",
      "        [0.7201],\n",
      "        [0.0808],\n",
      "        [0.2745],\n",
      "        [0.1551],\n",
      "        [0.0808],\n",
      "        [0.3206],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.1231],\n",
      "        [0.7310],\n",
      "        [0.4929],\n",
      "        [0.0808],\n",
      "        [0.6311],\n",
      "        [0.0808],\n",
      "        [0.7010],\n",
      "        [0.3161],\n",
      "        [0.2261],\n",
      "        [0.4806],\n",
      "        [0.6102],\n",
      "        [0.6451],\n",
      "        [0.6088],\n",
      "        [0.0808],\n",
      "        [0.7256],\n",
      "        [0.4771],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.0808],\n",
      "        [0.2684],\n",
      "        [0.6202],\n",
      "        [0.0808],\n",
      "        [0.6362],\n",
      "        [0.3456],\n",
      "        [0.2519],\n",
      "        [0.0808],\n",
      "        [0.4079],\n",
      "        [0.0808],\n",
      "        [0.5666],\n",
      "        [0.5530],\n",
      "        [0.6388],\n",
      "        [0.2661],\n",
      "        [0.0808],\n",
      "        [0.1315],\n",
      "        [0.2498],\n",
      "        [0.2431],\n",
      "        [0.2958],\n",
      "        [0.2848],\n",
      "        [0.0919],\n",
      "        [0.2923],\n",
      "        [0.0808],\n",
      "        [0.5461],\n",
      "        [0.2315],\n",
      "        [0.3729],\n",
      "        [0.1011],\n",
      "        [0.3650],\n",
      "        [0.3454],\n",
      "        [0.2839],\n",
      "        [0.1233],\n",
      "        [0.5532],\n",
      "        [0.2631],\n",
      "        [0.1326],\n",
      "        [0.4309],\n",
      "        [0.3767],\n",
      "        [0.1183],\n",
      "        [0.4224],\n",
      "        [0.3935],\n",
      "        [0.6055]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0009],\n",
      "        [    0.0012],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0017],\n",
      "        [    0.0022],\n",
      "        [    0.0026],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0035],\n",
      "        [    0.0039],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0050],\n",
      "        [    0.0053],\n",
      "        [    0.0054],\n",
      "        [    0.0055],\n",
      "        [    0.0063],\n",
      "        [    0.0076],\n",
      "        [    0.0076],\n",
      "        [    0.0078],\n",
      "        [    0.0078],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0086],\n",
      "        [    0.0097],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0100],\n",
      "        [    0.0105],\n",
      "        [    0.0117],\n",
      "        [    0.0120],\n",
      "        [    0.0125],\n",
      "        [    0.0133],\n",
      "        [    0.0134],\n",
      "        [    0.0135],\n",
      "        [    0.0148],\n",
      "        [    0.0158],\n",
      "        [    0.0165],\n",
      "        [    0.0168],\n",
      "        [    0.0170],\n",
      "        [    0.0176],\n",
      "        [    0.0185],\n",
      "        [    0.0191],\n",
      "        [    0.0196],\n",
      "        [    0.0217],\n",
      "        [    0.0219],\n",
      "        [    0.0228],\n",
      "        [    0.0229],\n",
      "        [    0.0230],\n",
      "        [    0.0235],\n",
      "        [    0.0238],\n",
      "        [    0.0242],\n",
      "        [    0.0244],\n",
      "        [    0.0249],\n",
      "        [    0.0254],\n",
      "        [    0.0260],\n",
      "        [    0.0261],\n",
      "        [    0.0268],\n",
      "        [    0.0269],\n",
      "        [    0.0274],\n",
      "        [    0.0277],\n",
      "        [    0.0278],\n",
      "        [    0.0278],\n",
      "        [    0.0279],\n",
      "        [    0.0280],\n",
      "        [    0.0281],\n",
      "        [    0.0290],\n",
      "        [    0.0296],\n",
      "        [    0.0299],\n",
      "        [    0.0309],\n",
      "        [    0.0309],\n",
      "        [    0.0320],\n",
      "        [    0.0321],\n",
      "        [    0.0321],\n",
      "        [    0.0329],\n",
      "        [    0.0331],\n",
      "        [    0.0332],\n",
      "        [    0.0337],\n",
      "        [    0.0345],\n",
      "        [    0.0349],\n",
      "        [    0.0351],\n",
      "        [    0.0354],\n",
      "        [    0.0355],\n",
      "        [    0.0357],\n",
      "        [    0.0362],\n",
      "        [    0.0366],\n",
      "        [    0.0368],\n",
      "        [    0.0370],\n",
      "        [    0.0370],\n",
      "        [    0.0372],\n",
      "        [    0.0374],\n",
      "        [    0.0379],\n",
      "        [    0.0384],\n",
      "        [    0.0384],\n",
      "        [    0.0389],\n",
      "        [    0.0396],\n",
      "        [    0.0400],\n",
      "        [    0.0402],\n",
      "        [    0.0409],\n",
      "        [    0.0412],\n",
      "        [    0.0413],\n",
      "        [    0.0418],\n",
      "        [    0.0420],\n",
      "        [    0.0424],\n",
      "        [    0.0433],\n",
      "        [    0.0456],\n",
      "        [    0.0477],\n",
      "        [    0.0479],\n",
      "        [    0.0484],\n",
      "        [    0.0494],\n",
      "        [    0.0494],\n",
      "        [    0.0497],\n",
      "        [    0.0507],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0526],\n",
      "        [    0.0531],\n",
      "        [    0.0533],\n",
      "        [    0.0543],\n",
      "        [    0.0555],\n",
      "        [    0.0567],\n",
      "        [    0.0569],\n",
      "        [    0.0572],\n",
      "        [    0.0577],\n",
      "        [    0.0578],\n",
      "        [    0.0592],\n",
      "        [    0.0599],\n",
      "        [    0.0606],\n",
      "        [    0.0622],\n",
      "        [    0.0622],\n",
      "        [    0.0627],\n",
      "        [    0.0646],\n",
      "        [    0.0649],\n",
      "        [    0.0664],\n",
      "        [    0.0696],\n",
      "        [    0.0709],\n",
      "        [    0.0711],\n",
      "        [    0.0781],\n",
      "        [    0.0791],\n",
      "        [    0.0872],\n",
      "        [    0.0912],\n",
      "        [    0.0930],\n",
      "        [    0.0945],\n",
      "        [    0.0964],\n",
      "        [    0.1003],\n",
      "        [    0.1013],\n",
      "        [    0.1045],\n",
      "        [    0.1117],\n",
      "        [    0.1183],\n",
      "        [    0.1357],\n",
      "        [    0.1661],\n",
      "        [    0.1818]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0018],\n",
      "        [    0.0033],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0042],\n",
      "        [    0.0024],\n",
      "        [    0.0004],\n",
      "        [    0.0050],\n",
      "        [    0.0063],\n",
      "        [    0.0059],\n",
      "        [    0.0017],\n",
      "        [    0.0026],\n",
      "        [    0.0049],\n",
      "        [    0.0073],\n",
      "        [    0.0055],\n",
      "        [    0.0078],\n",
      "        [    0.0034],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0058],\n",
      "        [    0.0060],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0061],\n",
      "        [    0.0104],\n",
      "        [    0.0098],\n",
      "        [    0.0096],\n",
      "        [    0.0077],\n",
      "        [    0.0098],\n",
      "        [    0.0132],\n",
      "        [    0.0070],\n",
      "        [    0.0118],\n",
      "        [    0.0139],\n",
      "        [    0.0127],\n",
      "        [    0.0137],\n",
      "        [    0.0119],\n",
      "        [    0.0112],\n",
      "        [    0.0127],\n",
      "        [    0.0175],\n",
      "        [    0.0168],\n",
      "        [    0.0193],\n",
      "        [    0.0172],\n",
      "        [    0.0195],\n",
      "        [    0.0206],\n",
      "        [    0.0169],\n",
      "        [    0.0186],\n",
      "        [    0.0215],\n",
      "        [    0.0221],\n",
      "        [    0.0262],\n",
      "        [    0.0214],\n",
      "        [    0.0246],\n",
      "        [    0.0222],\n",
      "        [    0.0262],\n",
      "        [    0.0263],\n",
      "        [    0.0246],\n",
      "        [    0.0247],\n",
      "        [    0.0271],\n",
      "        [    0.0258],\n",
      "        [    0.0263],\n",
      "        [    0.0270],\n",
      "        [    0.0270],\n",
      "        [    0.0272],\n",
      "        [    0.0268],\n",
      "        [    0.0298],\n",
      "        [    0.0276],\n",
      "        [    0.0259],\n",
      "        [    0.0260],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0298],\n",
      "        [    0.0297],\n",
      "        [    0.0313],\n",
      "        [    0.0288],\n",
      "        [    0.0331],\n",
      "        [    0.0322],\n",
      "        [    0.0338],\n",
      "        [    0.0331],\n",
      "        [    0.0353],\n",
      "        [    0.0316],\n",
      "        [    0.0316],\n",
      "        [    0.0346],\n",
      "        [    0.0329],\n",
      "        [    0.0373],\n",
      "        [    0.0373],\n",
      "        [    0.0339],\n",
      "        [    0.0335],\n",
      "        [    0.0364],\n",
      "        [    0.0348],\n",
      "        [    0.0370],\n",
      "        [    0.0368],\n",
      "        [    0.0367],\n",
      "        [    0.0370],\n",
      "        [    0.0364],\n",
      "        [    0.0377],\n",
      "        [    0.0386],\n",
      "        [    0.0382],\n",
      "        [    0.0393],\n",
      "        [    0.0376],\n",
      "        [    0.0424],\n",
      "        [    0.0400],\n",
      "        [    0.0440],\n",
      "        [    0.0414],\n",
      "        [    0.0391],\n",
      "        [    0.0407],\n",
      "        [    0.0420],\n",
      "        [    0.0447],\n",
      "        [    0.0401],\n",
      "        [    0.0481],\n",
      "        [    0.0456],\n",
      "        [    0.0481],\n",
      "        [    0.0465],\n",
      "        [    0.0514],\n",
      "        [    0.0496],\n",
      "        [    0.0495],\n",
      "        [    0.0505],\n",
      "        [    0.0505],\n",
      "        [    0.0537],\n",
      "        [    0.0524],\n",
      "        [    0.0559],\n",
      "        [    0.0528],\n",
      "        [    0.0544],\n",
      "        [    0.0554],\n",
      "        [    0.0589],\n",
      "        [    0.0570],\n",
      "        [    0.0590],\n",
      "        [    0.0549],\n",
      "        [    0.0602],\n",
      "        [    0.0590],\n",
      "        [    0.0601],\n",
      "        [    0.0610],\n",
      "        [    0.0623],\n",
      "        [    0.0623],\n",
      "        [    0.0621],\n",
      "        [    0.0648],\n",
      "        [    0.0644],\n",
      "        [    0.0658],\n",
      "        [    0.0694],\n",
      "        [    0.0730],\n",
      "        [    0.0712],\n",
      "        [    0.0773],\n",
      "        [    0.0796],\n",
      "        [    0.0864],\n",
      "        [    0.0906],\n",
      "        [    0.0932],\n",
      "        [    0.0953],\n",
      "        [    0.0933],\n",
      "        [    0.1004],\n",
      "        [    0.1009],\n",
      "        [    0.1028],\n",
      "        [    0.1101],\n",
      "        [    0.1178],\n",
      "        [    0.1343],\n",
      "        [    0.1647],\n",
      "        [    0.1784]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.22550773620605\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.280626045438112e-09, 157)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [157, 23, 16, 106, 137, 98, 47, 66, 136, 4, 43, 15, 54, 97, 62, 122, 60, 42, 61, 50, 22, 26, 51, 55, 68, 36, 92, 158, 156, 131, 46, 63, 53, 34, 49, 121, 1, 123, 13, 24, 93, 71, 94, 48, 147, 5, 87, 67, 40, 52, 8, 140, 110, 81, 35, 145, 148, 132, 59, 41, 32, 25, 120, 44, 86, 130, 141, 7, 107, 155, 138, 80, 14, 135, 39, 128, 105, 58, 76, 109, 56, 144, 77, 12, 0, 2, 146, 9, 17, 143, 83, 115, 95, 142, 149, 6, 69, 3, 154, 133, 127, 11, 114, 139, 27, 82, 129, 104, 64, 21, 65, 57, 10, 119, 19, 134, 126, 79, 150, 33, 153, 91, 45, 108, 31, 152, 20, 124, 70, 37, 96, 125, 18, 112, 85, 103, 102, 118, 99, 84, 151, 78, 38, 88, 113, 89, 90, 100, 30, 111, 101, 116, 72, 75, 117, 73, 74, 29, 28] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.0810],\n",
      "        [0.6449],\n",
      "        [0.6582],\n",
      "        [0.2328],\n",
      "        [0.0810],\n",
      "        [0.2637],\n",
      "        [0.6233],\n",
      "        [0.4691],\n",
      "        [0.0810],\n",
      "        [0.7082],\n",
      "        [0.6272],\n",
      "        [0.6625],\n",
      "        [0.6030],\n",
      "        [0.2759],\n",
      "        [0.5317],\n",
      "        [0.0810],\n",
      "        [0.5323],\n",
      "        [0.5748],\n",
      "        [0.5466],\n",
      "        [0.6130],\n",
      "        [0.6353],\n",
      "        [0.5958],\n",
      "        [0.6093],\n",
      "        [0.5984],\n",
      "        [0.4662],\n",
      "        [0.5773],\n",
      "        [0.3727],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6330],\n",
      "        [0.5240],\n",
      "        [0.6176],\n",
      "        [0.5411],\n",
      "        [0.6398],\n",
      "        [0.0810],\n",
      "        [0.7236],\n",
      "        [0.0810],\n",
      "        [0.6602],\n",
      "        [0.6231],\n",
      "        [0.3680],\n",
      "        [0.4265],\n",
      "        [0.3310],\n",
      "        [0.6502],\n",
      "        [0.0810],\n",
      "        [0.6985],\n",
      "        [0.3725],\n",
      "        [0.4680],\n",
      "        [0.5912],\n",
      "        [0.6234],\n",
      "        [0.7164],\n",
      "        [0.0810],\n",
      "        [0.1671],\n",
      "        [0.2849],\n",
      "        [0.5859],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.5739],\n",
      "        [0.5898],\n",
      "        [0.4852],\n",
      "        [0.6098],\n",
      "        [0.0810],\n",
      "        [0.6504],\n",
      "        [0.3367],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6780],\n",
      "        [0.2274],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.2797],\n",
      "        [0.6699],\n",
      "        [0.0810],\n",
      "        [0.5890],\n",
      "        [0.0810],\n",
      "        [0.2230],\n",
      "        [0.5874],\n",
      "        [0.2991],\n",
      "        [0.1913],\n",
      "        [0.5919],\n",
      "        [0.0810],\n",
      "        [0.2578],\n",
      "        [0.6688],\n",
      "        [0.7005],\n",
      "        [0.7373],\n",
      "        [0.0810],\n",
      "        [0.7184],\n",
      "        [0.6441],\n",
      "        [0.0810],\n",
      "        [0.3216],\n",
      "        [0.1548],\n",
      "        [0.2746],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6705],\n",
      "        [0.4346],\n",
      "        [0.7290],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6989],\n",
      "        [0.1227],\n",
      "        [0.0810],\n",
      "        [0.6070],\n",
      "        [0.3172],\n",
      "        [0.0810],\n",
      "        [0.2261],\n",
      "        [0.4906],\n",
      "        [0.6281],\n",
      "        [0.4783],\n",
      "        [0.6067],\n",
      "        [0.7237],\n",
      "        [0.0810],\n",
      "        [0.6426],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.2688],\n",
      "        [0.0810],\n",
      "        [0.4752],\n",
      "        [0.0810],\n",
      "        [0.3462],\n",
      "        [0.6180],\n",
      "        [0.2519],\n",
      "        [0.5503],\n",
      "        [0.0810],\n",
      "        [0.6334],\n",
      "        [0.0810],\n",
      "        [0.4056],\n",
      "        [0.5648],\n",
      "        [0.2662],\n",
      "        [0.0810],\n",
      "        [0.6364],\n",
      "        [0.1311],\n",
      "        [0.2965],\n",
      "        [0.2431],\n",
      "        [0.2499],\n",
      "        [0.0915],\n",
      "        [0.2849],\n",
      "        [0.2929],\n",
      "        [0.0810],\n",
      "        [0.2315],\n",
      "        [0.5440],\n",
      "        [0.3738],\n",
      "        [0.1007],\n",
      "        [0.3658],\n",
      "        [0.3459],\n",
      "        [0.2840],\n",
      "        [0.5502],\n",
      "        [0.1226],\n",
      "        [0.2631],\n",
      "        [0.1322],\n",
      "        [0.4292],\n",
      "        [0.3751],\n",
      "        [0.1178],\n",
      "        [0.4211],\n",
      "        [0.3921],\n",
      "        [0.6021],\n",
      "        [0.6104]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0015],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0018],\n",
      "        [    0.0024],\n",
      "        [    0.0026],\n",
      "        [    0.0033],\n",
      "        [    0.0034],\n",
      "        [    0.0042],\n",
      "        [    0.0049],\n",
      "        [    0.0050],\n",
      "        [    0.0055],\n",
      "        [    0.0058],\n",
      "        [    0.0059],\n",
      "        [    0.0060],\n",
      "        [    0.0061],\n",
      "        [    0.0063],\n",
      "        [    0.0070],\n",
      "        [    0.0073],\n",
      "        [    0.0077],\n",
      "        [    0.0078],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0101],\n",
      "        [    0.0104],\n",
      "        [    0.0112],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0127],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0137],\n",
      "        [    0.0139],\n",
      "        [    0.0168],\n",
      "        [    0.0169],\n",
      "        [    0.0172],\n",
      "        [    0.0175],\n",
      "        [    0.0186],\n",
      "        [    0.0193],\n",
      "        [    0.0195],\n",
      "        [    0.0206],\n",
      "        [    0.0214],\n",
      "        [    0.0215],\n",
      "        [    0.0221],\n",
      "        [    0.0222],\n",
      "        [    0.0246],\n",
      "        [    0.0246],\n",
      "        [    0.0247],\n",
      "        [    0.0258],\n",
      "        [    0.0259],\n",
      "        [    0.0260],\n",
      "        [    0.0262],\n",
      "        [    0.0262],\n",
      "        [    0.0263],\n",
      "        [    0.0263],\n",
      "        [    0.0268],\n",
      "        [    0.0270],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0272],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0288],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0298],\n",
      "        [    0.0313],\n",
      "        [    0.0316],\n",
      "        [    0.0316],\n",
      "        [    0.0322],\n",
      "        [    0.0329],\n",
      "        [    0.0331],\n",
      "        [    0.0331],\n",
      "        [    0.0335],\n",
      "        [    0.0338],\n",
      "        [    0.0339],\n",
      "        [    0.0346],\n",
      "        [    0.0348],\n",
      "        [    0.0353],\n",
      "        [    0.0364],\n",
      "        [    0.0364],\n",
      "        [    0.0367],\n",
      "        [    0.0368],\n",
      "        [    0.0370],\n",
      "        [    0.0370],\n",
      "        [    0.0373],\n",
      "        [    0.0373],\n",
      "        [    0.0376],\n",
      "        [    0.0377],\n",
      "        [    0.0382],\n",
      "        [    0.0386],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.0400],\n",
      "        [    0.0401],\n",
      "        [    0.0407],\n",
      "        [    0.0414],\n",
      "        [    0.0420],\n",
      "        [    0.0424],\n",
      "        [    0.0440],\n",
      "        [    0.0447],\n",
      "        [    0.0456],\n",
      "        [    0.0465],\n",
      "        [    0.0481],\n",
      "        [    0.0481],\n",
      "        [    0.0495],\n",
      "        [    0.0496],\n",
      "        [    0.0505],\n",
      "        [    0.0505],\n",
      "        [    0.0514],\n",
      "        [    0.0524],\n",
      "        [    0.0528],\n",
      "        [    0.0537],\n",
      "        [    0.0544],\n",
      "        [    0.0549],\n",
      "        [    0.0554],\n",
      "        [    0.0559],\n",
      "        [    0.0570],\n",
      "        [    0.0589],\n",
      "        [    0.0590],\n",
      "        [    0.0590],\n",
      "        [    0.0601],\n",
      "        [    0.0602],\n",
      "        [    0.0610],\n",
      "        [    0.0621],\n",
      "        [    0.0623],\n",
      "        [    0.0623],\n",
      "        [    0.0644],\n",
      "        [    0.0648],\n",
      "        [    0.0658],\n",
      "        [    0.0694],\n",
      "        [    0.0712],\n",
      "        [    0.0730],\n",
      "        [    0.0773],\n",
      "        [    0.0796],\n",
      "        [    0.0864],\n",
      "        [    0.0906],\n",
      "        [    0.0932],\n",
      "        [    0.0933],\n",
      "        [    0.0953],\n",
      "        [    0.1004],\n",
      "        [    0.1009],\n",
      "        [    0.1028],\n",
      "        [    0.1101],\n",
      "        [    0.1178],\n",
      "        [    0.1343],\n",
      "        [    0.1647],\n",
      "        [    0.1784],\n",
      "        [    0.1900]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0043],\n",
      "        [    0.0034],\n",
      "        [    0.0004],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0004],\n",
      "        [    0.0044],\n",
      "        [    0.0024],\n",
      "        [    0.0001],\n",
      "        [    0.0059],\n",
      "        [    0.0007],\n",
      "        [    0.0061],\n",
      "        [    0.0051],\n",
      "        [    0.0069],\n",
      "        [    0.0055],\n",
      "        [    0.0043],\n",
      "        [    0.0084],\n",
      "        [    0.0044],\n",
      "        [    0.0036],\n",
      "        [    0.0102],\n",
      "        [    0.0027],\n",
      "        [    0.0096],\n",
      "        [    0.0057],\n",
      "        [    0.0102],\n",
      "        [    0.0095],\n",
      "        [    0.0088],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0088],\n",
      "        [    0.0119],\n",
      "        [    0.0098],\n",
      "        [    0.0127],\n",
      "        [    0.0100],\n",
      "        [    0.0172],\n",
      "        [    0.0141],\n",
      "        [    0.0157],\n",
      "        [    0.0172],\n",
      "        [    0.0147],\n",
      "        [    0.0172],\n",
      "        [    0.0199],\n",
      "        [    0.0174],\n",
      "        [    0.0218],\n",
      "        [    0.0214],\n",
      "        [    0.0226],\n",
      "        [    0.0192],\n",
      "        [    0.0215],\n",
      "        [    0.0222],\n",
      "        [    0.0206],\n",
      "        [    0.0263],\n",
      "        [    0.0246],\n",
      "        [    0.0247],\n",
      "        [    0.0258],\n",
      "        [    0.0241],\n",
      "        [    0.0241],\n",
      "        [    0.0290],\n",
      "        [    0.0304],\n",
      "        [    0.0263],\n",
      "        [    0.0285],\n",
      "        [    0.0257],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0293],\n",
      "        [    0.0270],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0268],\n",
      "        [    0.0260],\n",
      "        [    0.0297],\n",
      "        [    0.0318],\n",
      "        [    0.0298],\n",
      "        [    0.0318],\n",
      "        [    0.0296],\n",
      "        [    0.0301],\n",
      "        [    0.0323],\n",
      "        [    0.0310],\n",
      "        [    0.0331],\n",
      "        [    0.0342],\n",
      "        [    0.0305],\n",
      "        [    0.0359],\n",
      "        [    0.0317],\n",
      "        [    0.0347],\n",
      "        [    0.0324],\n",
      "        [    0.0382],\n",
      "        [    0.0364],\n",
      "        [    0.0352],\n",
      "        [    0.0365],\n",
      "        [    0.0366],\n",
      "        [    0.0370],\n",
      "        [    0.0370],\n",
      "        [    0.0397],\n",
      "        [    0.0394],\n",
      "        [    0.0350],\n",
      "        [    0.0377],\n",
      "        [    0.0382],\n",
      "        [    0.0386],\n",
      "        [    0.0363],\n",
      "        [    0.0397],\n",
      "        [    0.0400],\n",
      "        [    0.0362],\n",
      "        [    0.0393],\n",
      "        [    0.0414],\n",
      "        [    0.0420],\n",
      "        [    0.0447],\n",
      "        [    0.0477],\n",
      "        [    0.0470],\n",
      "        [    0.0436],\n",
      "        [    0.0439],\n",
      "        [    0.0481],\n",
      "        [    0.0513],\n",
      "        [    0.0495],\n",
      "        [    0.0496],\n",
      "        [    0.0499],\n",
      "        [    0.0505],\n",
      "        [    0.0537],\n",
      "        [    0.0524],\n",
      "        [    0.0523],\n",
      "        [    0.0559],\n",
      "        [    0.0544],\n",
      "        [    0.0516],\n",
      "        [    0.0553],\n",
      "        [    0.0594],\n",
      "        [    0.0571],\n",
      "        [    0.0611],\n",
      "        [    0.0610],\n",
      "        [    0.0588],\n",
      "        [    0.0601],\n",
      "        [    0.0633],\n",
      "        [    0.0614],\n",
      "        [    0.0612],\n",
      "        [    0.0624],\n",
      "        [    0.0625],\n",
      "        [    0.0640],\n",
      "        [    0.0650],\n",
      "        [    0.0650],\n",
      "        [    0.0694],\n",
      "        [    0.0711],\n",
      "        [    0.0752],\n",
      "        [    0.0763],\n",
      "        [    0.0799],\n",
      "        [    0.0856],\n",
      "        [    0.0900],\n",
      "        [    0.0934],\n",
      "        [    0.0897],\n",
      "        [    0.0960],\n",
      "        [    0.1005],\n",
      "        [    0.1005],\n",
      "        [    0.1013],\n",
      "        [    0.1086],\n",
      "        [    0.1174],\n",
      "        [    0.1332],\n",
      "        [    0.1634],\n",
      "        [    0.1743],\n",
      "        [    0.1861]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.51254153251648\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 23 個區塊累積花費時間(s) 1.2113134860992432\n",
      "<<The performance of 23 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2113134860992432\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 973.36\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1278.69\n",
      "The accuracy(2000) for l = 1: 90.57%\n",
      "The accuracy(3000) for l = 1: 97.48%\n",
      "The maximum error: tensor(4751.2891)\n",
      "The minimum error: tensor(2.4609)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 985.5\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1069.1\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1331.04296875\n",
      "The minimum error: 309.04296875\n",
      "------------------------------------------------------------\n",
      "0.9056603773584906\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<24>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.289241376109203e-09, 0)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [0, 153, 43, 102, 11, 133, 94, 132, 22, 12, 46, 56, 19, 62, 57, 93, 118, 51, 39, 50, 58, 38, 88, 45, 32, 47, 154, 152, 127, 9, 64, 18, 117, 42, 59, 49, 155, 30, 119, 89, 44, 67, 90, 143, 20, 83, 4, 1, 77, 36, 136, 63, 106, 48, 55, 37, 141, 144, 82, 128, 10, 116, 31, 76, 103, 126, 137, 151, 134, 40, 28, 3, 54, 131, 124, 72, 21, 8, 52, 35, 101, 105, 5, 140, 73, 142, 79, 23, 7, 139, 111, 91, 138, 145, 150, 129, 13, 123, 156, 78, 65, 110, 2, 135, 125, 100, 53, 6, 60, 61, 17, 115, 130, 122, 75, 146, 15, 158, 27, 157, 87, 149, 29, 104, 148, 41, 120, 92, 16, 121, 33, 66, 81, 108, 99, 98, 14, 114, 95, 80, 147, 74, 34, 84, 109, 85, 26, 86, 96, 107, 112, 97, 68, 71, 113]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0024],\n",
      "        [    0.0027],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0043],\n",
      "        [    0.0043],\n",
      "        [    0.0044],\n",
      "        [    0.0044],\n",
      "        [    0.0051],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0061],\n",
      "        [    0.0069],\n",
      "        [    0.0084],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0095],\n",
      "        [    0.0096],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0102],\n",
      "        [    0.0119],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0121],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0127],\n",
      "        [    0.0141],\n",
      "        [    0.0147],\n",
      "        [    0.0157],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0192],\n",
      "        [    0.0199],\n",
      "        [    0.0206],\n",
      "        [    0.0214],\n",
      "        [    0.0215],\n",
      "        [    0.0218],\n",
      "        [    0.0222],\n",
      "        [    0.0226],\n",
      "        [    0.0241],\n",
      "        [    0.0241],\n",
      "        [    0.0246],\n",
      "        [    0.0247],\n",
      "        [    0.0257],\n",
      "        [    0.0258],\n",
      "        [    0.0260],\n",
      "        [    0.0263],\n",
      "        [    0.0263],\n",
      "        [    0.0268],\n",
      "        [    0.0270],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0285],\n",
      "        [    0.0290],\n",
      "        [    0.0293],\n",
      "        [    0.0296],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0301],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0310],\n",
      "        [    0.0318],\n",
      "        [    0.0318],\n",
      "        [    0.0323],\n",
      "        [    0.0324],\n",
      "        [    0.0331],\n",
      "        [    0.0342],\n",
      "        [    0.0347],\n",
      "        [    0.0352],\n",
      "        [    0.0362],\n",
      "        [    0.0363],\n",
      "        [    0.0364],\n",
      "        [    0.0365],\n",
      "        [    0.0366],\n",
      "        [    0.0370],\n",
      "        [    0.0370],\n",
      "        [    0.0377],\n",
      "        [    0.0382],\n",
      "        [    0.0382],\n",
      "        [    0.0386],\n",
      "        [    0.0386],\n",
      "        [    0.0393],\n",
      "        [    0.0394],\n",
      "        [    0.0397],\n",
      "        [    0.0397],\n",
      "        [    0.0400],\n",
      "        [    0.0414],\n",
      "        [    0.0420],\n",
      "        [    0.0436],\n",
      "        [    0.0439],\n",
      "        [    0.0447],\n",
      "        [    0.0470],\n",
      "        [    0.0477],\n",
      "        [    0.0481],\n",
      "        [    0.0495],\n",
      "        [    0.0496],\n",
      "        [    0.0499],\n",
      "        [    0.0505],\n",
      "        [    0.0513],\n",
      "        [    0.0516],\n",
      "        [    0.0516],\n",
      "        [    0.0521],\n",
      "        [    0.0523],\n",
      "        [    0.0524],\n",
      "        [    0.0537],\n",
      "        [    0.0544],\n",
      "        [    0.0553],\n",
      "        [    0.0559],\n",
      "        [    0.0571],\n",
      "        [    0.0588],\n",
      "        [    0.0594],\n",
      "        [    0.0601],\n",
      "        [    0.0610],\n",
      "        [    0.0611],\n",
      "        [    0.0612],\n",
      "        [    0.0614],\n",
      "        [    0.0624],\n",
      "        [    0.0625],\n",
      "        [    0.0633],\n",
      "        [    0.0640],\n",
      "        [    0.0650],\n",
      "        [    0.0650],\n",
      "        [    0.0694],\n",
      "        [    0.0711],\n",
      "        [    0.0752],\n",
      "        [    0.0763],\n",
      "        [    0.0799],\n",
      "        [    0.0856],\n",
      "        [    0.0897],\n",
      "        [    0.0900],\n",
      "        [    0.0934],\n",
      "        [    0.0960],\n",
      "        [    0.1005],\n",
      "        [    0.1005],\n",
      "        [    0.1013],\n",
      "        [    0.1086],\n",
      "        [    0.1174]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.289241376109203e-09, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [0, 153, 43, 102, 11, 133, 94, 132, 22, 12, 46, 56, 19, 62, 57, 93, 118, 51, 39, 50, 58, 38, 88, 45, 32, 47, 154, 152, 127, 9, 64, 18, 117, 42, 59, 49, 155, 30, 119, 89, 44, 67, 90, 143, 20, 83, 4, 1, 77, 36, 136, 63, 106, 48, 55, 37, 141, 144, 82, 128, 10, 116, 31, 76, 103, 126, 137, 151, 134, 40, 28, 3, 54, 131, 124, 72, 21, 8, 52, 35, 101, 105, 5, 140, 73, 142, 79, 23, 7, 139, 111, 91, 138, 145, 150, 129, 13, 123, 156, 78, 65, 110, 2, 135, 125, 100, 53, 6, 60, 61, 17, 115, 130, 122, 75, 146, 15, 158, 27, 157, 87, 149, 29, 104, 148, 41, 120, 92, 16, 121, 33, 66, 81, 108, 99, 98, 14, 114, 95, 80, 147, 74, 34, 84, 109, 85, 26, 86, 96, 107, 112, 97, 68, 71, 113, 69] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7055],\n",
      "        [0.0810],\n",
      "        [0.6212],\n",
      "        [0.2332],\n",
      "        [0.6598],\n",
      "        [0.0810],\n",
      "        [0.2639],\n",
      "        [0.0810],\n",
      "        [0.5915],\n",
      "        [0.6556],\n",
      "        [0.6106],\n",
      "        [0.5307],\n",
      "        [0.6410],\n",
      "        [0.4666],\n",
      "        [0.5450],\n",
      "        [0.2761],\n",
      "        [0.0810],\n",
      "        [0.5963],\n",
      "        [0.6247],\n",
      "        [0.6012],\n",
      "        [0.5297],\n",
      "        [0.5723],\n",
      "        [0.3733],\n",
      "        [0.6375],\n",
      "        [0.5756],\n",
      "        [0.6070],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6574],\n",
      "        [0.4638],\n",
      "        [0.6314],\n",
      "        [0.0810],\n",
      "        [0.6310],\n",
      "        [0.5219],\n",
      "        [0.6157],\n",
      "        [0.0810],\n",
      "        [0.5390],\n",
      "        [0.0810],\n",
      "        [0.3684],\n",
      "        [0.6480],\n",
      "        [0.4247],\n",
      "        [0.3314],\n",
      "        [0.0810],\n",
      "        [0.6192],\n",
      "        [0.3737],\n",
      "        [0.7142],\n",
      "        [0.6962],\n",
      "        [0.2865],\n",
      "        [0.5893],\n",
      "        [0.0810],\n",
      "        [0.4655],\n",
      "        [0.1669],\n",
      "        [0.6213],\n",
      "        [0.5720],\n",
      "        [0.5879],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.3377],\n",
      "        [0.0810],\n",
      "        [0.6671],\n",
      "        [0.0810],\n",
      "        [0.5842],\n",
      "        [0.2810],\n",
      "        [0.2276],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6482],\n",
      "        [0.4823],\n",
      "        [0.6757],\n",
      "        [0.5855],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.2976],\n",
      "        [0.6055],\n",
      "        [0.6658],\n",
      "        [0.5899],\n",
      "        [0.5870],\n",
      "        [0.2235],\n",
      "        [0.1914],\n",
      "        [0.7159],\n",
      "        [0.0810],\n",
      "        [0.2568],\n",
      "        [0.0810],\n",
      "        [0.3228],\n",
      "        [0.6031],\n",
      "        [0.6960],\n",
      "        [0.0810],\n",
      "        [0.1546],\n",
      "        [0.2749],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.6412],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.3186],\n",
      "        [0.4325],\n",
      "        [0.1224],\n",
      "        [0.6681],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.2261],\n",
      "        [0.6047],\n",
      "        [0.7211],\n",
      "        [0.4883],\n",
      "        [0.4761],\n",
      "        [0.6243],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.0810],\n",
      "        [0.2693],\n",
      "        [0.0810],\n",
      "        [0.6394],\n",
      "        [0.0810],\n",
      "        [0.5470],\n",
      "        [0.0810],\n",
      "        [0.3467],\n",
      "        [0.0810],\n",
      "        [0.4728],\n",
      "        [0.2519],\n",
      "        [0.0810],\n",
      "        [0.6158],\n",
      "        [0.0810],\n",
      "        [0.2665],\n",
      "        [0.6299],\n",
      "        [0.0810],\n",
      "        [0.5629],\n",
      "        [0.4035],\n",
      "        [0.2973],\n",
      "        [0.1307],\n",
      "        [0.2432],\n",
      "        [0.2501],\n",
      "        [0.6333],\n",
      "        [0.0910],\n",
      "        [0.2851],\n",
      "        [0.2937],\n",
      "        [0.0810],\n",
      "        [0.2316],\n",
      "        [0.5419],\n",
      "        [0.3747],\n",
      "        [0.1003],\n",
      "        [0.3666],\n",
      "        [0.5465],\n",
      "        [0.3465],\n",
      "        [0.2842],\n",
      "        [0.1218],\n",
      "        [0.1318],\n",
      "        [0.2633],\n",
      "        [0.4277],\n",
      "        [0.3735],\n",
      "        [0.1174],\n",
      "        [0.4199]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0015],\n",
      "        [    0.0017],\n",
      "        [    0.0024],\n",
      "        [    0.0027],\n",
      "        [    0.0034],\n",
      "        [    0.0036],\n",
      "        [    0.0043],\n",
      "        [    0.0043],\n",
      "        [    0.0044],\n",
      "        [    0.0044],\n",
      "        [    0.0051],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0061],\n",
      "        [    0.0069],\n",
      "        [    0.0084],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0095],\n",
      "        [    0.0096],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0102],\n",
      "        [    0.0119],\n",
      "        [    0.0120],\n",
      "        [    0.0121],\n",
      "        [    0.0121],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0127],\n",
      "        [    0.0141],\n",
      "        [    0.0147],\n",
      "        [    0.0157],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0192],\n",
      "        [    0.0199],\n",
      "        [    0.0206],\n",
      "        [    0.0214],\n",
      "        [    0.0215],\n",
      "        [    0.0218],\n",
      "        [    0.0222],\n",
      "        [    0.0226],\n",
      "        [    0.0241],\n",
      "        [    0.0241],\n",
      "        [    0.0246],\n",
      "        [    0.0247],\n",
      "        [    0.0257],\n",
      "        [    0.0258],\n",
      "        [    0.0260],\n",
      "        [    0.0263],\n",
      "        [    0.0263],\n",
      "        [    0.0268],\n",
      "        [    0.0270],\n",
      "        [    0.0270],\n",
      "        [    0.0271],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0285],\n",
      "        [    0.0290],\n",
      "        [    0.0293],\n",
      "        [    0.0296],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0301],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0310],\n",
      "        [    0.0318],\n",
      "        [    0.0318],\n",
      "        [    0.0323],\n",
      "        [    0.0324],\n",
      "        [    0.0331],\n",
      "        [    0.0342],\n",
      "        [    0.0347],\n",
      "        [    0.0352],\n",
      "        [    0.0362],\n",
      "        [    0.0363],\n",
      "        [    0.0364],\n",
      "        [    0.0365],\n",
      "        [    0.0366],\n",
      "        [    0.0370],\n",
      "        [    0.0370],\n",
      "        [    0.0377],\n",
      "        [    0.0382],\n",
      "        [    0.0382],\n",
      "        [    0.0386],\n",
      "        [    0.0386],\n",
      "        [    0.0393],\n",
      "        [    0.0394],\n",
      "        [    0.0397],\n",
      "        [    0.0397],\n",
      "        [    0.0400],\n",
      "        [    0.0414],\n",
      "        [    0.0420],\n",
      "        [    0.0436],\n",
      "        [    0.0439],\n",
      "        [    0.0447],\n",
      "        [    0.0470],\n",
      "        [    0.0477],\n",
      "        [    0.0481],\n",
      "        [    0.0495],\n",
      "        [    0.0496],\n",
      "        [    0.0499],\n",
      "        [    0.0505],\n",
      "        [    0.0513],\n",
      "        [    0.0516],\n",
      "        [    0.0516],\n",
      "        [    0.0521],\n",
      "        [    0.0523],\n",
      "        [    0.0524],\n",
      "        [    0.0537],\n",
      "        [    0.0544],\n",
      "        [    0.0553],\n",
      "        [    0.0559],\n",
      "        [    0.0571],\n",
      "        [    0.0588],\n",
      "        [    0.0594],\n",
      "        [    0.0601],\n",
      "        [    0.0610],\n",
      "        [    0.0611],\n",
      "        [    0.0612],\n",
      "        [    0.0614],\n",
      "        [    0.0624],\n",
      "        [    0.0625],\n",
      "        [    0.0633],\n",
      "        [    0.0640],\n",
      "        [    0.0650],\n",
      "        [    0.0650],\n",
      "        [    0.0694],\n",
      "        [    0.0711],\n",
      "        [    0.0752],\n",
      "        [    0.0763],\n",
      "        [    0.0799],\n",
      "        [    0.0856],\n",
      "        [    0.0897],\n",
      "        [    0.0900],\n",
      "        [    0.0934],\n",
      "        [    0.0960],\n",
      "        [    0.1005],\n",
      "        [    0.1005],\n",
      "        [    0.1013],\n",
      "        [    0.1086],\n",
      "        [    0.1174],\n",
      "        [    0.1332]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0079],\n",
      "        [    0.0033],\n",
      "        [    0.0066],\n",
      "        [    0.0012],\n",
      "        [    0.0085],\n",
      "        [    0.0018],\n",
      "        [    0.0038],\n",
      "        [    0.0008],\n",
      "        [    0.0085],\n",
      "        [    0.0043],\n",
      "        [    0.0102],\n",
      "        [    0.0100],\n",
      "        [    0.0025],\n",
      "        [    0.0001],\n",
      "        [    0.0103],\n",
      "        [    0.0075],\n",
      "        [    0.0087],\n",
      "        [    0.0123],\n",
      "        [    0.0010],\n",
      "        [    0.0007],\n",
      "        [    0.0013],\n",
      "        [    0.0023],\n",
      "        [    0.0127],\n",
      "        [    0.0158],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0064],\n",
      "        [    0.0065],\n",
      "        [    0.0131],\n",
      "        [    0.0175],\n",
      "        [    0.0057],\n",
      "        [    0.0034],\n",
      "        [    0.0151],\n",
      "        [    0.0049],\n",
      "        [    0.0068],\n",
      "        [    0.0051],\n",
      "        [    0.0089],\n",
      "        [    0.0063],\n",
      "        [    0.0160],\n",
      "        [    0.0179],\n",
      "        [    0.0219],\n",
      "        [    0.0116],\n",
      "        [    0.0204],\n",
      "        [    0.0204],\n",
      "        [    0.0108],\n",
      "        [    0.0132],\n",
      "        [    0.0276],\n",
      "        [    0.0118],\n",
      "        [    0.0168],\n",
      "        [    0.0147],\n",
      "        [    0.0183],\n",
      "        [    0.0173],\n",
      "        [    0.0219],\n",
      "        [    0.0156],\n",
      "        [    0.0302],\n",
      "        [    0.0307],\n",
      "        [    0.0279],\n",
      "        [    0.0215],\n",
      "        [    0.0219],\n",
      "        [    0.0225],\n",
      "        [    0.0337],\n",
      "        [    0.0295],\n",
      "        [    0.0195],\n",
      "        [    0.0232],\n",
      "        [    0.0255],\n",
      "        [    0.0303],\n",
      "        [    0.0303],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0211],\n",
      "        [    0.0240],\n",
      "        [    0.0216],\n",
      "        [    0.0360],\n",
      "        [    0.0264],\n",
      "        [    0.0330],\n",
      "        [    0.0327],\n",
      "        [    0.0243],\n",
      "        [    0.0382],\n",
      "        [    0.0375],\n",
      "        [    0.0250],\n",
      "        [    0.0334],\n",
      "        [    0.0330],\n",
      "        [    0.0408],\n",
      "        [    0.0363],\n",
      "        [    0.0319],\n",
      "        [    0.0379],\n",
      "        [    0.0314],\n",
      "        [    0.0424],\n",
      "        [    0.0444],\n",
      "        [    0.0396],\n",
      "        [    0.0368],\n",
      "        [    0.0342],\n",
      "        [    0.0402],\n",
      "        [    0.0338],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0309],\n",
      "        [    0.0418],\n",
      "        [    0.0354],\n",
      "        [    0.0353],\n",
      "        [    0.0353],\n",
      "        [    0.0398],\n",
      "        [    0.0321],\n",
      "        [    0.0368],\n",
      "        [    0.0446],\n",
      "        [    0.0434],\n",
      "        [    0.0504],\n",
      "        [    0.0525],\n",
      "        [    0.0400],\n",
      "        [    0.0423],\n",
      "        [    0.0412],\n",
      "        [    0.0514],\n",
      "        [    0.0462],\n",
      "        [    0.0529],\n",
      "        [    0.0466],\n",
      "        [    0.0473],\n",
      "        [    0.0443],\n",
      "        [    0.0484],\n",
      "        [    0.0573],\n",
      "        [    0.0489],\n",
      "        [    0.0486],\n",
      "        [    0.0491],\n",
      "        [    0.0486],\n",
      "        [    0.0560],\n",
      "        [    0.0521],\n",
      "        [    0.0491],\n",
      "        [    0.0603],\n",
      "        [    0.0564],\n",
      "        [    0.0526],\n",
      "        [    0.0633],\n",
      "        [    0.0546],\n",
      "        [    0.0574],\n",
      "        [    0.0580],\n",
      "        [    0.0616],\n",
      "        [    0.0640],\n",
      "        [    0.0643],\n",
      "        [    0.0562],\n",
      "        [    0.0635],\n",
      "        [    0.0674],\n",
      "        [    0.0618],\n",
      "        [    0.0661],\n",
      "        [    0.0685],\n",
      "        [    0.0693],\n",
      "        [    0.0722],\n",
      "        [    0.0805],\n",
      "        [    0.0815],\n",
      "        [    0.0951],\n",
      "        [    0.0864],\n",
      "        [    0.0958],\n",
      "        [    0.0967],\n",
      "        [    0.1003],\n",
      "        [    0.1025],\n",
      "        [    0.1057],\n",
      "        [    0.1123],\n",
      "        [    0.1171],\n",
      "        [    0.1376]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.0944344997406\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4546771787138368e-08, 62)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [62, 50, 132, 39, 102, 58, 133, 38, 19, 32, 47, 153, 18, 94, 12, 42, 49, 64, 30, 154, 152, 43, 59, 93, 0, 11, 22, 118, 155, 56, 46, 57, 20, 67, 1, 51, 88, 127, 83, 36, 117, 48, 45, 119, 77, 63, 9, 89, 136, 31, 90, 143, 40, 144, 3, 106, 82, 44, 128, 76, 28, 21, 151, 134, 35, 103, 131, 4, 141, 116, 55, 126, 137, 37, 13, 79, 73, 2, 72, 124, 105, 101, 10, 145, 91, 150, 129, 65, 78, 156, 54, 140, 135, 111, 52, 142, 8, 139, 110, 60, 138, 5, 17, 123, 61, 23, 100, 15, 7, 125, 130, 75, 146, 158, 29, 87, 157, 41, 149, 53, 115, 148, 6, 16, 122, 33, 104, 14, 92, 27, 66, 81, 120, 108, 80, 121, 114, 99, 98, 147, 95, 74, 34, 84, 109, 85, 86, 26, 96, 107, 112, 97, 68, 71, 113, 69, 70] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4711],\n",
      "        [0.6079],\n",
      "        [0.0842],\n",
      "        [0.6316],\n",
      "        [0.2348],\n",
      "        [0.5353],\n",
      "        [0.0842],\n",
      "        [0.5784],\n",
      "        [0.6479],\n",
      "        [0.5823],\n",
      "        [0.6137],\n",
      "        [0.0842],\n",
      "        [0.6381],\n",
      "        [0.2661],\n",
      "        [0.6633],\n",
      "        [0.6381],\n",
      "        [0.6226],\n",
      "        [0.4683],\n",
      "        [0.5451],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.6282],\n",
      "        [0.5272],\n",
      "        [0.2785],\n",
      "        [0.7136],\n",
      "        [0.6676],\n",
      "        [0.5974],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.5364],\n",
      "        [0.6172],\n",
      "        [0.5509],\n",
      "        [0.6255],\n",
      "        [0.4289],\n",
      "        [0.7043],\n",
      "        [0.6029],\n",
      "        [0.3772],\n",
      "        [0.0842],\n",
      "        [0.3779],\n",
      "        [0.5960],\n",
      "        [0.0842],\n",
      "        [0.6284],\n",
      "        [0.6444],\n",
      "        [0.0842],\n",
      "        [0.2903],\n",
      "        [0.4699],\n",
      "        [0.6650],\n",
      "        [0.3721],\n",
      "        [0.0842],\n",
      "        [0.5910],\n",
      "        [0.3346],\n",
      "        [0.0842],\n",
      "        [0.6556],\n",
      "        [0.0842],\n",
      "        [0.6834],\n",
      "        [0.1672],\n",
      "        [0.3415],\n",
      "        [0.6553],\n",
      "        [0.0842],\n",
      "        [0.2846],\n",
      "        [0.4873],\n",
      "        [0.6117],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.5937],\n",
      "        [0.2290],\n",
      "        [0.0842],\n",
      "        [0.7226],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.5782],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.5945],\n",
      "        [0.6486],\n",
      "        [0.3266],\n",
      "        [0.2591],\n",
      "        [0.6757],\n",
      "        [0.3002],\n",
      "        [0.0842],\n",
      "        [0.1921],\n",
      "        [0.2250],\n",
      "        [0.6749],\n",
      "        [0.0842],\n",
      "        [0.2772],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.4366],\n",
      "        [0.3225],\n",
      "        [0.0842],\n",
      "        [0.5919],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.1549],\n",
      "        [0.5965],\n",
      "        [0.0842],\n",
      "        [0.6735],\n",
      "        [0.0842],\n",
      "        [0.1223],\n",
      "        [0.4930],\n",
      "        [0.0842],\n",
      "        [0.7243],\n",
      "        [0.6308],\n",
      "        [0.0842],\n",
      "        [0.4807],\n",
      "        [0.6093],\n",
      "        [0.2274],\n",
      "        [0.6464],\n",
      "        [0.7041],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.2727],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.4779],\n",
      "        [0.3503],\n",
      "        [0.0842],\n",
      "        [0.6226],\n",
      "        [0.0842],\n",
      "        [0.6115],\n",
      "        [0.0842],\n",
      "        [0.0842],\n",
      "        [0.7296],\n",
      "        [0.6366],\n",
      "        [0.0842],\n",
      "        [0.5692],\n",
      "        [0.2535],\n",
      "        [0.6404],\n",
      "        [0.2688],\n",
      "        [0.5526],\n",
      "        [0.4071],\n",
      "        [0.3006],\n",
      "        [0.0842],\n",
      "        [0.1305],\n",
      "        [0.2969],\n",
      "        [0.0842],\n",
      "        [0.0905],\n",
      "        [0.2448],\n",
      "        [0.2520],\n",
      "        [0.0842],\n",
      "        [0.2875],\n",
      "        [0.2342],\n",
      "        [0.5478],\n",
      "        [0.3788],\n",
      "        [0.0997],\n",
      "        [0.3707],\n",
      "        [0.3501],\n",
      "        [0.5520],\n",
      "        [0.2867],\n",
      "        [0.1211],\n",
      "        [0.1316],\n",
      "        [0.2653],\n",
      "        [0.4320],\n",
      "        [0.3772],\n",
      "        [0.1171],\n",
      "        [0.4244],\n",
      "        [0.3948]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0007],\n",
      "        [    0.0008],\n",
      "        [    0.0010],\n",
      "        [    0.0012],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0023],\n",
      "        [    0.0025],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0033],\n",
      "        [    0.0034],\n",
      "        [    0.0038],\n",
      "        [    0.0043],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0057],\n",
      "        [    0.0063],\n",
      "        [    0.0064],\n",
      "        [    0.0065],\n",
      "        [    0.0066],\n",
      "        [    0.0068],\n",
      "        [    0.0075],\n",
      "        [    0.0079],\n",
      "        [    0.0085],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0103],\n",
      "        [    0.0108],\n",
      "        [    0.0116],\n",
      "        [    0.0118],\n",
      "        [    0.0123],\n",
      "        [    0.0127],\n",
      "        [    0.0131],\n",
      "        [    0.0132],\n",
      "        [    0.0147],\n",
      "        [    0.0151],\n",
      "        [    0.0156],\n",
      "        [    0.0158],\n",
      "        [    0.0160],\n",
      "        [    0.0168],\n",
      "        [    0.0173],\n",
      "        [    0.0175],\n",
      "        [    0.0179],\n",
      "        [    0.0183],\n",
      "        [    0.0195],\n",
      "        [    0.0204],\n",
      "        [    0.0204],\n",
      "        [    0.0211],\n",
      "        [    0.0215],\n",
      "        [    0.0216],\n",
      "        [    0.0219],\n",
      "        [    0.0219],\n",
      "        [    0.0219],\n",
      "        [    0.0225],\n",
      "        [    0.0232],\n",
      "        [    0.0240],\n",
      "        [    0.0243],\n",
      "        [    0.0244],\n",
      "        [    0.0246],\n",
      "        [    0.0250],\n",
      "        [    0.0255],\n",
      "        [    0.0264],\n",
      "        [    0.0276],\n",
      "        [    0.0279],\n",
      "        [    0.0295],\n",
      "        [    0.0302],\n",
      "        [    0.0303],\n",
      "        [    0.0303],\n",
      "        [    0.0307],\n",
      "        [    0.0309],\n",
      "        [    0.0314],\n",
      "        [    0.0319],\n",
      "        [    0.0321],\n",
      "        [    0.0327],\n",
      "        [    0.0330],\n",
      "        [    0.0330],\n",
      "        [    0.0334],\n",
      "        [    0.0337],\n",
      "        [    0.0338],\n",
      "        [    0.0342],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0353],\n",
      "        [    0.0353],\n",
      "        [    0.0354],\n",
      "        [    0.0360],\n",
      "        [    0.0363],\n",
      "        [    0.0368],\n",
      "        [    0.0368],\n",
      "        [    0.0375],\n",
      "        [    0.0379],\n",
      "        [    0.0382],\n",
      "        [    0.0396],\n",
      "        [    0.0398],\n",
      "        [    0.0400],\n",
      "        [    0.0402],\n",
      "        [    0.0408],\n",
      "        [    0.0412],\n",
      "        [    0.0418],\n",
      "        [    0.0423],\n",
      "        [    0.0424],\n",
      "        [    0.0434],\n",
      "        [    0.0443],\n",
      "        [    0.0444],\n",
      "        [    0.0446],\n",
      "        [    0.0462],\n",
      "        [    0.0466],\n",
      "        [    0.0473],\n",
      "        [    0.0484],\n",
      "        [    0.0486],\n",
      "        [    0.0486],\n",
      "        [    0.0489],\n",
      "        [    0.0491],\n",
      "        [    0.0491],\n",
      "        [    0.0504],\n",
      "        [    0.0514],\n",
      "        [    0.0521],\n",
      "        [    0.0525],\n",
      "        [    0.0526],\n",
      "        [    0.0529],\n",
      "        [    0.0546],\n",
      "        [    0.0560],\n",
      "        [    0.0562],\n",
      "        [    0.0564],\n",
      "        [    0.0573],\n",
      "        [    0.0574],\n",
      "        [    0.0580],\n",
      "        [    0.0603],\n",
      "        [    0.0616],\n",
      "        [    0.0618],\n",
      "        [    0.0633],\n",
      "        [    0.0635],\n",
      "        [    0.0640],\n",
      "        [    0.0643],\n",
      "        [    0.0661],\n",
      "        [    0.0674],\n",
      "        [    0.0685],\n",
      "        [    0.0693],\n",
      "        [    0.0722],\n",
      "        [    0.0805],\n",
      "        [    0.0815],\n",
      "        [    0.0864],\n",
      "        [    0.0951],\n",
      "        [    0.0958],\n",
      "        [    0.0967],\n",
      "        [    0.1003],\n",
      "        [    0.1025],\n",
      "        [    0.1057],\n",
      "        [    0.1123],\n",
      "        [    0.1171],\n",
      "        [    0.1376],\n",
      "        [    0.1674]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0028],\n",
      "        [0.0014],\n",
      "        [0.0009],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0038],\n",
      "        [0.0019],\n",
      "        [0.0040],\n",
      "        [0.0015],\n",
      "        [0.0033],\n",
      "        [0.0049],\n",
      "        [0.0034],\n",
      "        [0.0044],\n",
      "        [0.0018],\n",
      "        [0.0043],\n",
      "        [0.0062],\n",
      "        [0.0070],\n",
      "        [0.0087],\n",
      "        [0.0072],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0052],\n",
      "        [0.0093],\n",
      "        [0.0055],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0069],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0073],\n",
      "        [0.0083],\n",
      "        [0.0079],\n",
      "        [0.0121],\n",
      "        [0.0147],\n",
      "        [0.0114],\n",
      "        [0.0101],\n",
      "        [0.0116],\n",
      "        [0.0132],\n",
      "        [0.0140],\n",
      "        [0.0159],\n",
      "        [0.0152],\n",
      "        [0.0174],\n",
      "        [0.0142],\n",
      "        [0.0160],\n",
      "        [0.0183],\n",
      "        [0.0203],\n",
      "        [0.0176],\n",
      "        [0.0165],\n",
      "        [0.0182],\n",
      "        [0.0199],\n",
      "        [0.0185],\n",
      "        [0.0205],\n",
      "        [0.0223],\n",
      "        [0.0214],\n",
      "        [0.0211],\n",
      "        [0.0242],\n",
      "        [0.0230],\n",
      "        [0.0206],\n",
      "        [0.0225],\n",
      "        [0.0248],\n",
      "        [0.0258],\n",
      "        [0.0258],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0260],\n",
      "        [0.0272],\n",
      "        [0.0264],\n",
      "        [0.0286],\n",
      "        [0.0279],\n",
      "        [0.0296],\n",
      "        [0.0277],\n",
      "        [0.0303],\n",
      "        [0.0304],\n",
      "        [0.0293],\n",
      "        [0.0310],\n",
      "        [0.0327],\n",
      "        [0.0352],\n",
      "        [0.0320],\n",
      "        [0.0292],\n",
      "        [0.0331],\n",
      "        [0.0311],\n",
      "        [0.0316],\n",
      "        [0.0339],\n",
      "        [0.0337],\n",
      "        [0.0368],\n",
      "        [0.0344],\n",
      "        [0.0349],\n",
      "        [0.0386],\n",
      "        [0.0367],\n",
      "        [0.0353],\n",
      "        [0.0336],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0344],\n",
      "        [0.0354],\n",
      "        [0.0380],\n",
      "        [0.0383],\n",
      "        [0.0397],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0403],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0419],\n",
      "        [0.0452],\n",
      "        [0.0410],\n",
      "        [0.0414],\n",
      "        [0.0446],\n",
      "        [0.0448],\n",
      "        [0.0447],\n",
      "        [0.0462],\n",
      "        [0.0486],\n",
      "        [0.0472],\n",
      "        [0.0483],\n",
      "        [0.0500],\n",
      "        [0.0499],\n",
      "        [0.0488],\n",
      "        [0.0506],\n",
      "        [0.0491],\n",
      "        [0.0483],\n",
      "        [0.0514],\n",
      "        [0.0520],\n",
      "        [0.0533],\n",
      "        [0.0532],\n",
      "        [0.0529],\n",
      "        [0.0553],\n",
      "        [0.0544],\n",
      "        [0.0565],\n",
      "        [0.0589],\n",
      "        [0.0559],\n",
      "        [0.0609],\n",
      "        [0.0596],\n",
      "        [0.0604],\n",
      "        [0.0644],\n",
      "        [0.0634],\n",
      "        [0.0634],\n",
      "        [0.0604],\n",
      "        [0.0620],\n",
      "        [0.0623],\n",
      "        [0.0661],\n",
      "        [0.0655],\n",
      "        [0.0712],\n",
      "        [0.0705],\n",
      "        [0.0731],\n",
      "        [0.0836],\n",
      "        [0.0825],\n",
      "        [0.0876],\n",
      "        [0.0935],\n",
      "        [0.0939],\n",
      "        [0.0996],\n",
      "        [0.0977],\n",
      "        [0.1006],\n",
      "        [0.1027],\n",
      "        [0.1092],\n",
      "        [0.1144],\n",
      "        [0.1348],\n",
      "        [0.1644]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.38102889060974\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.560132147915283e-07, 102)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [102, 39, 132, 50, 19, 94, 133, 62, 32, 153, 58, 38, 12, 18, 47, 43, 93, 42, 154, 152, 22, 49, 30, 56, 57, 46, 11, 0, 64, 118, 155, 59, 51, 1, 88, 20, 127, 83, 45, 67, 117, 36, 119, 89, 48, 9, 136, 77, 90, 31, 63, 143, 44, 3, 144, 40, 128, 82, 106, 151, 134, 76, 28, 21, 35, 131, 103, 55, 141, 4, 72, 37, 116, 126, 137, 13, 105, 101, 2, 79, 124, 54, 145, 10, 150, 111, 129, 73, 156, 52, 140, 135, 78, 91, 142, 8, 65, 139, 138, 23, 100, 5, 123, 17, 110, 60, 15, 125, 7, 61, 130, 146, 158, 53, 75, 157, 149, 87, 29, 41, 115, 148, 122, 16, 6, 104, 33, 27, 14, 92, 81, 120, 114, 66, 99, 98, 121, 80, 108, 95, 147, 34, 74, 84, 85, 109, 86, 26, 96, 112, 107, 97, 68, 71, 113, 69, 70, 25] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2332],\n",
      "        [0.6302],\n",
      "        [0.0843],\n",
      "        [0.6059],\n",
      "        [0.6469],\n",
      "        [0.2640],\n",
      "        [0.0843],\n",
      "        [0.4681],\n",
      "        [0.5818],\n",
      "        [0.0843],\n",
      "        [0.5328],\n",
      "        [0.5767],\n",
      "        [0.6633],\n",
      "        [0.6371],\n",
      "        [0.6117],\n",
      "        [0.6268],\n",
      "        [0.2764],\n",
      "        [0.6367],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.5957],\n",
      "        [0.6208],\n",
      "        [0.5442],\n",
      "        [0.5337],\n",
      "        [0.5485],\n",
      "        [0.6152],\n",
      "        [0.6676],\n",
      "        [0.7142],\n",
      "        [0.4652],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.5247],\n",
      "        [0.6008],\n",
      "        [0.7047],\n",
      "        [0.3761],\n",
      "        [0.6243],\n",
      "        [0.0843],\n",
      "        [0.3771],\n",
      "        [0.6428],\n",
      "        [0.4257],\n",
      "        [0.0843],\n",
      "        [0.5949],\n",
      "        [0.0843],\n",
      "        [0.3708],\n",
      "        [0.6266],\n",
      "        [0.6650],\n",
      "        [0.0843],\n",
      "        [0.2888],\n",
      "        [0.3326],\n",
      "        [0.5906],\n",
      "        [0.4670],\n",
      "        [0.0843],\n",
      "        [0.6540],\n",
      "        [0.6839],\n",
      "        [0.0843],\n",
      "        [0.6544],\n",
      "        [0.0843],\n",
      "        [0.3404],\n",
      "        [0.1650],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.2830],\n",
      "        [0.4856],\n",
      "        [0.6102],\n",
      "        [0.5927],\n",
      "        [0.0843],\n",
      "        [0.2273],\n",
      "        [0.5757],\n",
      "        [0.0843],\n",
      "        [0.7236],\n",
      "        [0.2967],\n",
      "        [0.5931],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.6484],\n",
      "        [0.1902],\n",
      "        [0.2233],\n",
      "        [0.6758],\n",
      "        [0.3253],\n",
      "        [0.0843],\n",
      "        [0.5895],\n",
      "        [0.0843],\n",
      "        [0.6750],\n",
      "        [0.0843],\n",
      "        [0.1525],\n",
      "        [0.0843],\n",
      "        [0.2558],\n",
      "        [0.0843],\n",
      "        [0.5943],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.3211],\n",
      "        [0.2747],\n",
      "        [0.0843],\n",
      "        [0.6736],\n",
      "        [0.4333],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.6079],\n",
      "        [0.2255],\n",
      "        [0.7252],\n",
      "        [0.0843],\n",
      "        [0.6300],\n",
      "        [0.1195],\n",
      "        [0.4902],\n",
      "        [0.6461],\n",
      "        [0.0843],\n",
      "        [0.7045],\n",
      "        [0.4778],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.6094],\n",
      "        [0.2706],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.3491],\n",
      "        [0.4765],\n",
      "        [0.6211],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.0843],\n",
      "        [0.6361],\n",
      "        [0.7305],\n",
      "        [0.2520],\n",
      "        [0.5685],\n",
      "        [0.5512],\n",
      "        [0.6401],\n",
      "        [0.2664],\n",
      "        [0.2990],\n",
      "        [0.0843],\n",
      "        [0.0875],\n",
      "        [0.4037],\n",
      "        [0.2428],\n",
      "        [0.2500],\n",
      "        [0.0843],\n",
      "        [0.2953],\n",
      "        [0.1277],\n",
      "        [0.2857],\n",
      "        [0.0843],\n",
      "        [0.5466],\n",
      "        [0.2314],\n",
      "        [0.3779],\n",
      "        [0.3696],\n",
      "        [0.0967],\n",
      "        [0.3489],\n",
      "        [0.5504],\n",
      "        [0.2848],\n",
      "        [0.1290],\n",
      "        [0.1182],\n",
      "        [0.2634],\n",
      "        [0.4290],\n",
      "        [0.3742],\n",
      "        [0.1144],\n",
      "        [0.4215],\n",
      "        [0.3917],\n",
      "        [0.6025]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0004],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0034],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0044],\n",
      "        [0.0049],\n",
      "        [0.0052],\n",
      "        [0.0055],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0072],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0093],\n",
      "        [0.0101],\n",
      "        [0.0114],\n",
      "        [0.0116],\n",
      "        [0.0121],\n",
      "        [0.0132],\n",
      "        [0.0140],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0152],\n",
      "        [0.0159],\n",
      "        [0.0160],\n",
      "        [0.0165],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0214],\n",
      "        [0.0223],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0248],\n",
      "        [0.0258],\n",
      "        [0.0258],\n",
      "        [0.0260],\n",
      "        [0.0264],\n",
      "        [0.0272],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0293],\n",
      "        [0.0296],\n",
      "        [0.0303],\n",
      "        [0.0304],\n",
      "        [0.0310],\n",
      "        [0.0311],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0327],\n",
      "        [0.0331],\n",
      "        [0.0336],\n",
      "        [0.0337],\n",
      "        [0.0339],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0349],\n",
      "        [0.0352],\n",
      "        [0.0353],\n",
      "        [0.0354],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0367],\n",
      "        [0.0368],\n",
      "        [0.0380],\n",
      "        [0.0383],\n",
      "        [0.0386],\n",
      "        [0.0397],\n",
      "        [0.0403],\n",
      "        [0.0410],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.0419],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0428],\n",
      "        [0.0446],\n",
      "        [0.0447],\n",
      "        [0.0448],\n",
      "        [0.0452],\n",
      "        [0.0462],\n",
      "        [0.0472],\n",
      "        [0.0483],\n",
      "        [0.0483],\n",
      "        [0.0486],\n",
      "        [0.0488],\n",
      "        [0.0491],\n",
      "        [0.0499],\n",
      "        [0.0500],\n",
      "        [0.0506],\n",
      "        [0.0514],\n",
      "        [0.0520],\n",
      "        [0.0529],\n",
      "        [0.0532],\n",
      "        [0.0533],\n",
      "        [0.0544],\n",
      "        [0.0553],\n",
      "        [0.0559],\n",
      "        [0.0565],\n",
      "        [0.0589],\n",
      "        [0.0596],\n",
      "        [0.0604],\n",
      "        [0.0604],\n",
      "        [0.0609],\n",
      "        [0.0620],\n",
      "        [0.0623],\n",
      "        [0.0634],\n",
      "        [0.0634],\n",
      "        [0.0644],\n",
      "        [0.0655],\n",
      "        [0.0661],\n",
      "        [0.0705],\n",
      "        [0.0712],\n",
      "        [0.0731],\n",
      "        [0.0825],\n",
      "        [0.0836],\n",
      "        [0.0876],\n",
      "        [0.0935],\n",
      "        [0.0939],\n",
      "        [0.0977],\n",
      "        [0.0996],\n",
      "        [0.1006],\n",
      "        [0.1027],\n",
      "        [0.1092],\n",
      "        [0.1144],\n",
      "        [0.1348],\n",
      "        [0.1644],\n",
      "        [0.1788]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0031],\n",
      "        [    0.0010],\n",
      "        [    0.0037],\n",
      "        [    0.0020],\n",
      "        [    0.0017],\n",
      "        [    0.0020],\n",
      "        [    0.0056],\n",
      "        [    0.0050],\n",
      "        [    0.0035],\n",
      "        [    0.0062],\n",
      "        [    0.0065],\n",
      "        [    0.0018],\n",
      "        [    0.0080],\n",
      "        [    0.0075],\n",
      "        [    0.0028],\n",
      "        [    0.0053],\n",
      "        [    0.0086],\n",
      "        [    0.0062],\n",
      "        [    0.0063],\n",
      "        [    0.0030],\n",
      "        [    0.0094],\n",
      "        [    0.0091],\n",
      "        [    0.0051],\n",
      "        [    0.0058],\n",
      "        [    0.0055],\n",
      "        [    0.0059],\n",
      "        [    0.0061],\n",
      "        [    0.0113],\n",
      "        [    0.0089],\n",
      "        [    0.0087],\n",
      "        [    0.0117],\n",
      "        [    0.0077],\n",
      "        [    0.0136],\n",
      "        [    0.0117],\n",
      "        [    0.0158],\n",
      "        [    0.0133],\n",
      "        [    0.0134],\n",
      "        [    0.0115],\n",
      "        [    0.0169],\n",
      "        [    0.0153],\n",
      "        [    0.0179],\n",
      "        [    0.0161],\n",
      "        [    0.0164],\n",
      "        [    0.0198],\n",
      "        [    0.0150],\n",
      "        [    0.0181],\n",
      "        [    0.0173],\n",
      "        [    0.0183],\n",
      "        [    0.0216],\n",
      "        [    0.0230],\n",
      "        [    0.0206],\n",
      "        [    0.0182],\n",
      "        [    0.0232],\n",
      "        [    0.0213],\n",
      "        [    0.0248],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0243],\n",
      "        [    0.0242],\n",
      "        [    0.0244],\n",
      "        [    0.0241],\n",
      "        [    0.0284],\n",
      "        [    0.0297],\n",
      "        [    0.0281],\n",
      "        [    0.0262],\n",
      "        [    0.0271],\n",
      "        [    0.0253],\n",
      "        [    0.0280],\n",
      "        [    0.0265],\n",
      "        [    0.0274],\n",
      "        [    0.0272],\n",
      "        [    0.0297],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0337],\n",
      "        [    0.0312],\n",
      "        [    0.0319],\n",
      "        [    0.0343],\n",
      "        [    0.0321],\n",
      "        [    0.0332],\n",
      "        [    0.0311],\n",
      "        [    0.0336],\n",
      "        [    0.0312],\n",
      "        [    0.0343],\n",
      "        [    0.0342],\n",
      "        [    0.0348],\n",
      "        [    0.0365],\n",
      "        [    0.0352],\n",
      "        [    0.0329],\n",
      "        [    0.0365],\n",
      "        [    0.0366],\n",
      "        [    0.0360],\n",
      "        [    0.0370],\n",
      "        [    0.0381],\n",
      "        [    0.0355],\n",
      "        [    0.0411],\n",
      "        [    0.0398],\n",
      "        [    0.0404],\n",
      "        [    0.0374],\n",
      "        [    0.0413],\n",
      "        [    0.0394],\n",
      "        [    0.0420],\n",
      "        [    0.0454],\n",
      "        [    0.0428],\n",
      "        [    0.0453],\n",
      "        [    0.0475],\n",
      "        [    0.0448],\n",
      "        [    0.0422],\n",
      "        [    0.0478],\n",
      "        [    0.0461],\n",
      "        [    0.0471],\n",
      "        [    0.0482],\n",
      "        [    0.0458],\n",
      "        [    0.0485],\n",
      "        [    0.0487],\n",
      "        [    0.0490],\n",
      "        [    0.0498],\n",
      "        [    0.0522],\n",
      "        [    0.0530],\n",
      "        [    0.0516],\n",
      "        [    0.0519],\n",
      "        [    0.0530],\n",
      "        [    0.0564],\n",
      "        [    0.0509],\n",
      "        [    0.0544],\n",
      "        [    0.0573],\n",
      "        [    0.0528],\n",
      "        [    0.0593],\n",
      "        [    0.0591],\n",
      "        [    0.0592],\n",
      "        [    0.0605],\n",
      "        [    0.0600],\n",
      "        [    0.0634],\n",
      "        [    0.0619],\n",
      "        [    0.0622],\n",
      "        [    0.0635],\n",
      "        [    0.0631],\n",
      "        [    0.0647],\n",
      "        [    0.0654],\n",
      "        [    0.0660],\n",
      "        [    0.0727],\n",
      "        [    0.0716],\n",
      "        [    0.0727],\n",
      "        [    0.0822],\n",
      "        [    0.0839],\n",
      "        [    0.0875],\n",
      "        [    0.0901],\n",
      "        [    0.0938],\n",
      "        [    0.0973],\n",
      "        [    0.1003],\n",
      "        [    0.1005],\n",
      "        [    0.1007],\n",
      "        [    0.1073],\n",
      "        [    0.1139],\n",
      "        [    0.1331],\n",
      "        [    0.1626],\n",
      "        [    0.1750]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.66487288475037\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.101113955570554e-08, 102)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [102, 132, 94, 12, 133, 19, 43, 22, 39, 153, 50, 32, 56, 93, 46, 62, 57, 11, 0, 58, 154, 152, 38, 47, 51, 18, 42, 155, 118, 30, 49, 64, 45, 59, 88, 127, 83, 1, 9, 117, 20, 119, 89, 67, 77, 36, 136, 44, 90, 48, 143, 144, 31, 128, 82, 63, 3, 76, 151, 106, 134, 40, 55, 131, 4, 103, 37, 72, 141, 35, 28, 21, 116, 126, 137, 54, 105, 10, 101, 79, 52, 124, 145, 13, 111, 2, 150, 129, 156, 8, 78, 140, 73, 135, 91, 23, 142, 5, 139, 138, 65, 100, 123, 7, 110, 125, 60, 17, 53, 130, 146, 15, 61, 158, 75, 157, 149, 87, 6, 115, 148, 29, 27, 41, 122, 104, 16, 33, 92, 81, 14, 114, 120, 99, 98, 80, 66, 121, 108, 95, 147, 74, 34, 84, 85, 109, 86, 26, 96, 112, 107, 97, 68, 71, 113, 69, 70, 25, 24] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2335],\n",
      "        [0.0844],\n",
      "        [0.2639],\n",
      "        [0.6608],\n",
      "        [0.0844],\n",
      "        [0.6433],\n",
      "        [0.6244],\n",
      "        [0.5918],\n",
      "        [0.6275],\n",
      "        [0.0844],\n",
      "        [0.6035],\n",
      "        [0.5801],\n",
      "        [0.5316],\n",
      "        [0.2763],\n",
      "        [0.6125],\n",
      "        [0.4654],\n",
      "        [0.5464],\n",
      "        [0.6650],\n",
      "        [0.7117],\n",
      "        [0.5305],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.5742],\n",
      "        [0.6090],\n",
      "        [0.5983],\n",
      "        [0.6336],\n",
      "        [0.6344],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.5424],\n",
      "        [0.6184],\n",
      "        [0.4626],\n",
      "        [0.6402],\n",
      "        [0.5223],\n",
      "        [0.3762],\n",
      "        [0.0844],\n",
      "        [0.3777],\n",
      "        [0.7025],\n",
      "        [0.6624],\n",
      "        [0.0844],\n",
      "        [0.6206],\n",
      "        [0.0844],\n",
      "        [0.3707],\n",
      "        [0.4235],\n",
      "        [0.2898],\n",
      "        [0.5928],\n",
      "        [0.0844],\n",
      "        [0.6515],\n",
      "        [0.3325],\n",
      "        [0.6241],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.5889],\n",
      "        [0.0844],\n",
      "        [0.3409],\n",
      "        [0.4643],\n",
      "        [0.6818],\n",
      "        [0.2837],\n",
      "        [0.0844],\n",
      "        [0.1649],\n",
      "        [0.0844],\n",
      "        [0.6520],\n",
      "        [0.5733],\n",
      "        [0.0844],\n",
      "        [0.7215],\n",
      "        [0.2275],\n",
      "        [0.5910],\n",
      "        [0.2949],\n",
      "        [0.0844],\n",
      "        [0.5906],\n",
      "        [0.4829],\n",
      "        [0.6063],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.5870],\n",
      "        [0.1902],\n",
      "        [0.6724],\n",
      "        [0.2236],\n",
      "        [0.3259],\n",
      "        [0.5919],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.6457],\n",
      "        [0.1523],\n",
      "        [0.6735],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.6709],\n",
      "        [0.3219],\n",
      "        [0.0844],\n",
      "        [0.2544],\n",
      "        [0.0844],\n",
      "        [0.2744],\n",
      "        [0.6043],\n",
      "        [0.0844],\n",
      "        [0.7230],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.4308],\n",
      "        [0.2254],\n",
      "        [0.0844],\n",
      "        [0.7019],\n",
      "        [0.1192],\n",
      "        [0.0844],\n",
      "        [0.4876],\n",
      "        [0.6266],\n",
      "        [0.6069],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.6432],\n",
      "        [0.4753],\n",
      "        [0.0844],\n",
      "        [0.2707],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.3492],\n",
      "        [0.7281],\n",
      "        [0.0844],\n",
      "        [0.0844],\n",
      "        [0.4743],\n",
      "        [0.5481],\n",
      "        [0.6187],\n",
      "        [0.0844],\n",
      "        [0.2519],\n",
      "        [0.6329],\n",
      "        [0.5666],\n",
      "        [0.2662],\n",
      "        [0.2993],\n",
      "        [0.6373],\n",
      "        [0.0871],\n",
      "        [0.0844],\n",
      "        [0.2427],\n",
      "        [0.2499],\n",
      "        [0.2956],\n",
      "        [0.4012],\n",
      "        [0.0844],\n",
      "        [0.1274],\n",
      "        [0.2855],\n",
      "        [0.0844],\n",
      "        [0.2311],\n",
      "        [0.5444],\n",
      "        [0.3783],\n",
      "        [0.3699],\n",
      "        [0.0963],\n",
      "        [0.3490],\n",
      "        [0.5470],\n",
      "        [0.2847],\n",
      "        [0.1286],\n",
      "        [0.1175],\n",
      "        [0.2632],\n",
      "        [0.4271],\n",
      "        [0.3723],\n",
      "        [0.1139],\n",
      "        [0.4198],\n",
      "        [0.3899],\n",
      "        [0.5987],\n",
      "        [0.6078]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0010],\n",
      "        [    0.0017],\n",
      "        [    0.0018],\n",
      "        [    0.0020],\n",
      "        [    0.0020],\n",
      "        [    0.0028],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0035],\n",
      "        [    0.0037],\n",
      "        [    0.0050],\n",
      "        [    0.0051],\n",
      "        [    0.0053],\n",
      "        [    0.0055],\n",
      "        [    0.0056],\n",
      "        [    0.0058],\n",
      "        [    0.0059],\n",
      "        [    0.0061],\n",
      "        [    0.0062],\n",
      "        [    0.0062],\n",
      "        [    0.0063],\n",
      "        [    0.0065],\n",
      "        [    0.0075],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0086],\n",
      "        [    0.0087],\n",
      "        [    0.0089],\n",
      "        [    0.0091],\n",
      "        [    0.0094],\n",
      "        [    0.0113],\n",
      "        [    0.0115],\n",
      "        [    0.0117],\n",
      "        [    0.0117],\n",
      "        [    0.0133],\n",
      "        [    0.0134],\n",
      "        [    0.0136],\n",
      "        [    0.0150],\n",
      "        [    0.0153],\n",
      "        [    0.0158],\n",
      "        [    0.0161],\n",
      "        [    0.0164],\n",
      "        [    0.0169],\n",
      "        [    0.0173],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0182],\n",
      "        [    0.0183],\n",
      "        [    0.0198],\n",
      "        [    0.0206],\n",
      "        [    0.0213],\n",
      "        [    0.0216],\n",
      "        [    0.0224],\n",
      "        [    0.0225],\n",
      "        [    0.0230],\n",
      "        [    0.0232],\n",
      "        [    0.0241],\n",
      "        [    0.0242],\n",
      "        [    0.0243],\n",
      "        [    0.0244],\n",
      "        [    0.0248],\n",
      "        [    0.0253],\n",
      "        [    0.0262],\n",
      "        [    0.0265],\n",
      "        [    0.0271],\n",
      "        [    0.0272],\n",
      "        [    0.0274],\n",
      "        [    0.0280],\n",
      "        [    0.0281],\n",
      "        [    0.0284],\n",
      "        [    0.0297],\n",
      "        [    0.0297],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0311],\n",
      "        [    0.0312],\n",
      "        [    0.0312],\n",
      "        [    0.0319],\n",
      "        [    0.0321],\n",
      "        [    0.0329],\n",
      "        [    0.0332],\n",
      "        [    0.0336],\n",
      "        [    0.0337],\n",
      "        [    0.0342],\n",
      "        [    0.0343],\n",
      "        [    0.0343],\n",
      "        [    0.0348],\n",
      "        [    0.0352],\n",
      "        [    0.0355],\n",
      "        [    0.0360],\n",
      "        [    0.0365],\n",
      "        [    0.0365],\n",
      "        [    0.0366],\n",
      "        [    0.0370],\n",
      "        [    0.0374],\n",
      "        [    0.0381],\n",
      "        [    0.0394],\n",
      "        [    0.0398],\n",
      "        [    0.0404],\n",
      "        [    0.0411],\n",
      "        [    0.0413],\n",
      "        [    0.0420],\n",
      "        [    0.0422],\n",
      "        [    0.0428],\n",
      "        [    0.0448],\n",
      "        [    0.0453],\n",
      "        [    0.0454],\n",
      "        [    0.0458],\n",
      "        [    0.0461],\n",
      "        [    0.0471],\n",
      "        [    0.0475],\n",
      "        [    0.0478],\n",
      "        [    0.0482],\n",
      "        [    0.0485],\n",
      "        [    0.0487],\n",
      "        [    0.0490],\n",
      "        [    0.0498],\n",
      "        [    0.0509],\n",
      "        [    0.0516],\n",
      "        [    0.0519],\n",
      "        [    0.0522],\n",
      "        [    0.0528],\n",
      "        [    0.0530],\n",
      "        [    0.0530],\n",
      "        [    0.0544],\n",
      "        [    0.0564],\n",
      "        [    0.0573],\n",
      "        [    0.0591],\n",
      "        [    0.0592],\n",
      "        [    0.0593],\n",
      "        [    0.0600],\n",
      "        [    0.0605],\n",
      "        [    0.0619],\n",
      "        [    0.0622],\n",
      "        [    0.0631],\n",
      "        [    0.0634],\n",
      "        [    0.0635],\n",
      "        [    0.0647],\n",
      "        [    0.0654],\n",
      "        [    0.0660],\n",
      "        [    0.0716],\n",
      "        [    0.0727],\n",
      "        [    0.0727],\n",
      "        [    0.0822],\n",
      "        [    0.0839],\n",
      "        [    0.0875],\n",
      "        [    0.0901],\n",
      "        [    0.0938],\n",
      "        [    0.0973],\n",
      "        [    0.1003],\n",
      "        [    0.1005],\n",
      "        [    0.1007],\n",
      "        [    0.1073],\n",
      "        [    0.1139],\n",
      "        [    0.1331],\n",
      "        [    0.1626],\n",
      "        [    0.1750],\n",
      "        [    0.1873]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0011],\n",
      "        [0.0019],\n",
      "        [0.0008],\n",
      "        [0.0021],\n",
      "        [0.0058],\n",
      "        [0.0011],\n",
      "        [0.0010],\n",
      "        [0.0052],\n",
      "        [0.0037],\n",
      "        [0.0053],\n",
      "        [0.0064],\n",
      "        [0.0039],\n",
      "        [0.0055],\n",
      "        [0.0035],\n",
      "        [0.0077],\n",
      "        [0.0045],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0078],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0085],\n",
      "        [0.0095],\n",
      "        [0.0059],\n",
      "        [0.0116],\n",
      "        [0.0103],\n",
      "        [0.0086],\n",
      "        [0.0090],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0133],\n",
      "        [0.0095],\n",
      "        [0.0134],\n",
      "        [0.0122],\n",
      "        [0.0134],\n",
      "        [0.0123],\n",
      "        [0.0159],\n",
      "        [0.0122],\n",
      "        [0.0154],\n",
      "        [0.0195],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0184],\n",
      "        [0.0156],\n",
      "        [0.0194],\n",
      "        [0.0180],\n",
      "        [0.0163],\n",
      "        [0.0186],\n",
      "        [0.0216],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0230],\n",
      "        [0.0222],\n",
      "        [0.0214],\n",
      "        [0.0251],\n",
      "        [0.0255],\n",
      "        [0.0228],\n",
      "        [0.0241],\n",
      "        [0.0240],\n",
      "        [0.0243],\n",
      "        [0.0266],\n",
      "        [0.0238],\n",
      "        [0.0261],\n",
      "        [0.0242],\n",
      "        [0.0266],\n",
      "        [0.0257],\n",
      "        [0.0263],\n",
      "        [0.0282],\n",
      "        [0.0297],\n",
      "        [0.0310],\n",
      "        [0.0336],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0306],\n",
      "        [0.0294],\n",
      "        [0.0316],\n",
      "        [0.0284],\n",
      "        [0.0326],\n",
      "        [0.0308],\n",
      "        [0.0313],\n",
      "        [0.0333],\n",
      "        [0.0335],\n",
      "        [0.0366],\n",
      "        [0.0343],\n",
      "        [0.0367],\n",
      "        [0.0342],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0325],\n",
      "        [0.0345],\n",
      "        [0.0366],\n",
      "        [0.0372],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0337],\n",
      "        [0.0382],\n",
      "        [0.0369],\n",
      "        [0.0399],\n",
      "        [0.0406],\n",
      "        [0.0429],\n",
      "        [0.0416],\n",
      "        [0.0421],\n",
      "        [0.0392],\n",
      "        [0.0428],\n",
      "        [0.0449],\n",
      "        [0.0473],\n",
      "        [0.0490],\n",
      "        [0.0441],\n",
      "        [0.0459],\n",
      "        [0.0469],\n",
      "        [0.0506],\n",
      "        [0.0496],\n",
      "        [0.0480],\n",
      "        [0.0478],\n",
      "        [0.0486],\n",
      "        [0.0488],\n",
      "        [0.0494],\n",
      "        [0.0482],\n",
      "        [0.0517],\n",
      "        [0.0518],\n",
      "        [0.0542],\n",
      "        [0.0497],\n",
      "        [0.0549],\n",
      "        [0.0532],\n",
      "        [0.0545],\n",
      "        [0.0597],\n",
      "        [0.0589],\n",
      "        [0.0587],\n",
      "        [0.0583],\n",
      "        [0.0623],\n",
      "        [0.0601],\n",
      "        [0.0606],\n",
      "        [0.0622],\n",
      "        [0.0626],\n",
      "        [0.0622],\n",
      "        [0.0651],\n",
      "        [0.0636],\n",
      "        [0.0647],\n",
      "        [0.0657],\n",
      "        [0.0658],\n",
      "        [0.0713],\n",
      "        [0.0744],\n",
      "        [0.0718],\n",
      "        [0.0815],\n",
      "        [0.0839],\n",
      "        [0.0870],\n",
      "        [0.0866],\n",
      "        [0.0941],\n",
      "        [0.0973],\n",
      "        [0.1006],\n",
      "        [0.1008],\n",
      "        [0.0995],\n",
      "        [0.1061],\n",
      "        [0.1139],\n",
      "        [0.1321],\n",
      "        [0.1615],\n",
      "        [0.1712],\n",
      "        [0.1837]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.94691586494446\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 24 個區塊累積花費時間(s) 1.2187891006469727\n",
      "<<The performance of 24 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.2187891006469727\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 976.01\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1273.12\n",
      "The accuracy(2000) for l = 1: 90.57%\n",
      "The accuracy(3000) for l = 1: 97.48%\n",
      "The maximum error: tensor(4689.5781)\n",
      "The minimum error: tensor(13.4609)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1520.5\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1608.7\n",
      "The accuracy(2000) for l = 1: 75.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 2324.46484375\n",
      "The minimum error: 860.46484375\n",
      "------------------------------------------------------------\n",
      "0.9056603773584906\n",
      "<class 'float'>\n",
      "0.75\n",
      "<class 'float'>\n",
      "The <<25>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.780059276119573e-07, 98)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [98, 8, 18, 128, 39, 90, 129, 7, 42, 149, 52, 53, 35, 46, 89, 15, 47, 150, 148, 28, 58, 54, 34, 151, 114, 43, 41, 38, 26, 45, 14, 5, 84, 79, 60, 55, 123, 113, 73, 115, 40, 85, 132, 63, 86, 32, 16, 139, 140, 78, 44, 124, 72, 27, 51, 102, 147, 0, 130, 59, 33, 127, 68, 36, 99, 137, 6, 50, 31, 112, 122, 133, 75, 24, 48, 101, 4, 97, 120, 141, 17, 19, 156, 146, 107, 74, 125, 152, 131, 9, 136, 87, 1, 69, 138, 3, 135, 134, 96, 119, 106, 61, 49, 121, 126, 142, 56, 71, 154, 2, 153, 145, 13, 83, 57, 23, 11, 111, 144, 118, 155, 25, 100, 37, 77, 88, 29, 12, 157, 110, 116, 95, 76, 10, 94, 117, 104, 62, 91, 143, 70, 80, 30, 81, 105, 22, 82, 158, 92, 108, 64, 103, 93, 67, 109]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0011],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0090],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0103],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0134],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0163],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0214],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0251],\n",
      "        [0.0257],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0337],\n",
      "        [0.0337],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0369],\n",
      "        [0.0372],\n",
      "        [0.0382],\n",
      "        [0.0392],\n",
      "        [0.0399],\n",
      "        [0.0406],\n",
      "        [0.0416],\n",
      "        [0.0421],\n",
      "        [0.0428],\n",
      "        [0.0429],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0459],\n",
      "        [0.0469],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0480],\n",
      "        [0.0482],\n",
      "        [0.0486],\n",
      "        [0.0488],\n",
      "        [0.0490],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0497],\n",
      "        [0.0506],\n",
      "        [0.0517],\n",
      "        [0.0518],\n",
      "        [0.0532],\n",
      "        [0.0537],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0549],\n",
      "        [0.0583],\n",
      "        [0.0587],\n",
      "        [0.0589],\n",
      "        [0.0597],\n",
      "        [0.0598],\n",
      "        [0.0601],\n",
      "        [0.0606],\n",
      "        [0.0622],\n",
      "        [0.0622],\n",
      "        [0.0623],\n",
      "        [0.0626],\n",
      "        [0.0636],\n",
      "        [0.0647],\n",
      "        [0.0651],\n",
      "        [0.0657],\n",
      "        [0.0658],\n",
      "        [0.0713],\n",
      "        [0.0718],\n",
      "        [0.0744],\n",
      "        [0.0815],\n",
      "        [0.0839],\n",
      "        [0.0866],\n",
      "        [0.0870],\n",
      "        [0.0910],\n",
      "        [0.0941],\n",
      "        [0.0973],\n",
      "        [0.0995],\n",
      "        [0.1006],\n",
      "        [0.1008],\n",
      "        [0.1061],\n",
      "        [0.1139]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.780059276119573e-07, 98)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [98, 8, 18, 128, 39, 90, 129, 7, 42, 149, 52, 53, 35, 46, 89, 15, 47, 150, 148, 28, 58, 54, 34, 151, 114, 43, 41, 38, 26, 45, 14, 5, 84, 79, 60, 55, 123, 113, 73, 115, 40, 85, 132, 63, 86, 32, 16, 139, 140, 78, 44, 124, 72, 27, 51, 102, 147, 0, 130, 59, 33, 127, 68, 36, 99, 137, 6, 50, 31, 112, 122, 133, 75, 24, 48, 101, 4, 97, 120, 141, 17, 19, 156, 146, 107, 74, 125, 152, 131, 9, 136, 87, 1, 69, 138, 3, 135, 134, 96, 119, 106, 61, 49, 121, 126, 142, 56, 71, 154, 2, 153, 145, 13, 83, 57, 23, 11, 111, 144, 118, 155, 25, 100, 37, 77, 88, 29, 12, 157, 110, 116, 95, 76, 10, 94, 117, 104, 62, 91, 143, 70, 80, 30, 81, 105, 22, 82, 158, 92, 108, 64, 103, 93, 67, 109, 65] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.2341],\n",
      "        [0.6582],\n",
      "        [0.5878],\n",
      "        [0.0845],\n",
      "        [0.6227],\n",
      "        [0.2641],\n",
      "        [0.0845],\n",
      "        [0.6623],\n",
      "        [0.6104],\n",
      "        [0.0845],\n",
      "        [0.5303],\n",
      "        [0.5450],\n",
      "        [0.6253],\n",
      "        [0.6020],\n",
      "        [0.2765],\n",
      "        [0.6396],\n",
      "        [0.5966],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.5788],\n",
      "        [0.4632],\n",
      "        [0.5289],\n",
      "        [0.5722],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.6071],\n",
      "        [0.6382],\n",
      "        [0.6327],\n",
      "        [0.5406],\n",
      "        [0.6167],\n",
      "        [0.6299],\n",
      "        [0.6596],\n",
      "        [0.3767],\n",
      "        [0.3788],\n",
      "        [0.4606],\n",
      "        [0.5206],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.2915],\n",
      "        [0.0845],\n",
      "        [0.6497],\n",
      "        [0.3710],\n",
      "        [0.0845],\n",
      "        [0.4220],\n",
      "        [0.3328],\n",
      "        [0.5914],\n",
      "        [0.6169],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.3420],\n",
      "        [0.6224],\n",
      "        [0.0845],\n",
      "        [0.2850],\n",
      "        [0.5875],\n",
      "        [0.5717],\n",
      "        [0.1651],\n",
      "        [0.0845],\n",
      "        [0.7192],\n",
      "        [0.0845],\n",
      "        [0.4621],\n",
      "        [0.5895],\n",
      "        [0.0845],\n",
      "        [0.2937],\n",
      "        [0.6501],\n",
      "        [0.2279],\n",
      "        [0.0845],\n",
      "        [0.6696],\n",
      "        [0.5852],\n",
      "        [0.5891],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.3272],\n",
      "        [0.4803],\n",
      "        [0.5903],\n",
      "        [0.1907],\n",
      "        [0.6679],\n",
      "        [0.2243],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.6024],\n",
      "        [0.6006],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.1524],\n",
      "        [0.3234],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.6429],\n",
      "        [0.0845],\n",
      "        [0.2748],\n",
      "        [0.7204],\n",
      "        [0.2537],\n",
      "        [0.0845],\n",
      "        [0.6990],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.2256],\n",
      "        [0.0845],\n",
      "        [0.1193],\n",
      "        [0.4290],\n",
      "        [0.6052],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.4857],\n",
      "        [0.2715],\n",
      "        [0.0845],\n",
      "        [0.7254],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.6231],\n",
      "        [0.3496],\n",
      "        [0.4734],\n",
      "        [0.5450],\n",
      "        [0.6401],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.0845],\n",
      "        [0.4723],\n",
      "        [0.2521],\n",
      "        [0.6168],\n",
      "        [0.3003],\n",
      "        [0.2665],\n",
      "        [0.5650],\n",
      "        [0.6295],\n",
      "        [0.0845],\n",
      "        [0.0871],\n",
      "        [0.0845],\n",
      "        [0.2430],\n",
      "        [0.2965],\n",
      "        [0.6342],\n",
      "        [0.2502],\n",
      "        [0.0845],\n",
      "        [0.1273],\n",
      "        [0.3994],\n",
      "        [0.2858],\n",
      "        [0.0845],\n",
      "        [0.2314],\n",
      "        [0.3792],\n",
      "        [0.5427],\n",
      "        [0.3706],\n",
      "        [0.0963],\n",
      "        [0.5435],\n",
      "        [0.3495],\n",
      "        [0.0845],\n",
      "        [0.2850],\n",
      "        [0.1286],\n",
      "        [0.4259],\n",
      "        [0.1173],\n",
      "        [0.2635],\n",
      "        [0.3710],\n",
      "        [0.1139],\n",
      "        [0.4189]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0011],\n",
      "        [0.0019],\n",
      "        [0.0021],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0064],\n",
      "        [0.0077],\n",
      "        [0.0078],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0090],\n",
      "        [0.0095],\n",
      "        [0.0095],\n",
      "        [0.0103],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0134],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0163],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0214],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0228],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0240],\n",
      "        [0.0241],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0251],\n",
      "        [0.0257],\n",
      "        [0.0261],\n",
      "        [0.0263],\n",
      "        [0.0266],\n",
      "        [0.0266],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0333],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0337],\n",
      "        [0.0337],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0366],\n",
      "        [0.0367],\n",
      "        [0.0369],\n",
      "        [0.0372],\n",
      "        [0.0382],\n",
      "        [0.0392],\n",
      "        [0.0399],\n",
      "        [0.0406],\n",
      "        [0.0416],\n",
      "        [0.0421],\n",
      "        [0.0428],\n",
      "        [0.0429],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0459],\n",
      "        [0.0469],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0480],\n",
      "        [0.0482],\n",
      "        [0.0486],\n",
      "        [0.0488],\n",
      "        [0.0490],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0497],\n",
      "        [0.0506],\n",
      "        [0.0517],\n",
      "        [0.0518],\n",
      "        [0.0532],\n",
      "        [0.0537],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0549],\n",
      "        [0.0583],\n",
      "        [0.0587],\n",
      "        [0.0589],\n",
      "        [0.0597],\n",
      "        [0.0598],\n",
      "        [0.0601],\n",
      "        [0.0606],\n",
      "        [0.0622],\n",
      "        [0.0622],\n",
      "        [0.0623],\n",
      "        [0.0626],\n",
      "        [0.0636],\n",
      "        [0.0647],\n",
      "        [0.0651],\n",
      "        [0.0657],\n",
      "        [0.0658],\n",
      "        [0.0713],\n",
      "        [0.0718],\n",
      "        [0.0744],\n",
      "        [0.0815],\n",
      "        [0.0839],\n",
      "        [0.0866],\n",
      "        [0.0870],\n",
      "        [0.0910],\n",
      "        [0.0941],\n",
      "        [0.0973],\n",
      "        [0.0995],\n",
      "        [0.1006],\n",
      "        [0.1008],\n",
      "        [0.1061],\n",
      "        [0.1139],\n",
      "        [0.1321]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0026],\n",
      "        [    0.0033],\n",
      "        [    0.0026],\n",
      "        [    0.0054],\n",
      "        [    0.0065],\n",
      "        [    0.0042],\n",
      "        [    0.0064],\n",
      "        [    0.0072],\n",
      "        [    0.0085],\n",
      "        [    0.0080],\n",
      "        [    0.0088],\n",
      "        [    0.0094],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0080],\n",
      "        [    0.0018],\n",
      "        [    0.0111],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0015],\n",
      "        [    0.0035],\n",
      "        [    0.0031],\n",
      "        [    0.0037],\n",
      "        [    0.0043],\n",
      "        [    0.0133],\n",
      "        [    0.0043],\n",
      "        [    0.0146],\n",
      "        [    0.0050],\n",
      "        [    0.0062],\n",
      "        [    0.0057],\n",
      "        [    0.0077],\n",
      "        [    0.0159],\n",
      "        [    0.0149],\n",
      "        [    0.0094],\n",
      "        [    0.0092],\n",
      "        [    0.0090],\n",
      "        [    0.0177],\n",
      "        [    0.0197],\n",
      "        [    0.0118],\n",
      "        [    0.0206],\n",
      "        [    0.0215],\n",
      "        [    0.0192],\n",
      "        [    0.0137],\n",
      "        [    0.0142],\n",
      "        [    0.0213],\n",
      "        [    0.0143],\n",
      "        [    0.0157],\n",
      "        [    0.0250],\n",
      "        [    0.0169],\n",
      "        [    0.0183],\n",
      "        [    0.0162],\n",
      "        [    0.0179],\n",
      "        [    0.0188],\n",
      "        [    0.0182],\n",
      "        [    0.0287],\n",
      "        [    0.0225],\n",
      "        [    0.0198],\n",
      "        [    0.0277],\n",
      "        [    0.0200],\n",
      "        [    0.0210],\n",
      "        [    0.0307],\n",
      "        [    0.0218],\n",
      "        [    0.0299],\n",
      "        [    0.0213],\n",
      "        [    0.0247],\n",
      "        [    0.0325],\n",
      "        [    0.0323],\n",
      "        [    0.0345],\n",
      "        [    0.0247],\n",
      "        [    0.0341],\n",
      "        [    0.0349],\n",
      "        [    0.0349],\n",
      "        [    0.0273],\n",
      "        [    0.0270],\n",
      "        [    0.0365],\n",
      "        [    0.0333],\n",
      "        [    0.0363],\n",
      "        [    0.0347],\n",
      "        [    0.0376],\n",
      "        [    0.0292],\n",
      "        [    0.0299],\n",
      "        [    0.0376],\n",
      "        [    0.0294],\n",
      "        [    0.0299],\n",
      "        [    0.0360],\n",
      "        [    0.0308],\n",
      "        [    0.0303],\n",
      "        [    0.0307],\n",
      "        [    0.0321],\n",
      "        [    0.0327],\n",
      "        [    0.0409],\n",
      "        [    0.0341],\n",
      "        [    0.0405],\n",
      "        [    0.0335],\n",
      "        [    0.0425],\n",
      "        [    0.0430],\n",
      "        [    0.0442],\n",
      "        [    0.0449],\n",
      "        [    0.0435],\n",
      "        [    0.0464],\n",
      "        [    0.0411],\n",
      "        [    0.0388],\n",
      "        [    0.0493],\n",
      "        [    0.0492],\n",
      "        [    0.0416],\n",
      "        [    0.0426],\n",
      "        [    0.0431],\n",
      "        [    0.0438],\n",
      "        [    0.0437],\n",
      "        [    0.0520],\n",
      "        [    0.0443],\n",
      "        [    0.0445],\n",
      "        [    0.0453],\n",
      "        [    0.0467],\n",
      "        [    0.0454],\n",
      "        [    0.0536],\n",
      "        [    0.0469],\n",
      "        [    0.0560],\n",
      "        [    0.0475],\n",
      "        [    0.0575],\n",
      "        [    0.0494],\n",
      "        [    0.0499],\n",
      "        [    0.0564],\n",
      "        [    0.0496],\n",
      "        [    0.0551],\n",
      "        [    0.0561],\n",
      "        [    0.0542],\n",
      "        [    0.0561],\n",
      "        [    0.0555],\n",
      "        [    0.0619],\n",
      "        [    0.0649],\n",
      "        [    0.0643],\n",
      "        [    0.0590],\n",
      "        [    0.0586],\n",
      "        [    0.0649],\n",
      "        [    0.0679],\n",
      "        [    0.0634],\n",
      "        [    0.0612],\n",
      "        [    0.0681],\n",
      "        [    0.0615],\n",
      "        [    0.0674],\n",
      "        [    0.0690],\n",
      "        [    0.0697],\n",
      "        [    0.0786],\n",
      "        [    0.0825],\n",
      "        [    0.0903],\n",
      "        [    0.0844],\n",
      "        [    0.0868],\n",
      "        [    0.0967],\n",
      "        [    0.0989],\n",
      "        [    0.1039],\n",
      "        [    0.0994],\n",
      "        [    0.1031],\n",
      "        [    0.1101],\n",
      "        [    0.1156],\n",
      "        [    0.1364]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 72.49296450614929\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.827338611652522e-10, 46)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [46, 35, 28, 150, 15, 148, 98, 18, 54, 8, 58, 34, 90, 151, 43, 38, 128, 45, 26, 129, 39, 7, 14, 149, 89, 42, 52, 55, 60, 53, 79, 47, 73, 114, 132, 63, 32, 41, 84, 16, 5, 44, 140, 123, 124, 27, 78, 72, 85, 113, 147, 130, 115, 59, 36, 86, 40, 127, 102, 31, 99, 139, 24, 75, 0, 51, 141, 156, 17, 146, 68, 125, 33, 152, 74, 131, 6, 137, 9, 101, 69, 87, 112, 50, 97, 122, 133, 107, 4, 48, 120, 19, 61, 1, 136, 106, 126, 138, 142, 3, 56, 96, 154, 71, 135, 153, 145, 134, 13, 57, 119, 83, 11, 144, 121, 49, 155, 37, 25, 2, 23, 29, 77, 157, 111, 12, 88, 100, 118, 10, 76, 62, 143, 110, 104, 95, 94, 116, 70, 117, 91, 80, 30, 81, 105, 82, 158, 22, 92, 108, 103, 93, 64, 67, 109, 65, 66] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6073],\n",
      "        [0.6304],\n",
      "        [0.5836],\n",
      "        [0.0888],\n",
      "        [0.6436],\n",
      "        [0.0888],\n",
      "        [0.2362],\n",
      "        [0.5914],\n",
      "        [0.5335],\n",
      "        [0.6623],\n",
      "        [0.4674],\n",
      "        [0.5770],\n",
      "        [0.2664],\n",
      "        [0.0888],\n",
      "        [0.6123],\n",
      "        [0.6380],\n",
      "        [0.0888],\n",
      "        [0.6220],\n",
      "        [0.5453],\n",
      "        [0.0888],\n",
      "        [0.6280],\n",
      "        [0.6664],\n",
      "        [0.6338],\n",
      "        [0.0888],\n",
      "        [0.2789],\n",
      "        [0.6154],\n",
      "        [0.5352],\n",
      "        [0.5250],\n",
      "        [0.4648],\n",
      "        [0.5499],\n",
      "        [0.3817],\n",
      "        [0.6018],\n",
      "        [0.2953],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.4262],\n",
      "        [0.5964],\n",
      "        [0.6432],\n",
      "        [0.3793],\n",
      "        [0.6207],\n",
      "        [0.6634],\n",
      "        [0.6277],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.5923],\n",
      "        [0.3452],\n",
      "        [0.2890],\n",
      "        [0.3735],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.4663],\n",
      "        [0.6555],\n",
      "        [0.3355],\n",
      "        [0.6548],\n",
      "        [0.0888],\n",
      "        [0.1667],\n",
      "        [0.5940],\n",
      "        [0.2299],\n",
      "        [0.0888],\n",
      "        [0.4843],\n",
      "        [0.3307],\n",
      "        [0.7227],\n",
      "        [0.5767],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.6061],\n",
      "        [0.0888],\n",
      "        [0.2974],\n",
      "        [0.0888],\n",
      "        [0.5945],\n",
      "        [0.0888],\n",
      "        [0.3271],\n",
      "        [0.0888],\n",
      "        [0.6735],\n",
      "        [0.0888],\n",
      "        [0.6467],\n",
      "        [0.1924],\n",
      "        [0.2574],\n",
      "        [0.2773],\n",
      "        [0.0888],\n",
      "        [0.5903],\n",
      "        [0.2264],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.1541],\n",
      "        [0.6716],\n",
      "        [0.5955],\n",
      "        [0.0888],\n",
      "        [0.6045],\n",
      "        [0.4331],\n",
      "        [0.7240],\n",
      "        [0.0888],\n",
      "        [0.1210],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.7027],\n",
      "        [0.4898],\n",
      "        [0.2276],\n",
      "        [0.0888],\n",
      "        [0.2754],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.6267],\n",
      "        [0.4777],\n",
      "        [0.0888],\n",
      "        [0.3523],\n",
      "        [0.6438],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.6104],\n",
      "        [0.0888],\n",
      "        [0.6221],\n",
      "        [0.4766],\n",
      "        [0.7291],\n",
      "        [0.5489],\n",
      "        [0.5697],\n",
      "        [0.3035],\n",
      "        [0.0888],\n",
      "        [0.0888],\n",
      "        [0.6332],\n",
      "        [0.2692],\n",
      "        [0.2539],\n",
      "        [0.0888],\n",
      "        [0.6380],\n",
      "        [0.2997],\n",
      "        [0.4034],\n",
      "        [0.0888],\n",
      "        [0.0889],\n",
      "        [0.1287],\n",
      "        [0.2451],\n",
      "        [0.2525],\n",
      "        [0.0888],\n",
      "        [0.2353],\n",
      "        [0.0888],\n",
      "        [0.2882],\n",
      "        [0.3821],\n",
      "        [0.5474],\n",
      "        [0.3736],\n",
      "        [0.0978],\n",
      "        [0.3522],\n",
      "        [0.0888],\n",
      "        [0.5472],\n",
      "        [0.2876],\n",
      "        [0.1302],\n",
      "        [0.1184],\n",
      "        [0.2659],\n",
      "        [0.4302],\n",
      "        [0.3750],\n",
      "        [0.1156],\n",
      "        [0.4232],\n",
      "        [0.3931]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0015],\n",
      "        [    0.0018],\n",
      "        [    0.0018],\n",
      "        [    0.0019],\n",
      "        [    0.0026],\n",
      "        [    0.0026],\n",
      "        [    0.0031],\n",
      "        [    0.0033],\n",
      "        [    0.0035],\n",
      "        [    0.0037],\n",
      "        [    0.0042],\n",
      "        [    0.0043],\n",
      "        [    0.0043],\n",
      "        [    0.0050],\n",
      "        [    0.0054],\n",
      "        [    0.0057],\n",
      "        [    0.0062],\n",
      "        [    0.0064],\n",
      "        [    0.0065],\n",
      "        [    0.0072],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0085],\n",
      "        [    0.0088],\n",
      "        [    0.0090],\n",
      "        [    0.0092],\n",
      "        [    0.0094],\n",
      "        [    0.0094],\n",
      "        [    0.0111],\n",
      "        [    0.0118],\n",
      "        [    0.0133],\n",
      "        [    0.0137],\n",
      "        [    0.0142],\n",
      "        [    0.0143],\n",
      "        [    0.0146],\n",
      "        [    0.0149],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0169],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0182],\n",
      "        [    0.0183],\n",
      "        [    0.0188],\n",
      "        [    0.0192],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0206],\n",
      "        [    0.0210],\n",
      "        [    0.0213],\n",
      "        [    0.0213],\n",
      "        [    0.0215],\n",
      "        [    0.0218],\n",
      "        [    0.0225],\n",
      "        [    0.0247],\n",
      "        [    0.0247],\n",
      "        [    0.0250],\n",
      "        [    0.0270],\n",
      "        [    0.0273],\n",
      "        [    0.0277],\n",
      "        [    0.0287],\n",
      "        [    0.0292],\n",
      "        [    0.0294],\n",
      "        [    0.0299],\n",
      "        [    0.0299],\n",
      "        [    0.0299],\n",
      "        [    0.0303],\n",
      "        [    0.0307],\n",
      "        [    0.0307],\n",
      "        [    0.0308],\n",
      "        [    0.0321],\n",
      "        [    0.0323],\n",
      "        [    0.0325],\n",
      "        [    0.0327],\n",
      "        [    0.0333],\n",
      "        [    0.0335],\n",
      "        [    0.0341],\n",
      "        [    0.0341],\n",
      "        [    0.0345],\n",
      "        [    0.0347],\n",
      "        [    0.0349],\n",
      "        [    0.0349],\n",
      "        [    0.0360],\n",
      "        [    0.0363],\n",
      "        [    0.0365],\n",
      "        [    0.0376],\n",
      "        [    0.0376],\n",
      "        [    0.0388],\n",
      "        [    0.0405],\n",
      "        [    0.0409],\n",
      "        [    0.0411],\n",
      "        [    0.0416],\n",
      "        [    0.0425],\n",
      "        [    0.0426],\n",
      "        [    0.0430],\n",
      "        [    0.0431],\n",
      "        [    0.0435],\n",
      "        [    0.0437],\n",
      "        [    0.0438],\n",
      "        [    0.0442],\n",
      "        [    0.0443],\n",
      "        [    0.0445],\n",
      "        [    0.0449],\n",
      "        [    0.0453],\n",
      "        [    0.0454],\n",
      "        [    0.0464],\n",
      "        [    0.0467],\n",
      "        [    0.0469],\n",
      "        [    0.0475],\n",
      "        [    0.0492],\n",
      "        [    0.0493],\n",
      "        [    0.0494],\n",
      "        [    0.0496],\n",
      "        [    0.0499],\n",
      "        [    0.0520],\n",
      "        [    0.0536],\n",
      "        [    0.0542],\n",
      "        [    0.0551],\n",
      "        [    0.0555],\n",
      "        [    0.0560],\n",
      "        [    0.0561],\n",
      "        [    0.0561],\n",
      "        [    0.0564],\n",
      "        [    0.0575],\n",
      "        [    0.0586],\n",
      "        [    0.0590],\n",
      "        [    0.0612],\n",
      "        [    0.0615],\n",
      "        [    0.0619],\n",
      "        [    0.0634],\n",
      "        [    0.0643],\n",
      "        [    0.0649],\n",
      "        [    0.0649],\n",
      "        [    0.0674],\n",
      "        [    0.0679],\n",
      "        [    0.0681],\n",
      "        [    0.0690],\n",
      "        [    0.0697],\n",
      "        [    0.0786],\n",
      "        [    0.0825],\n",
      "        [    0.0844],\n",
      "        [    0.0868],\n",
      "        [    0.0903],\n",
      "        [    0.0967],\n",
      "        [    0.0989],\n",
      "        [    0.0994],\n",
      "        [    0.1031],\n",
      "        [    0.1039],\n",
      "        [    0.1101],\n",
      "        [    0.1156],\n",
      "        [    0.1364],\n",
      "        [    0.1657]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0011],\n",
      "        [0.0017],\n",
      "        [0.0023],\n",
      "        [0.0030],\n",
      "        [0.0025],\n",
      "        [0.0019],\n",
      "        [0.0012],\n",
      "        [0.0051],\n",
      "        [0.0026],\n",
      "        [0.0056],\n",
      "        [0.0046],\n",
      "        [0.0030],\n",
      "        [0.0048],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0049],\n",
      "        [0.0073],\n",
      "        [0.0063],\n",
      "        [0.0058],\n",
      "        [0.0054],\n",
      "        [0.0065],\n",
      "        [0.0088],\n",
      "        [0.0074],\n",
      "        [0.0066],\n",
      "        [0.0070],\n",
      "        [0.0066],\n",
      "        [0.0109],\n",
      "        [0.0115],\n",
      "        [0.0073],\n",
      "        [0.0106],\n",
      "        [0.0094],\n",
      "        [0.0127],\n",
      "        [0.0128],\n",
      "        [0.0142],\n",
      "        [0.0165],\n",
      "        [0.0151],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0169],\n",
      "        [0.0151],\n",
      "        [0.0177],\n",
      "        [0.0174],\n",
      "        [0.0172],\n",
      "        [0.0185],\n",
      "        [0.0182],\n",
      "        [0.0193],\n",
      "        [0.0198],\n",
      "        [0.0176],\n",
      "        [0.0192],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0200],\n",
      "        [0.0231],\n",
      "        [0.0223],\n",
      "        [0.0196],\n",
      "        [0.0203],\n",
      "        [0.0224],\n",
      "        [0.0230],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0245],\n",
      "        [0.0278],\n",
      "        [0.0283],\n",
      "        [0.0271],\n",
      "        [0.0266],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0312],\n",
      "        [0.0304],\n",
      "        [0.0281],\n",
      "        [0.0309],\n",
      "        [0.0299],\n",
      "        [0.0313],\n",
      "        [0.0318],\n",
      "        [0.0327],\n",
      "        [0.0315],\n",
      "        [0.0319],\n",
      "        [0.0335],\n",
      "        [0.0328],\n",
      "        [0.0352],\n",
      "        [0.0359],\n",
      "        [0.0336],\n",
      "        [0.0324],\n",
      "        [0.0339],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0353],\n",
      "        [0.0355],\n",
      "        [0.0347],\n",
      "        [0.0371],\n",
      "        [0.0363],\n",
      "        [0.0413],\n",
      "        [0.0399],\n",
      "        [0.0404],\n",
      "        [0.0418],\n",
      "        [0.0422],\n",
      "        [0.0420],\n",
      "        [0.0432],\n",
      "        [0.0423],\n",
      "        [0.0452],\n",
      "        [0.0427],\n",
      "        [0.0443],\n",
      "        [0.0449],\n",
      "        [0.0437],\n",
      "        [0.0448],\n",
      "        [0.0451],\n",
      "        [0.0443],\n",
      "        [0.0463],\n",
      "        [0.0474],\n",
      "        [0.0459],\n",
      "        [0.0481],\n",
      "        [0.0478],\n",
      "        [0.0480],\n",
      "        [0.0487],\n",
      "        [0.0474],\n",
      "        [0.0499],\n",
      "        [0.0506],\n",
      "        [0.0504],\n",
      "        [0.0514],\n",
      "        [0.0526],\n",
      "        [0.0544],\n",
      "        [0.0561],\n",
      "        [0.0560],\n",
      "        [0.0554],\n",
      "        [0.0570],\n",
      "        [0.0576],\n",
      "        [0.0556],\n",
      "        [0.0569],\n",
      "        [0.0595],\n",
      "        [0.0600],\n",
      "        [0.0636],\n",
      "        [0.0621],\n",
      "        [0.0612],\n",
      "        [0.0642],\n",
      "        [0.0633],\n",
      "        [0.0639],\n",
      "        [0.0644],\n",
      "        [0.0687],\n",
      "        [0.0674],\n",
      "        [0.0669],\n",
      "        [0.0703],\n",
      "        [0.0702],\n",
      "        [0.0798],\n",
      "        [0.0833],\n",
      "        [0.0857],\n",
      "        [0.0873],\n",
      "        [0.0891],\n",
      "        [0.0956],\n",
      "        [0.0981],\n",
      "        [0.1001],\n",
      "        [0.1021],\n",
      "        [0.1017],\n",
      "        [0.1080],\n",
      "        [0.1149],\n",
      "        [0.1343],\n",
      "        [0.1636]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 72.78274869918823\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1810723208327545e-06, 35)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [35, 18, 46, 28, 98, 150, 148, 8, 90, 15, 34, 151, 128, 54, 39, 58, 43, 129, 38, 26, 7, 52, 89, 42, 45, 53, 149, 14, 47, 79, 55, 60, 73, 114, 41, 84, 132, 32, 5, 63, 16, 123, 140, 85, 44, 27, 124, 113, 78, 86, 72, 115, 40, 147, 130, 36, 127, 102, 59, 139, 31, 99, 51, 0, 24, 68, 75, 141, 33, 156, 146, 125, 17, 152, 6, 74, 137, 50, 131, 101, 9, 112, 97, 122, 133, 48, 69, 107, 4, 87, 19, 120, 1, 136, 61, 106, 138, 126, 3, 96, 142, 135, 154, 134, 153, 71, 145, 56, 119, 13, 49, 57, 11, 144, 83, 121, 155, 25, 37, 2, 23, 29, 111, 100, 157, 77, 118, 12, 88, 10, 76, 110, 143, 95, 62, 94, 104, 116, 91, 117, 70, 30, 80, 81, 105, 82, 158, 22, 92, 108, 103, 64, 93, 67, 109, 65, 66, 21] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6295],\n",
      "        [0.5900],\n",
      "        [0.6056],\n",
      "        [0.5835],\n",
      "        [0.2355],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.6616],\n",
      "        [0.2652],\n",
      "        [0.6424],\n",
      "        [0.5761],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.5315],\n",
      "        [0.6269],\n",
      "        [0.4654],\n",
      "        [0.6108],\n",
      "        [0.0883],\n",
      "        [0.6370],\n",
      "        [0.5451],\n",
      "        [0.6657],\n",
      "        [0.5331],\n",
      "        [0.2776],\n",
      "        [0.6140],\n",
      "        [0.6205],\n",
      "        [0.5479],\n",
      "        [0.0883],\n",
      "        [0.6327],\n",
      "        [0.6001],\n",
      "        [0.3805],\n",
      "        [0.5231],\n",
      "        [0.4625],\n",
      "        [0.2944],\n",
      "        [0.0883],\n",
      "        [0.6419],\n",
      "        [0.3778],\n",
      "        [0.0883],\n",
      "        [0.5957],\n",
      "        [0.6625],\n",
      "        [0.4239],\n",
      "        [0.6195],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.3719],\n",
      "        [0.6263],\n",
      "        [0.5923],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.3442],\n",
      "        [0.3338],\n",
      "        [0.2880],\n",
      "        [0.0883],\n",
      "        [0.6536],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.6545],\n",
      "        [0.0883],\n",
      "        [0.1661],\n",
      "        [0.4641],\n",
      "        [0.0883],\n",
      "        [0.5935],\n",
      "        [0.2292],\n",
      "        [0.5745],\n",
      "        [0.7222],\n",
      "        [0.4835],\n",
      "        [0.2955],\n",
      "        [0.3297],\n",
      "        [0.0883],\n",
      "        [0.5936],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.6048],\n",
      "        [0.0883],\n",
      "        [0.6727],\n",
      "        [0.3261],\n",
      "        [0.0883],\n",
      "        [0.5882],\n",
      "        [0.0883],\n",
      "        [0.1919],\n",
      "        [0.6459],\n",
      "        [0.0883],\n",
      "        [0.2256],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.5936],\n",
      "        [0.2558],\n",
      "        [0.1534],\n",
      "        [0.6708],\n",
      "        [0.2756],\n",
      "        [0.6032],\n",
      "        [0.0883],\n",
      "        [0.7234],\n",
      "        [0.0883],\n",
      "        [0.4307],\n",
      "        [0.1203],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.7020],\n",
      "        [0.2267],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.2744],\n",
      "        [0.0883],\n",
      "        [0.4877],\n",
      "        [0.0883],\n",
      "        [0.6257],\n",
      "        [0.6085],\n",
      "        [0.4756],\n",
      "        [0.6430],\n",
      "        [0.0883],\n",
      "        [0.3509],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.4761],\n",
      "        [0.6211],\n",
      "        [0.7286],\n",
      "        [0.5479],\n",
      "        [0.5694],\n",
      "        [0.0883],\n",
      "        [0.2531],\n",
      "        [0.0883],\n",
      "        [0.3024],\n",
      "        [0.0883],\n",
      "        [0.6323],\n",
      "        [0.2677],\n",
      "        [0.6371],\n",
      "        [0.2987],\n",
      "        [0.0883],\n",
      "        [0.0883],\n",
      "        [0.2441],\n",
      "        [0.4010],\n",
      "        [0.2515],\n",
      "        [0.1279],\n",
      "        [0.0883],\n",
      "        [0.2870],\n",
      "        [0.0883],\n",
      "        [0.2340],\n",
      "        [0.5469],\n",
      "        [0.3807],\n",
      "        [0.3724],\n",
      "        [0.0969],\n",
      "        [0.3508],\n",
      "        [0.0883],\n",
      "        [0.5459],\n",
      "        [0.2865],\n",
      "        [0.1294],\n",
      "        [0.1177],\n",
      "        [0.4280],\n",
      "        [0.2649],\n",
      "        [0.3730],\n",
      "        [0.1149],\n",
      "        [0.4211],\n",
      "        [0.3910],\n",
      "        [0.5974]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0011],\n",
      "        [0.0012],\n",
      "        [0.0016],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0023],\n",
      "        [0.0025],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0046],\n",
      "        [0.0048],\n",
      "        [0.0049],\n",
      "        [0.0051],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0063],\n",
      "        [0.0065],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0074],\n",
      "        [0.0088],\n",
      "        [0.0094],\n",
      "        [0.0106],\n",
      "        [0.0109],\n",
      "        [0.0115],\n",
      "        [0.0127],\n",
      "        [0.0128],\n",
      "        [0.0133],\n",
      "        [0.0134],\n",
      "        [0.0142],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0176],\n",
      "        [0.0177],\n",
      "        [0.0182],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0196],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0223],\n",
      "        [0.0224],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0245],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0271],\n",
      "        [0.0278],\n",
      "        [0.0281],\n",
      "        [0.0283],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0300],\n",
      "        [0.0304],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0313],\n",
      "        [0.0315],\n",
      "        [0.0318],\n",
      "        [0.0319],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0339],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0347],\n",
      "        [0.0352],\n",
      "        [0.0353],\n",
      "        [0.0355],\n",
      "        [0.0359],\n",
      "        [0.0363],\n",
      "        [0.0371],\n",
      "        [0.0399],\n",
      "        [0.0404],\n",
      "        [0.0413],\n",
      "        [0.0418],\n",
      "        [0.0420],\n",
      "        [0.0422],\n",
      "        [0.0423],\n",
      "        [0.0427],\n",
      "        [0.0432],\n",
      "        [0.0437],\n",
      "        [0.0443],\n",
      "        [0.0443],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0459],\n",
      "        [0.0463],\n",
      "        [0.0474],\n",
      "        [0.0474],\n",
      "        [0.0478],\n",
      "        [0.0480],\n",
      "        [0.0481],\n",
      "        [0.0487],\n",
      "        [0.0499],\n",
      "        [0.0504],\n",
      "        [0.0506],\n",
      "        [0.0514],\n",
      "        [0.0526],\n",
      "        [0.0544],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0560],\n",
      "        [0.0561],\n",
      "        [0.0569],\n",
      "        [0.0570],\n",
      "        [0.0576],\n",
      "        [0.0595],\n",
      "        [0.0600],\n",
      "        [0.0612],\n",
      "        [0.0621],\n",
      "        [0.0633],\n",
      "        [0.0636],\n",
      "        [0.0639],\n",
      "        [0.0642],\n",
      "        [0.0644],\n",
      "        [0.0669],\n",
      "        [0.0674],\n",
      "        [0.0687],\n",
      "        [0.0702],\n",
      "        [0.0703],\n",
      "        [0.0798],\n",
      "        [0.0833],\n",
      "        [0.0857],\n",
      "        [0.0873],\n",
      "        [0.0891],\n",
      "        [0.0956],\n",
      "        [0.0981],\n",
      "        [0.1001],\n",
      "        [0.1017],\n",
      "        [0.1021],\n",
      "        [0.1080],\n",
      "        [0.1149],\n",
      "        [0.1343],\n",
      "        [0.1636],\n",
      "        [0.1737]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0035],\n",
      "        [0.0023],\n",
      "        [0.0038],\n",
      "        [0.0034],\n",
      "        [0.0018],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0005],\n",
      "        [0.0024],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0072],\n",
      "        [0.0032],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0061],\n",
      "        [0.0082],\n",
      "        [0.0081],\n",
      "        [0.0034],\n",
      "        [0.0046],\n",
      "        [0.0060],\n",
      "        [0.0045],\n",
      "        [0.0096],\n",
      "        [0.0052],\n",
      "        [0.0076],\n",
      "        [0.0123],\n",
      "        [0.0071],\n",
      "        [0.0112],\n",
      "        [0.0131],\n",
      "        [0.0136],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0108],\n",
      "        [0.0124],\n",
      "        [0.0140],\n",
      "        [0.0170],\n",
      "        [0.0119],\n",
      "        [0.0183],\n",
      "        [0.0204],\n",
      "        [0.0174],\n",
      "        [0.0172],\n",
      "        [0.0165],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0183],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0187],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0178],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0246],\n",
      "        [0.0221],\n",
      "        [0.0230],\n",
      "        [0.0253],\n",
      "        [0.0247],\n",
      "        [0.0272],\n",
      "        [0.0255],\n",
      "        [0.0243],\n",
      "        [0.0238],\n",
      "        [0.0300],\n",
      "        [0.0269],\n",
      "        [0.0285],\n",
      "        [0.0295],\n",
      "        [0.0279],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0307],\n",
      "        [0.0347],\n",
      "        [0.0311],\n",
      "        [0.0283],\n",
      "        [0.0318],\n",
      "        [0.0321],\n",
      "        [0.0300],\n",
      "        [0.0325],\n",
      "        [0.0328],\n",
      "        [0.0366],\n",
      "        [0.0338],\n",
      "        [0.0339],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0324],\n",
      "        [0.0359],\n",
      "        [0.0353],\n",
      "        [0.0322],\n",
      "        [0.0365],\n",
      "        [0.0330],\n",
      "        [0.0373],\n",
      "        [0.0365],\n",
      "        [0.0406],\n",
      "        [0.0433],\n",
      "        [0.0417],\n",
      "        [0.0422],\n",
      "        [0.0420],\n",
      "        [0.0389],\n",
      "        [0.0424],\n",
      "        [0.0430],\n",
      "        [0.0439],\n",
      "        [0.0441],\n",
      "        [0.0445],\n",
      "        [0.0446],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0474],\n",
      "        [0.0461],\n",
      "        [0.0497],\n",
      "        [0.0450],\n",
      "        [0.0495],\n",
      "        [0.0510],\n",
      "        [0.0478],\n",
      "        [0.0489],\n",
      "        [0.0489],\n",
      "        [0.0497],\n",
      "        [0.0521],\n",
      "        [0.0528],\n",
      "        [0.0480],\n",
      "        [0.0498],\n",
      "        [0.0563],\n",
      "        [0.0556],\n",
      "        [0.0552],\n",
      "        [0.0558],\n",
      "        [0.0563],\n",
      "        [0.0571],\n",
      "        [0.0603],\n",
      "        [0.0582],\n",
      "        [0.0626],\n",
      "        [0.0602],\n",
      "        [0.0614],\n",
      "        [0.0619],\n",
      "        [0.0629],\n",
      "        [0.0655],\n",
      "        [0.0635],\n",
      "        [0.0643],\n",
      "        [0.0646],\n",
      "        [0.0663],\n",
      "        [0.0676],\n",
      "        [0.0688],\n",
      "        [0.0721],\n",
      "        [0.0711],\n",
      "        [0.0805],\n",
      "        [0.0832],\n",
      "        [0.0865],\n",
      "        [0.0871],\n",
      "        [0.0861],\n",
      "        [0.0952],\n",
      "        [0.0981],\n",
      "        [0.1003],\n",
      "        [0.1000],\n",
      "        [0.1017],\n",
      "        [0.1066],\n",
      "        [0.1150],\n",
      "        [0.1328],\n",
      "        [0.1621],\n",
      "        [0.1703]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 73.07020878791809\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1526125237869564e-07, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [8, 98, 150, 148, 18, 90, 39, 28, 7, 35, 46, 42, 151, 52, 128, 53, 89, 129, 15, 34, 47, 54, 149, 58, 26, 43, 38, 45, 41, 79, 5, 14, 84, 73, 114, 55, 60, 132, 85, 32, 140, 123, 40, 124, 63, 86, 113, 78, 72, 44, 27, 147, 115, 130, 16, 127, 102, 0, 51, 36, 139, 59, 99, 68, 31, 33, 6, 75, 141, 156, 24, 50, 146, 125, 152, 74, 137, 4, 48, 131, 101, 19, 112, 97, 122, 133, 17, 107, 69, 87, 1, 9, 120, 3, 136, 106, 126, 138, 96, 142, 61, 135, 154, 134, 153, 145, 71, 49, 119, 56, 144, 2, 83, 121, 57, 13, 155, 23, 11, 25, 37, 100, 111, 157, 29, 77, 118, 88, 76, 12, 110, 143, 10, 95, 94, 104, 116, 62, 91, 117, 70, 80, 30, 81, 105, 22, 82, 158, 92, 108, 64, 103, 93, 67, 109, 65, 66, 21, 20] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6585],\n",
      "        [0.2354],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.5865],\n",
      "        [0.2646],\n",
      "        [0.6248],\n",
      "        [0.5817],\n",
      "        [0.6625],\n",
      "        [0.6271],\n",
      "        [0.6034],\n",
      "        [0.6115],\n",
      "        [0.0885],\n",
      "        [0.5311],\n",
      "        [0.0885],\n",
      "        [0.5458],\n",
      "        [0.2770],\n",
      "        [0.0885],\n",
      "        [0.6389],\n",
      "        [0.5740],\n",
      "        [0.5978],\n",
      "        [0.5294],\n",
      "        [0.0885],\n",
      "        [0.4632],\n",
      "        [0.5434],\n",
      "        [0.6085],\n",
      "        [0.6347],\n",
      "        [0.6182],\n",
      "        [0.6394],\n",
      "        [0.3799],\n",
      "        [0.6593],\n",
      "        [0.6293],\n",
      "        [0.3769],\n",
      "        [0.2946],\n",
      "        [0.0885],\n",
      "        [0.5209],\n",
      "        [0.4603],\n",
      "        [0.0885],\n",
      "        [0.3708],\n",
      "        [0.5937],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.6511],\n",
      "        [0.0885],\n",
      "        [0.4221],\n",
      "        [0.3329],\n",
      "        [0.0885],\n",
      "        [0.3439],\n",
      "        [0.2881],\n",
      "        [0.6240],\n",
      "        [0.5905],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.6160],\n",
      "        [0.0885],\n",
      "        [0.1661],\n",
      "        [0.7189],\n",
      "        [0.5723],\n",
      "        [0.6521],\n",
      "        [0.0885],\n",
      "        [0.4619],\n",
      "        [0.2290],\n",
      "        [0.2944],\n",
      "        [0.5915],\n",
      "        [0.5917],\n",
      "        [0.6695],\n",
      "        [0.3295],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.4813],\n",
      "        [0.5859],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.3261],\n",
      "        [0.0885],\n",
      "        [0.6675],\n",
      "        [0.5914],\n",
      "        [0.0885],\n",
      "        [0.1918],\n",
      "        [0.5999],\n",
      "        [0.0885],\n",
      "        [0.2255],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.6013],\n",
      "        [0.1534],\n",
      "        [0.2550],\n",
      "        [0.2749],\n",
      "        [0.7201],\n",
      "        [0.6428],\n",
      "        [0.0885],\n",
      "        [0.6986],\n",
      "        [0.0885],\n",
      "        [0.1203],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.2264],\n",
      "        [0.0885],\n",
      "        [0.4286],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.2743],\n",
      "        [0.6061],\n",
      "        [0.0885],\n",
      "        [0.4855],\n",
      "        [0.0885],\n",
      "        [0.7251],\n",
      "        [0.3500],\n",
      "        [0.0885],\n",
      "        [0.4735],\n",
      "        [0.6223],\n",
      "        [0.0885],\n",
      "        [0.5451],\n",
      "        [0.6397],\n",
      "        [0.4744],\n",
      "        [0.6189],\n",
      "        [0.2527],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.5676],\n",
      "        [0.3022],\n",
      "        [0.0885],\n",
      "        [0.2671],\n",
      "        [0.2985],\n",
      "        [0.6290],\n",
      "        [0.0885],\n",
      "        [0.0885],\n",
      "        [0.6340],\n",
      "        [0.2437],\n",
      "        [0.2511],\n",
      "        [0.1278],\n",
      "        [0.0885],\n",
      "        [0.3990],\n",
      "        [0.2865],\n",
      "        [0.0885],\n",
      "        [0.2339],\n",
      "        [0.3800],\n",
      "        [0.5450],\n",
      "        [0.3716],\n",
      "        [0.0970],\n",
      "        [0.5429],\n",
      "        [0.3500],\n",
      "        [0.0885],\n",
      "        [0.2860],\n",
      "        [0.1294],\n",
      "        [0.4264],\n",
      "        [0.1176],\n",
      "        [0.2645],\n",
      "        [0.3715],\n",
      "        [0.1150],\n",
      "        [0.4195],\n",
      "        [0.3895],\n",
      "        [0.5940],\n",
      "        [0.6035]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0018],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0024],\n",
      "        [0.0032],\n",
      "        [0.0034],\n",
      "        [0.0034],\n",
      "        [0.0035],\n",
      "        [0.0038],\n",
      "        [0.0045],\n",
      "        [0.0046],\n",
      "        [0.0046],\n",
      "        [0.0051],\n",
      "        [0.0052],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0096],\n",
      "        [0.0108],\n",
      "        [0.0112],\n",
      "        [0.0119],\n",
      "        [0.0123],\n",
      "        [0.0124],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0131],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0178],\n",
      "        [0.0183],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0194],\n",
      "        [0.0196],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0200],\n",
      "        [0.0201],\n",
      "        [0.0202],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0221],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0243],\n",
      "        [0.0246],\n",
      "        [0.0247],\n",
      "        [0.0253],\n",
      "        [0.0255],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.0279],\n",
      "        [0.0283],\n",
      "        [0.0285],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0300],\n",
      "        [0.0302],\n",
      "        [0.0307],\n",
      "        [0.0311],\n",
      "        [0.0318],\n",
      "        [0.0321],\n",
      "        [0.0322],\n",
      "        [0.0324],\n",
      "        [0.0325],\n",
      "        [0.0328],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0339],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0347],\n",
      "        [0.0353],\n",
      "        [0.0359],\n",
      "        [0.0365],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0373],\n",
      "        [0.0389],\n",
      "        [0.0406],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0422],\n",
      "        [0.0424],\n",
      "        [0.0430],\n",
      "        [0.0433],\n",
      "        [0.0439],\n",
      "        [0.0441],\n",
      "        [0.0445],\n",
      "        [0.0446],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0461],\n",
      "        [0.0474],\n",
      "        [0.0478],\n",
      "        [0.0480],\n",
      "        [0.0489],\n",
      "        [0.0489],\n",
      "        [0.0495],\n",
      "        [0.0497],\n",
      "        [0.0497],\n",
      "        [0.0498],\n",
      "        [0.0510],\n",
      "        [0.0521],\n",
      "        [0.0528],\n",
      "        [0.0552],\n",
      "        [0.0556],\n",
      "        [0.0558],\n",
      "        [0.0563],\n",
      "        [0.0563],\n",
      "        [0.0571],\n",
      "        [0.0582],\n",
      "        [0.0602],\n",
      "        [0.0603],\n",
      "        [0.0614],\n",
      "        [0.0619],\n",
      "        [0.0626],\n",
      "        [0.0629],\n",
      "        [0.0635],\n",
      "        [0.0643],\n",
      "        [0.0646],\n",
      "        [0.0655],\n",
      "        [0.0663],\n",
      "        [0.0676],\n",
      "        [0.0688],\n",
      "        [0.0711],\n",
      "        [0.0721],\n",
      "        [0.0805],\n",
      "        [0.0832],\n",
      "        [0.0861],\n",
      "        [0.0865],\n",
      "        [0.0871],\n",
      "        [0.0952],\n",
      "        [0.0981],\n",
      "        [0.1000],\n",
      "        [0.1003],\n",
      "        [0.1017],\n",
      "        [0.1066],\n",
      "        [0.1150],\n",
      "        [0.1328],\n",
      "        [0.1621],\n",
      "        [0.1703],\n",
      "        [0.1830]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0039],\n",
      "        [    0.0019],\n",
      "        [    0.0019],\n",
      "        [    0.0021],\n",
      "        [    0.0063],\n",
      "        [    0.0020],\n",
      "        [    0.0011],\n",
      "        [    0.0052],\n",
      "        [    0.0001],\n",
      "        [    0.0059],\n",
      "        [    0.0057],\n",
      "        [    0.0022],\n",
      "        [    0.0045],\n",
      "        [    0.0030],\n",
      "        [    0.0052],\n",
      "        [    0.0035],\n",
      "        [    0.0056],\n",
      "        [    0.0062],\n",
      "        [    0.0104],\n",
      "        [    0.0089],\n",
      "        [    0.0050],\n",
      "        [    0.0090],\n",
      "        [    0.0078],\n",
      "        [    0.0098],\n",
      "        [    0.0101],\n",
      "        [    0.0103],\n",
      "        [    0.0103],\n",
      "        [    0.0116],\n",
      "        [    0.0084],\n",
      "        [    0.0113],\n",
      "        [    0.0082],\n",
      "        [    0.0162],\n",
      "        [    0.0118],\n",
      "        [    0.0117],\n",
      "        [    0.0131],\n",
      "        [    0.0150],\n",
      "        [    0.0156],\n",
      "        [    0.0139],\n",
      "        [    0.0158],\n",
      "        [    0.0189],\n",
      "        [    0.0171],\n",
      "        [    0.0175],\n",
      "        [    0.0156],\n",
      "        [    0.0181],\n",
      "        [    0.0198],\n",
      "        [    0.0181],\n",
      "        [    0.0195],\n",
      "        [    0.0195],\n",
      "        [    0.0190],\n",
      "        [    0.0221],\n",
      "        [    0.0219],\n",
      "        [    0.0200],\n",
      "        [    0.0204],\n",
      "        [    0.0202],\n",
      "        [    0.0243],\n",
      "        [    0.0220],\n",
      "        [    0.0229],\n",
      "        [    0.0201],\n",
      "        [    0.0224],\n",
      "        [    0.0269],\n",
      "        [    0.0248],\n",
      "        [    0.0274],\n",
      "        [    0.0255],\n",
      "        [    0.0260],\n",
      "        [    0.0292],\n",
      "        [    0.0260],\n",
      "        [    0.0246],\n",
      "        [    0.0281],\n",
      "        [    0.0294],\n",
      "        [    0.0296],\n",
      "        [    0.0326],\n",
      "        [    0.0280],\n",
      "        [    0.0301],\n",
      "        [    0.0305],\n",
      "        [    0.0309],\n",
      "        [    0.0312],\n",
      "        [    0.0323],\n",
      "        [    0.0283],\n",
      "        [    0.0304],\n",
      "        [    0.0323],\n",
      "        [    0.0329],\n",
      "        [    0.0292],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0347],\n",
      "        [    0.0347],\n",
      "        [    0.0387],\n",
      "        [    0.0353],\n",
      "        [    0.0365],\n",
      "        [    0.0368],\n",
      "        [    0.0327],\n",
      "        [    0.0402],\n",
      "        [    0.0374],\n",
      "        [    0.0350],\n",
      "        [    0.0407],\n",
      "        [    0.0416],\n",
      "        [    0.0418],\n",
      "        [    0.0423],\n",
      "        [    0.0422],\n",
      "        [    0.0428],\n",
      "        [    0.0451],\n",
      "        [    0.0440],\n",
      "        [    0.0439],\n",
      "        [    0.0447],\n",
      "        [    0.0445],\n",
      "        [    0.0447],\n",
      "        [    0.0446],\n",
      "        [    0.0429],\n",
      "        [    0.0462],\n",
      "        [    0.0494],\n",
      "        [    0.0477],\n",
      "        [    0.0441],\n",
      "        [    0.0495],\n",
      "        [    0.0491],\n",
      "        [    0.0514],\n",
      "        [    0.0536],\n",
      "        [    0.0496],\n",
      "        [    0.0466],\n",
      "        [    0.0547],\n",
      "        [    0.0542],\n",
      "        [    0.0550],\n",
      "        [    0.0549],\n",
      "        [    0.0558],\n",
      "        [    0.0557],\n",
      "        [    0.0582],\n",
      "        [    0.0562],\n",
      "        [    0.0573],\n",
      "        [    0.0584],\n",
      "        [    0.0600],\n",
      "        [    0.0641],\n",
      "        [    0.0617],\n",
      "        [    0.0617],\n",
      "        [    0.0663],\n",
      "        [    0.0627],\n",
      "        [    0.0633],\n",
      "        [    0.0644],\n",
      "        [    0.0647],\n",
      "        [    0.0672],\n",
      "        [    0.0660],\n",
      "        [    0.0677],\n",
      "        [    0.0686],\n",
      "        [    0.0714],\n",
      "        [    0.0740],\n",
      "        [    0.0809],\n",
      "        [    0.0831],\n",
      "        [    0.0826],\n",
      "        [    0.0871],\n",
      "        [    0.0869],\n",
      "        [    0.0949],\n",
      "        [    0.0982],\n",
      "        [    0.0987],\n",
      "        [    0.1004],\n",
      "        [    0.1015],\n",
      "        [    0.1053],\n",
      "        [    0.1151],\n",
      "        [    0.1316],\n",
      "        [    0.1610],\n",
      "        [    0.1665],\n",
      "        [    0.1793]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 73.35846161842346\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 25 個區塊累積花費時間(s) 1.221837043762207\n",
      "<<The performance of 25 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.221837043762207\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 996.57\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1286.69\n",
      "The accuracy(2000) for l = 1: 89.94%\n",
      "The accuracy(3000) for l = 1: 97.48%\n",
      "The maximum error: tensor(4576.8945)\n",
      "The minimum error: tensor(3.8047)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 8470.7\n",
      "The MAPE for l = 1: 0.2%\n",
      "The RMSE for l = 1: 8663.3\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "The maximum error: 10393.73828125\n",
      "The minimum error: 5643.73828125\n",
      "------------------------------------------------------------\n",
      "0.89937106918239\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<26>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.220446049250313e-08, 3)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [3, 35, 94, 146, 86, 144, 38, 48, 49, 4, 147, 43, 24, 124, 85, 42, 31, 125, 14, 145, 1, 37, 30, 50, 54, 22, 39, 34, 11, 75, 41, 69, 80, 110, 128, 51, 36, 56, 81, 10, 136, 119, 120, 82, 28, 68, 74, 109, 59, 143, 126, 111, 23, 123, 40, 47, 98, 12, 2, 135, 95, 64, 29, 32, 55, 46, 71, 0, 27, 15, 137, 152, 142, 44, 121, 148, 70, 133, 127, 20, 97, 108, 93, 118, 129, 103, 65, 83, 116, 13, 5, 132, 102, 122, 92, 134, 138, 45, 150, 131, 149, 67, 130, 141, 57, 115, 19, 140, 117, 52, 79, 151, 53, 9, 21, 7, 96, 33, 153, 107, 73, 114, 25, 84, 72, 106, 139, 91, 90, 8, 100, 112, 87, 6, 58, 113, 66, 76, 26, 77, 18, 101, 154, 78, 88, 104, 60, 99, 89, 63, 105, 61, 62, 17, 16]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0011],\n",
      "        [    0.0019],\n",
      "        [    0.0019],\n",
      "        [    0.0020],\n",
      "        [    0.0021],\n",
      "        [    0.0022],\n",
      "        [    0.0030],\n",
      "        [    0.0035],\n",
      "        [    0.0039],\n",
      "        [    0.0045],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0052],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0062],\n",
      "        [    0.0063],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0084],\n",
      "        [    0.0089],\n",
      "        [    0.0090],\n",
      "        [    0.0098],\n",
      "        [    0.0101],\n",
      "        [    0.0103],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0113],\n",
      "        [    0.0116],\n",
      "        [    0.0117],\n",
      "        [    0.0118],\n",
      "        [    0.0131],\n",
      "        [    0.0139],\n",
      "        [    0.0150],\n",
      "        [    0.0156],\n",
      "        [    0.0156],\n",
      "        [    0.0158],\n",
      "        [    0.0162],\n",
      "        [    0.0171],\n",
      "        [    0.0175],\n",
      "        [    0.0181],\n",
      "        [    0.0181],\n",
      "        [    0.0189],\n",
      "        [    0.0190],\n",
      "        [    0.0195],\n",
      "        [    0.0195],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0204],\n",
      "        [    0.0219],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0224],\n",
      "        [    0.0229],\n",
      "        [    0.0243],\n",
      "        [    0.0246],\n",
      "        [    0.0248],\n",
      "        [    0.0255],\n",
      "        [    0.0260],\n",
      "        [    0.0260],\n",
      "        [    0.0269],\n",
      "        [    0.0274],\n",
      "        [    0.0280],\n",
      "        [    0.0281],\n",
      "        [    0.0283],\n",
      "        [    0.0292],\n",
      "        [    0.0292],\n",
      "        [    0.0294],\n",
      "        [    0.0296],\n",
      "        [    0.0301],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0309],\n",
      "        [    0.0312],\n",
      "        [    0.0323],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0329],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0347],\n",
      "        [    0.0347],\n",
      "        [    0.0353],\n",
      "        [    0.0365],\n",
      "        [    0.0368],\n",
      "        [    0.0374],\n",
      "        [    0.0387],\n",
      "        [    0.0402],\n",
      "        [    0.0407],\n",
      "        [    0.0416],\n",
      "        [    0.0418],\n",
      "        [    0.0422],\n",
      "        [    0.0423],\n",
      "        [    0.0428],\n",
      "        [    0.0429],\n",
      "        [    0.0439],\n",
      "        [    0.0440],\n",
      "        [    0.0445],\n",
      "        [    0.0446],\n",
      "        [    0.0447],\n",
      "        [    0.0447],\n",
      "        [    0.0451],\n",
      "        [    0.0462],\n",
      "        [    0.0466],\n",
      "        [    0.0477],\n",
      "        [    0.0491],\n",
      "        [    0.0494],\n",
      "        [    0.0495],\n",
      "        [    0.0496],\n",
      "        [    0.0514],\n",
      "        [    0.0536],\n",
      "        [    0.0542],\n",
      "        [    0.0547],\n",
      "        [    0.0549],\n",
      "        [    0.0550],\n",
      "        [    0.0557],\n",
      "        [    0.0558],\n",
      "        [    0.0562],\n",
      "        [    0.0573],\n",
      "        [    0.0582],\n",
      "        [    0.0584],\n",
      "        [    0.0600],\n",
      "        [    0.0617],\n",
      "        [    0.0617],\n",
      "        [    0.0627],\n",
      "        [    0.0633],\n",
      "        [    0.0641],\n",
      "        [    0.0644],\n",
      "        [    0.0647],\n",
      "        [    0.0660],\n",
      "        [    0.0663],\n",
      "        [    0.0672],\n",
      "        [    0.0677],\n",
      "        [    0.0686],\n",
      "        [    0.0714],\n",
      "        [    0.0740],\n",
      "        [    0.0809],\n",
      "        [    0.0826],\n",
      "        [    0.0831],\n",
      "        [    0.0869],\n",
      "        [    0.0871],\n",
      "        [    0.0949],\n",
      "        [    0.0982],\n",
      "        [    0.0987],\n",
      "        [    0.1004],\n",
      "        [    0.1015],\n",
      "        [    0.1053],\n",
      "        [    0.1151],\n",
      "        [    0.1316],\n",
      "        [    0.1610],\n",
      "        [    0.1665],\n",
      "        [    0.1793]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.220446049250313e-08, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [3, 35, 94, 146, 86, 144, 38, 48, 49, 4, 147, 43, 24, 124, 85, 42, 31, 125, 14, 145, 1, 37, 30, 50, 54, 22, 39, 34, 11, 75, 41, 69, 80, 110, 128, 51, 36, 56, 81, 10, 136, 119, 120, 82, 28, 68, 74, 109, 59, 143, 126, 111, 23, 123, 40, 47, 98, 12, 2, 135, 95, 64, 29, 32, 55, 46, 71, 0, 27, 15, 137, 152, 142, 44, 121, 148, 70, 133, 127, 20, 97, 108, 93, 118, 129, 103, 65, 83, 116, 13, 5, 132, 102, 122, 92, 134, 138, 45, 150, 131, 149, 67, 130, 141, 57, 115, 19, 140, 117, 52, 79, 151, 53, 9, 21, 7, 96, 33, 153, 107, 73, 114, 25, 84, 72, 106, 139, 91, 90, 8, 100, 112, 87, 6, 58, 113, 66, 76, 26, 77, 18, 101, 154, 78, 88, 104, 60, 99, 89, 63, 105, 61, 62, 17, 16, 155] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6590],\n",
      "        [0.6227],\n",
      "        [0.2355],\n",
      "        [0.0887],\n",
      "        [0.2642],\n",
      "        [0.0887],\n",
      "        [0.6092],\n",
      "        [0.5295],\n",
      "        [0.5441],\n",
      "        [0.6551],\n",
      "        [0.0887],\n",
      "        [0.5957],\n",
      "        [0.5799],\n",
      "        [0.0887],\n",
      "        [0.2765],\n",
      "        [0.6015],\n",
      "        [0.6247],\n",
      "        [0.0887],\n",
      "        [0.5825],\n",
      "        [0.0887],\n",
      "        [0.6556],\n",
      "        [0.6371],\n",
      "        [0.5718],\n",
      "        [0.5276],\n",
      "        [0.4612],\n",
      "        [0.5414],\n",
      "        [0.6063],\n",
      "        [0.6327],\n",
      "        [0.6350],\n",
      "        [0.3798],\n",
      "        [0.6162],\n",
      "        [0.2954],\n",
      "        [0.3762],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.5190],\n",
      "        [0.6489],\n",
      "        [0.4583],\n",
      "        [0.3701],\n",
      "        [0.6254],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.3323],\n",
      "        [0.5919],\n",
      "        [0.2888],\n",
      "        [0.3439],\n",
      "        [0.0887],\n",
      "        [0.4206],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.5886],\n",
      "        [0.0887],\n",
      "        [0.6219],\n",
      "        [0.5704],\n",
      "        [0.1662],\n",
      "        [0.6121],\n",
      "        [0.6658],\n",
      "        [0.0887],\n",
      "        [0.2290],\n",
      "        [0.2934],\n",
      "        [0.5898],\n",
      "        [0.6499],\n",
      "        [0.4598],\n",
      "        [0.5838],\n",
      "        [0.3299],\n",
      "        [0.6637],\n",
      "        [0.5895],\n",
      "        [0.5961],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.5894],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.3267],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.4788],\n",
      "        [0.1920],\n",
      "        [0.0887],\n",
      "        [0.2257],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.1534],\n",
      "        [0.2545],\n",
      "        [0.2746],\n",
      "        [0.0887],\n",
      "        [0.5973],\n",
      "        [0.6392],\n",
      "        [0.0887],\n",
      "        [0.1204],\n",
      "        [0.0887],\n",
      "        [0.2263],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.6040],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.2746],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.4268],\n",
      "        [0.0887],\n",
      "        [0.5420],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.4835],\n",
      "        [0.3495],\n",
      "        [0.0887],\n",
      "        [0.4716],\n",
      "        [0.6184],\n",
      "        [0.4723],\n",
      "        [0.6360],\n",
      "        [0.2524],\n",
      "        [0.6168],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.3024],\n",
      "        [0.0887],\n",
      "        [0.5656],\n",
      "        [0.2668],\n",
      "        [0.2987],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.2435],\n",
      "        [0.2510],\n",
      "        [0.6251],\n",
      "        [0.1277],\n",
      "        [0.0887],\n",
      "        [0.2861],\n",
      "        [0.6303],\n",
      "        [0.3974],\n",
      "        [0.0887],\n",
      "        [0.2340],\n",
      "        [0.3797],\n",
      "        [0.5431],\n",
      "        [0.3712],\n",
      "        [0.5395],\n",
      "        [0.0971],\n",
      "        [0.0887],\n",
      "        [0.3494],\n",
      "        [0.2858],\n",
      "        [0.1294],\n",
      "        [0.4251],\n",
      "        [0.1174],\n",
      "        [0.2642],\n",
      "        [0.3703],\n",
      "        [0.1151],\n",
      "        [0.4184],\n",
      "        [0.3884],\n",
      "        [0.5902],\n",
      "        [0.5998],\n",
      "        [0.0887]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0011],\n",
      "        [    0.0019],\n",
      "        [    0.0019],\n",
      "        [    0.0020],\n",
      "        [    0.0021],\n",
      "        [    0.0022],\n",
      "        [    0.0030],\n",
      "        [    0.0035],\n",
      "        [    0.0039],\n",
      "        [    0.0045],\n",
      "        [    0.0050],\n",
      "        [    0.0052],\n",
      "        [    0.0052],\n",
      "        [    0.0056],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0062],\n",
      "        [    0.0063],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0084],\n",
      "        [    0.0089],\n",
      "        [    0.0090],\n",
      "        [    0.0098],\n",
      "        [    0.0101],\n",
      "        [    0.0103],\n",
      "        [    0.0103],\n",
      "        [    0.0104],\n",
      "        [    0.0113],\n",
      "        [    0.0116],\n",
      "        [    0.0117],\n",
      "        [    0.0118],\n",
      "        [    0.0131],\n",
      "        [    0.0139],\n",
      "        [    0.0150],\n",
      "        [    0.0156],\n",
      "        [    0.0156],\n",
      "        [    0.0158],\n",
      "        [    0.0162],\n",
      "        [    0.0171],\n",
      "        [    0.0175],\n",
      "        [    0.0181],\n",
      "        [    0.0181],\n",
      "        [    0.0189],\n",
      "        [    0.0190],\n",
      "        [    0.0195],\n",
      "        [    0.0195],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0202],\n",
      "        [    0.0204],\n",
      "        [    0.0219],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0224],\n",
      "        [    0.0229],\n",
      "        [    0.0243],\n",
      "        [    0.0246],\n",
      "        [    0.0248],\n",
      "        [    0.0255],\n",
      "        [    0.0260],\n",
      "        [    0.0260],\n",
      "        [    0.0269],\n",
      "        [    0.0274],\n",
      "        [    0.0280],\n",
      "        [    0.0281],\n",
      "        [    0.0283],\n",
      "        [    0.0292],\n",
      "        [    0.0292],\n",
      "        [    0.0294],\n",
      "        [    0.0296],\n",
      "        [    0.0301],\n",
      "        [    0.0304],\n",
      "        [    0.0305],\n",
      "        [    0.0309],\n",
      "        [    0.0312],\n",
      "        [    0.0323],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0329],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0347],\n",
      "        [    0.0347],\n",
      "        [    0.0353],\n",
      "        [    0.0365],\n",
      "        [    0.0368],\n",
      "        [    0.0374],\n",
      "        [    0.0387],\n",
      "        [    0.0402],\n",
      "        [    0.0407],\n",
      "        [    0.0416],\n",
      "        [    0.0418],\n",
      "        [    0.0422],\n",
      "        [    0.0423],\n",
      "        [    0.0428],\n",
      "        [    0.0429],\n",
      "        [    0.0439],\n",
      "        [    0.0440],\n",
      "        [    0.0445],\n",
      "        [    0.0446],\n",
      "        [    0.0447],\n",
      "        [    0.0447],\n",
      "        [    0.0451],\n",
      "        [    0.0462],\n",
      "        [    0.0466],\n",
      "        [    0.0477],\n",
      "        [    0.0491],\n",
      "        [    0.0494],\n",
      "        [    0.0495],\n",
      "        [    0.0496],\n",
      "        [    0.0514],\n",
      "        [    0.0536],\n",
      "        [    0.0542],\n",
      "        [    0.0547],\n",
      "        [    0.0549],\n",
      "        [    0.0550],\n",
      "        [    0.0557],\n",
      "        [    0.0558],\n",
      "        [    0.0562],\n",
      "        [    0.0573],\n",
      "        [    0.0582],\n",
      "        [    0.0584],\n",
      "        [    0.0600],\n",
      "        [    0.0617],\n",
      "        [    0.0617],\n",
      "        [    0.0627],\n",
      "        [    0.0633],\n",
      "        [    0.0641],\n",
      "        [    0.0644],\n",
      "        [    0.0647],\n",
      "        [    0.0660],\n",
      "        [    0.0663],\n",
      "        [    0.0672],\n",
      "        [    0.0677],\n",
      "        [    0.0686],\n",
      "        [    0.0714],\n",
      "        [    0.0740],\n",
      "        [    0.0809],\n",
      "        [    0.0826],\n",
      "        [    0.0831],\n",
      "        [    0.0869],\n",
      "        [    0.0871],\n",
      "        [    0.0949],\n",
      "        [    0.0982],\n",
      "        [    0.0987],\n",
      "        [    0.1004],\n",
      "        [    0.1015],\n",
      "        [    0.1053],\n",
      "        [    0.1151],\n",
      "        [    0.1316],\n",
      "        [    0.1610],\n",
      "        [    0.1665],\n",
      "        [    0.1793],\n",
      "        [    0.2211]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0050],\n",
      "        [0.0038],\n",
      "        [0.0030],\n",
      "        [0.0018],\n",
      "        [0.0027],\n",
      "        [0.0016],\n",
      "        [0.0042],\n",
      "        [0.0038],\n",
      "        [0.0047],\n",
      "        [0.0012],\n",
      "        [0.0007],\n",
      "        [0.0067],\n",
      "        [0.0014],\n",
      "        [0.0090],\n",
      "        [0.0064],\n",
      "        [0.0038],\n",
      "        [0.0032],\n",
      "        [0.0099],\n",
      "        [0.0042],\n",
      "        [0.0115],\n",
      "        [0.0135],\n",
      "        [0.0110],\n",
      "        [0.0067],\n",
      "        [0.0081],\n",
      "        [0.0098],\n",
      "        [0.0072],\n",
      "        [0.0084],\n",
      "        [0.0075],\n",
      "        [0.0071],\n",
      "        [0.0082],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0143],\n",
      "        [0.0169],\n",
      "        [0.0101],\n",
      "        [0.0142],\n",
      "        [0.0185],\n",
      "        [0.0156],\n",
      "        [0.0180],\n",
      "        [0.0129],\n",
      "        [0.0133],\n",
      "        [0.0213],\n",
      "        [0.0144],\n",
      "        [0.0195],\n",
      "        [0.0159],\n",
      "        [0.0173],\n",
      "        [0.0169],\n",
      "        [0.0233],\n",
      "        [0.0200],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0241],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0199],\n",
      "        [0.0236],\n",
      "        [0.0227],\n",
      "        [0.0215],\n",
      "        [0.0299],\n",
      "        [0.0286],\n",
      "        [0.0245],\n",
      "        [0.0251],\n",
      "        [0.0288],\n",
      "        [0.0238],\n",
      "        [0.0273],\n",
      "        [0.0294],\n",
      "        [0.0258],\n",
      "        [0.0337],\n",
      "        [0.0260],\n",
      "        [0.0316],\n",
      "        [0.0256],\n",
      "        [0.0259],\n",
      "        [0.0263],\n",
      "        [0.0321],\n",
      "        [0.0268],\n",
      "        [0.0272],\n",
      "        [0.0289],\n",
      "        [0.0360],\n",
      "        [0.0286],\n",
      "        [0.0311],\n",
      "        [0.0335],\n",
      "        [0.0377],\n",
      "        [0.0349],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0355],\n",
      "        [0.0374],\n",
      "        [0.0366],\n",
      "        [0.0412],\n",
      "        [0.0364],\n",
      "        [0.0355],\n",
      "        [0.0445],\n",
      "        [0.0421],\n",
      "        [0.0381],\n",
      "        [0.0428],\n",
      "        [0.0461],\n",
      "        [0.0391],\n",
      "        [0.0448],\n",
      "        [0.0402],\n",
      "        [0.0478],\n",
      "        [0.0407],\n",
      "        [0.0436],\n",
      "        [0.0484],\n",
      "        [0.0410],\n",
      "        [0.0453],\n",
      "        [0.0500],\n",
      "        [0.0487],\n",
      "        [0.0440],\n",
      "        [0.0528],\n",
      "        [0.0491],\n",
      "        [0.0472],\n",
      "        [0.0458],\n",
      "        [0.0513],\n",
      "        [0.0501],\n",
      "        [0.0523],\n",
      "        [0.0503],\n",
      "        [0.0561],\n",
      "        [0.0524],\n",
      "        [0.0519],\n",
      "        [0.0595],\n",
      "        [0.0544],\n",
      "        [0.0610],\n",
      "        [0.0548],\n",
      "        [0.0581],\n",
      "        [0.0583],\n",
      "        [0.0653],\n",
      "        [0.0580],\n",
      "        [0.0633],\n",
      "        [0.0640],\n",
      "        [0.0601],\n",
      "        [0.0648],\n",
      "        [0.0685],\n",
      "        [0.0670],\n",
      "        [0.0618],\n",
      "        [0.0678],\n",
      "        [0.0715],\n",
      "        [0.0688],\n",
      "        [0.0684],\n",
      "        [0.0714],\n",
      "        [0.0782],\n",
      "        [0.0844],\n",
      "        [0.0840],\n",
      "        [0.0832],\n",
      "        [0.0847],\n",
      "        [0.0959],\n",
      "        [0.0978],\n",
      "        [0.0989],\n",
      "        [0.1012],\n",
      "        [0.1023],\n",
      "        [0.1053],\n",
      "        [0.1144],\n",
      "        [0.1320],\n",
      "        [0.1611],\n",
      "        [0.1687],\n",
      "        [0.1816],\n",
      "        [0.2173]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 73.87379217147827\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.098975179862464e-07, 147)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [147, 4, 24, 144, 146, 86, 94, 31, 42, 35, 48, 14, 38, 49, 3, 85, 30, 43, 11, 22, 34, 50, 75, 39, 124, 41, 69, 54, 125, 128, 37, 145, 10, 136, 1, 51, 80, 120, 56, 28, 143, 126, 110, 74, 68, 81, 23, 123, 36, 82, 40, 59, 119, 12, 98, 109, 47, 32, 111, 95, 64, 137, 71, 152, 27, 142, 121, 148, 55, 135, 127, 29, 70, 46, 2, 20, 15, 44, 97, 0, 93, 103, 5, 133, 13, 83, 65, 108, 122, 118, 129, 138, 150, 149, 141, 116, 102, 92, 67, 140, 132, 45, 57, 151, 134, 79, 131, 130, 19, 52, 115, 9, 7, 53, 153, 21, 33, 117, 73, 25, 96, 139, 84, 72, 107, 8, 114, 6, 91, 90, 100, 106, 87, 58, 76, 112, 66, 26, 113, 77, 154, 101, 18, 78, 88, 104, 60, 99, 89, 63, 105, 61, 62, 17, 16, 155, 156] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.0924],\n",
      "        [0.6602],\n",
      "        [0.5837],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.2649],\n",
      "        [0.2366],\n",
      "        [0.6274],\n",
      "        [0.6034],\n",
      "        [0.6254],\n",
      "        [0.5303],\n",
      "        [0.5846],\n",
      "        [0.6112],\n",
      "        [0.5453],\n",
      "        [0.6642],\n",
      "        [0.2774],\n",
      "        [0.5740],\n",
      "        [0.5974],\n",
      "        [0.6383],\n",
      "        [0.5442],\n",
      "        [0.6355],\n",
      "        [0.5285],\n",
      "        [0.3829],\n",
      "        [0.6082],\n",
      "        [0.0924],\n",
      "        [0.6184],\n",
      "        [0.2976],\n",
      "        [0.4612],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.6396],\n",
      "        [0.0924],\n",
      "        [0.6287],\n",
      "        [0.0924],\n",
      "        [0.6609],\n",
      "        [0.5198],\n",
      "        [0.3788],\n",
      "        [0.0924],\n",
      "        [0.4584],\n",
      "        [0.5949],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.3465],\n",
      "        [0.2905],\n",
      "        [0.3723],\n",
      "        [0.5924],\n",
      "        [0.0924],\n",
      "        [0.6519],\n",
      "        [0.3337],\n",
      "        [0.6241],\n",
      "        [0.4205],\n",
      "        [0.0924],\n",
      "        [0.6148],\n",
      "        [0.1665],\n",
      "        [0.0924],\n",
      "        [0.5715],\n",
      "        [0.6529],\n",
      "        [0.0924],\n",
      "        [0.2300],\n",
      "        [0.2925],\n",
      "        [0.0924],\n",
      "        [0.3322],\n",
      "        [0.0924],\n",
      "        [0.5927],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.4600],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.5925],\n",
      "        [0.3290],\n",
      "        [0.5852],\n",
      "        [0.6711],\n",
      "        [0.4803],\n",
      "        [0.5985],\n",
      "        [0.5910],\n",
      "        [0.1926],\n",
      "        [0.6690],\n",
      "        [0.2266],\n",
      "        [0.1536],\n",
      "        [0.6440],\n",
      "        [0.0924],\n",
      "        [0.5996],\n",
      "        [0.2749],\n",
      "        [0.2536],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.1200],\n",
      "        [0.2269],\n",
      "        [0.2757],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.6058],\n",
      "        [0.4266],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.3518],\n",
      "        [0.0924],\n",
      "        [0.0924],\n",
      "        [0.5441],\n",
      "        [0.4838],\n",
      "        [0.0924],\n",
      "        [0.6220],\n",
      "        [0.6405],\n",
      "        [0.4718],\n",
      "        [0.0924],\n",
      "        [0.4742],\n",
      "        [0.6193],\n",
      "        [0.0924],\n",
      "        [0.3042],\n",
      "        [0.5690],\n",
      "        [0.2537],\n",
      "        [0.0924],\n",
      "        [0.2672],\n",
      "        [0.3004],\n",
      "        [0.0924],\n",
      "        [0.6291],\n",
      "        [0.0924],\n",
      "        [0.6348],\n",
      "        [0.2441],\n",
      "        [0.2516],\n",
      "        [0.1273],\n",
      "        [0.0924],\n",
      "        [0.2871],\n",
      "        [0.3967],\n",
      "        [0.3826],\n",
      "        [0.0924],\n",
      "        [0.2339],\n",
      "        [0.5457],\n",
      "        [0.0924],\n",
      "        [0.3740],\n",
      "        [0.0924],\n",
      "        [0.0962],\n",
      "        [0.5413],\n",
      "        [0.3518],\n",
      "        [0.2867],\n",
      "        [0.1291],\n",
      "        [0.4252],\n",
      "        [0.1166],\n",
      "        [0.2651],\n",
      "        [0.3702],\n",
      "        [0.1144],\n",
      "        [0.4188],\n",
      "        [0.3884],\n",
      "        [0.5924],\n",
      "        [0.6021],\n",
      "        [0.0924],\n",
      "        [0.0924]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0012],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0018],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0038],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0050],\n",
      "        [0.0064],\n",
      "        [0.0067],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0075],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0090],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.0129],\n",
      "        [0.0133],\n",
      "        [0.0135],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0144],\n",
      "        [0.0156],\n",
      "        [0.0159],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0173],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0195],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0213],\n",
      "        [0.0215],\n",
      "        [0.0227],\n",
      "        [0.0233],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0241],\n",
      "        [0.0245],\n",
      "        [0.0251],\n",
      "        [0.0256],\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0268],\n",
      "        [0.0272],\n",
      "        [0.0273],\n",
      "        [0.0286],\n",
      "        [0.0286],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0294],\n",
      "        [0.0299],\n",
      "        [0.0311],\n",
      "        [0.0316],\n",
      "        [0.0321],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0349],\n",
      "        [0.0355],\n",
      "        [0.0355],\n",
      "        [0.0360],\n",
      "        [0.0364],\n",
      "        [0.0366],\n",
      "        [0.0374],\n",
      "        [0.0377],\n",
      "        [0.0381],\n",
      "        [0.0384],\n",
      "        [0.0385],\n",
      "        [0.0391],\n",
      "        [0.0402],\n",
      "        [0.0407],\n",
      "        [0.0410],\n",
      "        [0.0412],\n",
      "        [0.0421],\n",
      "        [0.0428],\n",
      "        [0.0436],\n",
      "        [0.0440],\n",
      "        [0.0445],\n",
      "        [0.0448],\n",
      "        [0.0453],\n",
      "        [0.0458],\n",
      "        [0.0461],\n",
      "        [0.0472],\n",
      "        [0.0478],\n",
      "        [0.0484],\n",
      "        [0.0487],\n",
      "        [0.0491],\n",
      "        [0.0500],\n",
      "        [0.0501],\n",
      "        [0.0503],\n",
      "        [0.0513],\n",
      "        [0.0519],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0528],\n",
      "        [0.0544],\n",
      "        [0.0548],\n",
      "        [0.0561],\n",
      "        [0.0580],\n",
      "        [0.0581],\n",
      "        [0.0583],\n",
      "        [0.0595],\n",
      "        [0.0601],\n",
      "        [0.0610],\n",
      "        [0.0618],\n",
      "        [0.0633],\n",
      "        [0.0640],\n",
      "        [0.0648],\n",
      "        [0.0653],\n",
      "        [0.0670],\n",
      "        [0.0678],\n",
      "        [0.0684],\n",
      "        [0.0685],\n",
      "        [0.0688],\n",
      "        [0.0714],\n",
      "        [0.0715],\n",
      "        [0.0782],\n",
      "        [0.0832],\n",
      "        [0.0840],\n",
      "        [0.0844],\n",
      "        [0.0847],\n",
      "        [0.0959],\n",
      "        [0.0978],\n",
      "        [0.0989],\n",
      "        [0.1012],\n",
      "        [0.1023],\n",
      "        [0.1053],\n",
      "        [0.1144],\n",
      "        [0.1320],\n",
      "        [0.1611],\n",
      "        [0.1687],\n",
      "        [0.1816],\n",
      "        [0.2173],\n",
      "        [0.3167]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.31\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.31\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 55\n",
      "Number of shrink: 10\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0061],\n",
      "        [0.0025],\n",
      "        [0.0003],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0022],\n",
      "        [0.0029],\n",
      "        [0.0029],\n",
      "        [0.0039],\n",
      "        [0.0043],\n",
      "        [0.0032],\n",
      "        [0.0044],\n",
      "        [0.0041],\n",
      "        [0.0044],\n",
      "        [0.0063],\n",
      "        [0.0058],\n",
      "        [0.0067],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0066],\n",
      "        [0.0070],\n",
      "        [0.0086],\n",
      "        [0.0073],\n",
      "        [0.0086],\n",
      "        [0.0158],\n",
      "        [0.0093],\n",
      "        [0.0086],\n",
      "        [0.0106],\n",
      "        [0.0168],\n",
      "        [0.0033],\n",
      "        [0.0112],\n",
      "        [0.0183],\n",
      "        [0.0125],\n",
      "        [0.0065],\n",
      "        [0.0148],\n",
      "        [0.0147],\n",
      "        [0.0148],\n",
      "        [0.0076],\n",
      "        [0.0164],\n",
      "        [0.0153],\n",
      "        [0.0094],\n",
      "        [0.0096],\n",
      "        [0.0237],\n",
      "        [0.0163],\n",
      "        [0.0166],\n",
      "        [0.0182],\n",
      "        [0.0169],\n",
      "        [0.0114],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0198],\n",
      "        [0.0209],\n",
      "        [0.0281],\n",
      "        [0.0215],\n",
      "        [0.0235],\n",
      "        [0.0301],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0309],\n",
      "        [0.0247],\n",
      "        [0.0239],\n",
      "        [0.0188],\n",
      "        [0.0251],\n",
      "        [0.0190],\n",
      "        [0.0252],\n",
      "        [0.0195],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0280],\n",
      "        [0.0354],\n",
      "        [0.0218],\n",
      "        [0.0291],\n",
      "        [0.0282],\n",
      "        [0.0291],\n",
      "        [0.0313],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0330],\n",
      "        [0.0350],\n",
      "        [0.0347],\n",
      "        [0.0348],\n",
      "        [0.0344],\n",
      "        [0.0429],\n",
      "        [0.0365],\n",
      "        [0.0377],\n",
      "        [0.0384],\n",
      "        [0.0445],\n",
      "        [0.0313],\n",
      "        [0.0452],\n",
      "        [0.0453],\n",
      "        [0.0323],\n",
      "        [0.0334],\n",
      "        [0.0339],\n",
      "        [0.0342],\n",
      "        [0.0480],\n",
      "        [0.0431],\n",
      "        [0.0424],\n",
      "        [0.0432],\n",
      "        [0.0371],\n",
      "        [0.0513],\n",
      "        [0.0448],\n",
      "        [0.0463],\n",
      "        [0.0390],\n",
      "        [0.0529],\n",
      "        [0.0468],\n",
      "        [0.0546],\n",
      "        [0.0552],\n",
      "        [0.0487],\n",
      "        [0.0499],\n",
      "        [0.0568],\n",
      "        [0.0495],\n",
      "        [0.0493],\n",
      "        [0.0521],\n",
      "        [0.0451],\n",
      "        [0.0523],\n",
      "        [0.0522],\n",
      "        [0.0596],\n",
      "        [0.0541],\n",
      "        [0.0540],\n",
      "        [0.0561],\n",
      "        [0.0512],\n",
      "        [0.0590],\n",
      "        [0.0580],\n",
      "        [0.0664],\n",
      "        [0.0594],\n",
      "        [0.0678],\n",
      "        [0.0608],\n",
      "        [0.0628],\n",
      "        [0.0635],\n",
      "        [0.0660],\n",
      "        [0.0722],\n",
      "        [0.0665],\n",
      "        [0.0690],\n",
      "        [0.0676],\n",
      "        [0.0753],\n",
      "        [0.0692],\n",
      "        [0.0709],\n",
      "        [0.0783],\n",
      "        [0.0776],\n",
      "        [0.0764],\n",
      "        [0.0810],\n",
      "        [0.0842],\n",
      "        [0.0843],\n",
      "        [0.0954],\n",
      "        [0.0968],\n",
      "        [0.0982],\n",
      "        [0.1028],\n",
      "        [0.1019],\n",
      "        [0.1045],\n",
      "        [0.1134],\n",
      "        [0.1315],\n",
      "        [0.1603],\n",
      "        [0.1686],\n",
      "        [0.1816],\n",
      "        [0.2105],\n",
      "        [0.3099]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.31335684]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 16\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0060],\n",
      "        [0.0022],\n",
      "        [0.0006],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0020],\n",
      "        [0.0027],\n",
      "        [0.0031],\n",
      "        [0.0042],\n",
      "        [0.0040],\n",
      "        [0.0030],\n",
      "        [0.0047],\n",
      "        [0.0038],\n",
      "        [0.0041],\n",
      "        [0.0061],\n",
      "        [0.0056],\n",
      "        [0.0070],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0089],\n",
      "        [0.0075],\n",
      "        [0.0088],\n",
      "        [0.0157],\n",
      "        [0.0096],\n",
      "        [0.0088],\n",
      "        [0.0108],\n",
      "        [0.0167],\n",
      "        [0.0034],\n",
      "        [0.0109],\n",
      "        [0.0182],\n",
      "        [0.0127],\n",
      "        [0.0066],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0146],\n",
      "        [0.0076],\n",
      "        [0.0166],\n",
      "        [0.0155],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0236],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0180],\n",
      "        [0.0172],\n",
      "        [0.0115],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0201],\n",
      "        [0.0211],\n",
      "        [0.0280],\n",
      "        [0.0217],\n",
      "        [0.0237],\n",
      "        [0.0300],\n",
      "        [0.0228],\n",
      "        [0.0235],\n",
      "        [0.0309],\n",
      "        [0.0249],\n",
      "        [0.0237],\n",
      "        [0.0189],\n",
      "        [0.0253],\n",
      "        [0.0191],\n",
      "        [0.0254],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0282],\n",
      "        [0.0353],\n",
      "        [0.0219],\n",
      "        [0.0289],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0310],\n",
      "        [0.0316],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0329],\n",
      "        [0.0347],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0428],\n",
      "        [0.0368],\n",
      "        [0.0378],\n",
      "        [0.0386],\n",
      "        [0.0444],\n",
      "        [0.0313],\n",
      "        [0.0452],\n",
      "        [0.0452],\n",
      "        [0.0324],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0479],\n",
      "        [0.0432],\n",
      "        [0.0422],\n",
      "        [0.0434],\n",
      "        [0.0372],\n",
      "        [0.0512],\n",
      "        [0.0445],\n",
      "        [0.0465],\n",
      "        [0.0391],\n",
      "        [0.0528],\n",
      "        [0.0470],\n",
      "        [0.0545],\n",
      "        [0.0551],\n",
      "        [0.0484],\n",
      "        [0.0501],\n",
      "        [0.0567],\n",
      "        [0.0498],\n",
      "        [0.0495],\n",
      "        [0.0523],\n",
      "        [0.0452],\n",
      "        [0.0526],\n",
      "        [0.0524],\n",
      "        [0.0595],\n",
      "        [0.0543],\n",
      "        [0.0542],\n",
      "        [0.0559],\n",
      "        [0.0512],\n",
      "        [0.0592],\n",
      "        [0.0582],\n",
      "        [0.0663],\n",
      "        [0.0597],\n",
      "        [0.0678],\n",
      "        [0.0610],\n",
      "        [0.0626],\n",
      "        [0.0633],\n",
      "        [0.0662],\n",
      "        [0.0721],\n",
      "        [0.0664],\n",
      "        [0.0692],\n",
      "        [0.0678],\n",
      "        [0.0752],\n",
      "        [0.0694],\n",
      "        [0.0711],\n",
      "        [0.0782],\n",
      "        [0.0778],\n",
      "        [0.0765],\n",
      "        [0.0811],\n",
      "        [0.0840],\n",
      "        [0.0845],\n",
      "        [0.0952],\n",
      "        [0.0966],\n",
      "        [0.0980],\n",
      "        [0.1030],\n",
      "        [0.1017],\n",
      "        [0.1043],\n",
      "        [0.1132],\n",
      "        [0.1313],\n",
      "        [0.1601],\n",
      "        [0.1684],\n",
      "        [0.1813],\n",
      "        [0.2106],\n",
      "        [0.3100]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 74.13577675819397\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.3296657875325764e-07, 24)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [24, 86, 4, 94, 48, 31, 128, 38, 35, 49, 42, 14, 85, 147, 3, 43, 136, 22, 11, 30, 34, 75, 120, 144, 146, 69, 39, 50, 143, 41, 126, 54, 37, 123, 10, 1, 80, 51, 28, 124, 74, 56, 125, 68, 23, 81, 145, 36, 137, 82, 152, 142, 121, 40, 148, 59, 12, 127, 47, 32, 110, 98, 64, 95, 71, 27, 119, 55, 70, 46, 29, 109, 111, 2, 15, 122, 44, 20, 138, 97, 150, 149, 141, 93, 5, 103, 0, 135, 13, 140, 83, 65, 151, 92, 133, 102, 67, 108, 45, 118, 129, 153, 57, 79, 116, 19, 7, 9, 52, 132, 139, 53, 33, 21, 134, 25, 73, 131, 130, 96, 115, 72, 84, 117, 8, 6, 91, 90, 100, 107, 87, 114, 76, 58, 66, 26, 106, 112, 154, 77, 113, 101, 18, 78, 88, 104, 60, 89, 99, 63, 105, 61, 62, 17, 16, 155, 156, 157] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5845],\n",
      "        [0.2642],\n",
      "        [0.6612],\n",
      "        [0.2363],\n",
      "        [0.5294],\n",
      "        [0.6274],\n",
      "        [0.0991],\n",
      "        [0.6108],\n",
      "        [0.6256],\n",
      "        [0.5447],\n",
      "        [0.6031],\n",
      "        [0.5842],\n",
      "        [0.2766],\n",
      "        [0.0991],\n",
      "        [0.6652],\n",
      "        [0.5970],\n",
      "        [0.0991],\n",
      "        [0.5445],\n",
      "        [0.6385],\n",
      "        [0.5737],\n",
      "        [0.6357],\n",
      "        [0.3836],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2983],\n",
      "        [0.6077],\n",
      "        [0.5278],\n",
      "        [0.0991],\n",
      "        [0.6182],\n",
      "        [0.0991],\n",
      "        [0.4601],\n",
      "        [0.6396],\n",
      "        [0.0991],\n",
      "        [0.6288],\n",
      "        [0.6620],\n",
      "        [0.3791],\n",
      "        [0.5191],\n",
      "        [0.5953],\n",
      "        [0.0991],\n",
      "        [0.3470],\n",
      "        [0.4573],\n",
      "        [0.0991],\n",
      "        [0.2911],\n",
      "        [0.5933],\n",
      "        [0.3723],\n",
      "        [0.0991],\n",
      "        [0.6522],\n",
      "        [0.0991],\n",
      "        [0.3331],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.6239],\n",
      "        [0.0991],\n",
      "        [0.4193],\n",
      "        [0.6146],\n",
      "        [0.0991],\n",
      "        [0.5708],\n",
      "        [0.6532],\n",
      "        [0.0991],\n",
      "        [0.1654],\n",
      "        [0.2912],\n",
      "        [0.2297],\n",
      "        [0.3327],\n",
      "        [0.5933],\n",
      "        [0.0991],\n",
      "        [0.4590],\n",
      "        [0.3295],\n",
      "        [0.5846],\n",
      "        [0.5926],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.6722],\n",
      "        [0.5982],\n",
      "        [0.0991],\n",
      "        [0.5906],\n",
      "        [0.4798],\n",
      "        [0.0991],\n",
      "        [0.1919],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2262],\n",
      "        [0.6448],\n",
      "        [0.1527],\n",
      "        [0.6700],\n",
      "        [0.0991],\n",
      "        [0.5992],\n",
      "        [0.0991],\n",
      "        [0.2736],\n",
      "        [0.2524],\n",
      "        [0.0991],\n",
      "        [0.2263],\n",
      "        [0.0991],\n",
      "        [0.1188],\n",
      "        [0.2759],\n",
      "        [0.0991],\n",
      "        [0.6056],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.4254],\n",
      "        [0.3520],\n",
      "        [0.0991],\n",
      "        [0.5438],\n",
      "        [0.6412],\n",
      "        [0.6222],\n",
      "        [0.4828],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.4707],\n",
      "        [0.6193],\n",
      "        [0.4740],\n",
      "        [0.0991],\n",
      "        [0.5696],\n",
      "        [0.3043],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2534],\n",
      "        [0.0991],\n",
      "        [0.3005],\n",
      "        [0.2660],\n",
      "        [0.0991],\n",
      "        [0.6296],\n",
      "        [0.6356],\n",
      "        [0.2434],\n",
      "        [0.2510],\n",
      "        [0.1259],\n",
      "        [0.0991],\n",
      "        [0.2865],\n",
      "        [0.0991],\n",
      "        [0.3832],\n",
      "        [0.3953],\n",
      "        [0.2333],\n",
      "        [0.5460],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.3744],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.5409],\n",
      "        [0.3520],\n",
      "        [0.2861],\n",
      "        [0.1279],\n",
      "        [0.4244],\n",
      "        [0.2644],\n",
      "        [0.1149],\n",
      "        [0.3692],\n",
      "        [0.1132],\n",
      "        [0.4181],\n",
      "        [0.3875],\n",
      "        [0.5921],\n",
      "        [0.6018],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0115],\n",
      "        [0.0127],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0211],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0228],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0237],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0300],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0347],\n",
      "        [0.0353],\n",
      "        [0.0368],\n",
      "        [0.0372],\n",
      "        [0.0378],\n",
      "        [0.0386],\n",
      "        [0.0391],\n",
      "        [0.0422],\n",
      "        [0.0428],\n",
      "        [0.0432],\n",
      "        [0.0434],\n",
      "        [0.0444],\n",
      "        [0.0445],\n",
      "        [0.0452],\n",
      "        [0.0452],\n",
      "        [0.0452],\n",
      "        [0.0465],\n",
      "        [0.0470],\n",
      "        [0.0479],\n",
      "        [0.0484],\n",
      "        [0.0495],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0526],\n",
      "        [0.0528],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0545],\n",
      "        [0.0551],\n",
      "        [0.0559],\n",
      "        [0.0567],\n",
      "        [0.0582],\n",
      "        [0.0592],\n",
      "        [0.0595],\n",
      "        [0.0597],\n",
      "        [0.0610],\n",
      "        [0.0626],\n",
      "        [0.0633],\n",
      "        [0.0662],\n",
      "        [0.0663],\n",
      "        [0.0664],\n",
      "        [0.0678],\n",
      "        [0.0678],\n",
      "        [0.0692],\n",
      "        [0.0694],\n",
      "        [0.0711],\n",
      "        [0.0721],\n",
      "        [0.0752],\n",
      "        [0.0765],\n",
      "        [0.0778],\n",
      "        [0.0782],\n",
      "        [0.0811],\n",
      "        [0.0840],\n",
      "        [0.0845],\n",
      "        [0.0952],\n",
      "        [0.0966],\n",
      "        [0.0980],\n",
      "        [0.1017],\n",
      "        [0.1030],\n",
      "        [0.1043],\n",
      "        [0.1132],\n",
      "        [0.1313],\n",
      "        [0.1601],\n",
      "        [0.1684],\n",
      "        [0.1813],\n",
      "        [0.2106],\n",
      "        [0.3100],\n",
      "        [0.3680]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.31\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.31\n",
      "Matching的第10000回合\n",
      "Matching finished - the network is Unacceptable\n",
      "Number of enlarge: 6629\n",
      "Number of shrink: 3371\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0038],\n",
      "        [0.0040],\n",
      "        [0.0041],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0075],\n",
      "        [0.0076],\n",
      "        [0.0084],\n",
      "        [0.0085],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0095],\n",
      "        [0.0096],\n",
      "        [0.0097],\n",
      "        [0.0108],\n",
      "        [0.0109],\n",
      "        [0.0115],\n",
      "        [0.0127],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0167],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0191],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0211],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0228],\n",
      "        [0.0235],\n",
      "        [0.0236],\n",
      "        [0.0237],\n",
      "        [0.0237],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0280],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0300],\n",
      "        [0.0309],\n",
      "        [0.0310],\n",
      "        [0.0313],\n",
      "        [0.0313],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0347],\n",
      "        [0.0353],\n",
      "        [0.0368],\n",
      "        [0.0372],\n",
      "        [0.0378],\n",
      "        [0.0386],\n",
      "        [0.0391],\n",
      "        [0.0422],\n",
      "        [0.0428],\n",
      "        [0.0432],\n",
      "        [0.0434],\n",
      "        [0.0444],\n",
      "        [0.0445],\n",
      "        [0.0452],\n",
      "        [0.0452],\n",
      "        [0.0452],\n",
      "        [0.0465],\n",
      "        [0.0470],\n",
      "        [0.0479],\n",
      "        [0.0484],\n",
      "        [0.0495],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0526],\n",
      "        [0.0528],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0545],\n",
      "        [0.0551],\n",
      "        [0.0559],\n",
      "        [0.0567],\n",
      "        [0.0582],\n",
      "        [0.0592],\n",
      "        [0.0595],\n",
      "        [0.0597],\n",
      "        [0.0610],\n",
      "        [0.0626],\n",
      "        [0.0633],\n",
      "        [0.0662],\n",
      "        [0.0663],\n",
      "        [0.0664],\n",
      "        [0.0678],\n",
      "        [0.0678],\n",
      "        [0.0692],\n",
      "        [0.0694],\n",
      "        [0.0711],\n",
      "        [0.0721],\n",
      "        [0.0752],\n",
      "        [0.0765],\n",
      "        [0.0778],\n",
      "        [0.0782],\n",
      "        [0.0811],\n",
      "        [0.0840],\n",
      "        [0.0845],\n",
      "        [0.0952],\n",
      "        [0.0966],\n",
      "        [0.0980],\n",
      "        [0.1017],\n",
      "        [0.1030],\n",
      "        [0.1043],\n",
      "        [0.1132],\n",
      "        [0.1313],\n",
      "        [0.1601],\n",
      "        [0.1684],\n",
      "        [0.1813],\n",
      "        [0.2106],\n",
      "        [0.3100],\n",
      "        [0.3680]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.31335684]\n",
      "<<Cramming module>>\n",
      "threshold_for_error: 0.31\n",
      "不滿足個數： 1\n",
      "The index of the undesired data: tensor([[157,   0]], device='cuda:0')\n",
      "Cramming success!\n",
      "<<Cramming後看一下差異>>\n",
      "tensor([[    0.0006],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0027],\n",
      "        [    0.0030],\n",
      "        [    0.0031],\n",
      "        [    0.0034],\n",
      "        [    0.0038],\n",
      "        [    0.0040],\n",
      "        [    0.0041],\n",
      "        [    0.0041],\n",
      "        [    0.0047],\n",
      "        [    0.0056],\n",
      "        [    0.0060],\n",
      "        [    0.0061],\n",
      "        [    0.0064],\n",
      "        [    0.0066],\n",
      "        [    0.0069],\n",
      "        [    0.0069],\n",
      "        [    0.0070],\n",
      "        [    0.0073],\n",
      "        [    0.0075],\n",
      "        [    0.0076],\n",
      "        [    0.0084],\n",
      "        [    0.0085],\n",
      "        [    0.0088],\n",
      "        [    0.0089],\n",
      "        [    0.0089],\n",
      "        [    0.0095],\n",
      "        [    0.0095],\n",
      "        [    0.0097],\n",
      "        [    0.0108],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0127],\n",
      "        [    0.0145],\n",
      "        [    0.0146],\n",
      "        [    0.0150],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0167],\n",
      "        [    0.0167],\n",
      "        [    0.0172],\n",
      "        [    0.0180],\n",
      "        [    0.0182],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0189],\n",
      "        [    0.0191],\n",
      "        [    0.0196],\n",
      "        [    0.0201],\n",
      "        [    0.0202],\n",
      "        [    0.0205],\n",
      "        [    0.0211],\n",
      "        [    0.0218],\n",
      "        [    0.0219],\n",
      "        [    0.0228],\n",
      "        [    0.0235],\n",
      "        [    0.0236],\n",
      "        [    0.0237],\n",
      "        [    0.0238],\n",
      "        [    0.0249],\n",
      "        [    0.0253],\n",
      "        [    0.0255],\n",
      "        [    0.0280],\n",
      "        [    0.0282],\n",
      "        [    0.0284],\n",
      "        [    0.0288],\n",
      "        [    0.0290],\n",
      "        [    0.0300],\n",
      "        [    0.0309],\n",
      "        [    0.0311],\n",
      "        [    0.0313],\n",
      "        [    0.0313],\n",
      "        [    0.0315],\n",
      "        [    0.0316],\n",
      "        [    0.0324],\n",
      "        [    0.0329],\n",
      "        [    0.0335],\n",
      "        [    0.0340],\n",
      "        [    0.0342],\n",
      "        [    0.0345],\n",
      "        [    0.0345],\n",
      "        [    0.0346],\n",
      "        [    0.0347],\n",
      "        [    0.0353],\n",
      "        [    0.0367],\n",
      "        [    0.0372],\n",
      "        [    0.0379],\n",
      "        [    0.0386],\n",
      "        [    0.0391],\n",
      "        [    0.0422],\n",
      "        [    0.0428],\n",
      "        [    0.0432],\n",
      "        [    0.0434],\n",
      "        [    0.0444],\n",
      "        [    0.0445],\n",
      "        [    0.0452],\n",
      "        [    0.0452],\n",
      "        [    0.0452],\n",
      "        [    0.0465],\n",
      "        [    0.0470],\n",
      "        [    0.0479],\n",
      "        [    0.0484],\n",
      "        [    0.0496],\n",
      "        [    0.0498],\n",
      "        [    0.0501],\n",
      "        [    0.0512],\n",
      "        [    0.0512],\n",
      "        [    0.0523],\n",
      "        [    0.0524],\n",
      "        [    0.0526],\n",
      "        [    0.0528],\n",
      "        [    0.0542],\n",
      "        [    0.0543],\n",
      "        [    0.0545],\n",
      "        [    0.0551],\n",
      "        [    0.0559],\n",
      "        [    0.0567],\n",
      "        [    0.0582],\n",
      "        [    0.0592],\n",
      "        [    0.0595],\n",
      "        [    0.0597],\n",
      "        [    0.0611],\n",
      "        [    0.0626],\n",
      "        [    0.0633],\n",
      "        [    0.0662],\n",
      "        [    0.0663],\n",
      "        [    0.0664],\n",
      "        [    0.0678],\n",
      "        [    0.0678],\n",
      "        [    0.0692],\n",
      "        [    0.0694],\n",
      "        [    0.0710],\n",
      "        [    0.0721],\n",
      "        [    0.0752],\n",
      "        [    0.0765],\n",
      "        [    0.0778],\n",
      "        [    0.0782],\n",
      "        [    0.0811],\n",
      "        [    0.0840],\n",
      "        [    0.0845],\n",
      "        [    0.0952],\n",
      "        [    0.0966],\n",
      "        [    0.0980],\n",
      "        [    0.1017],\n",
      "        [    0.1029],\n",
      "        [    0.1043],\n",
      "        [    0.1132],\n",
      "        [    0.1313],\n",
      "        [    0.1601],\n",
      "        [    0.1684],\n",
      "        [    0.1813],\n",
      "        [    0.2106],\n",
      "        [    0.3100],\n",
      "        [    0.0000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 64\n",
      "Number of shrink: 36\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0093],\n",
      "        [0.0148],\n",
      "        [0.0063],\n",
      "        [0.0027],\n",
      "        [0.0063],\n",
      "        [0.0008],\n",
      "        [0.0034],\n",
      "        [0.0012],\n",
      "        [0.0031],\n",
      "        [0.0046],\n",
      "        [0.0092],\n",
      "        [0.0176],\n",
      "        [0.0162],\n",
      "        [0.0060],\n",
      "        [0.0031],\n",
      "        [0.0007],\n",
      "        [0.0066],\n",
      "        [0.0017],\n",
      "        [0.0208],\n",
      "        [0.0028],\n",
      "        [0.0089],\n",
      "        [0.0071],\n",
      "        [0.0077],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0151],\n",
      "        [0.0165],\n",
      "        [0.0095],\n",
      "        [0.0143],\n",
      "        [0.0098],\n",
      "        [0.0140],\n",
      "        [0.0088],\n",
      "        [0.0116],\n",
      "        [0.0239],\n",
      "        [0.0068],\n",
      "        [0.0226],\n",
      "        [0.0209],\n",
      "        [0.0105],\n",
      "        [0.0157],\n",
      "        [0.0019],\n",
      "        [0.0218],\n",
      "        [0.0166],\n",
      "        [0.0019],\n",
      "        [0.0089],\n",
      "        [0.0239],\n",
      "        [0.0182],\n",
      "        [0.0179],\n",
      "        [0.0189],\n",
      "        [0.0223],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0261],\n",
      "        [0.0205],\n",
      "        [0.0289],\n",
      "        [0.0368],\n",
      "        [0.0219],\n",
      "        [0.0106],\n",
      "        [0.0242],\n",
      "        [0.0236],\n",
      "        [0.0238],\n",
      "        [0.0251],\n",
      "        [0.0250],\n",
      "        [0.0084],\n",
      "        [0.0181],\n",
      "        [0.0280],\n",
      "        [0.0311],\n",
      "        [0.0108],\n",
      "        [0.0160],\n",
      "        [0.0328],\n",
      "        [0.0300],\n",
      "        [0.0308],\n",
      "        [0.0207],\n",
      "        [0.0188],\n",
      "        [0.0314],\n",
      "        [0.0206],\n",
      "        [0.0404],\n",
      "        [0.0324],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0396],\n",
      "        [0.0346],\n",
      "        [0.0273],\n",
      "        [0.0353],\n",
      "        [0.0520],\n",
      "        [0.0373],\n",
      "        [0.0327],\n",
      "        [0.0389],\n",
      "        [0.0391],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0433],\n",
      "        [0.0358],\n",
      "        [0.0444],\n",
      "        [0.0325],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0453],\n",
      "        [0.0582],\n",
      "        [0.0375],\n",
      "        [0.0479],\n",
      "        [0.0349],\n",
      "        [0.0570],\n",
      "        [0.0572],\n",
      "        [0.0543],\n",
      "        [0.0512],\n",
      "        [0.0513],\n",
      "        [0.0557],\n",
      "        [0.0526],\n",
      "        [0.0554],\n",
      "        [0.0528],\n",
      "        [0.0479],\n",
      "        [0.0380],\n",
      "        [0.0545],\n",
      "        [0.0551],\n",
      "        [0.0558],\n",
      "        [0.0567],\n",
      "        [0.0418],\n",
      "        [0.0524],\n",
      "        [0.0595],\n",
      "        [0.0692],\n",
      "        [0.0665],\n",
      "        [0.0839],\n",
      "        [0.0824],\n",
      "        [0.0662],\n",
      "        [0.0662],\n",
      "        [0.0812],\n",
      "        [0.0677],\n",
      "        [0.0551],\n",
      "        [0.0788],\n",
      "        [0.0693],\n",
      "        [0.0622],\n",
      "        [0.0720],\n",
      "        [0.0752],\n",
      "        [0.0765],\n",
      "        [0.0673],\n",
      "        [0.0782],\n",
      "        [0.0811],\n",
      "        [0.0687],\n",
      "        [0.0744],\n",
      "        [0.1100],\n",
      "        [0.0965],\n",
      "        [0.0924],\n",
      "        [0.1200],\n",
      "        [0.1030],\n",
      "        [0.1011],\n",
      "        [0.1131],\n",
      "        [0.1226],\n",
      "        [0.1554],\n",
      "        [0.1585],\n",
      "        [0.1699],\n",
      "        [0.1130],\n",
      "        [0.2119],\n",
      "        [0.0209]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 95.15657806396484\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.229939006312634e-07, 38)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [38, 31, 43, 74, 22, 68, 94, 30, 35, 3, 128, 49, 48, 147, 4, 136, 1, 75, 120, 71, 37, 144, 146, 23, 34, 24, 143, 69, 126, 42, 28, 47, 70, 123, 54, 41, 86, 39, 46, 124, 85, 50, 125, 36, 14, 27, 145, 15, 137, 152, 142, 121, 148, 2, 44, 11, 157, 51, 56, 127, 80, 82, 110, 10, 32, 98, 81, 64, 95, 40, 0, 119, 59, 109, 55, 111, 122, 138, 45, 97, 83, 29, 150, 149, 141, 93, 103, 19, 135, 67, 12, 140, 79, 73, 65, 151, 5, 20, 72, 92, 133, 102, 108, 118, 129, 153, 116, 25, 132, 139, 84, 13, 33, 134, 52, 131, 76, 130, 21, 53, 96, 115, 9, 7, 57, 117, 26, 107, 100, 6, 77, 114, 8, 18, 66, 106, 78, 112, 154, 113, 58, 101, 87, 90, 91, 60, 104, 63, 99, 88, 155, 105, 89, 61, 62, 17, 16, 156, 158] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6082],\n",
      "        [0.6298],\n",
      "        [0.5899],\n",
      "        [0.3616],\n",
      "        [0.5498],\n",
      "        [0.3059],\n",
      "        [0.2363],\n",
      "        [0.5779],\n",
      "        [0.6247],\n",
      "        [0.6560],\n",
      "        [0.0991],\n",
      "        [0.5360],\n",
      "        [0.5201],\n",
      "        [0.0991],\n",
      "        [0.6527],\n",
      "        [0.0991],\n",
      "        [0.6543],\n",
      "        [0.3982],\n",
      "        [0.0991],\n",
      "        [0.3496],\n",
      "        [0.6374],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.6016],\n",
      "        [0.6341],\n",
      "        [0.5945],\n",
      "        [0.0991],\n",
      "        [0.3169],\n",
      "        [0.0991],\n",
      "        [0.5981],\n",
      "        [0.6002],\n",
      "        [0.5585],\n",
      "        [0.3471],\n",
      "        [0.0991],\n",
      "        [0.4570],\n",
      "        [0.6135],\n",
      "        [0.2770],\n",
      "        [0.6015],\n",
      "        [0.5719],\n",
      "        [0.0991],\n",
      "        [0.2872],\n",
      "        [0.5202],\n",
      "        [0.0991],\n",
      "        [0.6512],\n",
      "        [0.5712],\n",
      "        [0.6006],\n",
      "        [0.0991],\n",
      "        [0.5857],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.6618],\n",
      "        [0.5796],\n",
      "        [0.6246],\n",
      "        [0.4881],\n",
      "        [0.5131],\n",
      "        [0.4522],\n",
      "        [0.0991],\n",
      "        [0.3870],\n",
      "        [0.3365],\n",
      "        [0.0991],\n",
      "        [0.6176],\n",
      "        [0.6526],\n",
      "        [0.1654],\n",
      "        [0.3782],\n",
      "        [0.2925],\n",
      "        [0.2296],\n",
      "        [0.6179],\n",
      "        [0.6626],\n",
      "        [0.0991],\n",
      "        [0.4115],\n",
      "        [0.0991],\n",
      "        [0.4561],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.5936],\n",
      "        [0.1919],\n",
      "        [0.2787],\n",
      "        [0.5966],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2261],\n",
      "        [0.1526],\n",
      "        [0.5302],\n",
      "        [0.0991],\n",
      "        [0.2835],\n",
      "        [0.5996],\n",
      "        [0.0991],\n",
      "        [0.3614],\n",
      "        [0.3205],\n",
      "        [0.2521],\n",
      "        [0.0991],\n",
      "        [0.6399],\n",
      "        [0.4709],\n",
      "        [0.3169],\n",
      "        [0.2262],\n",
      "        [0.0991],\n",
      "        [0.1187],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.5760],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2728],\n",
      "        [0.5840],\n",
      "        [0.6191],\n",
      "        [0.0991],\n",
      "        [0.4786],\n",
      "        [0.0991],\n",
      "        [0.3960],\n",
      "        [0.0991],\n",
      "        [0.4712],\n",
      "        [0.4673],\n",
      "        [0.2534],\n",
      "        [0.0991],\n",
      "        [0.6148],\n",
      "        [0.6338],\n",
      "        [0.4137],\n",
      "        [0.0991],\n",
      "        [0.5549],\n",
      "        [0.0991],\n",
      "        [0.1259],\n",
      "        [0.6301],\n",
      "        [0.3849],\n",
      "        [0.0991],\n",
      "        [0.6200],\n",
      "        [0.5256],\n",
      "        [0.2334],\n",
      "        [0.0991],\n",
      "        [0.3621],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.3857],\n",
      "        [0.0991],\n",
      "        [0.3013],\n",
      "        [0.2700],\n",
      "        [0.2647],\n",
      "        [0.4188],\n",
      "        [0.1278],\n",
      "        [0.3661],\n",
      "        [0.1148],\n",
      "        [0.3008],\n",
      "        [0.1967],\n",
      "        [0.1131],\n",
      "        [0.2828],\n",
      "        [0.4094],\n",
      "        [0.3828],\n",
      "        [0.5822],\n",
      "        [0.5904],\n",
      "        [0.1972],\n",
      "        [0.1965]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0008],\n",
      "        [0.0007],\n",
      "        [0.0019],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0027],\n",
      "        [0.0028],\n",
      "        [0.0031],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0046],\n",
      "        [0.0063],\n",
      "        [0.0060],\n",
      "        [0.0063],\n",
      "        [0.0066],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0077],\n",
      "        [0.0084],\n",
      "        [0.0088],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0089],\n",
      "        [0.0093],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0092],\n",
      "        [0.0105],\n",
      "        [0.0106],\n",
      "        [0.0108],\n",
      "        [0.0116],\n",
      "        [0.0140],\n",
      "        [0.0143],\n",
      "        [0.0148],\n",
      "        [0.0151],\n",
      "        [0.0160],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0176],\n",
      "        [0.0181],\n",
      "        [0.0182],\n",
      "        [0.0188],\n",
      "        [0.0189],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0226],\n",
      "        [0.0223],\n",
      "        [0.0236],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0238],\n",
      "        [0.0239],\n",
      "        [0.0251],\n",
      "        [0.0250],\n",
      "        [0.0261],\n",
      "        [0.0273],\n",
      "        [0.0280],\n",
      "        [0.0289],\n",
      "        [0.0300],\n",
      "        [0.0311],\n",
      "        [0.0308],\n",
      "        [0.0314],\n",
      "        [0.0324],\n",
      "        [0.0325],\n",
      "        [0.0328],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.0353],\n",
      "        [0.0358],\n",
      "        [0.0368],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0380],\n",
      "        [0.0389],\n",
      "        [0.0391],\n",
      "        [0.0396],\n",
      "        [0.0404],\n",
      "        [0.0418],\n",
      "        [0.0421],\n",
      "        [0.0427],\n",
      "        [0.0433],\n",
      "        [0.0444],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0453],\n",
      "        [0.0479],\n",
      "        [0.0479],\n",
      "        [0.0512],\n",
      "        [0.0513],\n",
      "        [0.0524],\n",
      "        [0.0520],\n",
      "        [0.0526],\n",
      "        [0.0528],\n",
      "        [0.0543],\n",
      "        [0.0545],\n",
      "        [0.0551],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0557],\n",
      "        [0.0558],\n",
      "        [0.0567],\n",
      "        [0.0572],\n",
      "        [0.0570],\n",
      "        [0.0582],\n",
      "        [0.0595],\n",
      "        [0.0622],\n",
      "        [0.0662],\n",
      "        [0.0662],\n",
      "        [0.0665],\n",
      "        [0.0673],\n",
      "        [0.0677],\n",
      "        [0.0692],\n",
      "        [0.0687],\n",
      "        [0.0693],\n",
      "        [0.0720],\n",
      "        [0.0744],\n",
      "        [0.0752],\n",
      "        [0.0765],\n",
      "        [0.0782],\n",
      "        [0.0788],\n",
      "        [0.0811],\n",
      "        [0.0812],\n",
      "        [0.0824],\n",
      "        [0.0839],\n",
      "        [0.0924],\n",
      "        [0.0965],\n",
      "        [0.1011],\n",
      "        [0.1030],\n",
      "        [0.1100],\n",
      "        [0.1130],\n",
      "        [0.1131],\n",
      "        [0.1200],\n",
      "        [0.1226],\n",
      "        [0.1554],\n",
      "        [0.1585],\n",
      "        [0.1699],\n",
      "        [0.2119],\n",
      "        [0.2993]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 64\n",
      "Number of shrink: 36\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0036],\n",
      "        [0.0057],\n",
      "        [0.0029],\n",
      "        [0.0045],\n",
      "        [0.0041],\n",
      "        [0.0020],\n",
      "        [0.0026],\n",
      "        [0.0057],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0035],\n",
      "        [0.0104],\n",
      "        [0.0127],\n",
      "        [0.0059],\n",
      "        [0.0107],\n",
      "        [0.0067],\n",
      "        [0.0042],\n",
      "        [0.0137],\n",
      "        [0.0077],\n",
      "        [0.0005],\n",
      "        [0.0122],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0012],\n",
      "        [0.0051],\n",
      "        [0.0192],\n",
      "        [0.0096],\n",
      "        [0.0169],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0024],\n",
      "        [0.0028],\n",
      "        [0.0035],\n",
      "        [0.0116],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0197],\n",
      "        [0.0161],\n",
      "        [0.0074],\n",
      "        [0.0156],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0166],\n",
      "        [0.0221],\n",
      "        [0.0224],\n",
      "        [0.0079],\n",
      "        [0.0182],\n",
      "        [0.0138],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0197],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0153],\n",
      "        [0.0142],\n",
      "        [0.0270],\n",
      "        [0.0377],\n",
      "        [0.0235],\n",
      "        [0.0226],\n",
      "        [0.0219],\n",
      "        [0.0237],\n",
      "        [0.0194],\n",
      "        [0.0235],\n",
      "        [0.0275],\n",
      "        [0.0205],\n",
      "        [0.0239],\n",
      "        [0.0232],\n",
      "        [0.0255],\n",
      "        [0.0250],\n",
      "        [0.0273],\n",
      "        [0.0252],\n",
      "        [0.0279],\n",
      "        [0.0344],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0308],\n",
      "        [0.0314],\n",
      "        [0.0324],\n",
      "        [0.0249],\n",
      "        [0.0327],\n",
      "        [0.0336],\n",
      "        [0.0398],\n",
      "        [0.0335],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0345],\n",
      "        [0.0253],\n",
      "        [0.0352],\n",
      "        [0.0367],\n",
      "        [0.0440],\n",
      "        [0.0373],\n",
      "        [0.0355],\n",
      "        [0.0303],\n",
      "        [0.0416],\n",
      "        [0.0392],\n",
      "        [0.0408],\n",
      "        [0.0468],\n",
      "        [0.0340],\n",
      "        [0.0420],\n",
      "        [0.0427],\n",
      "        [0.0434],\n",
      "        [0.0443],\n",
      "        [0.0451],\n",
      "        [0.0451],\n",
      "        [0.0453],\n",
      "        [0.0478],\n",
      "        [0.0400],\n",
      "        [0.0511],\n",
      "        [0.0513],\n",
      "        [0.0522],\n",
      "        [0.0590],\n",
      "        [0.0481],\n",
      "        [0.0527],\n",
      "        [0.0554],\n",
      "        [0.0544],\n",
      "        [0.0502],\n",
      "        [0.0551],\n",
      "        [0.0570],\n",
      "        [0.0562],\n",
      "        [0.0557],\n",
      "        [0.0566],\n",
      "        [0.0576],\n",
      "        [0.0590],\n",
      "        [0.0655],\n",
      "        [0.0594],\n",
      "        [0.0510],\n",
      "        [0.0662],\n",
      "        [0.0663],\n",
      "        [0.0678],\n",
      "        [0.0645],\n",
      "        [0.0677],\n",
      "        [0.0723],\n",
      "        [0.0586],\n",
      "        [0.0752],\n",
      "        [0.0720],\n",
      "        [0.0720],\n",
      "        [0.0751],\n",
      "        [0.0765],\n",
      "        [0.0781],\n",
      "        [0.0852],\n",
      "        [0.0812],\n",
      "        [0.0880],\n",
      "        [0.0930],\n",
      "        [0.0967],\n",
      "        [0.0886],\n",
      "        [0.0965],\n",
      "        [0.0977],\n",
      "        [0.1031],\n",
      "        [0.1172],\n",
      "        [0.0331],\n",
      "        [0.1131],\n",
      "        [0.1298],\n",
      "        [0.1145],\n",
      "        [0.1502],\n",
      "        [0.1555],\n",
      "        [0.1652],\n",
      "        [0.1318],\n",
      "        [0.2179]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 95.44593238830566\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 26 個區塊累積花費時間(s) 21.925105333328247\n",
      "<<The performance of 26 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 21.925105333328247\n",
      "<<The percentage of each step>>\n",
      "Step 4: 98.74%\n",
      "Step 6.1: 0.63%\n",
      "Step 6.2: 0.63%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 1\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1039.95\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1382.41\n",
      "The accuracy(2000) for l = 1: 88.68%\n",
      "The accuracy(3000) for l = 1: 96.23%\n",
      "The maximum error: tensor(5562.6914)\n",
      "The minimum error: tensor(12.9375)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 4731.0\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 4798.2\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "The maximum error: 5627.2890625\n",
      "The minimum error: 3919.703125\n",
      "------------------------------------------------------------\n",
      "0.8867924528301887\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<27>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.852485809195059e-07, 67)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [67, 19, 64, 24, 90, 43, 39, 66, 124, 34, 18, 70, 30, 26, 27, 143, 132, 31, 42, 116, 23, 140, 142, 139, 38, 122, 45, 0, 119, 33, 44, 11, 71, 40, 50, 37, 120, 35, 121, 65, 141, 133, 148, 81, 20, 78, 138, 82, 117, 46, 28, 144, 123, 10, 32, 52, 77, 47, 106, 76, 94, 41, 91, 15, 60, 7, 36, 6, 115, 105, 51, 69, 107, 118, 134, 93, 151, 146, 79, 68, 145, 55, 137, 89, 99, 131, 75, 63, 136, 153, 147, 25, 21, 1, 61, 88, 129, 98, 8, 104, 114, 125, 149, 16, 112, 29, 72, 22, 128, 135, 80, 130, 127, 48, 126, 92, 49, 111, 17, 5, 14, 9, 3, 113, 73, 53, 103, 96, 2, 110, 74, 102, 4, 108, 62, 150, 109, 97, 54, 83, 56, 86, 100, 87, 59, 95, 101, 57, 84, 85, 152, 58, 158, 157, 13]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0012],\n",
      "        [0.0020],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0041],\n",
      "        [0.0045],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0067],\n",
      "        [0.0075],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0107],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0127],\n",
      "        [0.0138],\n",
      "        [0.0137],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0156],\n",
      "        [0.0161],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0182],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0219],\n",
      "        [0.0224],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0232],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0253],\n",
      "        [0.0255],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0275],\n",
      "        [0.0279],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0308],\n",
      "        [0.0314],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0344],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0345],\n",
      "        [0.0352],\n",
      "        [0.0355],\n",
      "        [0.0367],\n",
      "        [0.0373],\n",
      "        [0.0377],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0400],\n",
      "        [0.0408],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0427],\n",
      "        [0.0434],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0451],\n",
      "        [0.0451],\n",
      "        [0.0453],\n",
      "        [0.0468],\n",
      "        [0.0478],\n",
      "        [0.0481],\n",
      "        [0.0502],\n",
      "        [0.0510],\n",
      "        [0.0511],\n",
      "        [0.0513],\n",
      "        [0.0522],\n",
      "        [0.0527],\n",
      "        [0.0544],\n",
      "        [0.0554],\n",
      "        [0.0551],\n",
      "        [0.0557],\n",
      "        [0.0562],\n",
      "        [0.0566],\n",
      "        [0.0570],\n",
      "        [0.0576],\n",
      "        [0.0586],\n",
      "        [0.0590],\n",
      "        [0.0590],\n",
      "        [0.0594],\n",
      "        [0.0645],\n",
      "        [0.0655],\n",
      "        [0.0662],\n",
      "        [0.0663],\n",
      "        [0.0678],\n",
      "        [0.0677],\n",
      "        [0.0720],\n",
      "        [0.0720],\n",
      "        [0.0723],\n",
      "        [0.0751],\n",
      "        [0.0752],\n",
      "        [0.0765],\n",
      "        [0.0781],\n",
      "        [0.0812],\n",
      "        [0.0852],\n",
      "        [0.0880],\n",
      "        [0.0886],\n",
      "        [0.0930],\n",
      "        [0.0965],\n",
      "        [0.0967],\n",
      "        [0.0977],\n",
      "        [0.1031],\n",
      "        [0.1131],\n",
      "        [0.1145],\n",
      "        [0.1172],\n",
      "        [0.1298],\n",
      "        [0.1318],\n",
      "        [0.1502],\n",
      "        [0.1537],\n",
      "        [0.1547],\n",
      "        [0.1555]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.852485809195059e-07, 67)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [67, 19, 64, 24, 90, 43, 39, 66, 124, 34, 18, 70, 30, 26, 27, 143, 132, 31, 42, 116, 23, 140, 142, 139, 38, 122, 45, 0, 119, 33, 44, 11, 71, 40, 50, 37, 120, 35, 121, 65, 141, 133, 148, 81, 20, 78, 138, 82, 117, 46, 28, 144, 123, 10, 32, 52, 77, 47, 106, 76, 94, 41, 91, 15, 60, 7, 36, 6, 115, 105, 51, 69, 107, 118, 134, 93, 151, 146, 79, 68, 145, 55, 137, 89, 99, 131, 75, 63, 136, 153, 147, 25, 21, 1, 61, 88, 129, 98, 8, 104, 114, 125, 149, 16, 112, 29, 72, 22, 128, 135, 80, 130, 127, 48, 126, 92, 49, 111, 17, 5, 14, 9, 3, 113, 73, 53, 103, 96, 2, 110, 74, 102, 4, 108, 62, 150, 109, 97, 54, 83, 56, 86, 100, 87, 59, 95, 101, 57, 84, 85, 152, 58, 158, 157, 13, 12] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3575],\n",
      "        [0.6093],\n",
      "        [0.3098],\n",
      "        [0.6084],\n",
      "        [0.2362],\n",
      "        [0.5507],\n",
      "        [0.5878],\n",
      "        [0.3544],\n",
      "        [0.0991],\n",
      "        [0.6106],\n",
      "        [0.5555],\n",
      "        [0.3679],\n",
      "        [0.6379],\n",
      "        [0.5864],\n",
      "        [0.6362],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.6291],\n",
      "        [0.5633],\n",
      "        [0.0991],\n",
      "        [0.6108],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.5974],\n",
      "        [0.0991],\n",
      "        [0.5302],\n",
      "        [0.6483],\n",
      "        [0.0991],\n",
      "        [0.6408],\n",
      "        [0.5138],\n",
      "        [0.5807],\n",
      "        [0.4048],\n",
      "        [0.5732],\n",
      "        [0.4566],\n",
      "        [0.6131],\n",
      "        [0.0991],\n",
      "        [0.6005],\n",
      "        [0.0991],\n",
      "        [0.3240],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2905],\n",
      "        [0.6044],\n",
      "        [0.3335],\n",
      "        [0.0991],\n",
      "        [0.2819],\n",
      "        [0.0991],\n",
      "        [0.5165],\n",
      "        [0.6562],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.5665],\n",
      "        [0.6554],\n",
      "        [0.4513],\n",
      "        [0.3775],\n",
      "        [0.5105],\n",
      "        [0.0991],\n",
      "        [0.3881],\n",
      "        [0.1653],\n",
      "        [0.5860],\n",
      "        [0.2295],\n",
      "        [0.5207],\n",
      "        [0.2930],\n",
      "        [0.6184],\n",
      "        [0.6166],\n",
      "        [0.6141],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.4570],\n",
      "        [0.3283],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.1918],\n",
      "        [0.2766],\n",
      "        [0.0991],\n",
      "        [0.2779],\n",
      "        [0.3247],\n",
      "        [0.0991],\n",
      "        [0.4060],\n",
      "        [0.0991],\n",
      "        [0.2261],\n",
      "        [0.1526],\n",
      "        [0.0991],\n",
      "        [0.3635],\n",
      "        [0.2825],\n",
      "        [0.0991],\n",
      "        [0.5048],\n",
      "        [0.0991],\n",
      "        [0.6036],\n",
      "        [0.5839],\n",
      "        [0.6387],\n",
      "        [0.2494],\n",
      "        [0.2261],\n",
      "        [0.0991],\n",
      "        [0.1187],\n",
      "        [0.5924],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.4646],\n",
      "        [0.0991],\n",
      "        [0.6236],\n",
      "        [0.4008],\n",
      "        [0.5661],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.2731],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.4775],\n",
      "        [0.0991],\n",
      "        [0.2533],\n",
      "        [0.4668],\n",
      "        [0.0991],\n",
      "        [0.4695],\n",
      "        [0.6144],\n",
      "        [0.5154],\n",
      "        [0.5770],\n",
      "        [0.6317],\n",
      "        [0.0991],\n",
      "        [0.3876],\n",
      "        [0.4064],\n",
      "        [0.0991],\n",
      "        [0.1258],\n",
      "        [0.6288],\n",
      "        [0.0991],\n",
      "        [0.3645],\n",
      "        [0.0991],\n",
      "        [0.6169],\n",
      "        [0.0991],\n",
      "        [0.2275],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.3794],\n",
      "        [0.3082],\n",
      "        [0.4150],\n",
      "        [0.2806],\n",
      "        [0.1278],\n",
      "        [0.2775],\n",
      "        [0.3627],\n",
      "        [0.1147],\n",
      "        [0.1131],\n",
      "        [0.4012],\n",
      "        [0.3081],\n",
      "        [0.2925],\n",
      "        [0.2773],\n",
      "        [0.3776],\n",
      "        [0.2459],\n",
      "        [0.2594],\n",
      "        [0.5792],\n",
      "        [0.5857]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0012],\n",
      "        [0.0020],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0029],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0041],\n",
      "        [0.0045],\n",
      "        [0.0051],\n",
      "        [0.0057],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0067],\n",
      "        [0.0075],\n",
      "        [0.0074],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0104],\n",
      "        [0.0107],\n",
      "        [0.0116],\n",
      "        [0.0122],\n",
      "        [0.0127],\n",
      "        [0.0138],\n",
      "        [0.0137],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0146],\n",
      "        [0.0156],\n",
      "        [0.0161],\n",
      "        [0.0166],\n",
      "        [0.0169],\n",
      "        [0.0182],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0201],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0205],\n",
      "        [0.0219],\n",
      "        [0.0224],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0232],\n",
      "        [0.0235],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0249],\n",
      "        [0.0250],\n",
      "        [0.0253],\n",
      "        [0.0255],\n",
      "        [0.0270],\n",
      "        [0.0273],\n",
      "        [0.0275],\n",
      "        [0.0279],\n",
      "        [0.0299],\n",
      "        [0.0302],\n",
      "        [0.0303],\n",
      "        [0.0308],\n",
      "        [0.0314],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0336],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0344],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0345],\n",
      "        [0.0352],\n",
      "        [0.0355],\n",
      "        [0.0367],\n",
      "        [0.0373],\n",
      "        [0.0377],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0400],\n",
      "        [0.0408],\n",
      "        [0.0416],\n",
      "        [0.0420],\n",
      "        [0.0427],\n",
      "        [0.0434],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0451],\n",
      "        [0.0451],\n",
      "        [0.0453],\n",
      "        [0.0468],\n",
      "        [0.0478],\n",
      "        [0.0481],\n",
      "        [0.0502],\n",
      "        [0.0510],\n",
      "        [0.0511],\n",
      "        [0.0513],\n",
      "        [0.0522],\n",
      "        [0.0527],\n",
      "        [0.0544],\n",
      "        [0.0554],\n",
      "        [0.0551],\n",
      "        [0.0557],\n",
      "        [0.0562],\n",
      "        [0.0566],\n",
      "        [0.0570],\n",
      "        [0.0576],\n",
      "        [0.0586],\n",
      "        [0.0590],\n",
      "        [0.0590],\n",
      "        [0.0594],\n",
      "        [0.0645],\n",
      "        [0.0655],\n",
      "        [0.0662],\n",
      "        [0.0663],\n",
      "        [0.0678],\n",
      "        [0.0677],\n",
      "        [0.0720],\n",
      "        [0.0720],\n",
      "        [0.0723],\n",
      "        [0.0751],\n",
      "        [0.0752],\n",
      "        [0.0765],\n",
      "        [0.0781],\n",
      "        [0.0812],\n",
      "        [0.0852],\n",
      "        [0.0880],\n",
      "        [0.0886],\n",
      "        [0.0930],\n",
      "        [0.0965],\n",
      "        [0.0967],\n",
      "        [0.0977],\n",
      "        [0.1031],\n",
      "        [0.1131],\n",
      "        [0.1145],\n",
      "        [0.1172],\n",
      "        [0.1298],\n",
      "        [0.1318],\n",
      "        [0.1502],\n",
      "        [0.1537],\n",
      "        [0.1547],\n",
      "        [0.1555],\n",
      "        [0.1652]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 63\n",
      "Number of shrink: 37\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0031],\n",
      "        [0.0081],\n",
      "        [0.0015],\n",
      "        [0.0132],\n",
      "        [0.0025],\n",
      "        [0.0055],\n",
      "        [0.0038],\n",
      "        [0.0004],\n",
      "        [0.0035],\n",
      "        [0.0147],\n",
      "        [0.0130],\n",
      "        [0.0093],\n",
      "        [0.0082],\n",
      "        [0.0222],\n",
      "        [0.0206],\n",
      "        [0.0059],\n",
      "        [0.0067],\n",
      "        [0.0213],\n",
      "        [0.0106],\n",
      "        [0.0078],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0084],\n",
      "        [0.0096],\n",
      "        [0.0007],\n",
      "        [0.0098],\n",
      "        [0.0063],\n",
      "        [0.0008],\n",
      "        [0.0116],\n",
      "        [0.0240],\n",
      "        [0.0088],\n",
      "        [0.0220],\n",
      "        [0.0193],\n",
      "        [0.0182],\n",
      "        [0.0083],\n",
      "        [0.0050],\n",
      "        [0.0156],\n",
      "        [0.0072],\n",
      "        [0.0166],\n",
      "        [0.0187],\n",
      "        [0.0181],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0227],\n",
      "        [0.0311],\n",
      "        [0.0200],\n",
      "        [0.0197],\n",
      "        [0.0234],\n",
      "        [0.0202],\n",
      "        [0.0151],\n",
      "        [0.0066],\n",
      "        [0.0206],\n",
      "        [0.0220],\n",
      "        [0.0130],\n",
      "        [0.0351],\n",
      "        [0.0165],\n",
      "        [0.0244],\n",
      "        [0.0176],\n",
      "        [0.0235],\n",
      "        [0.0251],\n",
      "        [0.0239],\n",
      "        [0.0282],\n",
      "        [0.0251],\n",
      "        [0.0279],\n",
      "        [0.0293],\n",
      "        [0.0163],\n",
      "        [0.0185],\n",
      "        [0.0167],\n",
      "        [0.0279],\n",
      "        [0.0299],\n",
      "        [0.0239],\n",
      "        [0.0253],\n",
      "        [0.0307],\n",
      "        [0.0315],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0116],\n",
      "        [0.0336],\n",
      "        [0.0311],\n",
      "        [0.0292],\n",
      "        [0.0341],\n",
      "        [0.0321],\n",
      "        [0.0344],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0352],\n",
      "        [0.0338],\n",
      "        [0.0393],\n",
      "        [0.0373],\n",
      "        [0.0435],\n",
      "        [0.0392],\n",
      "        [0.0556],\n",
      "        [0.0277],\n",
      "        [0.0285],\n",
      "        [0.0412],\n",
      "        [0.0420],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0345],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0451],\n",
      "        [0.0453],\n",
      "        [0.0438],\n",
      "        [0.0478],\n",
      "        [0.0346],\n",
      "        [0.0456],\n",
      "        [0.0345],\n",
      "        [0.0511],\n",
      "        [0.0514],\n",
      "        [0.0499],\n",
      "        [0.0527],\n",
      "        [0.0544],\n",
      "        [0.0489],\n",
      "        [0.0550],\n",
      "        [0.0557],\n",
      "        [0.0501],\n",
      "        [0.0566],\n",
      "        [0.0524],\n",
      "        [0.0443],\n",
      "        [0.0615],\n",
      "        [0.0503],\n",
      "        [0.0476],\n",
      "        [0.0594],\n",
      "        [0.0612],\n",
      "        [0.0633],\n",
      "        [0.0662],\n",
      "        [0.0664],\n",
      "        [0.0560],\n",
      "        [0.0676],\n",
      "        [0.0697],\n",
      "        [0.0719],\n",
      "        [0.0616],\n",
      "        [0.0751],\n",
      "        [0.0791],\n",
      "        [0.0766],\n",
      "        [0.0781],\n",
      "        [0.0812],\n",
      "        [0.0825],\n",
      "        [0.0926],\n",
      "        [0.0920],\n",
      "        [0.0983],\n",
      "        [0.0964],\n",
      "        [0.1031],\n",
      "        [0.1002],\n",
      "        [0.1032],\n",
      "        [0.1130],\n",
      "        [0.1132],\n",
      "        [0.1214],\n",
      "        [0.1353],\n",
      "        [0.0863],\n",
      "        [0.1505],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1640],\n",
      "        [0.1729]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.04012680053711\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.585782585943889e-08, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [66, 38, 0, 64, 90, 67, 124, 39, 37, 43, 143, 45, 28, 132, 35, 116, 50, 19, 140, 23, 142, 30, 44, 70, 139, 122, 42, 151, 119, 10, 24, 18, 34, 46, 120, 7, 6, 121, 52, 47, 141, 40, 36, 65, 133, 148, 71, 138, 78, 117, 144, 27, 31, 11, 123, 26, 81, 106, 51, 82, 94, 33, 77, 91, 69, 76, 15, 115, 21, 41, 68, 1, 60, 105, 107, 20, 79, 118, 55, 134, 93, 146, 75, 145, 89, 8, 137, 99, 22, 29, 32, 131, 136, 147, 63, 61, 88, 129, 98, 153, 16, 5, 104, 114, 125, 149, 72, 3, 112, 48, 80, 9, 49, 128, 135, 17, 130, 127, 126, 25, 92, 2, 111, 113, 73, 4, 14, 53, 103, 96, 110, 74, 102, 108, 150, 109, 62, 97, 54, 152, 56, 83, 100, 86, 59, 95, 87, 157, 158, 101, 57, 84, 85, 58, 13, 156, 154] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3575],\n",
      "        [0.6065],\n",
      "        [0.6582],\n",
      "        [0.3093],\n",
      "        [0.2361],\n",
      "        [0.3611],\n",
      "        [0.0990],\n",
      "        [0.5945],\n",
      "        [0.6228],\n",
      "        [0.5534],\n",
      "        [0.0990],\n",
      "        [0.5343],\n",
      "        [0.6701],\n",
      "        [0.0990],\n",
      "        [0.6094],\n",
      "        [0.0990],\n",
      "        [0.4626],\n",
      "        [0.6186],\n",
      "        [0.0990],\n",
      "        [0.6269],\n",
      "        [0.0990],\n",
      "        [0.6512],\n",
      "        [0.5177],\n",
      "        [0.3728],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.5664],\n",
      "        [0.3213],\n",
      "        [0.0990],\n",
      "        [0.5759],\n",
      "        [0.6240],\n",
      "        [0.5645],\n",
      "        [0.6217],\n",
      "        [0.5216],\n",
      "        [0.0990],\n",
      "        [0.6290],\n",
      "        [0.6248],\n",
      "        [0.0990],\n",
      "        [0.4574],\n",
      "        [0.5164],\n",
      "        [0.0990],\n",
      "        [0.5772],\n",
      "        [0.6254],\n",
      "        [0.3258],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.4104],\n",
      "        [0.0990],\n",
      "        [0.3342],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.6512],\n",
      "        [0.6428],\n",
      "        [0.5889],\n",
      "        [0.0990],\n",
      "        [0.6029],\n",
      "        [0.2937],\n",
      "        [0.0990],\n",
      "        [0.4634],\n",
      "        [0.2856],\n",
      "        [0.1652],\n",
      "        [0.6527],\n",
      "        [0.3787],\n",
      "        [0.2294],\n",
      "        [0.3333],\n",
      "        [0.3896],\n",
      "        [0.5233],\n",
      "        [0.0990],\n",
      "        [0.5961],\n",
      "        [0.5893],\n",
      "        [0.3295],\n",
      "        [0.6510],\n",
      "        [0.2968],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.6162],\n",
      "        [0.2804],\n",
      "        [0.0990],\n",
      "        [0.4084],\n",
      "        [0.0990],\n",
      "        [0.1917],\n",
      "        [0.0990],\n",
      "        [0.3651],\n",
      "        [0.0990],\n",
      "        [0.2260],\n",
      "        [0.6019],\n",
      "        [0.0990],\n",
      "        [0.1525],\n",
      "        [0.5825],\n",
      "        [0.6371],\n",
      "        [0.6684],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.2799],\n",
      "        [0.2497],\n",
      "        [0.2260],\n",
      "        [0.0990],\n",
      "        [0.1186],\n",
      "        [0.5107],\n",
      "        [0.4676],\n",
      "        [0.6277],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.4055],\n",
      "        [0.6431],\n",
      "        [0.0990],\n",
      "        [0.4841],\n",
      "        [0.2754],\n",
      "        [0.5857],\n",
      "        [0.4729],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.4741],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.6194],\n",
      "        [0.2532],\n",
      "        [0.6406],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.3910],\n",
      "        [0.6277],\n",
      "        [0.5184],\n",
      "        [0.4086],\n",
      "        [0.0990],\n",
      "        [0.1257],\n",
      "        [0.0990],\n",
      "        [0.3668],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.2236],\n",
      "        [0.0990],\n",
      "        [0.3820],\n",
      "        [0.3228],\n",
      "        [0.4183],\n",
      "        [0.3127],\n",
      "        [0.1277],\n",
      "        [0.2859],\n",
      "        [0.3652],\n",
      "        [0.1146],\n",
      "        [0.2839],\n",
      "        [0.3057],\n",
      "        [0.2912],\n",
      "        [0.1130],\n",
      "        [0.3999],\n",
      "        [0.3123],\n",
      "        [0.2980],\n",
      "        [0.3779],\n",
      "        [0.5877],\n",
      "        [0.3192],\n",
      "        [0.3254]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0015],\n",
      "        [0.0025],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.0038],\n",
      "        [0.0050],\n",
      "        [0.0055],\n",
      "        [0.0059],\n",
      "        [0.0063],\n",
      "        [0.0066],\n",
      "        [0.0067],\n",
      "        [0.0072],\n",
      "        [0.0078],\n",
      "        [0.0083],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0106],\n",
      "        [0.0116],\n",
      "        [0.0116],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0130],\n",
      "        [0.0147],\n",
      "        [0.0151],\n",
      "        [0.0156],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0166],\n",
      "        [0.0165],\n",
      "        [0.0176],\n",
      "        [0.0181],\n",
      "        [0.0182],\n",
      "        [0.0185],\n",
      "        [0.0187],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0206],\n",
      "        [0.0213],\n",
      "        [0.0220],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0227],\n",
      "        [0.0235],\n",
      "        [0.0239],\n",
      "        [0.0234],\n",
      "        [0.0239],\n",
      "        [0.0240],\n",
      "        [0.0244],\n",
      "        [0.0251],\n",
      "        [0.0253],\n",
      "        [0.0251],\n",
      "        [0.0279],\n",
      "        [0.0279],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0292],\n",
      "        [0.0285],\n",
      "        [0.0293],\n",
      "        [0.0299],\n",
      "        [0.0307],\n",
      "        [0.0311],\n",
      "        [0.0311],\n",
      "        [0.0315],\n",
      "        [0.0321],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0336],\n",
      "        [0.0338],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0351],\n",
      "        [0.0352],\n",
      "        [0.0373],\n",
      "        [0.0392],\n",
      "        [0.0393],\n",
      "        [0.0412],\n",
      "        [0.0420],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0435],\n",
      "        [0.0438],\n",
      "        [0.0443],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0451],\n",
      "        [0.0453],\n",
      "        [0.0456],\n",
      "        [0.0476],\n",
      "        [0.0478],\n",
      "        [0.0489],\n",
      "        [0.0499],\n",
      "        [0.0503],\n",
      "        [0.0501],\n",
      "        [0.0511],\n",
      "        [0.0514],\n",
      "        [0.0524],\n",
      "        [0.0527],\n",
      "        [0.0544],\n",
      "        [0.0550],\n",
      "        [0.0556],\n",
      "        [0.0557],\n",
      "        [0.0560],\n",
      "        [0.0566],\n",
      "        [0.0594],\n",
      "        [0.0612],\n",
      "        [0.0616],\n",
      "        [0.0615],\n",
      "        [0.0633],\n",
      "        [0.0662],\n",
      "        [0.0664],\n",
      "        [0.0676],\n",
      "        [0.0697],\n",
      "        [0.0719],\n",
      "        [0.0751],\n",
      "        [0.0766],\n",
      "        [0.0781],\n",
      "        [0.0791],\n",
      "        [0.0812],\n",
      "        [0.0825],\n",
      "        [0.0863],\n",
      "        [0.0920],\n",
      "        [0.0926],\n",
      "        [0.0964],\n",
      "        [0.0983],\n",
      "        [0.1002],\n",
      "        [0.1032],\n",
      "        [0.1031],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1130],\n",
      "        [0.1132],\n",
      "        [0.1214],\n",
      "        [0.1353],\n",
      "        [0.1505],\n",
      "        [0.1640],\n",
      "        [0.1639],\n",
      "        [0.1704]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 63\n",
      "Number of shrink: 37\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0008],\n",
      "        [0.0026],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0046],\n",
      "        [0.0035],\n",
      "        [0.0030],\n",
      "        [0.0041],\n",
      "        [0.0002],\n",
      "        [0.0059],\n",
      "        [0.0101],\n",
      "        [0.0012],\n",
      "        [0.0067],\n",
      "        [0.0065],\n",
      "        [0.0078],\n",
      "        [0.0070],\n",
      "        [0.0111],\n",
      "        [0.0082],\n",
      "        [0.0183],\n",
      "        [0.0084],\n",
      "        [0.0138],\n",
      "        [0.0132],\n",
      "        [0.0126],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0049],\n",
      "        [0.0646],\n",
      "        [0.0117],\n",
      "        [0.0082],\n",
      "        [0.0219],\n",
      "        [0.0162],\n",
      "        [0.0185],\n",
      "        [0.0169],\n",
      "        [0.0155],\n",
      "        [0.0111],\n",
      "        [0.0104],\n",
      "        [0.0165],\n",
      "        [0.0150],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0134],\n",
      "        [0.0190],\n",
      "        [0.0176],\n",
      "        [0.0190],\n",
      "        [0.0193],\n",
      "        [0.0237],\n",
      "        [0.0197],\n",
      "        [0.0184],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0281],\n",
      "        [0.0276],\n",
      "        [0.0252],\n",
      "        [0.0220],\n",
      "        [0.0319],\n",
      "        [0.0247],\n",
      "        [0.0235],\n",
      "        [0.0213],\n",
      "        [0.0266],\n",
      "        [0.0240],\n",
      "        [0.0283],\n",
      "        [0.0242],\n",
      "        [0.0252],\n",
      "        [0.0219],\n",
      "        [0.0259],\n",
      "        [0.0247],\n",
      "        [0.0278],\n",
      "        [0.0209],\n",
      "        [0.0230],\n",
      "        [0.0264],\n",
      "        [0.0218],\n",
      "        [0.0303],\n",
      "        [0.0298],\n",
      "        [0.0307],\n",
      "        [0.0376],\n",
      "        [0.0308],\n",
      "        [0.0315],\n",
      "        [0.0344],\n",
      "        [0.0325],\n",
      "        [0.0325],\n",
      "        [0.0336],\n",
      "        [0.0329],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0300],\n",
      "        [0.0344],\n",
      "        [0.0343],\n",
      "        [0.0238],\n",
      "        [0.0293],\n",
      "        [0.0410],\n",
      "        [0.0352],\n",
      "        [0.0374],\n",
      "        [0.0393],\n",
      "        [0.0452],\n",
      "        [0.0440],\n",
      "        [0.0419],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0501],\n",
      "        [0.0470],\n",
      "        [0.0352],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0422],\n",
      "        [0.0409],\n",
      "        [0.0478],\n",
      "        [0.0473],\n",
      "        [0.0498],\n",
      "        [0.0457],\n",
      "        [0.0492],\n",
      "        [0.0511],\n",
      "        [0.0514],\n",
      "        [0.0533],\n",
      "        [0.0526],\n",
      "        [0.0544],\n",
      "        [0.0550],\n",
      "        [0.0641],\n",
      "        [0.0556],\n",
      "        [0.0487],\n",
      "        [0.0566],\n",
      "        [0.0594],\n",
      "        [0.0599],\n",
      "        [0.0550],\n",
      "        [0.0599],\n",
      "        [0.0653],\n",
      "        [0.0661],\n",
      "        [0.0665],\n",
      "        [0.0676],\n",
      "        [0.0684],\n",
      "        [0.0719],\n",
      "        [0.0750],\n",
      "        [0.0766],\n",
      "        [0.0781],\n",
      "        [0.0871],\n",
      "        [0.0812],\n",
      "        [0.0844],\n",
      "        [0.0327],\n",
      "        [0.0905],\n",
      "        [0.0961],\n",
      "        [0.0963],\n",
      "        [0.1037],\n",
      "        [0.0983],\n",
      "        [0.1033],\n",
      "        [0.1103],\n",
      "        [0.0538],\n",
      "        [0.0557],\n",
      "        [0.1129],\n",
      "        [0.1066],\n",
      "        [0.1250],\n",
      "        [0.1402],\n",
      "        [0.1459],\n",
      "        [0.1682],\n",
      "        [0.1064],\n",
      "        [0.1135]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.32776236534119\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.3274675580987605e-08, 43)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [43, 38, 66, 28, 64, 90, 0, 39, 124, 37, 42, 67, 143, 35, 132, 50, 116, 10, 140, 142, 139, 122, 45, 6, 19, 7, 119, 70, 40, 44, 30, 52, 120, 18, 121, 46, 65, 141, 78, 47, 23, 34, 36, 133, 148, 138, 117, 144, 21, 51, 1, 24, 69, 123, 41, 22, 106, 71, 94, 77, 15, 81, 11, 91, 76, 82, 68, 27, 31, 115, 33, 29, 105, 8, 60, 107, 79, 118, 26, 134, 93, 152, 75, 146, 145, 89, 99, 137, 55, 5, 131, 20, 136, 147, 32, 3, 88, 72, 129, 98, 61, 104, 114, 125, 149, 63, 9, 16, 48, 112, 49, 2, 80, 153, 128, 135, 130, 157, 17, 127, 126, 4, 92, 158, 111, 113, 14, 73, 25, 151, 53, 103, 96, 110, 74, 102, 108, 150, 109, 97, 54, 62, 56, 83, 100, 59, 95, 86, 156, 57, 87, 101, 154, 155, 84, 85, 58, 13] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5477],\n",
      "        [0.6064],\n",
      "        [0.3574],\n",
      "        [0.6755],\n",
      "        [0.3054],\n",
      "        [0.2360],\n",
      "        [0.6616],\n",
      "        [0.5937],\n",
      "        [0.0990],\n",
      "        [0.6236],\n",
      "        [0.5607],\n",
      "        [0.3626],\n",
      "        [0.0990],\n",
      "        [0.6100],\n",
      "        [0.0990],\n",
      "        [0.4639],\n",
      "        [0.0990],\n",
      "        [0.5806],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.5305],\n",
      "        [0.6311],\n",
      "        [0.6216],\n",
      "        [0.6343],\n",
      "        [0.0990],\n",
      "        [0.3761],\n",
      "        [0.5724],\n",
      "        [0.5132],\n",
      "        [0.6568],\n",
      "        [0.4589],\n",
      "        [0.0990],\n",
      "        [0.5677],\n",
      "        [0.0990],\n",
      "        [0.5198],\n",
      "        [0.3246],\n",
      "        [0.0990],\n",
      "        [0.3326],\n",
      "        [0.5160],\n",
      "        [0.6371],\n",
      "        [0.6255],\n",
      "        [0.6250],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.6029],\n",
      "        [0.4659],\n",
      "        [0.6576],\n",
      "        [0.6326],\n",
      "        [0.3367],\n",
      "        [0.0990],\n",
      "        [0.5840],\n",
      "        [0.5933],\n",
      "        [0.0990],\n",
      "        [0.4148],\n",
      "        [0.1651],\n",
      "        [0.3785],\n",
      "        [0.5201],\n",
      "        [0.2957],\n",
      "        [0.5921],\n",
      "        [0.2294],\n",
      "        [0.3903],\n",
      "        [0.2888],\n",
      "        [0.3323],\n",
      "        [0.6587],\n",
      "        [0.6491],\n",
      "        [0.0990],\n",
      "        [0.6569],\n",
      "        [0.6424],\n",
      "        [0.0990],\n",
      "        [0.6064],\n",
      "        [0.2978],\n",
      "        [0.0990],\n",
      "        [0.2806],\n",
      "        [0.0990],\n",
      "        [0.6126],\n",
      "        [0.0990],\n",
      "        [0.1916],\n",
      "        [0.3764],\n",
      "        [0.3661],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.2259],\n",
      "        [0.1524],\n",
      "        [0.0990],\n",
      "        [0.4060],\n",
      "        [0.6368],\n",
      "        [0.0990],\n",
      "        [0.6227],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.6743],\n",
      "        [0.6498],\n",
      "        [0.2259],\n",
      "        [0.4089],\n",
      "        [0.0990],\n",
      "        [0.1185],\n",
      "        [0.2469],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.2740],\n",
      "        [0.5902],\n",
      "        [0.4644],\n",
      "        [0.4856],\n",
      "        [0.0990],\n",
      "        [0.4738],\n",
      "        [0.6479],\n",
      "        [0.2754],\n",
      "        [0.5173],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.3604],\n",
      "        [0.4732],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.6343],\n",
      "        [0.2531],\n",
      "        [0.3439],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.5168],\n",
      "        [0.3923],\n",
      "        [0.6279],\n",
      "        [0.3743],\n",
      "        [0.4066],\n",
      "        [0.0990],\n",
      "        [0.1256],\n",
      "        [0.0990],\n",
      "        [0.3681],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.0990],\n",
      "        [0.3802],\n",
      "        [0.2156],\n",
      "        [0.4168],\n",
      "        [0.3163],\n",
      "        [0.1276],\n",
      "        [0.3633],\n",
      "        [0.1146],\n",
      "        [0.2913],\n",
      "        [0.3766],\n",
      "        [0.3933],\n",
      "        [0.2911],\n",
      "        [0.1129],\n",
      "        [0.3822],\n",
      "        [0.3849],\n",
      "        [0.3159],\n",
      "        [0.3030],\n",
      "        [0.3733],\n",
      "        [0.5919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0008],\n",
      "        [0.0005],\n",
      "        [0.0012],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0035],\n",
      "        [0.0041],\n",
      "        [0.0049],\n",
      "        [0.0046],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0067],\n",
      "        [0.0070],\n",
      "        [0.0078],\n",
      "        [0.0082],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0104],\n",
      "        [0.0111],\n",
      "        [0.0111],\n",
      "        [0.0117],\n",
      "        [0.0126],\n",
      "        [0.0134],\n",
      "        [0.0132],\n",
      "        [0.0138],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0169],\n",
      "        [0.0176],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0180],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0193],\n",
      "        [0.0197],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0209],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0219],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0230],\n",
      "        [0.0238],\n",
      "        [0.0235],\n",
      "        [0.0237],\n",
      "        [0.0240],\n",
      "        [0.0242],\n",
      "        [0.0247],\n",
      "        [0.0247],\n",
      "        [0.0252],\n",
      "        [0.0252],\n",
      "        [0.0259],\n",
      "        [0.0266],\n",
      "        [0.0264],\n",
      "        [0.0281],\n",
      "        [0.0276],\n",
      "        [0.0278],\n",
      "        [0.0283],\n",
      "        [0.0293],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0303],\n",
      "        [0.0307],\n",
      "        [0.0308],\n",
      "        [0.0315],\n",
      "        [0.0319],\n",
      "        [0.0325],\n",
      "        [0.0325],\n",
      "        [0.0327],\n",
      "        [0.0329],\n",
      "        [0.0336],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0352],\n",
      "        [0.0352],\n",
      "        [0.0376],\n",
      "        [0.0374],\n",
      "        [0.0393],\n",
      "        [0.0410],\n",
      "        [0.0409],\n",
      "        [0.0419],\n",
      "        [0.0422],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0440],\n",
      "        [0.0443],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0452],\n",
      "        [0.0457],\n",
      "        [0.0470],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0492],\n",
      "        [0.0487],\n",
      "        [0.0498],\n",
      "        [0.0501],\n",
      "        [0.0511],\n",
      "        [0.0514],\n",
      "        [0.0526],\n",
      "        [0.0538],\n",
      "        [0.0533],\n",
      "        [0.0544],\n",
      "        [0.0550],\n",
      "        [0.0550],\n",
      "        [0.0556],\n",
      "        [0.0557],\n",
      "        [0.0566],\n",
      "        [0.0594],\n",
      "        [0.0599],\n",
      "        [0.0599],\n",
      "        [0.0641],\n",
      "        [0.0646],\n",
      "        [0.0653],\n",
      "        [0.0661],\n",
      "        [0.0665],\n",
      "        [0.0676],\n",
      "        [0.0684],\n",
      "        [0.0719],\n",
      "        [0.0750],\n",
      "        [0.0766],\n",
      "        [0.0781],\n",
      "        [0.0812],\n",
      "        [0.0844],\n",
      "        [0.0871],\n",
      "        [0.0905],\n",
      "        [0.0961],\n",
      "        [0.0963],\n",
      "        [0.0983],\n",
      "        [0.1033],\n",
      "        [0.1037],\n",
      "        [0.1064],\n",
      "        [0.1066],\n",
      "        [0.1103],\n",
      "        [0.1129],\n",
      "        [0.1135],\n",
      "        [0.1136],\n",
      "        [0.1250],\n",
      "        [0.1402],\n",
      "        [0.1459],\n",
      "        [0.1682]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 62\n",
      "Number of shrink: 38\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0055],\n",
      "        [0.0018],\n",
      "        [0.0030],\n",
      "        [0.0018],\n",
      "        [0.0065],\n",
      "        [0.0024],\n",
      "        [0.0037],\n",
      "        [0.0011],\n",
      "        [0.0036],\n",
      "        [0.0053],\n",
      "        [0.0004],\n",
      "        [0.0025],\n",
      "        [0.0058],\n",
      "        [0.0085],\n",
      "        [0.0068],\n",
      "        [0.0080],\n",
      "        [0.0078],\n",
      "        [0.0072],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0149],\n",
      "        [0.0092],\n",
      "        [0.0103],\n",
      "        [0.0105],\n",
      "        [0.0117],\n",
      "        [0.0106],\n",
      "        [0.0097],\n",
      "        [0.0182],\n",
      "        [0.0140],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0163],\n",
      "        [0.0165],\n",
      "        [0.0207],\n",
      "        [0.0149],\n",
      "        [0.0181],\n",
      "        [0.0147],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0188],\n",
      "        [0.0206],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0198],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0195],\n",
      "        [0.0213],\n",
      "        [0.0195],\n",
      "        [0.0237],\n",
      "        [0.0234],\n",
      "        [0.0220],\n",
      "        [0.0182],\n",
      "        [0.0199],\n",
      "        [0.0234],\n",
      "        [0.0219],\n",
      "        [0.0241],\n",
      "        [0.0214],\n",
      "        [0.0202],\n",
      "        [0.0227],\n",
      "        [0.0254],\n",
      "        [0.0252],\n",
      "        [0.0234],\n",
      "        [0.0247],\n",
      "        [0.0276],\n",
      "        [0.0287],\n",
      "        [0.0282],\n",
      "        [0.0278],\n",
      "        [0.0286],\n",
      "        [0.0296],\n",
      "        [0.0298],\n",
      "        [0.0297],\n",
      "        [0.0289],\n",
      "        [0.0307],\n",
      "        [0.0337],\n",
      "        [0.0315],\n",
      "        [0.0341],\n",
      "        [0.0325],\n",
      "        [0.0325],\n",
      "        [0.0019],\n",
      "        [0.0353],\n",
      "        [0.0336],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0381],\n",
      "        [0.0327],\n",
      "        [0.0351],\n",
      "        [0.0381],\n",
      "        [0.0374],\n",
      "        [0.0393],\n",
      "        [0.0417],\n",
      "        [0.0390],\n",
      "        [0.0418],\n",
      "        [0.0448],\n",
      "        [0.0426],\n",
      "        [0.0436],\n",
      "        [0.0478],\n",
      "        [0.0442],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0513],\n",
      "        [0.0455],\n",
      "        [0.0505],\n",
      "        [0.0492],\n",
      "        [0.0477],\n",
      "        [0.0506],\n",
      "        [0.0468],\n",
      "        [0.0527],\n",
      "        [0.0512],\n",
      "        [0.0510],\n",
      "        [0.0514],\n",
      "        [0.0526],\n",
      "        [0.0207],\n",
      "        [0.0555],\n",
      "        [0.0543],\n",
      "        [0.0550],\n",
      "        [0.0538],\n",
      "        [0.0555],\n",
      "        [0.0236],\n",
      "        [0.0565],\n",
      "        [0.0593],\n",
      "        [0.0561],\n",
      "        [0.0629],\n",
      "        [0.0655],\n",
      "        [0.0950],\n",
      "        [0.0680],\n",
      "        [0.0661],\n",
      "        [0.0665],\n",
      "        [0.0676],\n",
      "        [0.0708],\n",
      "        [0.0719],\n",
      "        [0.0750],\n",
      "        [0.0766],\n",
      "        [0.0780],\n",
      "        [0.0813],\n",
      "        [0.0870],\n",
      "        [0.0940],\n",
      "        [0.0874],\n",
      "        [0.0942],\n",
      "        [0.0963],\n",
      "        [0.0952],\n",
      "        [0.1033],\n",
      "        [0.1025],\n",
      "        [0.0725],\n",
      "        [0.1005],\n",
      "        [0.1101],\n",
      "        [0.1129],\n",
      "        [0.0812],\n",
      "        [0.0805],\n",
      "        [0.1225],\n",
      "        [0.1387],\n",
      "        [0.1412],\n",
      "        [0.1681]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.61738419532776\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.046214361456805e-08, 42)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [42, 39, 38, 152, 28, 90, 67, 0, 66, 124, 37, 43, 143, 64, 10, 132, 116, 50, 140, 142, 35, 6, 40, 139, 122, 19, 7, 70, 119, 30, 78, 45, 65, 52, 120, 18, 121, 41, 141, 34, 44, 133, 148, 1, 21, 138, 22, 117, 15, 36, 144, 46, 157, 51, 47, 77, 23, 71, 123, 81, 69, 106, 158, 76, 24, 94, 82, 11, 91, 68, 115, 31, 33, 27, 60, 8, 29, 105, 107, 118, 5, 93, 134, 146, 79, 26, 89, 145, 99, 137, 131, 75, 136, 55, 20, 3, 147, 32, 88, 129, 98, 104, 72, 114, 125, 149, 9, 2, 112, 61, 48, 16, 49, 128, 63, 153, 135, 130, 80, 4, 127, 126, 92, 17, 14, 111, 113, 73, 25, 103, 96, 110, 53, 74, 102, 156, 108, 150, 109, 155, 97, 154, 54, 56, 62, 83, 151, 59, 100, 57, 86, 95, 87, 101, 84, 85, 58, 13, 12] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5554],\n",
      "        [0.5917],\n",
      "        [0.6054],\n",
      "        [0.4073],\n",
      "        [0.6749],\n",
      "        [0.2360],\n",
      "        [0.3605],\n",
      "        [0.6627],\n",
      "        [0.3549],\n",
      "        [0.0989],\n",
      "        [0.6225],\n",
      "        [0.5425],\n",
      "        [0.0989],\n",
      "        [0.3012],\n",
      "        [0.5817],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4629],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6081],\n",
      "        [0.6324],\n",
      "        [0.5687],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6208],\n",
      "        [0.6348],\n",
      "        [0.3740],\n",
      "        [0.0989],\n",
      "        [0.6570],\n",
      "        [0.3289],\n",
      "        [0.5256],\n",
      "        [0.3220],\n",
      "        [0.4590],\n",
      "        [0.0989],\n",
      "        [0.5677],\n",
      "        [0.0989],\n",
      "        [0.5792],\n",
      "        [0.0989],\n",
      "        [0.6258],\n",
      "        [0.5083],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6600],\n",
      "        [0.6043],\n",
      "        [0.0989],\n",
      "        [0.5972],\n",
      "        [0.0989],\n",
      "        [0.5155],\n",
      "        [0.6233],\n",
      "        [0.0989],\n",
      "        [0.5159],\n",
      "        [0.3934],\n",
      "        [0.4659],\n",
      "        [0.5126],\n",
      "        [0.3757],\n",
      "        [0.6405],\n",
      "        [0.4130],\n",
      "        [0.0989],\n",
      "        [0.2937],\n",
      "        [0.3352],\n",
      "        [0.0989],\n",
      "        [0.3760],\n",
      "        [0.3879],\n",
      "        [0.6345],\n",
      "        [0.1651],\n",
      "        [0.2869],\n",
      "        [0.5923],\n",
      "        [0.2293],\n",
      "        [0.3311],\n",
      "        [0.0989],\n",
      "        [0.6498],\n",
      "        [0.6572],\n",
      "        [0.6593],\n",
      "        [0.2964],\n",
      "        [0.6067],\n",
      "        [0.6422],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6394],\n",
      "        [0.1916],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2777],\n",
      "        [0.6148],\n",
      "        [0.2259],\n",
      "        [0.0989],\n",
      "        [0.1524],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3637],\n",
      "        [0.0989],\n",
      "        [0.4024],\n",
      "        [0.6232],\n",
      "        [0.6517],\n",
      "        [0.0989],\n",
      "        [0.6750],\n",
      "        [0.2259],\n",
      "        [0.0989],\n",
      "        [0.1185],\n",
      "        [0.0989],\n",
      "        [0.4063],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5905],\n",
      "        [0.6498],\n",
      "        [0.0989],\n",
      "        [0.2432],\n",
      "        [0.4837],\n",
      "        [0.4609],\n",
      "        [0.4724],\n",
      "        [0.0989],\n",
      "        [0.2680],\n",
      "        [0.5184],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2725],\n",
      "        [0.6354],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2531],\n",
      "        [0.4710],\n",
      "        [0.5130],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3892],\n",
      "        [0.6293],\n",
      "        [0.0989],\n",
      "        [0.1256],\n",
      "        [0.0989],\n",
      "        [0.4039],\n",
      "        [0.3657],\n",
      "        [0.0989],\n",
      "        [0.4105],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4180],\n",
      "        [0.0989],\n",
      "        [0.4146],\n",
      "        [0.3775],\n",
      "        [0.4138],\n",
      "        [0.2087],\n",
      "        [0.3144],\n",
      "        [0.4047],\n",
      "        [0.3601],\n",
      "        [0.1276],\n",
      "        [0.3872],\n",
      "        [0.2901],\n",
      "        [0.1145],\n",
      "        [0.2909],\n",
      "        [0.1129],\n",
      "        [0.3134],\n",
      "        [0.3014],\n",
      "        [0.3686],\n",
      "        [0.5918],\n",
      "        [0.5956]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0011],\n",
      "        [0.0018],\n",
      "        [0.0019],\n",
      "        [0.0018],\n",
      "        [0.0024],\n",
      "        [0.0025],\n",
      "        [0.0037],\n",
      "        [0.0030],\n",
      "        [0.0036],\n",
      "        [0.0053],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0072],\n",
      "        [0.0068],\n",
      "        [0.0078],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0099],\n",
      "        [0.0103],\n",
      "        [0.0105],\n",
      "        [0.0106],\n",
      "        [0.0117],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0155],\n",
      "        [0.0163],\n",
      "        [0.0165],\n",
      "        [0.0182],\n",
      "        [0.0181],\n",
      "        [0.0188],\n",
      "        [0.0182],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0202],\n",
      "        [0.0202],\n",
      "        [0.0206],\n",
      "        [0.0206],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0217],\n",
      "        [0.0219],\n",
      "        [0.0220],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0234],\n",
      "        [0.0236],\n",
      "        [0.0234],\n",
      "        [0.0237],\n",
      "        [0.0241],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0252],\n",
      "        [0.0276],\n",
      "        [0.0278],\n",
      "        [0.0282],\n",
      "        [0.0286],\n",
      "        [0.0287],\n",
      "        [0.0289],\n",
      "        [0.0297],\n",
      "        [0.0296],\n",
      "        [0.0298],\n",
      "        [0.0307],\n",
      "        [0.0315],\n",
      "        [0.0327],\n",
      "        [0.0325],\n",
      "        [0.0325],\n",
      "        [0.0336],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0351],\n",
      "        [0.0353],\n",
      "        [0.0374],\n",
      "        [0.0381],\n",
      "        [0.0381],\n",
      "        [0.0390],\n",
      "        [0.0393],\n",
      "        [0.0417],\n",
      "        [0.0418],\n",
      "        [0.0426],\n",
      "        [0.0436],\n",
      "        [0.0442],\n",
      "        [0.0448],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0455],\n",
      "        [0.0468],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0492],\n",
      "        [0.0505],\n",
      "        [0.0506],\n",
      "        [0.0510],\n",
      "        [0.0513],\n",
      "        [0.0512],\n",
      "        [0.0514],\n",
      "        [0.0526],\n",
      "        [0.0527],\n",
      "        [0.0538],\n",
      "        [0.0543],\n",
      "        [0.0550],\n",
      "        [0.0555],\n",
      "        [0.0555],\n",
      "        [0.0561],\n",
      "        [0.0565],\n",
      "        [0.0593],\n",
      "        [0.0629],\n",
      "        [0.0655],\n",
      "        [0.0661],\n",
      "        [0.0665],\n",
      "        [0.0676],\n",
      "        [0.0680],\n",
      "        [0.0708],\n",
      "        [0.0719],\n",
      "        [0.0725],\n",
      "        [0.0750],\n",
      "        [0.0766],\n",
      "        [0.0780],\n",
      "        [0.0805],\n",
      "        [0.0813],\n",
      "        [0.0812],\n",
      "        [0.0870],\n",
      "        [0.0874],\n",
      "        [0.0940],\n",
      "        [0.0942],\n",
      "        [0.0950],\n",
      "        [0.0952],\n",
      "        [0.0963],\n",
      "        [0.1005],\n",
      "        [0.1025],\n",
      "        [0.1033],\n",
      "        [0.1101],\n",
      "        [0.1129],\n",
      "        [0.1225],\n",
      "        [0.1387],\n",
      "        [0.1412],\n",
      "        [0.1681],\n",
      "        [0.1751]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 46\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0020],\n",
      "        [    0.0000],\n",
      "        [    0.0025],\n",
      "        [    0.0035],\n",
      "        [    0.0058],\n",
      "        [    0.0023],\n",
      "        [    0.0010],\n",
      "        [    0.0013],\n",
      "        [    0.0040],\n",
      "        [    0.0036],\n",
      "        [    0.0067],\n",
      "        [    0.0069],\n",
      "        [    0.0058],\n",
      "        [    0.0079],\n",
      "        [    0.0137],\n",
      "        [    0.0068],\n",
      "        [    0.0078],\n",
      "        [    0.0107],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0112],\n",
      "        [    0.0155],\n",
      "        [    0.0083],\n",
      "        [    0.0097],\n",
      "        [    0.0099],\n",
      "        [    0.0043],\n",
      "        [    0.0169],\n",
      "        [    0.0087],\n",
      "        [    0.0117],\n",
      "        [    0.0109],\n",
      "        [    0.0130],\n",
      "        [    0.0168],\n",
      "        [    0.0141],\n",
      "        [    0.0166],\n",
      "        [    0.0155],\n",
      "        [    0.0103],\n",
      "        [    0.0165],\n",
      "        [    0.0161],\n",
      "        [    0.0180],\n",
      "        [    0.0163],\n",
      "        [    0.0194],\n",
      "        [    0.0191],\n",
      "        [    0.0193],\n",
      "        [    0.0248],\n",
      "        [    0.0244],\n",
      "        [    0.0198],\n",
      "        [    0.0232],\n",
      "        [    0.0203],\n",
      "        [    0.0127],\n",
      "        [    0.0229],\n",
      "        [    0.0207],\n",
      "        [    0.0238],\n",
      "        [    0.0132],\n",
      "        [    0.0229],\n",
      "        [    0.0244],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0191],\n",
      "        [    0.0221],\n",
      "        [    0.0193],\n",
      "        [    0.0251],\n",
      "        [    0.0234],\n",
      "        [    0.0157],\n",
      "        [    0.0210],\n",
      "        [    0.0210],\n",
      "        [    0.0241],\n",
      "        [    0.0211],\n",
      "        [    0.0185],\n",
      "        [    0.0253],\n",
      "        [    0.0293],\n",
      "        [    0.0278],\n",
      "        [    0.0262],\n",
      "        [    0.0255],\n",
      "        [    0.0245],\n",
      "        [    0.0273],\n",
      "        [    0.0362],\n",
      "        [    0.0335],\n",
      "        [    0.0298],\n",
      "        [    0.0307],\n",
      "        [    0.0315],\n",
      "        [    0.0391],\n",
      "        [    0.0325],\n",
      "        [    0.0326],\n",
      "        [    0.0337],\n",
      "        [    0.0351],\n",
      "        [    0.0307],\n",
      "        [    0.0341],\n",
      "        [    0.0342],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0351],\n",
      "        [    0.0382],\n",
      "        [    0.0374],\n",
      "        [    0.0393],\n",
      "        [    0.0332],\n",
      "        [    0.0450],\n",
      "        [    0.0393],\n",
      "        [    0.0390],\n",
      "        [    0.0418],\n",
      "        [    0.0426],\n",
      "        [    0.0436],\n",
      "        [    0.0442],\n",
      "        [    0.0475],\n",
      "        [    0.0450],\n",
      "        [    0.0450],\n",
      "        [    0.0454],\n",
      "        [    0.0518],\n",
      "        [    0.0526],\n",
      "        [    0.0477],\n",
      "        [    0.0500],\n",
      "        [    0.0520],\n",
      "        [    0.0571],\n",
      "        [    0.0534],\n",
      "        [    0.0510],\n",
      "        [    0.0536],\n",
      "        [    0.0493],\n",
      "        [    0.0514],\n",
      "        [    0.0526],\n",
      "        [    0.0549],\n",
      "        [    0.0604],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0555],\n",
      "        [    0.0614],\n",
      "        [    0.0478],\n",
      "        [    0.0565],\n",
      "        [    0.0593],\n",
      "        [    0.0658],\n",
      "        [    0.0632],\n",
      "        [    0.0661],\n",
      "        [    0.0665],\n",
      "        [    0.0676],\n",
      "        [    0.0688],\n",
      "        [    0.0738],\n",
      "        [    0.0719],\n",
      "        [    0.0648],\n",
      "        [    0.0750],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0741],\n",
      "        [    0.0813],\n",
      "        [    0.0747],\n",
      "        [    0.0881],\n",
      "        [    0.0871],\n",
      "        [    0.0962],\n",
      "        [    0.0901],\n",
      "        [    0.0997],\n",
      "        [    0.0933],\n",
      "        [    0.0963],\n",
      "        [    0.0989],\n",
      "        [    0.0979],\n",
      "        [    0.1033],\n",
      "        [    0.1054],\n",
      "        [    0.1129],\n",
      "        [    0.1184],\n",
      "        [    0.1338],\n",
      "        [    0.1394],\n",
      "        [    0.1604],\n",
      "        [    0.1682]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.86308336257935\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 27 個區塊累積花費時間(s) 1.1758372783660889\n",
      "<<The performance of 27 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 1.1758372783660889\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1024.92\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1317.45\n",
      "The accuracy(2000) for l = 1: 88.68%\n",
      "The accuracy(3000) for l = 1: 96.86%\n",
      "The maximum error: tensor(4293.9570)\n",
      "The minimum error: tensor(1.1797)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1943.3\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 2031.1\n",
      "The accuracy(2000) for l = 1: 25.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 2415.1581250000017\n",
      "The minimum error: 938.19921875\n",
      "------------------------------------------------------------\n",
      "0.8867924528301887\n",
      "<class 'float'>\n",
      "0.25\n",
      "<class 'float'>\n",
      "The <<28>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.42593550487436e-08, 35)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [35, 63, 86, 38, 34, 120, 62, 148, 15, 139, 24, 33, 128, 39, 60, 112, 136, 138, 36, 66, 135, 118, 14, 46, 26, 31, 115, 74, 11, 153, 6, 61, 2, 116, 154, 37, 30, 117, 48, 3, 41, 137, 73, 7, 19, 129, 67, 144, 77, 40, 134, 113, 140, 72, 78, 20, 119, 47, 32, 18, 42, 102, 17, 90, 43, 23, 65, 87, 29, 27, 56, 111, 64, 101, 103, 22, 114, 89, 130, 16, 25, 142, 85, 141, 95, 133, 127, 75, 4, 155, 132, 71, 143, 1, 28, 51, 84, 125, 94, 100, 110, 121, 145, 10, 68, 108, 149, 57, 124, 131, 44, 5, 126, 45, 59, 123, 122, 76, 88, 107, 12, 109, 0, 13, 21, 152, 69, 99, 92, 106, 49, 98, 70, 151, 150, 104, 146, 105, 93, 156, 52, 50, 79, 157, 55, 158, 58, 96, 82, 53, 147, 91, 83, 97, 80]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0010],\n",
      "        [    0.0023],\n",
      "        [    0.0020],\n",
      "        [    0.0025],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0035],\n",
      "        [    0.0043],\n",
      "        [    0.0058],\n",
      "        [    0.0058],\n",
      "        [    0.0067],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0079],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0083],\n",
      "        [    0.0087],\n",
      "        [    0.0097],\n",
      "        [    0.0099],\n",
      "        [    0.0103],\n",
      "        [    0.0107],\n",
      "        [    0.0109],\n",
      "        [    0.0112],\n",
      "        [    0.0117],\n",
      "        [    0.0130],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0137],\n",
      "        [    0.0141],\n",
      "        [    0.0155],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.0161],\n",
      "        [    0.0163],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0169],\n",
      "        [    0.0168],\n",
      "        [    0.0180],\n",
      "        [    0.0188],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0191],\n",
      "        [    0.0191],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0194],\n",
      "        [    0.0198],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0210],\n",
      "        [    0.0211],\n",
      "        [    0.0210],\n",
      "        [    0.0221],\n",
      "        [    0.0229],\n",
      "        [    0.0229],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0234],\n",
      "        [    0.0244],\n",
      "        [    0.0241],\n",
      "        [    0.0244],\n",
      "        [    0.0245],\n",
      "        [    0.0251],\n",
      "        [    0.0253],\n",
      "        [    0.0255],\n",
      "        [    0.0262],\n",
      "        [    0.0273],\n",
      "        [    0.0278],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0307],\n",
      "        [    0.0307],\n",
      "        [    0.0315],\n",
      "        [    0.0325],\n",
      "        [    0.0326],\n",
      "        [    0.0332],\n",
      "        [    0.0335],\n",
      "        [    0.0337],\n",
      "        [    0.0341],\n",
      "        [    0.0342],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0351],\n",
      "        [    0.0351],\n",
      "        [    0.0362],\n",
      "        [    0.0366],\n",
      "        [    0.0374],\n",
      "        [    0.0382],\n",
      "        [    0.0393],\n",
      "        [    0.0391],\n",
      "        [    0.0390],\n",
      "        [    0.0393],\n",
      "        [    0.0418],\n",
      "        [    0.0426],\n",
      "        [    0.0436],\n",
      "        [    0.0442],\n",
      "        [    0.0450],\n",
      "        [    0.0450],\n",
      "        [    0.0454],\n",
      "        [    0.0478],\n",
      "        [    0.0475],\n",
      "        [    0.0477],\n",
      "        [    0.0493],\n",
      "        [    0.0500],\n",
      "        [    0.0510],\n",
      "        [    0.0514],\n",
      "        [    0.0520],\n",
      "        [    0.0518],\n",
      "        [    0.0526],\n",
      "        [    0.0534],\n",
      "        [    0.0536],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0549],\n",
      "        [    0.0555],\n",
      "        [    0.0565],\n",
      "        [    0.0571],\n",
      "        [    0.0593],\n",
      "        [    0.0604],\n",
      "        [    0.0614],\n",
      "        [    0.0632],\n",
      "        [    0.0648],\n",
      "        [    0.0658],\n",
      "        [    0.0661],\n",
      "        [    0.0665],\n",
      "        [    0.0676],\n",
      "        [    0.0688],\n",
      "        [    0.0719],\n",
      "        [    0.0738],\n",
      "        [    0.0741],\n",
      "        [    0.0747],\n",
      "        [    0.0750],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0813],\n",
      "        [    0.0826],\n",
      "        [    0.0871],\n",
      "        [    0.0881],\n",
      "        [    0.0901],\n",
      "        [    0.0909],\n",
      "        [    0.0933],\n",
      "        [    0.0951],\n",
      "        [    0.0962],\n",
      "        [    0.0963],\n",
      "        [    0.0979],\n",
      "        [    0.0989],\n",
      "        [    0.0997],\n",
      "        [    0.1033],\n",
      "        [    0.1054],\n",
      "        [    0.1129],\n",
      "        [    0.1184]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (8.42593550487436e-08, 35)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [35, 63, 86, 38, 34, 120, 62, 148, 15, 139, 24, 33, 128, 39, 60, 112, 136, 138, 36, 66, 135, 118, 14, 46, 26, 31, 115, 74, 11, 153, 6, 61, 2, 116, 154, 37, 30, 117, 48, 3, 41, 137, 73, 7, 19, 129, 67, 144, 77, 40, 134, 113, 140, 72, 78, 20, 119, 47, 32, 18, 42, 102, 17, 90, 43, 23, 65, 87, 29, 27, 56, 111, 64, 101, 103, 22, 114, 89, 130, 16, 25, 142, 85, 141, 95, 133, 127, 75, 4, 155, 132, 71, 143, 1, 28, 51, 84, 125, 94, 100, 110, 121, 145, 10, 68, 108, 149, 57, 124, 131, 44, 5, 126, 45, 59, 123, 122, 76, 88, 107, 12, 109, 0, 13, 21, 152, 69, 99, 92, 106, 49, 98, 70, 151, 150, 104, 146, 105, 93, 156, 52, 50, 79, 157, 55, 158, 58, 96, 82, 53, 147, 91, 83, 97, 80, 81] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5906],\n",
      "        [0.3590],\n",
      "        [0.2359],\n",
      "        [0.5538],\n",
      "        [0.6047],\n",
      "        [0.0989],\n",
      "        [0.3539],\n",
      "        [0.4126],\n",
      "        [0.6148],\n",
      "        [0.0989],\n",
      "        [0.6710],\n",
      "        [0.6211],\n",
      "        [0.0989],\n",
      "        [0.5410],\n",
      "        [0.2999],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5673],\n",
      "        [0.3722],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5618],\n",
      "        [0.4602],\n",
      "        [0.6539],\n",
      "        [0.6053],\n",
      "        [0.0989],\n",
      "        [0.3272],\n",
      "        [0.5081],\n",
      "        [0.4009],\n",
      "        [0.5751],\n",
      "        [0.3212],\n",
      "        [0.6261],\n",
      "        [0.0989],\n",
      "        [0.3840],\n",
      "        [0.5771],\n",
      "        [0.6233],\n",
      "        [0.0989],\n",
      "        [0.4574],\n",
      "        [0.6285],\n",
      "        [0.5237],\n",
      "        [0.0989],\n",
      "        [0.3731],\n",
      "        [0.5854],\n",
      "        [0.6376],\n",
      "        [0.0989],\n",
      "        [0.4102],\n",
      "        [0.0989],\n",
      "        [0.2903],\n",
      "        [0.5071],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3855],\n",
      "        [0.2833],\n",
      "        [0.6318],\n",
      "        [0.0989],\n",
      "        [0.4643],\n",
      "        [0.6211],\n",
      "        [0.5939],\n",
      "        [0.5129],\n",
      "        [0.0989],\n",
      "        [0.5995],\n",
      "        [0.1651],\n",
      "        [0.5096],\n",
      "        [0.6550],\n",
      "        [0.3335],\n",
      "        [0.2293],\n",
      "        [0.6542],\n",
      "        [0.6478],\n",
      "        [0.2948],\n",
      "        [0.0989],\n",
      "        [0.3294],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6114],\n",
      "        [0.0989],\n",
      "        [0.1916],\n",
      "        [0.0989],\n",
      "        [0.6183],\n",
      "        [0.6382],\n",
      "        [0.0989],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.1524],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2764],\n",
      "        [0.6002],\n",
      "        [0.3898],\n",
      "        [0.0989],\n",
      "        [0.3607],\n",
      "        [0.0989],\n",
      "        [0.6329],\n",
      "        [0.6724],\n",
      "        [0.4011],\n",
      "        [0.2259],\n",
      "        [0.0989],\n",
      "        [0.1184],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5046],\n",
      "        [0.4036],\n",
      "        [0.0989],\n",
      "        [0.5165],\n",
      "        [0.2410],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4810],\n",
      "        [0.5842],\n",
      "        [0.0989],\n",
      "        [0.4697],\n",
      "        [0.2657],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2703],\n",
      "        [0.2530],\n",
      "        [0.0989],\n",
      "        [0.4542],\n",
      "        [0.0989],\n",
      "        [0.6288],\n",
      "        [0.4651],\n",
      "        [0.6270],\n",
      "        [0.4183],\n",
      "        [0.3864],\n",
      "        [0.0989],\n",
      "        [0.1256],\n",
      "        [0.0989],\n",
      "        [0.4031],\n",
      "        [0.0989],\n",
      "        [0.3627],\n",
      "        [0.4244],\n",
      "        [0.4211],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3856],\n",
      "        [0.4135],\n",
      "        [0.3765],\n",
      "        [0.3102],\n",
      "        [0.3926],\n",
      "        [0.3582],\n",
      "        [0.4053],\n",
      "        [0.2065],\n",
      "        [0.1276],\n",
      "        [0.2855],\n",
      "        [0.3856],\n",
      "        [0.4094],\n",
      "        [0.1145],\n",
      "        [0.2862],\n",
      "        [0.1129],\n",
      "        [0.3093],\n",
      "        [0.2966]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0010],\n",
      "        [    0.0023],\n",
      "        [    0.0020],\n",
      "        [    0.0025],\n",
      "        [    0.0036],\n",
      "        [    0.0040],\n",
      "        [    0.0035],\n",
      "        [    0.0043],\n",
      "        [    0.0058],\n",
      "        [    0.0058],\n",
      "        [    0.0067],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0079],\n",
      "        [    0.0078],\n",
      "        [    0.0082],\n",
      "        [    0.0083],\n",
      "        [    0.0083],\n",
      "        [    0.0087],\n",
      "        [    0.0097],\n",
      "        [    0.0099],\n",
      "        [    0.0103],\n",
      "        [    0.0107],\n",
      "        [    0.0109],\n",
      "        [    0.0112],\n",
      "        [    0.0117],\n",
      "        [    0.0130],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0137],\n",
      "        [    0.0141],\n",
      "        [    0.0155],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.0161],\n",
      "        [    0.0163],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0169],\n",
      "        [    0.0168],\n",
      "        [    0.0180],\n",
      "        [    0.0188],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0191],\n",
      "        [    0.0191],\n",
      "        [    0.0193],\n",
      "        [    0.0193],\n",
      "        [    0.0194],\n",
      "        [    0.0198],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0210],\n",
      "        [    0.0211],\n",
      "        [    0.0210],\n",
      "        [    0.0221],\n",
      "        [    0.0229],\n",
      "        [    0.0229],\n",
      "        [    0.0232],\n",
      "        [    0.0238],\n",
      "        [    0.0234],\n",
      "        [    0.0244],\n",
      "        [    0.0241],\n",
      "        [    0.0244],\n",
      "        [    0.0245],\n",
      "        [    0.0251],\n",
      "        [    0.0253],\n",
      "        [    0.0255],\n",
      "        [    0.0262],\n",
      "        [    0.0273],\n",
      "        [    0.0278],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0307],\n",
      "        [    0.0307],\n",
      "        [    0.0315],\n",
      "        [    0.0325],\n",
      "        [    0.0326],\n",
      "        [    0.0332],\n",
      "        [    0.0335],\n",
      "        [    0.0337],\n",
      "        [    0.0341],\n",
      "        [    0.0342],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0351],\n",
      "        [    0.0351],\n",
      "        [    0.0362],\n",
      "        [    0.0366],\n",
      "        [    0.0374],\n",
      "        [    0.0382],\n",
      "        [    0.0393],\n",
      "        [    0.0391],\n",
      "        [    0.0390],\n",
      "        [    0.0393],\n",
      "        [    0.0418],\n",
      "        [    0.0426],\n",
      "        [    0.0436],\n",
      "        [    0.0442],\n",
      "        [    0.0450],\n",
      "        [    0.0450],\n",
      "        [    0.0454],\n",
      "        [    0.0478],\n",
      "        [    0.0475],\n",
      "        [    0.0477],\n",
      "        [    0.0493],\n",
      "        [    0.0500],\n",
      "        [    0.0510],\n",
      "        [    0.0514],\n",
      "        [    0.0520],\n",
      "        [    0.0518],\n",
      "        [    0.0526],\n",
      "        [    0.0534],\n",
      "        [    0.0536],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0549],\n",
      "        [    0.0555],\n",
      "        [    0.0565],\n",
      "        [    0.0571],\n",
      "        [    0.0593],\n",
      "        [    0.0604],\n",
      "        [    0.0614],\n",
      "        [    0.0632],\n",
      "        [    0.0648],\n",
      "        [    0.0658],\n",
      "        [    0.0661],\n",
      "        [    0.0665],\n",
      "        [    0.0676],\n",
      "        [    0.0688],\n",
      "        [    0.0719],\n",
      "        [    0.0738],\n",
      "        [    0.0741],\n",
      "        [    0.0747],\n",
      "        [    0.0750],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0813],\n",
      "        [    0.0826],\n",
      "        [    0.0871],\n",
      "        [    0.0881],\n",
      "        [    0.0901],\n",
      "        [    0.0909],\n",
      "        [    0.0933],\n",
      "        [    0.0951],\n",
      "        [    0.0962],\n",
      "        [    0.0963],\n",
      "        [    0.0979],\n",
      "        [    0.0989],\n",
      "        [    0.0997],\n",
      "        [    0.1033],\n",
      "        [    0.1054],\n",
      "        [    0.1129],\n",
      "        [    0.1184],\n",
      "        [    0.1338]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 62\n",
      "Number of shrink: 38\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0051],\n",
      "        [    0.0003],\n",
      "        [    0.0023],\n",
      "        [    0.0013],\n",
      "        [    0.0028],\n",
      "        [    0.0036],\n",
      "        [    0.0054],\n",
      "        [    0.0218],\n",
      "        [    0.0013],\n",
      "        [    0.0058],\n",
      "        [    0.0049],\n",
      "        [    0.0017],\n",
      "        [    0.0068],\n",
      "        [    0.0035],\n",
      "        [    0.0092],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0125],\n",
      "        [    0.0066],\n",
      "        [    0.0097],\n",
      "        [    0.0099],\n",
      "        [    0.0085],\n",
      "        [    0.0048],\n",
      "        [    0.0132],\n",
      "        [    0.0075],\n",
      "        [    0.0118],\n",
      "        [    0.0140],\n",
      "        [    0.0123],\n",
      "        [    0.0141],\n",
      "        [    0.0080],\n",
      "        [    0.0128],\n",
      "        [    0.0112],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.0191],\n",
      "        [    0.0201],\n",
      "        [    0.0165],\n",
      "        [    0.0087],\n",
      "        [    0.0122],\n",
      "        [    0.0142],\n",
      "        [    0.0180],\n",
      "        [    0.0186],\n",
      "        [    0.0238],\n",
      "        [    0.0203],\n",
      "        [    0.0191],\n",
      "        [    0.0167],\n",
      "        [    0.0193],\n",
      "        [    0.0197],\n",
      "        [    0.0163],\n",
      "        [    0.0198],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0209],\n",
      "        [    0.0203],\n",
      "        [    0.0212],\n",
      "        [    0.0221],\n",
      "        [    0.0160],\n",
      "        [    0.0187],\n",
      "        [    0.0221],\n",
      "        [    0.0208],\n",
      "        [    0.0234],\n",
      "        [    0.0259],\n",
      "        [    0.0241],\n",
      "        [    0.0223],\n",
      "        [    0.0258],\n",
      "        [    0.0257],\n",
      "        [    0.0253],\n",
      "        [    0.0276],\n",
      "        [    0.0294],\n",
      "        [    0.0346],\n",
      "        [    0.0278],\n",
      "        [    0.0298],\n",
      "        [    0.0298],\n",
      "        [    0.0306],\n",
      "        [    0.0333],\n",
      "        [    0.0316],\n",
      "        [    0.0324],\n",
      "        [    0.0326],\n",
      "        [    0.0295],\n",
      "        [    0.0319],\n",
      "        [    0.0337],\n",
      "        [    0.0341],\n",
      "        [    0.0342],\n",
      "        [    0.0342],\n",
      "        [    0.0345],\n",
      "        [    0.0351],\n",
      "        [    0.0322],\n",
      "        [    0.0309],\n",
      "        [    0.0001],\n",
      "        [    0.0374],\n",
      "        [    0.0381],\n",
      "        [    0.0393],\n",
      "        [    0.0355],\n",
      "        [    0.0416],\n",
      "        [    0.0335],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0436],\n",
      "        [    0.0442],\n",
      "        [    0.0449],\n",
      "        [    0.0450],\n",
      "        [    0.0454],\n",
      "        [    0.0484],\n",
      "        [    0.0499],\n",
      "        [    0.0477],\n",
      "        [    0.0455],\n",
      "        [    0.0439],\n",
      "        [    0.0510],\n",
      "        [    0.0515],\n",
      "        [    0.0479],\n",
      "        [    0.0461],\n",
      "        [    0.0526],\n",
      "        [    0.0480],\n",
      "        [    0.0541],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0535],\n",
      "        [    0.0555],\n",
      "        [    0.0565],\n",
      "        [    0.0574],\n",
      "        [    0.0593],\n",
      "        [    0.0576],\n",
      "        [    0.0612],\n",
      "        [    0.0639],\n",
      "        [    0.0408],\n",
      "        [    0.0673],\n",
      "        [    0.0660],\n",
      "        [    0.0666],\n",
      "        [    0.0675],\n",
      "        [    0.0618],\n",
      "        [    0.0718],\n",
      "        [    0.0742],\n",
      "        [    0.0534],\n",
      "        [    0.0550],\n",
      "        [    0.0750],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0813],\n",
      "        [    0.0435],\n",
      "        [    0.0928],\n",
      "        [    0.0811],\n",
      "        [    0.0882],\n",
      "        [    0.0511],\n",
      "        [    0.0995],\n",
      "        [    0.0547],\n",
      "        [    0.0945],\n",
      "        [    0.0962],\n",
      "        [    0.0940],\n",
      "        [    0.1024],\n",
      "        [    0.1171],\n",
      "        [    0.1034],\n",
      "        [    0.1015],\n",
      "        [    0.1128],\n",
      "        [    0.1151],\n",
      "        [    0.1297]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 97.39663743972778\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.452706837630103e-08, 155)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [155, 63, 15, 38, 86, 33, 34, 120, 39, 46, 24, 35, 62, 139, 128, 66, 31, 112, 6, 136, 138, 48, 14, 60, 135, 118, 2, 115, 3, 11, 36, 61, 26, 74, 41, 153, 116, 47, 154, 40, 117, 67, 137, 73, 37, 32, 129, 144, 77, 134, 78, 30, 19, 113, 140, 42, 72, 20, 148, 119, 43, 18, 102, 7, 90, 87, 23, 65, 17, 29, 111, 27, 16, 64, 101, 103, 4, 114, 25, 75, 89, 130, 51, 22, 142, 85, 95, 141, 133, 56, 127, 1, 132, 71, 143, 152, 28, 84, 125, 156, 94, 57, 100, 110, 121, 145, 149, 5, 44, 108, 45, 10, 68, 124, 157, 131, 126, 151, 76, 59, 123, 122, 150, 158, 88, 107, 0, 12, 109, 13, 49, 21, 99, 92, 69, 106, 98, 70, 104, 146, 105, 50, 93, 79, 52, 82, 58, 96, 55, 83, 53, 91, 97, 80, 147, 81, 54] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4263],\n",
      "        [0.3577],\n",
      "        [0.6092],\n",
      "        [0.5571],\n",
      "        [0.2359],\n",
      "        [0.6261],\n",
      "        [0.6101],\n",
      "        [0.0989],\n",
      "        [0.5444],\n",
      "        [0.4661],\n",
      "        [0.6718],\n",
      "        [0.5957],\n",
      "        [0.3525],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3701],\n",
      "        [0.6091],\n",
      "        [0.0989],\n",
      "        [0.5808],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4652],\n",
      "        [0.5600],\n",
      "        [0.2986],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6304],\n",
      "        [0.0989],\n",
      "        [0.6332],\n",
      "        [0.5077],\n",
      "        [0.5715],\n",
      "        [0.3199],\n",
      "        [0.6562],\n",
      "        [0.3282],\n",
      "        [0.5264],\n",
      "        [0.4282],\n",
      "        [0.0989],\n",
      "        [0.4712],\n",
      "        [0.4153],\n",
      "        [0.5101],\n",
      "        [0.0989],\n",
      "        [0.4078],\n",
      "        [0.0989],\n",
      "        [0.3729],\n",
      "        [0.5802],\n",
      "        [0.6253],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2907],\n",
      "        [0.0989],\n",
      "        [0.2825],\n",
      "        [0.6271],\n",
      "        [0.6390],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5158],\n",
      "        [0.3854],\n",
      "        [0.6320],\n",
      "        [0.4309],\n",
      "        [0.0989],\n",
      "        [0.5117],\n",
      "        [0.5950],\n",
      "        [0.0989],\n",
      "        [0.5907],\n",
      "        [0.1650],\n",
      "        [0.2292],\n",
      "        [0.6564],\n",
      "        [0.3328],\n",
      "        [0.5980],\n",
      "        [0.6563],\n",
      "        [0.0989],\n",
      "        [0.6510],\n",
      "        [0.6146],\n",
      "        [0.3290],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6055],\n",
      "        [0.0989],\n",
      "        [0.6398],\n",
      "        [0.2792],\n",
      "        [0.1915],\n",
      "        [0.0989],\n",
      "        [0.4069],\n",
      "        [0.6140],\n",
      "        [0.0989],\n",
      "        [0.2258],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3021],\n",
      "        [0.0989],\n",
      "        [0.6366],\n",
      "        [0.0989],\n",
      "        [0.3609],\n",
      "        [0.0989],\n",
      "        [0.4423],\n",
      "        [0.6749],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.4247],\n",
      "        [0.1184],\n",
      "        [0.2471],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5127],\n",
      "        [0.5899],\n",
      "        [0.4851],\n",
      "        [0.0989],\n",
      "        [0.4751],\n",
      "        [0.5053],\n",
      "        [0.4011],\n",
      "        [0.0989],\n",
      "        [0.4324],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4451],\n",
      "        [0.2718],\n",
      "        [0.2651],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4408],\n",
      "        [0.4457],\n",
      "        [0.2530],\n",
      "        [0.0989],\n",
      "        [0.6317],\n",
      "        [0.4540],\n",
      "        [0.0989],\n",
      "        [0.4653],\n",
      "        [0.4101],\n",
      "        [0.6277],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.3849],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3623],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3835],\n",
      "        [0.0989],\n",
      "        [0.3083],\n",
      "        [0.4192],\n",
      "        [0.2817],\n",
      "        [0.2082],\n",
      "        [0.1275],\n",
      "        [0.3644],\n",
      "        [0.2823],\n",
      "        [0.3892],\n",
      "        [0.1144],\n",
      "        [0.1128],\n",
      "        [0.3060],\n",
      "        [0.4268],\n",
      "        [0.2924],\n",
      "        [0.3718]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0013],\n",
      "        [    0.0013],\n",
      "        [    0.0023],\n",
      "        [    0.0017],\n",
      "        [    0.0028],\n",
      "        [    0.0036],\n",
      "        [    0.0035],\n",
      "        [    0.0048],\n",
      "        [    0.0049],\n",
      "        [    0.0051],\n",
      "        [    0.0054],\n",
      "        [    0.0058],\n",
      "        [    0.0068],\n",
      "        [    0.0066],\n",
      "        [    0.0075],\n",
      "        [    0.0079],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0087],\n",
      "        [    0.0085],\n",
      "        [    0.0092],\n",
      "        [    0.0097],\n",
      "        [    0.0099],\n",
      "        [    0.0112],\n",
      "        [    0.0118],\n",
      "        [    0.0122],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0128],\n",
      "        [    0.0132],\n",
      "        [    0.0140],\n",
      "        [    0.0142],\n",
      "        [    0.0141],\n",
      "        [    0.0155],\n",
      "        [    0.0160],\n",
      "        [    0.0157],\n",
      "        [    0.0163],\n",
      "        [    0.0165],\n",
      "        [    0.0167],\n",
      "        [    0.0180],\n",
      "        [    0.0186],\n",
      "        [    0.0191],\n",
      "        [    0.0187],\n",
      "        [    0.0191],\n",
      "        [    0.0193],\n",
      "        [    0.0197],\n",
      "        [    0.0198],\n",
      "        [    0.0203],\n",
      "        [    0.0201],\n",
      "        [    0.0203],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0208],\n",
      "        [    0.0209],\n",
      "        [    0.0212],\n",
      "        [    0.0218],\n",
      "        [    0.0221],\n",
      "        [    0.0223],\n",
      "        [    0.0221],\n",
      "        [    0.0234],\n",
      "        [    0.0238],\n",
      "        [    0.0241],\n",
      "        [    0.0253],\n",
      "        [    0.0258],\n",
      "        [    0.0257],\n",
      "        [    0.0259],\n",
      "        [    0.0276],\n",
      "        [    0.0278],\n",
      "        [    0.0294],\n",
      "        [    0.0295],\n",
      "        [    0.0298],\n",
      "        [    0.0298],\n",
      "        [    0.0306],\n",
      "        [    0.0309],\n",
      "        [    0.0316],\n",
      "        [    0.0319],\n",
      "        [    0.0322],\n",
      "        [    0.0324],\n",
      "        [    0.0326],\n",
      "        [    0.0335],\n",
      "        [    0.0333],\n",
      "        [    0.0337],\n",
      "        [    0.0341],\n",
      "        [    0.0342],\n",
      "        [    0.0342],\n",
      "        [    0.0345],\n",
      "        [    0.0346],\n",
      "        [    0.0351],\n",
      "        [    0.0355],\n",
      "        [    0.0374],\n",
      "        [    0.0381],\n",
      "        [    0.0393],\n",
      "        [    0.0408],\n",
      "        [    0.0416],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0435],\n",
      "        [    0.0436],\n",
      "        [    0.0439],\n",
      "        [    0.0442],\n",
      "        [    0.0449],\n",
      "        [    0.0450],\n",
      "        [    0.0454],\n",
      "        [    0.0455],\n",
      "        [    0.0461],\n",
      "        [    0.0479],\n",
      "        [    0.0477],\n",
      "        [    0.0480],\n",
      "        [    0.0484],\n",
      "        [    0.0499],\n",
      "        [    0.0510],\n",
      "        [    0.0511],\n",
      "        [    0.0515],\n",
      "        [    0.0526],\n",
      "        [    0.0534],\n",
      "        [    0.0535],\n",
      "        [    0.0541],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0550],\n",
      "        [    0.0547],\n",
      "        [    0.0555],\n",
      "        [    0.0565],\n",
      "        [    0.0576],\n",
      "        [    0.0574],\n",
      "        [    0.0593],\n",
      "        [    0.0612],\n",
      "        [    0.0618],\n",
      "        [    0.0639],\n",
      "        [    0.0660],\n",
      "        [    0.0666],\n",
      "        [    0.0673],\n",
      "        [    0.0675],\n",
      "        [    0.0718],\n",
      "        [    0.0742],\n",
      "        [    0.0750],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0811],\n",
      "        [    0.0813],\n",
      "        [    0.0882],\n",
      "        [    0.0928],\n",
      "        [    0.0940],\n",
      "        [    0.0945],\n",
      "        [    0.0962],\n",
      "        [    0.0995],\n",
      "        [    0.1015],\n",
      "        [    0.1024],\n",
      "        [    0.1034],\n",
      "        [    0.1128],\n",
      "        [    0.1151],\n",
      "        [    0.1171],\n",
      "        [    0.1297],\n",
      "        [    0.1444]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 4\n",
      "Number of shrink: 9\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0057],\n",
      "        [0.0035],\n",
      "        [0.0053],\n",
      "        [0.0035],\n",
      "        [0.0023],\n",
      "        [0.0004],\n",
      "        [0.0055],\n",
      "        [0.0036],\n",
      "        [0.0015],\n",
      "        [0.0027],\n",
      "        [0.0048],\n",
      "        [0.0079],\n",
      "        [0.0091],\n",
      "        [0.0058],\n",
      "        [0.0068],\n",
      "        [0.0038],\n",
      "        [0.0059],\n",
      "        [0.0079],\n",
      "        [0.0032],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0054],\n",
      "        [0.0066],\n",
      "        [0.0129],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0066],\n",
      "        [0.0118],\n",
      "        [0.0072],\n",
      "        [0.0138],\n",
      "        [0.0148],\n",
      "        [0.0091],\n",
      "        [0.0134],\n",
      "        [0.0149],\n",
      "        [0.0122],\n",
      "        [0.0161],\n",
      "        [0.0155],\n",
      "        [0.0132],\n",
      "        [0.0199],\n",
      "        [0.0138],\n",
      "        [0.0164],\n",
      "        [0.0148],\n",
      "        [0.0180],\n",
      "        [0.0191],\n",
      "        [0.0207],\n",
      "        [0.0169],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0192],\n",
      "        [0.0221],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0193],\n",
      "        [0.0214],\n",
      "        [0.0212],\n",
      "        [0.0184],\n",
      "        [0.0221],\n",
      "        [0.0209],\n",
      "        [0.0228],\n",
      "        [0.0234],\n",
      "        [0.0281],\n",
      "        [0.0241],\n",
      "        [0.0253],\n",
      "        [0.0255],\n",
      "        [0.0287],\n",
      "        [0.0283],\n",
      "        [0.0286],\n",
      "        [0.0278],\n",
      "        [0.0302],\n",
      "        [0.0261],\n",
      "        [0.0327],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0260],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0310],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0315],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0351],\n",
      "        [0.0318],\n",
      "        [0.0375],\n",
      "        [0.0384],\n",
      "        [0.0393],\n",
      "        [0.0405],\n",
      "        [0.0426],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0366],\n",
      "        [0.0437],\n",
      "        [0.0450],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0426],\n",
      "        [0.0410],\n",
      "        [0.0460],\n",
      "        [0.0477],\n",
      "        [0.0462],\n",
      "        [0.0502],\n",
      "        [0.0512],\n",
      "        [0.0510],\n",
      "        [0.0438],\n",
      "        [0.0515],\n",
      "        [0.0526],\n",
      "        [0.0555],\n",
      "        [0.0533],\n",
      "        [0.0579],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0582],\n",
      "        [0.0477],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0543],\n",
      "        [0.0575],\n",
      "        [0.0593],\n",
      "        [0.0623],\n",
      "        [0.0587],\n",
      "        [0.0641],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0681],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0744],\n",
      "        [0.0750],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0785],\n",
      "        [0.0813],\n",
      "        [0.0858],\n",
      "        [0.0945],\n",
      "        [0.0898],\n",
      "        [0.0971],\n",
      "        [0.0962],\n",
      "        [0.0999],\n",
      "        [0.0972],\n",
      "        [0.1026],\n",
      "        [0.1034],\n",
      "        [0.1128],\n",
      "        [0.1118],\n",
      "        [0.1133],\n",
      "        [0.1253],\n",
      "        [0.1446]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 97.51526045799255\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.798498203697818e-07, 33)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [33, 39, 86, 46, 120, 6, 63, 38, 66, 34, 24, 15, 155, 48, 139, 31, 14, 2, 128, 3, 35, 112, 136, 138, 62, 61, 135, 118, 115, 41, 60, 47, 26, 11, 40, 36, 74, 67, 116, 153, 117, 32, 137, 148, 73, 42, 129, 77, 144, 78, 134, 19, 154, 113, 140, 37, 43, 20, 72, 30, 119, 18, 102, 90, 87, 16, 23, 4, 111, 7, 17, 65, 29, 101, 27, 103, 75, 1, 114, 51, 89, 64, 130, 25, 22, 142, 85, 95, 141, 133, 56, 127, 156, 132, 71, 143, 152, 5, 84, 28, 125, 149, 157, 94, 100, 57, 110, 121, 145, 45, 44, 158, 108, 10, 124, 68, 131, 126, 76, 123, 0, 122, 151, 88, 107, 12, 150, 59, 49, 109, 13, 21, 99, 92, 106, 69, 98, 70, 104, 146, 105, 50, 93, 79, 82, 52, 96, 58, 83, 55, 53, 91, 80, 97, 147, 81, 54, 9] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6282],\n",
      "        [0.5464],\n",
      "        [0.2359],\n",
      "        [0.4683],\n",
      "        [0.0989],\n",
      "        [0.5856],\n",
      "        [0.3545],\n",
      "        [0.5593],\n",
      "        [0.3673],\n",
      "        [0.6128],\n",
      "        [0.6719],\n",
      "        [0.6052],\n",
      "        [0.4321],\n",
      "        [0.4685],\n",
      "        [0.0989],\n",
      "        [0.6107],\n",
      "        [0.5581],\n",
      "        [0.6349],\n",
      "        [0.0989],\n",
      "        [0.6382],\n",
      "        [0.5985],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3488],\n",
      "        [0.3162],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5283],\n",
      "        [0.2949],\n",
      "        [0.4740],\n",
      "        [0.6563],\n",
      "        [0.5091],\n",
      "        [0.5127],\n",
      "        [0.5738],\n",
      "        [0.3291],\n",
      "        [0.4059],\n",
      "        [0.0989],\n",
      "        [0.4303],\n",
      "        [0.0989],\n",
      "        [0.6271],\n",
      "        [0.0989],\n",
      "        [0.4275],\n",
      "        [0.3734],\n",
      "        [0.5173],\n",
      "        [0.0989],\n",
      "        [0.2905],\n",
      "        [0.0989],\n",
      "        [0.2814],\n",
      "        [0.0989],\n",
      "        [0.6387],\n",
      "        [0.4196],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5817],\n",
      "        [0.5131],\n",
      "        [0.6319],\n",
      "        [0.3859],\n",
      "        [0.6290],\n",
      "        [0.0989],\n",
      "        [0.5943],\n",
      "        [0.0989],\n",
      "        [0.1650],\n",
      "        [0.2292],\n",
      "        [0.6112],\n",
      "        [0.6560],\n",
      "        [0.6104],\n",
      "        [0.0989],\n",
      "        [0.5950],\n",
      "        [0.5955],\n",
      "        [0.3299],\n",
      "        [0.6572],\n",
      "        [0.0989],\n",
      "        [0.6518],\n",
      "        [0.0989],\n",
      "        [0.2804],\n",
      "        [0.6403],\n",
      "        [0.0989],\n",
      "        [0.4089],\n",
      "        [0.1915],\n",
      "        [0.3260],\n",
      "        [0.0989],\n",
      "        [0.6394],\n",
      "        [0.6142],\n",
      "        [0.0989],\n",
      "        [0.2258],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3022],\n",
      "        [0.0989],\n",
      "        [0.4316],\n",
      "        [0.0989],\n",
      "        [0.3605],\n",
      "        [0.0989],\n",
      "        [0.4426],\n",
      "        [0.5950],\n",
      "        [0.2258],\n",
      "        [0.6759],\n",
      "        [0.0989],\n",
      "        [0.5097],\n",
      "        [0.4398],\n",
      "        [0.1184],\n",
      "        [0.0989],\n",
      "        [0.2460],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4768],\n",
      "        [0.4869],\n",
      "        [0.4527],\n",
      "        [0.0989],\n",
      "        [0.5070],\n",
      "        [0.0989],\n",
      "        [0.3998],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2719],\n",
      "        [0.0989],\n",
      "        [0.6350],\n",
      "        [0.0989],\n",
      "        [0.4430],\n",
      "        [0.2530],\n",
      "        [0.0989],\n",
      "        [0.4539],\n",
      "        [0.4375],\n",
      "        [0.2613],\n",
      "        [0.4133],\n",
      "        [0.0989],\n",
      "        [0.4642],\n",
      "        [0.6279],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.3841],\n",
      "        [0.0989],\n",
      "        [0.3621],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3860],\n",
      "        [0.0989],\n",
      "        [0.3059],\n",
      "        [0.2774],\n",
      "        [0.4209],\n",
      "        [0.1275],\n",
      "        [0.2056],\n",
      "        [0.2780],\n",
      "        [0.3649],\n",
      "        [0.3894],\n",
      "        [0.1144],\n",
      "        [0.3026],\n",
      "        [0.1128],\n",
      "        [0.4231],\n",
      "        [0.2881],\n",
      "        [0.3720],\n",
      "        [0.5905]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0015],\n",
      "        [0.0023],\n",
      "        [0.0027],\n",
      "        [0.0036],\n",
      "        [0.0032],\n",
      "        [0.0035],\n",
      "        [0.0035],\n",
      "        [0.0038],\n",
      "        [0.0055],\n",
      "        [0.0048],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0054],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0068],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0091],\n",
      "        [0.0091],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0129],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0148],\n",
      "        [0.0155],\n",
      "        [0.0161],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0191],\n",
      "        [0.0193],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0194],\n",
      "        [0.0192],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0214],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0228],\n",
      "        [0.0234],\n",
      "        [0.0241],\n",
      "        [0.0253],\n",
      "        [0.0261],\n",
      "        [0.0255],\n",
      "        [0.0260],\n",
      "        [0.0278],\n",
      "        [0.0281],\n",
      "        [0.0283],\n",
      "        [0.0287],\n",
      "        [0.0286],\n",
      "        [0.0298],\n",
      "        [0.0302],\n",
      "        [0.0306],\n",
      "        [0.0310],\n",
      "        [0.0318],\n",
      "        [0.0316],\n",
      "        [0.0315],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0326],\n",
      "        [0.0324],\n",
      "        [0.0335],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0351],\n",
      "        [0.0366],\n",
      "        [0.0375],\n",
      "        [0.0384],\n",
      "        [0.0393],\n",
      "        [0.0405],\n",
      "        [0.0410],\n",
      "        [0.0417],\n",
      "        [0.0426],\n",
      "        [0.0425],\n",
      "        [0.0426],\n",
      "        [0.0438],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0450],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0462],\n",
      "        [0.0460],\n",
      "        [0.0477],\n",
      "        [0.0477],\n",
      "        [0.0502],\n",
      "        [0.0510],\n",
      "        [0.0512],\n",
      "        [0.0515],\n",
      "        [0.0526],\n",
      "        [0.0533],\n",
      "        [0.0543],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0575],\n",
      "        [0.0582],\n",
      "        [0.0579],\n",
      "        [0.0587],\n",
      "        [0.0593],\n",
      "        [0.0623],\n",
      "        [0.0641],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0681],\n",
      "        [0.0718],\n",
      "        [0.0744],\n",
      "        [0.0750],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0785],\n",
      "        [0.0813],\n",
      "        [0.0858],\n",
      "        [0.0898],\n",
      "        [0.0945],\n",
      "        [0.0962],\n",
      "        [0.0971],\n",
      "        [0.0972],\n",
      "        [0.0999],\n",
      "        [0.1026],\n",
      "        [0.1034],\n",
      "        [0.1118],\n",
      "        [0.1128],\n",
      "        [0.1133],\n",
      "        [0.1253],\n",
      "        [0.1446],\n",
      "        [0.1668]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 17\n",
      "Number of shrink: 16\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0050],\n",
      "        [0.0064],\n",
      "        [0.0023],\n",
      "        [0.0073],\n",
      "        [0.0036],\n",
      "        [0.0102],\n",
      "        [0.0064],\n",
      "        [0.0019],\n",
      "        [0.0010],\n",
      "        [0.0006],\n",
      "        [0.0108],\n",
      "        [0.0113],\n",
      "        [0.0056],\n",
      "        [0.0097],\n",
      "        [0.0058],\n",
      "        [0.0116],\n",
      "        [0.0006],\n",
      "        [0.0136],\n",
      "        [0.0068],\n",
      "        [0.0140],\n",
      "        [0.0026],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0118],\n",
      "        [0.0070],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0118],\n",
      "        [0.0170],\n",
      "        [0.0154],\n",
      "        [0.0179],\n",
      "        [0.0079],\n",
      "        [0.0070],\n",
      "        [0.0181],\n",
      "        [0.0098],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0155],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0222],\n",
      "        [0.0180],\n",
      "        [0.0177],\n",
      "        [0.0160],\n",
      "        [0.0240],\n",
      "        [0.0191],\n",
      "        [0.0157],\n",
      "        [0.0194],\n",
      "        [0.0160],\n",
      "        [0.0198],\n",
      "        [0.0146],\n",
      "        [0.0195],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0154],\n",
      "        [0.0257],\n",
      "        [0.0157],\n",
      "        [0.0185],\n",
      "        [0.0165],\n",
      "        [0.0221],\n",
      "        [0.0283],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0253],\n",
      "        [0.0202],\n",
      "        [0.0194],\n",
      "        [0.0328],\n",
      "        [0.0278],\n",
      "        [0.0217],\n",
      "        [0.0340],\n",
      "        [0.0315],\n",
      "        [0.0226],\n",
      "        [0.0298],\n",
      "        [0.0247],\n",
      "        [0.0306],\n",
      "        [0.0339],\n",
      "        [0.0381],\n",
      "        [0.0316],\n",
      "        [0.0356],\n",
      "        [0.0324],\n",
      "        [0.0355],\n",
      "        [0.0326],\n",
      "        [0.0382],\n",
      "        [0.0275],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0345],\n",
      "        [0.0315],\n",
      "        [0.0351],\n",
      "        [0.0361],\n",
      "        [0.0375],\n",
      "        [0.0415],\n",
      "        [0.0393],\n",
      "        [0.0407],\n",
      "        [0.0478],\n",
      "        [0.0417],\n",
      "        [0.0370],\n",
      "        [0.0425],\n",
      "        [0.0411],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0480],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0507],\n",
      "        [0.0506],\n",
      "        [0.0473],\n",
      "        [0.0477],\n",
      "        [0.0437],\n",
      "        [0.0510],\n",
      "        [0.0543],\n",
      "        [0.0515],\n",
      "        [0.0526],\n",
      "        [0.0565],\n",
      "        [0.0543],\n",
      "        [0.0611],\n",
      "        [0.0549],\n",
      "        [0.0560],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0636],\n",
      "        [0.0586],\n",
      "        [0.0607],\n",
      "        [0.0625],\n",
      "        [0.0593],\n",
      "        [0.0680],\n",
      "        [0.0584],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0715],\n",
      "        [0.0718],\n",
      "        [0.0778],\n",
      "        [0.0750],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0823],\n",
      "        [0.0813],\n",
      "        [0.0821],\n",
      "        [0.0862],\n",
      "        [0.0909],\n",
      "        [0.0962],\n",
      "        [0.0998],\n",
      "        [0.0937],\n",
      "        [0.0961],\n",
      "        [0.0989],\n",
      "        [0.1034],\n",
      "        [0.1082],\n",
      "        [0.1128],\n",
      "        [0.1125],\n",
      "        [0.1216],\n",
      "        [0.1408],\n",
      "        [0.1598]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 97.70183181762695\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.9467985263618175e-09, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 14, 66, 38, 35, 86, 120, 33, 155, 139, 63, 39, 61, 128, 11, 46, 26, 112, 136, 138, 135, 48, 36, 118, 6, 24, 15, 31, 115, 62, 74, 67, 2, 3, 19, 60, 116, 37, 20, 153, 77, 73, 78, 117, 30, 41, 148, 47, 137, 72, 40, 129, 144, 23, 154, 134, 113, 16, 140, 7, 119, 32, 29, 102, 42, 90, 27, 87, 43, 111, 22, 18, 101, 103, 65, 114, 56, 4, 89, 130, 142, 75, 85, 95, 141, 17, 133, 127, 51, 64, 156, 28, 132, 25, 1, 143, 152, 149, 71, 84, 125, 157, 94, 10, 100, 110, 121, 145, 108, 158, 5, 57, 45, 44, 124, 131, 126, 123, 68, 122, 88, 151, 76, 107, 21, 150, 109, 59, 0, 49, 12, 99, 92, 106, 13, 69, 98, 104, 146, 70, 105, 93, 79, 50, 82, 52, 83, 96, 55, 53, 58, 91, 80, 147, 97, 81, 54, 9, 8] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6078],\n",
      "        [0.5520],\n",
      "        [0.3644],\n",
      "        [0.5539],\n",
      "        [0.5933],\n",
      "        [0.2359],\n",
      "        [0.0989],\n",
      "        [0.6228],\n",
      "        [0.4319],\n",
      "        [0.0989],\n",
      "        [0.3516],\n",
      "        [0.5415],\n",
      "        [0.3141],\n",
      "        [0.0989],\n",
      "        [0.5023],\n",
      "        [0.4637],\n",
      "        [0.6508],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4643],\n",
      "        [0.5687],\n",
      "        [0.0989],\n",
      "        [0.5786],\n",
      "        [0.6659],\n",
      "        [0.5992],\n",
      "        [0.6049],\n",
      "        [0.0989],\n",
      "        [0.3461],\n",
      "        [0.3260],\n",
      "        [0.4029],\n",
      "        [0.6279],\n",
      "        [0.6314],\n",
      "        [0.6333],\n",
      "        [0.2923],\n",
      "        [0.0989],\n",
      "        [0.5764],\n",
      "        [0.6265],\n",
      "        [0.4300],\n",
      "        [0.2867],\n",
      "        [0.3703],\n",
      "        [0.2782],\n",
      "        [0.0989],\n",
      "        [0.6234],\n",
      "        [0.5236],\n",
      "        [0.4268],\n",
      "        [0.4694],\n",
      "        [0.0989],\n",
      "        [0.3830],\n",
      "        [0.5083],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6499],\n",
      "        [0.4192],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6053],\n",
      "        [0.0989],\n",
      "        [0.5886],\n",
      "        [0.0989],\n",
      "        [0.6217],\n",
      "        [0.6513],\n",
      "        [0.0989],\n",
      "        [0.5127],\n",
      "        [0.1650],\n",
      "        [0.6463],\n",
      "        [0.2292],\n",
      "        [0.5083],\n",
      "        [0.0989],\n",
      "        [0.6082],\n",
      "        [0.5888],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3271],\n",
      "        [0.0989],\n",
      "        [0.2990],\n",
      "        [0.6036],\n",
      "        [0.1915],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2776],\n",
      "        [0.2258],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.5899],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4049],\n",
      "        [0.3232],\n",
      "        [0.4321],\n",
      "        [0.6704],\n",
      "        [0.0989],\n",
      "        [0.6335],\n",
      "        [0.6339],\n",
      "        [0.0989],\n",
      "        [0.4424],\n",
      "        [0.5083],\n",
      "        [0.3574],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.4398],\n",
      "        [0.1184],\n",
      "        [0.5006],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4530],\n",
      "        [0.5882],\n",
      "        [0.2429],\n",
      "        [0.4723],\n",
      "        [0.4823],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3967],\n",
      "        [0.0989],\n",
      "        [0.2530],\n",
      "        [0.4425],\n",
      "        [0.2688],\n",
      "        [0.0989],\n",
      "        [0.6222],\n",
      "        [0.4372],\n",
      "        [0.0989],\n",
      "        [0.2586],\n",
      "        [0.6282],\n",
      "        [0.4094],\n",
      "        [0.4477],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4585],\n",
      "        [0.3807],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3588],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3022],\n",
      "        [0.3823],\n",
      "        [0.2738],\n",
      "        [0.4173],\n",
      "        [0.2745],\n",
      "        [0.1275],\n",
      "        [0.3611],\n",
      "        [0.3857],\n",
      "        [0.2029],\n",
      "        [0.1144],\n",
      "        [0.2991],\n",
      "        [0.4222],\n",
      "        [0.1128],\n",
      "        [0.2843],\n",
      "        [0.3682],\n",
      "        [0.5835],\n",
      "        [0.5891]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0006],\n",
      "        [0.0010],\n",
      "        [0.0019],\n",
      "        [0.0026],\n",
      "        [0.0023],\n",
      "        [0.0036],\n",
      "        [0.0050],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0064],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0146],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0154],\n",
      "        [0.0157],\n",
      "        [0.0159],\n",
      "        [0.0157],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0177],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0185],\n",
      "        [0.0181],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0195],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0226],\n",
      "        [0.0234],\n",
      "        [0.0240],\n",
      "        [0.0242],\n",
      "        [0.0247],\n",
      "        [0.0253],\n",
      "        [0.0257],\n",
      "        [0.0278],\n",
      "        [0.0275],\n",
      "        [0.0283],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0315],\n",
      "        [0.0316],\n",
      "        [0.0315],\n",
      "        [0.0328],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0339],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0340],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0356],\n",
      "        [0.0355],\n",
      "        [0.0361],\n",
      "        [0.0370],\n",
      "        [0.0375],\n",
      "        [0.0382],\n",
      "        [0.0381],\n",
      "        [0.0393],\n",
      "        [0.0407],\n",
      "        [0.0411],\n",
      "        [0.0415],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0477],\n",
      "        [0.0473],\n",
      "        [0.0478],\n",
      "        [0.0480],\n",
      "        [0.0507],\n",
      "        [0.0506],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0526],\n",
      "        [0.0543],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0560],\n",
      "        [0.0565],\n",
      "        [0.0565],\n",
      "        [0.0584],\n",
      "        [0.0586],\n",
      "        [0.0593],\n",
      "        [0.0607],\n",
      "        [0.0611],\n",
      "        [0.0625],\n",
      "        [0.0636],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0680],\n",
      "        [0.0715],\n",
      "        [0.0718],\n",
      "        [0.0750],\n",
      "        [0.0767],\n",
      "        [0.0778],\n",
      "        [0.0780],\n",
      "        [0.0813],\n",
      "        [0.0821],\n",
      "        [0.0823],\n",
      "        [0.0862],\n",
      "        [0.0909],\n",
      "        [0.0937],\n",
      "        [0.0962],\n",
      "        [0.0961],\n",
      "        [0.0989],\n",
      "        [0.0998],\n",
      "        [0.1034],\n",
      "        [0.1082],\n",
      "        [0.1125],\n",
      "        [0.1128],\n",
      "        [0.1216],\n",
      "        [0.1408],\n",
      "        [0.1598],\n",
      "        [0.1686]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 1\n",
      "Number of shrink: 7\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0020],\n",
      "        [0.0017],\n",
      "        [0.0005],\n",
      "        [0.0041],\n",
      "        [0.0005],\n",
      "        [0.0023],\n",
      "        [0.0036],\n",
      "        [0.0069],\n",
      "        [0.0041],\n",
      "        [0.0058],\n",
      "        [0.0077],\n",
      "        [0.0089],\n",
      "        [0.0056],\n",
      "        [0.0068],\n",
      "        [0.0046],\n",
      "        [0.0093],\n",
      "        [0.0053],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0097],\n",
      "        [0.0115],\n",
      "        [0.0077],\n",
      "        [0.0100],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0137],\n",
      "        [0.0138],\n",
      "        [0.0118],\n",
      "        [0.0130],\n",
      "        [0.0103],\n",
      "        [0.0103],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0123],\n",
      "        [0.0169],\n",
      "        [0.0155],\n",
      "        [0.0134],\n",
      "        [0.0129],\n",
      "        [0.0142],\n",
      "        [0.0142],\n",
      "        [0.0147],\n",
      "        [0.0143],\n",
      "        [0.0164],\n",
      "        [0.0139],\n",
      "        [0.0193],\n",
      "        [0.0163],\n",
      "        [0.0197],\n",
      "        [0.0180],\n",
      "        [0.0169],\n",
      "        [0.0203],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0169],\n",
      "        [0.0181],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0179],\n",
      "        [0.0207],\n",
      "        [0.0188],\n",
      "        [0.0221],\n",
      "        [0.0244],\n",
      "        [0.0207],\n",
      "        [0.0234],\n",
      "        [0.0263],\n",
      "        [0.0242],\n",
      "        [0.0227],\n",
      "        [0.0253],\n",
      "        [0.0279],\n",
      "        [0.0278],\n",
      "        [0.0251],\n",
      "        [0.0306],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0328],\n",
      "        [0.0316],\n",
      "        [0.0298],\n",
      "        [0.0354],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0355],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0363],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0372],\n",
      "        [0.0369],\n",
      "        [0.0377],\n",
      "        [0.0349],\n",
      "        [0.0375],\n",
      "        [0.0406],\n",
      "        [0.0409],\n",
      "        [0.0393],\n",
      "        [0.0420],\n",
      "        [0.0409],\n",
      "        [0.0429],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0454],\n",
      "        [0.0437],\n",
      "        [0.0414],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0477],\n",
      "        [0.0489],\n",
      "        [0.0504],\n",
      "        [0.0495],\n",
      "        [0.0527],\n",
      "        [0.0526],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0526],\n",
      "        [0.0543],\n",
      "        [0.0558],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0573],\n",
      "        [0.0581],\n",
      "        [0.0565],\n",
      "        [0.0561],\n",
      "        [0.0601],\n",
      "        [0.0593],\n",
      "        [0.0621],\n",
      "        [0.0634],\n",
      "        [0.0643],\n",
      "        [0.0660],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0703],\n",
      "        [0.0728],\n",
      "        [0.0718],\n",
      "        [0.0750],\n",
      "        [0.0767],\n",
      "        [0.0792],\n",
      "        [0.0780],\n",
      "        [0.0813],\n",
      "        [0.0807],\n",
      "        [0.0843],\n",
      "        [0.0848],\n",
      "        [0.0892],\n",
      "        [0.0924],\n",
      "        [0.0962],\n",
      "        [0.0947],\n",
      "        [0.0971],\n",
      "        [0.1013],\n",
      "        [0.1034],\n",
      "        [0.1066],\n",
      "        [0.1111],\n",
      "        [0.1128],\n",
      "        [0.1201],\n",
      "        [0.1391],\n",
      "        [0.1571],\n",
      "        [0.1661]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 97.80924391746521\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 28 個區塊累積花費時間(s) 0.7786896228790283\n",
      "<<The performance of 28 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.7786896228790283\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1010.61\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1295.57\n",
      "The accuracy(2000) for l = 1: 88.05%\n",
      "The accuracy(3000) for l = 1: 97.48%\n",
      "The maximum error: tensor(4241.5508)\n",
      "The minimum error: tensor(11.7969)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 850.0\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1071.1\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1919.7109375\n",
      "The minimum error: 283.80859375\n",
      "------------------------------------------------------------\n",
      "0.8805031446540881\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<29>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.8978772093068983e-07, 62)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [62, 31, 10, 30, 82, 116, 34, 151, 7, 22, 57, 135, 124, 29, 59, 108, 32, 132, 134, 35, 42, 131, 114, 70, 63, 156, 44, 111, 15, 2, 16, 58, 33, 11, 20, 26, 27, 149, 155, 73, 74, 69, 112, 144, 113, 56, 68, 19, 133, 150, 12, 37, 3, 125, 140, 43, 130, 36, 109, 25, 136, 115, 23, 98, 86, 28, 18, 83, 38, 107, 39, 97, 52, 99, 14, 110, 85, 158, 126, 61, 138, 81, 91, 137, 24, 129, 123, 71, 0, 13, 60, 47, 128, 152, 139, 21, 145, 6, 80, 148, 121, 67, 90, 96, 106, 117, 153, 141, 104, 154, 53, 1, 120, 127, 41, 122, 40, 119, 118, 84, 64, 17, 103, 147, 72, 105, 146, 55, 45, 8, 95, 88, 102, 9, 94, 65, 100, 157, 142, 101, 66, 75, 89, 46, 78, 48, 79, 51, 92, 49, 54, 87, 76, 143, 93]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0023],\n",
      "        [0.0036],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0046],\n",
      "        [0.0053],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0089],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0103],\n",
      "        [0.0111],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0134],\n",
      "        [0.0137],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0138],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0155],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0179],\n",
      "        [0.0193],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0251],\n",
      "        [0.0253],\n",
      "        [0.0263],\n",
      "        [0.0278],\n",
      "        [0.0279],\n",
      "        [0.0298],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0328],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0349],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0355],\n",
      "        [0.0354],\n",
      "        [0.0363],\n",
      "        [0.0369],\n",
      "        [0.0372],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0393],\n",
      "        [0.0406],\n",
      "        [0.0409],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0454],\n",
      "        [0.0477],\n",
      "        [0.0489],\n",
      "        [0.0495],\n",
      "        [0.0504],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0527],\n",
      "        [0.0526],\n",
      "        [0.0526],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0558],\n",
      "        [0.0561],\n",
      "        [0.0565],\n",
      "        [0.0573],\n",
      "        [0.0581],\n",
      "        [0.0593],\n",
      "        [0.0601],\n",
      "        [0.0621],\n",
      "        [0.0643],\n",
      "        [0.0660],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0703],\n",
      "        [0.0718],\n",
      "        [0.0728],\n",
      "        [0.0750],\n",
      "        [0.0752],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0792],\n",
      "        [0.0807],\n",
      "        [0.0813],\n",
      "        [0.0843],\n",
      "        [0.0848],\n",
      "        [0.0892],\n",
      "        [0.0924],\n",
      "        [0.0947],\n",
      "        [0.0962],\n",
      "        [0.0971],\n",
      "        [0.1013],\n",
      "        [0.1034],\n",
      "        [0.1066],\n",
      "        [0.1111],\n",
      "        [0.1128]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.8978772093068983e-07, 62)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [62, 31, 10, 30, 82, 116, 34, 151, 7, 22, 57, 135, 124, 29, 59, 108, 32, 132, 134, 35, 42, 131, 114, 70, 63, 156, 44, 111, 15, 2, 16, 58, 33, 11, 20, 26, 27, 149, 155, 73, 74, 69, 112, 144, 113, 56, 68, 19, 133, 150, 12, 37, 3, 125, 140, 43, 130, 36, 109, 25, 136, 115, 23, 98, 86, 28, 18, 83, 38, 107, 39, 97, 52, 99, 14, 110, 85, 158, 126, 61, 138, 81, 91, 137, 24, 129, 123, 71, 0, 13, 60, 47, 128, 152, 139, 21, 145, 6, 80, 148, 121, 67, 90, 96, 106, 117, 153, 141, 104, 154, 53, 1, 120, 127, 41, 122, 40, 119, 118, 84, 64, 17, 103, 147, 72, 105, 146, 55, 45, 8, 95, 88, 102, 9, 94, 65, 100, 157, 142, 101, 66, 75, 89, 46, 78, 48, 79, 51, 92, 49, 54, 87, 76, 143, 93, 77] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3630],\n",
      "        [0.5912],\n",
      "        [0.5497],\n",
      "        [0.6052],\n",
      "        [0.2359],\n",
      "        [0.0989],\n",
      "        [0.5518],\n",
      "        [0.4305],\n",
      "        [0.5000],\n",
      "        [0.6483],\n",
      "        [0.3127],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6209],\n",
      "        [0.3504],\n",
      "        [0.0989],\n",
      "        [0.5667],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5391],\n",
      "        [0.4616],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3245],\n",
      "        [0.4014],\n",
      "        [0.4986],\n",
      "        [0.4625],\n",
      "        [0.0989],\n",
      "        [0.6310],\n",
      "        [0.5762],\n",
      "        [0.6237],\n",
      "        [0.3449],\n",
      "        [0.5744],\n",
      "        [0.5968],\n",
      "        [0.6635],\n",
      "        [0.6209],\n",
      "        [0.6027],\n",
      "        [0.4283],\n",
      "        [0.4772],\n",
      "        [0.2852],\n",
      "        [0.2765],\n",
      "        [0.3690],\n",
      "        [0.0989],\n",
      "        [0.4254],\n",
      "        [0.0989],\n",
      "        [0.2909],\n",
      "        [0.3813],\n",
      "        [0.6475],\n",
      "        [0.0989],\n",
      "        [0.4177],\n",
      "        [0.6030],\n",
      "        [0.5213],\n",
      "        [0.5857],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4675],\n",
      "        [0.0989],\n",
      "        [0.5061],\n",
      "        [0.0989],\n",
      "        [0.6493],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6442],\n",
      "        [0.0989],\n",
      "        [0.1650],\n",
      "        [0.6195],\n",
      "        [0.6058],\n",
      "        [0.2292],\n",
      "        [0.5103],\n",
      "        [0.0989],\n",
      "        [0.5061],\n",
      "        [0.0989],\n",
      "        [0.2973],\n",
      "        [0.0989],\n",
      "        [0.5865],\n",
      "        [0.0989],\n",
      "        [0.1915],\n",
      "        [0.5087],\n",
      "        [0.0989],\n",
      "        [0.3257],\n",
      "        [0.0989],\n",
      "        [0.2258],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.6682],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2760],\n",
      "        [0.6010],\n",
      "        [0.5876],\n",
      "        [0.3218],\n",
      "        [0.4032],\n",
      "        [0.0989],\n",
      "        [0.4305],\n",
      "        [0.0989],\n",
      "        [0.6311],\n",
      "        [0.5080],\n",
      "        [0.4982],\n",
      "        [0.2258],\n",
      "        [0.4411],\n",
      "        [0.0989],\n",
      "        [0.3561],\n",
      "        [0.1184],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4382],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4514],\n",
      "        [0.2415],\n",
      "        [0.5856],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4704],\n",
      "        [0.0989],\n",
      "        [0.4803],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2530],\n",
      "        [0.3953],\n",
      "        [0.6198],\n",
      "        [0.0989],\n",
      "        [0.4412],\n",
      "        [0.2671],\n",
      "        [0.0989],\n",
      "        [0.4357],\n",
      "        [0.2572],\n",
      "        [0.4076],\n",
      "        [0.4454],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4562],\n",
      "        [0.0989],\n",
      "        [0.3794],\n",
      "        [0.0989],\n",
      "        [0.4944],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3573],\n",
      "        [0.3008],\n",
      "        [0.0989],\n",
      "        [0.3803],\n",
      "        [0.2724],\n",
      "        [0.4156],\n",
      "        [0.2732],\n",
      "        [0.3596],\n",
      "        [0.1275],\n",
      "        [0.3839],\n",
      "        [0.2014],\n",
      "        [0.1144],\n",
      "        [0.2974],\n",
      "        [0.4208],\n",
      "        [0.1128],\n",
      "        [0.2829]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0023],\n",
      "        [0.0036],\n",
      "        [0.0041],\n",
      "        [0.0041],\n",
      "        [0.0046],\n",
      "        [0.0053],\n",
      "        [0.0056],\n",
      "        [0.0058],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0077],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0089],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0103],\n",
      "        [0.0111],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0134],\n",
      "        [0.0137],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0138],\n",
      "        [0.0142],\n",
      "        [0.0144],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0147],\n",
      "        [0.0155],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0181],\n",
      "        [0.0179],\n",
      "        [0.0193],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0227],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0244],\n",
      "        [0.0251],\n",
      "        [0.0253],\n",
      "        [0.0263],\n",
      "        [0.0278],\n",
      "        [0.0279],\n",
      "        [0.0298],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0328],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0342],\n",
      "        [0.0349],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0355],\n",
      "        [0.0354],\n",
      "        [0.0363],\n",
      "        [0.0369],\n",
      "        [0.0372],\n",
      "        [0.0375],\n",
      "        [0.0377],\n",
      "        [0.0393],\n",
      "        [0.0406],\n",
      "        [0.0409],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0454],\n",
      "        [0.0477],\n",
      "        [0.0489],\n",
      "        [0.0495],\n",
      "        [0.0504],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0527],\n",
      "        [0.0526],\n",
      "        [0.0526],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0558],\n",
      "        [0.0561],\n",
      "        [0.0565],\n",
      "        [0.0573],\n",
      "        [0.0581],\n",
      "        [0.0593],\n",
      "        [0.0601],\n",
      "        [0.0621],\n",
      "        [0.0643],\n",
      "        [0.0660],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0703],\n",
      "        [0.0718],\n",
      "        [0.0728],\n",
      "        [0.0750],\n",
      "        [0.0752],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0792],\n",
      "        [0.0807],\n",
      "        [0.0813],\n",
      "        [0.0843],\n",
      "        [0.0848],\n",
      "        [0.0892],\n",
      "        [0.0924],\n",
      "        [0.0947],\n",
      "        [0.0962],\n",
      "        [0.0971],\n",
      "        [0.1013],\n",
      "        [0.1034],\n",
      "        [0.1066],\n",
      "        [0.1111],\n",
      "        [0.1128],\n",
      "        [0.1201]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 24\n",
      "Number of shrink: 19\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0034],\n",
      "        [0.0063],\n",
      "        [0.0008],\n",
      "        [0.0039],\n",
      "        [0.0023],\n",
      "        [0.0036],\n",
      "        [0.0017],\n",
      "        [0.0154],\n",
      "        [0.0071],\n",
      "        [0.0098],\n",
      "        [0.0097],\n",
      "        [0.0058],\n",
      "        [0.0068],\n",
      "        [0.0017],\n",
      "        [0.0037],\n",
      "        [0.0079],\n",
      "        [0.0132],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0026],\n",
      "        [0.0040],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0151],\n",
      "        [0.0141],\n",
      "        [0.0230],\n",
      "        [0.0057],\n",
      "        [0.0118],\n",
      "        [0.0148],\n",
      "        [0.0092],\n",
      "        [0.0163],\n",
      "        [0.0090],\n",
      "        [0.0187],\n",
      "        [0.0125],\n",
      "        [0.0094],\n",
      "        [0.0185],\n",
      "        [0.0088],\n",
      "        [0.0225],\n",
      "        [0.0016],\n",
      "        [0.0183],\n",
      "        [0.0182],\n",
      "        [0.0187],\n",
      "        [0.0155],\n",
      "        [0.0222],\n",
      "        [0.0164],\n",
      "        [0.0123],\n",
      "        [0.0213],\n",
      "        [0.0200],\n",
      "        [0.0180],\n",
      "        [0.0277],\n",
      "        [0.0194],\n",
      "        [0.0137],\n",
      "        [0.0222],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0143],\n",
      "        [0.0198],\n",
      "        [0.0146],\n",
      "        [0.0203],\n",
      "        [0.0249],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0269],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0195],\n",
      "        [0.0283],\n",
      "        [0.0254],\n",
      "        [0.0207],\n",
      "        [0.0278],\n",
      "        [0.0231],\n",
      "        [0.0298],\n",
      "        [0.0358],\n",
      "        [0.0306],\n",
      "        [0.0281],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0207],\n",
      "        [0.0326],\n",
      "        [0.0289],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0387],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0298],\n",
      "        [0.0320],\n",
      "        [0.0342],\n",
      "        [0.0329],\n",
      "        [0.0315],\n",
      "        [0.0375],\n",
      "        [0.0253],\n",
      "        [0.0393],\n",
      "        [0.0368],\n",
      "        [0.0396],\n",
      "        [0.0438],\n",
      "        [0.0417],\n",
      "        [0.0348],\n",
      "        [0.0425],\n",
      "        [0.0387],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0326],\n",
      "        [0.0455],\n",
      "        [0.0477],\n",
      "        [0.0365],\n",
      "        [0.0438],\n",
      "        [0.0471],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0471],\n",
      "        [0.0525],\n",
      "        [0.0476],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0520],\n",
      "        [0.0594],\n",
      "        [0.0565],\n",
      "        [0.0510],\n",
      "        [0.0533],\n",
      "        [0.0593],\n",
      "        [0.0536],\n",
      "        [0.0576],\n",
      "        [0.0585],\n",
      "        [0.0635],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0691],\n",
      "        [0.0750],\n",
      "        [0.0635],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0751],\n",
      "        [0.0842],\n",
      "        [0.0813],\n",
      "        [0.0781],\n",
      "        [0.0870],\n",
      "        [0.0951],\n",
      "        [0.0949],\n",
      "        [0.1005],\n",
      "        [0.0962],\n",
      "        [0.1029],\n",
      "        [0.0959],\n",
      "        [0.1034],\n",
      "        [0.1099],\n",
      "        [0.1169],\n",
      "        [0.1128],\n",
      "        [0.1225]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 98.22702431678772\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.108660505153239e-07, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 29, 155, 34, 82, 35, 62, 59, 116, 30, 42, 44, 135, 31, 124, 7, 108, 132, 134, 58, 27, 2, 20, 22, 57, 131, 114, 111, 11, 56, 32, 37, 63, 36, 43, 151, 70, 15, 112, 113, 16, 133, 74, 26, 73, 69, 33, 125, 28, 140, 12, 130, 19, 109, 136, 158, 38, 68, 115, 3, 144, 149, 156, 39, 98, 86, 25, 152, 83, 23, 14, 107, 150, 18, 61, 71, 97, 99, 47, 110, 0, 85, 126, 153, 60, 138, 81, 91, 137, 129, 13, 148, 123, 52, 154, 21, 128, 24, 67, 139, 145, 80, 121, 90, 53, 6, 96, 106, 117, 141, 1, 41, 104, 40, 147, 120, 127, 64, 122, 72, 146, 119, 118, 84, 103, 55, 45, 105, 17, 157, 8, 95, 88, 102, 9, 65, 94, 100, 66, 142, 101, 46, 89, 75, 78, 79, 48, 54, 92, 51, 49, 87, 76, 93, 143, 77, 50] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5523],\n",
      "        [0.6260],\n",
      "        [0.4901],\n",
      "        [0.5576],\n",
      "        [0.2359],\n",
      "        [0.5454],\n",
      "        [0.3668],\n",
      "        [0.3543],\n",
      "        [0.0989],\n",
      "        [0.6112],\n",
      "        [0.4669],\n",
      "        [0.4683],\n",
      "        [0.0989],\n",
      "        [0.5970],\n",
      "        [0.0989],\n",
      "        [0.5024],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3489],\n",
      "        [0.6077],\n",
      "        [0.5797],\n",
      "        [0.6674],\n",
      "        [0.6528],\n",
      "        [0.3168],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5980],\n",
      "        [0.2955],\n",
      "        [0.5722],\n",
      "        [0.5269],\n",
      "        [0.4053],\n",
      "        [0.5118],\n",
      "        [0.4729],\n",
      "        [0.4418],\n",
      "        [0.3293],\n",
      "        [0.6336],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6271],\n",
      "        [0.0989],\n",
      "        [0.2804],\n",
      "        [0.6254],\n",
      "        [0.2893],\n",
      "        [0.3730],\n",
      "        [0.5797],\n",
      "        [0.0989],\n",
      "        [0.6245],\n",
      "        [0.0989],\n",
      "        [0.6046],\n",
      "        [0.0989],\n",
      "        [0.6506],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5205],\n",
      "        [0.5159],\n",
      "        [0.3857],\n",
      "        [0.0989],\n",
      "        [0.5891],\n",
      "        [0.4313],\n",
      "        [0.4367],\n",
      "        [0.5105],\n",
      "        [0.5109],\n",
      "        [0.0989],\n",
      "        [0.1650],\n",
      "        [0.6535],\n",
      "        [0.4429],\n",
      "        [0.2292],\n",
      "        [0.6485],\n",
      "        [0.5889],\n",
      "        [0.0989],\n",
      "        [0.4274],\n",
      "        [0.6090],\n",
      "        [0.3296],\n",
      "        [0.2817],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4089],\n",
      "        [0.0989],\n",
      "        [0.6044],\n",
      "        [0.1915],\n",
      "        [0.0989],\n",
      "        [0.4510],\n",
      "        [0.3258],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5897],\n",
      "        [0.4483],\n",
      "        [0.0989],\n",
      "        [0.3032],\n",
      "        [0.4639],\n",
      "        [0.6349],\n",
      "        [0.0989],\n",
      "        [0.6720],\n",
      "        [0.3603],\n",
      "        [0.0989],\n",
      "        [0.5067],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.1184],\n",
      "        [0.2472],\n",
      "        [0.5007],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5889],\n",
      "        [0.4759],\n",
      "        [0.0989],\n",
      "        [0.4854],\n",
      "        [0.4475],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3990],\n",
      "        [0.0989],\n",
      "        [0.2719],\n",
      "        [0.4421],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2530],\n",
      "        [0.0989],\n",
      "        [0.2617],\n",
      "        [0.4135],\n",
      "        [0.0989],\n",
      "        [0.6232],\n",
      "        [0.5061],\n",
      "        [0.4478],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4590],\n",
      "        [0.3831],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3614],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3864],\n",
      "        [0.0989],\n",
      "        [0.3043],\n",
      "        [0.2747],\n",
      "        [0.2757],\n",
      "        [0.4214],\n",
      "        [0.2067],\n",
      "        [0.1275],\n",
      "        [0.3654],\n",
      "        [0.3896],\n",
      "        [0.1144],\n",
      "        [0.3008],\n",
      "        [0.1128],\n",
      "        [0.4266],\n",
      "        [0.2853],\n",
      "        [0.3724]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0017],\n",
      "        [0.0016],\n",
      "        [0.0017],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0040],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.0071],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0090],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0123],\n",
      "        [0.0132],\n",
      "        [0.0137],\n",
      "        [0.0141],\n",
      "        [0.0146],\n",
      "        [0.0143],\n",
      "        [0.0154],\n",
      "        [0.0151],\n",
      "        [0.0148],\n",
      "        [0.0155],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0185],\n",
      "        [0.0183],\n",
      "        [0.0187],\n",
      "        [0.0187],\n",
      "        [0.0191],\n",
      "        [0.0195],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0222],\n",
      "        [0.0225],\n",
      "        [0.0230],\n",
      "        [0.0231],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0253],\n",
      "        [0.0254],\n",
      "        [0.0269],\n",
      "        [0.0281],\n",
      "        [0.0278],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0289],\n",
      "        [0.0298],\n",
      "        [0.0298],\n",
      "        [0.0306],\n",
      "        [0.0315],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0329],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0342],\n",
      "        [0.0348],\n",
      "        [0.0351],\n",
      "        [0.0358],\n",
      "        [0.0365],\n",
      "        [0.0368],\n",
      "        [0.0375],\n",
      "        [0.0387],\n",
      "        [0.0387],\n",
      "        [0.0393],\n",
      "        [0.0396],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0438],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0471],\n",
      "        [0.0471],\n",
      "        [0.0477],\n",
      "        [0.0476],\n",
      "        [0.0510],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0520],\n",
      "        [0.0525],\n",
      "        [0.0533],\n",
      "        [0.0536],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0576],\n",
      "        [0.0585],\n",
      "        [0.0593],\n",
      "        [0.0594],\n",
      "        [0.0635],\n",
      "        [0.0635],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0675],\n",
      "        [0.0691],\n",
      "        [0.0718],\n",
      "        [0.0750],\n",
      "        [0.0751],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0781],\n",
      "        [0.0813],\n",
      "        [0.0842],\n",
      "        [0.0870],\n",
      "        [0.0949],\n",
      "        [0.0951],\n",
      "        [0.0959],\n",
      "        [0.0962],\n",
      "        [0.1005],\n",
      "        [0.1029],\n",
      "        [0.1034],\n",
      "        [0.1099],\n",
      "        [0.1128],\n",
      "        [0.1169],\n",
      "        [0.1225],\n",
      "        [0.1450]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 3\n",
      "Number of shrink: 8\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0019],\n",
      "        [    0.0034],\n",
      "        [    0.0012],\n",
      "        [    0.0001],\n",
      "        [    0.0023],\n",
      "        [    0.0045],\n",
      "        [    0.0003],\n",
      "        [    0.0067],\n",
      "        [    0.0036],\n",
      "        [    0.0022],\n",
      "        [    0.0054],\n",
      "        [    0.0068],\n",
      "        [    0.0058],\n",
      "        [    0.0052],\n",
      "        [    0.0068],\n",
      "        [    0.0047],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0118],\n",
      "        [    0.0106],\n",
      "        [    0.0117],\n",
      "        [    0.0123],\n",
      "        [    0.0075],\n",
      "        [    0.0070],\n",
      "        [    0.0097],\n",
      "        [    0.0100],\n",
      "        [    0.0118],\n",
      "        [    0.0159],\n",
      "        [    0.0149],\n",
      "        [    0.0115],\n",
      "        [    0.0150],\n",
      "        [    0.0112],\n",
      "        [    0.0161],\n",
      "        [    0.0156],\n",
      "        [    0.0151],\n",
      "        [    0.0127],\n",
      "        [    0.0125],\n",
      "        [    0.0155],\n",
      "        [    0.0164],\n",
      "        [    0.0132],\n",
      "        [    0.0180],\n",
      "        [    0.0157],\n",
      "        [    0.0169],\n",
      "        [    0.0160],\n",
      "        [    0.0159],\n",
      "        [    0.0169],\n",
      "        [    0.0191],\n",
      "        [    0.0210],\n",
      "        [    0.0194],\n",
      "        [    0.0159],\n",
      "        [    0.0198],\n",
      "        [    0.0174],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0205],\n",
      "        [    0.0225],\n",
      "        [    0.0183],\n",
      "        [    0.0221],\n",
      "        [    0.0203],\n",
      "        [    0.0198],\n",
      "        [    0.0205],\n",
      "        [    0.0229],\n",
      "        [    0.0247],\n",
      "        [    0.0234],\n",
      "        [    0.0242],\n",
      "        [    0.0218],\n",
      "        [    0.0250],\n",
      "        [    0.0254],\n",
      "        [    0.0249],\n",
      "        [    0.0305],\n",
      "        [    0.0278],\n",
      "        [    0.0268],\n",
      "        [    0.0262],\n",
      "        [    0.0318],\n",
      "        [    0.0320],\n",
      "        [    0.0297],\n",
      "        [    0.0306],\n",
      "        [    0.0326],\n",
      "        [    0.0316],\n",
      "        [    0.0345],\n",
      "        [    0.0324],\n",
      "        [    0.0326],\n",
      "        [    0.0322],\n",
      "        [    0.0359],\n",
      "        [    0.0337],\n",
      "        [    0.0340],\n",
      "        [    0.0342],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0376],\n",
      "        [    0.0369],\n",
      "        [    0.0351],\n",
      "        [    0.0343],\n",
      "        [    0.0355],\n",
      "        [    0.0390],\n",
      "        [    0.0375],\n",
      "        [    0.0363],\n",
      "        [    0.0413],\n",
      "        [    0.0393],\n",
      "        [    0.0387],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0437],\n",
      "        [    0.0452],\n",
      "        [    0.0415],\n",
      "        [    0.0442],\n",
      "        [    0.0449],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0491],\n",
      "        [    0.0484],\n",
      "        [    0.0477],\n",
      "        [    0.0490],\n",
      "        [    0.0534],\n",
      "        [    0.0510],\n",
      "        [    0.0515],\n",
      "        [    0.0551],\n",
      "        [    0.0525],\n",
      "        [    0.0555],\n",
      "        [    0.0558],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0554],\n",
      "        [    0.0565],\n",
      "        [    0.0599],\n",
      "        [    0.0593],\n",
      "        [    0.0593],\n",
      "        [    0.0572],\n",
      "        [    0.0635],\n",
      "        [    0.0656],\n",
      "        [    0.0660],\n",
      "        [    0.0666],\n",
      "        [    0.0675],\n",
      "        [    0.0698],\n",
      "        [    0.0716],\n",
      "        [    0.0718],\n",
      "        [    0.0749],\n",
      "        [    0.0778],\n",
      "        [    0.0767],\n",
      "        [    0.0780],\n",
      "        [    0.0796],\n",
      "        [    0.0813],\n",
      "        [    0.0815],\n",
      "        [    0.0841],\n",
      "        [    0.0916],\n",
      "        [    0.0936],\n",
      "        [    0.0976],\n",
      "        [    0.0962],\n",
      "        [    0.0986],\n",
      "        [    0.1009],\n",
      "        [    0.1034],\n",
      "        [    0.1066],\n",
      "        [    0.1128],\n",
      "        [    0.1145],\n",
      "        [    0.1191],\n",
      "        [    0.1430]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 98.34106707572937\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.976406924901312e-09, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 62, 155, 10, 30, 82, 116, 29, 35, 7, 31, 42, 135, 44, 59, 124, 57, 22, 108, 132, 134, 131, 114, 63, 27, 2, 58, 111, 32, 20, 70, 15, 16, 151, 37, 56, 43, 112, 74, 69, 36, 73, 11, 12, 33, 113, 26, 19, 133, 68, 125, 140, 130, 144, 3, 149, 109, 158, 136, 28, 25, 38, 115, 156, 98, 86, 39, 23, 152, 83, 18, 150, 107, 97, 14, 99, 110, 61, 71, 153, 85, 47, 126, 138, 81, 91, 137, 129, 52, 0, 123, 154, 60, 24, 148, 13, 128, 145, 21, 139, 6, 67, 80, 121, 90, 96, 53, 106, 117, 141, 104, 41, 40, 1, 120, 127, 122, 147, 119, 118, 64, 84, 72, 146, 103, 17, 105, 45, 55, 157, 8, 95, 88, 102, 9, 65, 94, 100, 142, 66, 101, 46, 89, 75, 78, 79, 48, 92, 54, 51, 49, 87, 76, 93, 143, 77, 50, 5] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5560],\n",
      "        [0.3638],\n",
      "        [0.4905],\n",
      "        [0.5496],\n",
      "        [0.6094],\n",
      "        [0.2359],\n",
      "        [0.0989],\n",
      "        [0.6243],\n",
      "        [0.5435],\n",
      "        [0.5001],\n",
      "        [0.5959],\n",
      "        [0.4655],\n",
      "        [0.0989],\n",
      "        [0.4671],\n",
      "        [0.3513],\n",
      "        [0.0989],\n",
      "        [0.3141],\n",
      "        [0.6505],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4023],\n",
      "        [0.6060],\n",
      "        [0.5771],\n",
      "        [0.3461],\n",
      "        [0.0989],\n",
      "        [0.5704],\n",
      "        [0.6644],\n",
      "        [0.3269],\n",
      "        [0.6312],\n",
      "        [0.6239],\n",
      "        [0.4415],\n",
      "        [0.5256],\n",
      "        [0.2929],\n",
      "        [0.4717],\n",
      "        [0.0989],\n",
      "        [0.2779],\n",
      "        [0.3702],\n",
      "        [0.5104],\n",
      "        [0.2870],\n",
      "        [0.5946],\n",
      "        [0.6010],\n",
      "        [0.5780],\n",
      "        [0.0989],\n",
      "        [0.6239],\n",
      "        [0.6480],\n",
      "        [0.0989],\n",
      "        [0.3828],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4289],\n",
      "        [0.5872],\n",
      "        [0.4346],\n",
      "        [0.0989],\n",
      "        [0.5208],\n",
      "        [0.0989],\n",
      "        [0.6230],\n",
      "        [0.6505],\n",
      "        [0.5141],\n",
      "        [0.0989],\n",
      "        [0.5104],\n",
      "        [0.0989],\n",
      "        [0.1650],\n",
      "        [0.5093],\n",
      "        [0.6465],\n",
      "        [0.4432],\n",
      "        [0.2292],\n",
      "        [0.6069],\n",
      "        [0.4265],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5866],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3267],\n",
      "        [0.2795],\n",
      "        [0.4513],\n",
      "        [0.1915],\n",
      "        [0.4078],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3018],\n",
      "        [0.6019],\n",
      "        [0.0989],\n",
      "        [0.4648],\n",
      "        [0.3228],\n",
      "        [0.6697],\n",
      "        [0.4462],\n",
      "        [0.5862],\n",
      "        [0.0989],\n",
      "        [0.5059],\n",
      "        [0.6328],\n",
      "        [0.0989],\n",
      "        [0.4984],\n",
      "        [0.3576],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.1184],\n",
      "        [0.0989],\n",
      "        [0.2457],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4747],\n",
      "        [0.4840],\n",
      "        [0.5869],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4451],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3960],\n",
      "        [0.2530],\n",
      "        [0.2698],\n",
      "        [0.4400],\n",
      "        [0.0989],\n",
      "        [0.6210],\n",
      "        [0.0989],\n",
      "        [0.4126],\n",
      "        [0.2593],\n",
      "        [0.5060],\n",
      "        [0.4458],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4567],\n",
      "        [0.3805],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3588],\n",
      "        [0.0989],\n",
      "        [0.3850],\n",
      "        [0.0989],\n",
      "        [0.3016],\n",
      "        [0.2717],\n",
      "        [0.2725],\n",
      "        [0.4200],\n",
      "        [0.1275],\n",
      "        [0.2051],\n",
      "        [0.3636],\n",
      "        [0.3877],\n",
      "        [0.1144],\n",
      "        [0.2975],\n",
      "        [0.1128],\n",
      "        [0.4242],\n",
      "        [0.2819],\n",
      "        [0.3704],\n",
      "        [0.5811]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0012],\n",
      "        [    0.0019],\n",
      "        [    0.0022],\n",
      "        [    0.0023],\n",
      "        [    0.0036],\n",
      "        [    0.0034],\n",
      "        [    0.0045],\n",
      "        [    0.0047],\n",
      "        [    0.0052],\n",
      "        [    0.0054],\n",
      "        [    0.0058],\n",
      "        [    0.0068],\n",
      "        [    0.0067],\n",
      "        [    0.0068],\n",
      "        [    0.0070],\n",
      "        [    0.0075],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0097],\n",
      "        [    0.0100],\n",
      "        [    0.0112],\n",
      "        [    0.0106],\n",
      "        [    0.0117],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0115],\n",
      "        [    0.0123],\n",
      "        [    0.0127],\n",
      "        [    0.0125],\n",
      "        [    0.0132],\n",
      "        [    0.0151],\n",
      "        [    0.0150],\n",
      "        [    0.0149],\n",
      "        [    0.0156],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.0159],\n",
      "        [    0.0161],\n",
      "        [    0.0160],\n",
      "        [    0.0159],\n",
      "        [    0.0159],\n",
      "        [    0.0169],\n",
      "        [    0.0164],\n",
      "        [    0.0169],\n",
      "        [    0.0174],\n",
      "        [    0.0180],\n",
      "        [    0.0183],\n",
      "        [    0.0191],\n",
      "        [    0.0194],\n",
      "        [    0.0198],\n",
      "        [    0.0198],\n",
      "        [    0.0203],\n",
      "        [    0.0205],\n",
      "        [    0.0203],\n",
      "        [    0.0205],\n",
      "        [    0.0207],\n",
      "        [    0.0210],\n",
      "        [    0.0218],\n",
      "        [    0.0225],\n",
      "        [    0.0221],\n",
      "        [    0.0229],\n",
      "        [    0.0234],\n",
      "        [    0.0242],\n",
      "        [    0.0247],\n",
      "        [    0.0249],\n",
      "        [    0.0250],\n",
      "        [    0.0254],\n",
      "        [    0.0262],\n",
      "        [    0.0268],\n",
      "        [    0.0278],\n",
      "        [    0.0297],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0316],\n",
      "        [    0.0318],\n",
      "        [    0.0320],\n",
      "        [    0.0322],\n",
      "        [    0.0324],\n",
      "        [    0.0326],\n",
      "        [    0.0326],\n",
      "        [    0.0337],\n",
      "        [    0.0340],\n",
      "        [    0.0342],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0351],\n",
      "        [    0.0355],\n",
      "        [    0.0359],\n",
      "        [    0.0363],\n",
      "        [    0.0369],\n",
      "        [    0.0376],\n",
      "        [    0.0375],\n",
      "        [    0.0387],\n",
      "        [    0.0390],\n",
      "        [    0.0393],\n",
      "        [    0.0415],\n",
      "        [    0.0413],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0437],\n",
      "        [    0.0442],\n",
      "        [    0.0452],\n",
      "        [    0.0449],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0477],\n",
      "        [    0.0484],\n",
      "        [    0.0490],\n",
      "        [    0.0491],\n",
      "        [    0.0510],\n",
      "        [    0.0515],\n",
      "        [    0.0525],\n",
      "        [    0.0534],\n",
      "        [    0.0543],\n",
      "        [    0.0549],\n",
      "        [    0.0551],\n",
      "        [    0.0554],\n",
      "        [    0.0555],\n",
      "        [    0.0558],\n",
      "        [    0.0565],\n",
      "        [    0.0572],\n",
      "        [    0.0593],\n",
      "        [    0.0593],\n",
      "        [    0.0599],\n",
      "        [    0.0635],\n",
      "        [    0.0656],\n",
      "        [    0.0660],\n",
      "        [    0.0666],\n",
      "        [    0.0675],\n",
      "        [    0.0698],\n",
      "        [    0.0716],\n",
      "        [    0.0718],\n",
      "        [    0.0749],\n",
      "        [    0.0767],\n",
      "        [    0.0778],\n",
      "        [    0.0780],\n",
      "        [    0.0796],\n",
      "        [    0.0813],\n",
      "        [    0.0815],\n",
      "        [    0.0841],\n",
      "        [    0.0916],\n",
      "        [    0.0936],\n",
      "        [    0.0962],\n",
      "        [    0.0976],\n",
      "        [    0.0986],\n",
      "        [    0.1009],\n",
      "        [    0.1034],\n",
      "        [    0.1066],\n",
      "        [    0.1128],\n",
      "        [    0.1145],\n",
      "        [    0.1191],\n",
      "        [    0.1430],\n",
      "        [    0.1574]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 15\n",
      "Number of shrink: 15\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0015],\n",
      "        [0.0013],\n",
      "        [0.0020],\n",
      "        [0.0046],\n",
      "        [0.0010],\n",
      "        [0.0022],\n",
      "        [0.0036],\n",
      "        [0.0047],\n",
      "        [0.0052],\n",
      "        [0.0011],\n",
      "        [0.0037],\n",
      "        [0.0069],\n",
      "        [0.0058],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0068],\n",
      "        [0.0061],\n",
      "        [0.0056],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0095],\n",
      "        [0.0126],\n",
      "        [0.0149],\n",
      "        [0.0127],\n",
      "        [0.0118],\n",
      "        [0.0105],\n",
      "        [0.0146],\n",
      "        [0.0115],\n",
      "        [0.0099],\n",
      "        [0.0111],\n",
      "        [0.0170],\n",
      "        [0.0165],\n",
      "        [0.0158],\n",
      "        [0.0167],\n",
      "        [0.0154],\n",
      "        [0.0142],\n",
      "        [0.0146],\n",
      "        [0.0167],\n",
      "        [0.0146],\n",
      "        [0.0189],\n",
      "        [0.0128],\n",
      "        [0.0154],\n",
      "        [0.0164],\n",
      "        [0.0148],\n",
      "        [0.0146],\n",
      "        [0.0180],\n",
      "        [0.0172],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0207],\n",
      "        [0.0169],\n",
      "        [0.0217],\n",
      "        [0.0203],\n",
      "        [0.0182],\n",
      "        [0.0207],\n",
      "        [0.0227],\n",
      "        [0.0200],\n",
      "        [0.0237],\n",
      "        [0.0221],\n",
      "        [0.0255],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0264],\n",
      "        [0.0227],\n",
      "        [0.0223],\n",
      "        [0.0254],\n",
      "        [0.0236],\n",
      "        [0.0284],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0334],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0332],\n",
      "        [0.0329],\n",
      "        [0.0294],\n",
      "        [0.0324],\n",
      "        [0.0334],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0331],\n",
      "        [0.0376],\n",
      "        [0.0351],\n",
      "        [0.0326],\n",
      "        [0.0373],\n",
      "        [0.0340],\n",
      "        [0.0358],\n",
      "        [0.0405],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0417],\n",
      "        [0.0394],\n",
      "        [0.0376],\n",
      "        [0.0429],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0463],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0477],\n",
      "        [0.0498],\n",
      "        [0.0504],\n",
      "        [0.0525],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0528],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0566],\n",
      "        [0.0554],\n",
      "        [0.0566],\n",
      "        [0.0548],\n",
      "        [0.0565],\n",
      "        [0.0547],\n",
      "        [0.0593],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0610],\n",
      "        [0.0684],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0724],\n",
      "        [0.0733],\n",
      "        [0.0718],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0793],\n",
      "        [0.0780],\n",
      "        [0.0802],\n",
      "        [0.0813],\n",
      "        [0.0797],\n",
      "        [0.0819],\n",
      "        [0.0899],\n",
      "        [0.0931],\n",
      "        [0.0962],\n",
      "        [0.0987],\n",
      "        [0.0975],\n",
      "        [0.0997],\n",
      "        [0.1034],\n",
      "        [0.1047],\n",
      "        [0.1128],\n",
      "        [0.1149],\n",
      "        [0.1171],\n",
      "        [0.1422],\n",
      "        [0.1539]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 98.4934310913086\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.826799795409897e-07, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [7, 62, 34, 30, 155, 82, 116, 31, 29, 10, 22, 35, 135, 57, 124, 42, 44, 59, 108, 132, 134, 63, 131, 114, 15, 32, 16, 70, 111, 27, 12, 58, 74, 19, 69, 20, 26, 73, 2, 33, 112, 56, 113, 37, 43, 36, 151, 68, 3, 133, 158, 125, 11, 140, 130, 25, 109, 136, 144, 149, 115, 28, 152, 23, 18, 98, 38, 86, 83, 156, 39, 107, 150, 153, 97, 99, 110, 85, 71, 154, 126, 52, 14, 61, 47, 138, 24, 81, 91, 137, 129, 123, 148, 6, 60, 128, 145, 0, 139, 13, 21, 80, 121, 67, 90, 96, 106, 117, 141, 53, 104, 41, 40, 120, 127, 147, 122, 1, 119, 17, 118, 146, 84, 103, 72, 64, 105, 45, 157, 55, 95, 88, 102, 8, 94, 9, 65, 100, 142, 101, 66, 75, 46, 89, 78, 79, 48, 92, 51, 54, 49, 87, 76, 93, 143, 77, 50, 5, 4] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4965],\n",
      "        [0.3622],\n",
      "        [0.5544],\n",
      "        [0.6083],\n",
      "        [0.4937],\n",
      "        [0.2359],\n",
      "        [0.0989],\n",
      "        [0.5944],\n",
      "        [0.6230],\n",
      "        [0.5468],\n",
      "        [0.6486],\n",
      "        [0.5427],\n",
      "        [0.0989],\n",
      "        [0.3132],\n",
      "        [0.0989],\n",
      "        [0.4640],\n",
      "        [0.4660],\n",
      "        [0.3501],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4006],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6287],\n",
      "        [0.5695],\n",
      "        [0.6218],\n",
      "        [0.3257],\n",
      "        [0.0989],\n",
      "        [0.6040],\n",
      "        [0.5980],\n",
      "        [0.3452],\n",
      "        [0.2764],\n",
      "        [0.6451],\n",
      "        [0.3689],\n",
      "        [0.6621],\n",
      "        [0.6218],\n",
      "        [0.2856],\n",
      "        [0.5739],\n",
      "        [0.5764],\n",
      "        [0.0989],\n",
      "        [0.2920],\n",
      "        [0.0989],\n",
      "        [0.5241],\n",
      "        [0.4706],\n",
      "        [0.5098],\n",
      "        [0.4434],\n",
      "        [0.3817],\n",
      "        [0.5838],\n",
      "        [0.0989],\n",
      "        [0.5231],\n",
      "        [0.0989],\n",
      "        [0.5916],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6486],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4298],\n",
      "        [0.4358],\n",
      "        [0.0989],\n",
      "        [0.6212],\n",
      "        [0.4459],\n",
      "        [0.6443],\n",
      "        [0.6042],\n",
      "        [0.0989],\n",
      "        [0.5129],\n",
      "        [0.1650],\n",
      "        [0.2292],\n",
      "        [0.5130],\n",
      "        [0.5076],\n",
      "        [0.0989],\n",
      "        [0.4280],\n",
      "        [0.4541],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.1915],\n",
      "        [0.2786],\n",
      "        [0.4677],\n",
      "        [0.0989],\n",
      "        [0.3005],\n",
      "        [0.5837],\n",
      "        [0.3254],\n",
      "        [0.4070],\n",
      "        [0.0989],\n",
      "        [0.6674],\n",
      "        [0.2257],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4473],\n",
      "        [0.4945],\n",
      "        [0.3214],\n",
      "        [0.0989],\n",
      "        [0.5048],\n",
      "        [0.5988],\n",
      "        [0.0989],\n",
      "        [0.5834],\n",
      "        [0.6301],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.3561],\n",
      "        [0.1184],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2447],\n",
      "        [0.0989],\n",
      "        [0.4733],\n",
      "        [0.4825],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4457],\n",
      "        [0.0989],\n",
      "        [0.5835],\n",
      "        [0.0989],\n",
      "        [0.6185],\n",
      "        [0.0989],\n",
      "        [0.4409],\n",
      "        [0.2529],\n",
      "        [0.0989],\n",
      "        [0.2687],\n",
      "        [0.3944],\n",
      "        [0.0989],\n",
      "        [0.4116],\n",
      "        [0.5085],\n",
      "        [0.2583],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4430],\n",
      "        [0.0989],\n",
      "        [0.4541],\n",
      "        [0.3789],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3572],\n",
      "        [0.2999],\n",
      "        [0.3844],\n",
      "        [0.0989],\n",
      "        [0.2695],\n",
      "        [0.2707],\n",
      "        [0.4194],\n",
      "        [0.1275],\n",
      "        [0.3625],\n",
      "        [0.2040],\n",
      "        [0.3865],\n",
      "        [0.1144],\n",
      "        [0.2956],\n",
      "        [0.1128],\n",
      "        [0.4246],\n",
      "        [0.2799],\n",
      "        [0.3695],\n",
      "        [0.5776],\n",
      "        [0.5839]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0011],\n",
      "        [0.0013],\n",
      "        [0.0015],\n",
      "        [0.0010],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0047],\n",
      "        [0.0046],\n",
      "        [0.0056],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0061],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0099],\n",
      "        [0.0105],\n",
      "        [0.0111],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0128],\n",
      "        [0.0127],\n",
      "        [0.0142],\n",
      "        [0.0146],\n",
      "        [0.0146],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0146],\n",
      "        [0.0149],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0167],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0191],\n",
      "        [0.0189],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0217],\n",
      "        [0.0221],\n",
      "        [0.0227],\n",
      "        [0.0223],\n",
      "        [0.0227],\n",
      "        [0.0236],\n",
      "        [0.0234],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0255],\n",
      "        [0.0264],\n",
      "        [0.0277],\n",
      "        [0.0284],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0329],\n",
      "        [0.0326],\n",
      "        [0.0326],\n",
      "        [0.0331],\n",
      "        [0.0334],\n",
      "        [0.0332],\n",
      "        [0.0334],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0358],\n",
      "        [0.0376],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0376],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0463],\n",
      "        [0.0477],\n",
      "        [0.0498],\n",
      "        [0.0504],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0528],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0543],\n",
      "        [0.0547],\n",
      "        [0.0549],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0566],\n",
      "        [0.0566],\n",
      "        [0.0593],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0610],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0684],\n",
      "        [0.0718],\n",
      "        [0.0724],\n",
      "        [0.0733],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0793],\n",
      "        [0.0797],\n",
      "        [0.0802],\n",
      "        [0.0813],\n",
      "        [0.0819],\n",
      "        [0.0899],\n",
      "        [0.0931],\n",
      "        [0.0962],\n",
      "        [0.0975],\n",
      "        [0.0987],\n",
      "        [0.0997],\n",
      "        [0.1034],\n",
      "        [0.1047],\n",
      "        [0.1128],\n",
      "        [0.1149],\n",
      "        [0.1171],\n",
      "        [0.1422],\n",
      "        [0.1539],\n",
      "        [0.1634]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 15\n",
      "Number of shrink: 15\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0050],\n",
      "        [0.0029],\n",
      "        [0.0033],\n",
      "        [0.0012],\n",
      "        [0.0051],\n",
      "        [0.0022],\n",
      "        [0.0036],\n",
      "        [0.0012],\n",
      "        [0.0074],\n",
      "        [0.0098],\n",
      "        [0.0013],\n",
      "        [0.0076],\n",
      "        [0.0058],\n",
      "        [0.0055],\n",
      "        [0.0068],\n",
      "        [0.0094],\n",
      "        [0.0095],\n",
      "        [0.0092],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0077],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0055],\n",
      "        [0.0081],\n",
      "        [0.0069],\n",
      "        [0.0105],\n",
      "        [0.0118],\n",
      "        [0.0163],\n",
      "        [0.0075],\n",
      "        [0.0136],\n",
      "        [0.0119],\n",
      "        [0.0092],\n",
      "        [0.0128],\n",
      "        [0.0194],\n",
      "        [0.0110],\n",
      "        [0.0125],\n",
      "        [0.0212],\n",
      "        [0.0127],\n",
      "        [0.0154],\n",
      "        [0.0168],\n",
      "        [0.0164],\n",
      "        [0.0187],\n",
      "        [0.0190],\n",
      "        [0.0184],\n",
      "        [0.0191],\n",
      "        [0.0154],\n",
      "        [0.0113],\n",
      "        [0.0180],\n",
      "        [0.0158],\n",
      "        [0.0191],\n",
      "        [0.0243],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0158],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0229],\n",
      "        [0.0221],\n",
      "        [0.0265],\n",
      "        [0.0197],\n",
      "        [0.0190],\n",
      "        [0.0189],\n",
      "        [0.0234],\n",
      "        [0.0262],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0277],\n",
      "        [0.0299],\n",
      "        [0.0266],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0339],\n",
      "        [0.0299],\n",
      "        [0.0326],\n",
      "        [0.0315],\n",
      "        [0.0378],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0337],\n",
      "        [0.0304],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0346],\n",
      "        [0.0314],\n",
      "        [0.0387],\n",
      "        [0.0375],\n",
      "        [0.0372],\n",
      "        [0.0439],\n",
      "        [0.0394],\n",
      "        [0.0456],\n",
      "        [0.0459],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0447],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0478],\n",
      "        [0.0477],\n",
      "        [0.0522],\n",
      "        [0.0528],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0519],\n",
      "        [0.0525],\n",
      "        [0.0589],\n",
      "        [0.0543],\n",
      "        [0.0504],\n",
      "        [0.0549],\n",
      "        [0.0542],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0580],\n",
      "        [0.0586],\n",
      "        [0.0593],\n",
      "        [0.0616],\n",
      "        [0.0588],\n",
      "        [0.0625],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0740],\n",
      "        [0.0718],\n",
      "        [0.0778],\n",
      "        [0.0754],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0811],\n",
      "        [0.0772],\n",
      "        [0.0817],\n",
      "        [0.0813],\n",
      "        [0.0795],\n",
      "        [0.0872],\n",
      "        [0.0921],\n",
      "        [0.0962],\n",
      "        [0.0956],\n",
      "        [0.0999],\n",
      "        [0.0984],\n",
      "        [0.1034],\n",
      "        [0.1026],\n",
      "        [0.1128],\n",
      "        [0.1149],\n",
      "        [0.1144],\n",
      "        [0.1402],\n",
      "        [0.1474],\n",
      "        [0.1570]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 98.64570116996765\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 29 個區塊累積花費時間(s) 0.6654477119445801\n",
      "<<The performance of 29 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.6654477119445801\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 995.22\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1277.38\n",
      "The accuracy(2000) for l = 1: 88.68%\n",
      "The accuracy(3000) for l = 1: 98.11%\n",
      "The maximum error: tensor(4007.2930)\n",
      "The minimum error: tensor(31.4570)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 296.0\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 344.1\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 547.66015625\n",
      "The minimum error: 101.73046875\n",
      "------------------------------------------------------------\n",
      "0.8867924528301887\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<30>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.478658178821206e-07, 26)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [26, 27, 18, 78, 58, 112, 30, 155, 3, 151, 53, 11, 131, 157, 12, 120, 8, 25, 31, 59, 104, 128, 130, 28, 55, 38, 15, 40, 127, 110, 6, 66, 22, 70, 107, 69, 65, 29, 54, 156, 64, 108, 21, 154, 109, 23, 52, 129, 33, 32, 14, 39, 147, 121, 136, 19, 148, 16, 126, 105, 132, 140, 158, 111, 145, 94, 82, 7, 79, 34, 24, 149, 103, 152, 35, 93, 146, 150, 20, 95, 2, 106, 48, 81, 122, 134, 77, 67, 87, 133, 125, 144, 57, 43, 119, 141, 124, 10, 56, 135, 76, 117, 86, 92, 63, 102, 113, 137, 17, 9, 100, 49, 13, 116, 123, 37, 143, 118, 36, 142, 115, 114, 80, 99, 68, 60, 153, 101, 41, 51, 91, 84, 98, 90, 4, 96, 61, 138, 71, 5, 97, 74, 85, 62, 42, 75, 44, 47, 88, 45, 50, 72, 83, 89, 73]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0022],\n",
      "        [0.0029],\n",
      "        [0.0036],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0074],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0081],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0098],\n",
      "        [0.0105],\n",
      "        [0.0110],\n",
      "        [0.0119],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0128],\n",
      "        [0.0127],\n",
      "        [0.0136],\n",
      "        [0.0144],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0168],\n",
      "        [0.0180],\n",
      "        [0.0187],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0221],\n",
      "        [0.0229],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0254],\n",
      "        [0.0262],\n",
      "        [0.0265],\n",
      "        [0.0266],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0315],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0339],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0351],\n",
      "        [0.0372],\n",
      "        [0.0375],\n",
      "        [0.0378],\n",
      "        [0.0387],\n",
      "        [0.0394],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0447],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0459],\n",
      "        [0.0456],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0504],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0519],\n",
      "        [0.0525],\n",
      "        [0.0528],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0580],\n",
      "        [0.0586],\n",
      "        [0.0588],\n",
      "        [0.0593],\n",
      "        [0.0616],\n",
      "        [0.0625],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0740],\n",
      "        [0.0749],\n",
      "        [0.0754],\n",
      "        [0.0767],\n",
      "        [0.0772],\n",
      "        [0.0778],\n",
      "        [0.0780],\n",
      "        [0.0795],\n",
      "        [0.0813],\n",
      "        [0.0811],\n",
      "        [0.0817],\n",
      "        [0.0872],\n",
      "        [0.0921],\n",
      "        [0.0956],\n",
      "        [0.0962],\n",
      "        [0.0984],\n",
      "        [0.0999],\n",
      "        [0.1026],\n",
      "        [0.1034],\n",
      "        [0.1128],\n",
      "        [0.1144]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.478658178821206e-07, 26)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [26, 27, 18, 78, 58, 112, 30, 155, 3, 151, 53, 11, 131, 157, 12, 120, 8, 25, 31, 59, 104, 128, 130, 28, 55, 38, 15, 40, 127, 110, 6, 66, 22, 70, 107, 69, 65, 29, 54, 156, 64, 108, 21, 154, 109, 23, 52, 129, 33, 32, 14, 39, 147, 121, 136, 19, 148, 16, 126, 105, 132, 140, 158, 111, 145, 94, 82, 7, 79, 34, 24, 149, 103, 152, 35, 93, 146, 150, 20, 95, 2, 106, 48, 81, 122, 134, 77, 67, 87, 133, 125, 144, 57, 43, 119, 141, 124, 10, 56, 135, 76, 117, 86, 92, 63, 102, 113, 137, 17, 9, 100, 49, 13, 116, 123, 37, 143, 118, 36, 142, 115, 114, 80, 99, 68, 60, 153, 101, 41, 51, 91, 84, 98, 90, 4, 96, 61, 138, 71, 5, 97, 74, 85, 62, 42, 75, 44, 47, 88, 45, 50, 72, 83, 89, 73, 139] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6060],\n",
      "        [0.5919],\n",
      "        [0.6442],\n",
      "        [0.2358],\n",
      "        [0.3605],\n",
      "        [0.0989],\n",
      "        [0.5525],\n",
      "        [0.5211],\n",
      "        [0.4904],\n",
      "        [0.4968],\n",
      "        [0.3126],\n",
      "        [0.6242],\n",
      "        [0.0989],\n",
      "        [0.5060],\n",
      "        [0.6177],\n",
      "        [0.0989],\n",
      "        [0.5926],\n",
      "        [0.6204],\n",
      "        [0.5403],\n",
      "        [0.3989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5671],\n",
      "        [0.3489],\n",
      "        [0.4615],\n",
      "        [0.6397],\n",
      "        [0.4644],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5416],\n",
      "        [0.3246],\n",
      "        [0.6180],\n",
      "        [0.2741],\n",
      "        [0.0989],\n",
      "        [0.2835],\n",
      "        [0.3671],\n",
      "        [0.5738],\n",
      "        [0.3443],\n",
      "        [0.5070],\n",
      "        [0.3799],\n",
      "        [0.0989],\n",
      "        [0.6444],\n",
      "        [0.5255],\n",
      "        [0.0989],\n",
      "        [0.6003],\n",
      "        [0.2910],\n",
      "        [0.0989],\n",
      "        [0.5218],\n",
      "        [0.5081],\n",
      "        [0.5995],\n",
      "        [0.4682],\n",
      "        [0.4454],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6406],\n",
      "        [0.4485],\n",
      "        [0.6573],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4303],\n",
      "        [0.5098],\n",
      "        [0.0989],\n",
      "        [0.4371],\n",
      "        [0.0989],\n",
      "        [0.1650],\n",
      "        [0.5862],\n",
      "        [0.2292],\n",
      "        [0.5104],\n",
      "        [0.6175],\n",
      "        [0.4570],\n",
      "        [0.0989],\n",
      "        [0.5157],\n",
      "        [0.5053],\n",
      "        [0.0989],\n",
      "        [0.4296],\n",
      "        [0.4705],\n",
      "        [0.6637],\n",
      "        [0.0989],\n",
      "        [0.4883],\n",
      "        [0.0989],\n",
      "        [0.2989],\n",
      "        [0.1915],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.2776],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4485],\n",
      "        [0.3238],\n",
      "        [0.4054],\n",
      "        [0.0989],\n",
      "        [0.5044],\n",
      "        [0.0989],\n",
      "        [0.5793],\n",
      "        [0.3200],\n",
      "        [0.0989],\n",
      "        [0.2258],\n",
      "        [0.0989],\n",
      "        [0.1184],\n",
      "        [0.0989],\n",
      "        [0.3543],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6258],\n",
      "        [0.5782],\n",
      "        [0.0989],\n",
      "        [0.2432],\n",
      "        [0.6142],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4708],\n",
      "        [0.4466],\n",
      "        [0.0989],\n",
      "        [0.4802],\n",
      "        [0.4416],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2529],\n",
      "        [0.0989],\n",
      "        [0.2672],\n",
      "        [0.3925],\n",
      "        [0.5108],\n",
      "        [0.0989],\n",
      "        [0.4103],\n",
      "        [0.2567],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4374],\n",
      "        [0.0989],\n",
      "        [0.3768],\n",
      "        [0.0989],\n",
      "        [0.2973],\n",
      "        [0.4487],\n",
      "        [0.0989],\n",
      "        [0.2671],\n",
      "        [0.0989],\n",
      "        [0.3554],\n",
      "        [0.3828],\n",
      "        [0.2680],\n",
      "        [0.4185],\n",
      "        [0.3606],\n",
      "        [0.1275],\n",
      "        [0.3852],\n",
      "        [0.2028],\n",
      "        [0.2934],\n",
      "        [0.1144],\n",
      "        [0.1128],\n",
      "        [0.2772],\n",
      "        [0.4247]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0012],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0022],\n",
      "        [0.0029],\n",
      "        [0.0036],\n",
      "        [0.0033],\n",
      "        [0.0040],\n",
      "        [0.0050],\n",
      "        [0.0051],\n",
      "        [0.0055],\n",
      "        [0.0055],\n",
      "        [0.0058],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0074],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0081],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0098],\n",
      "        [0.0105],\n",
      "        [0.0110],\n",
      "        [0.0119],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0128],\n",
      "        [0.0127],\n",
      "        [0.0136],\n",
      "        [0.0144],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0158],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0168],\n",
      "        [0.0180],\n",
      "        [0.0187],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0190],\n",
      "        [0.0197],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0221],\n",
      "        [0.0229],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0254],\n",
      "        [0.0262],\n",
      "        [0.0265],\n",
      "        [0.0266],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0287],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0315],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0339],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0351],\n",
      "        [0.0372],\n",
      "        [0.0375],\n",
      "        [0.0378],\n",
      "        [0.0387],\n",
      "        [0.0394],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0442],\n",
      "        [0.0447],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0459],\n",
      "        [0.0456],\n",
      "        [0.0477],\n",
      "        [0.0478],\n",
      "        [0.0504],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0522],\n",
      "        [0.0519],\n",
      "        [0.0525],\n",
      "        [0.0528],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0580],\n",
      "        [0.0586],\n",
      "        [0.0588],\n",
      "        [0.0593],\n",
      "        [0.0616],\n",
      "        [0.0625],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0740],\n",
      "        [0.0749],\n",
      "        [0.0754],\n",
      "        [0.0767],\n",
      "        [0.0772],\n",
      "        [0.0778],\n",
      "        [0.0780],\n",
      "        [0.0795],\n",
      "        [0.0813],\n",
      "        [0.0811],\n",
      "        [0.0817],\n",
      "        [0.0872],\n",
      "        [0.0921],\n",
      "        [0.0956],\n",
      "        [0.0962],\n",
      "        [0.0984],\n",
      "        [0.0999],\n",
      "        [0.1026],\n",
      "        [0.1034],\n",
      "        [0.1128],\n",
      "        [0.1144],\n",
      "        [0.1149]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 14\n",
      "Number of shrink: 14\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0058],\n",
      "        [0.0085],\n",
      "        [0.0078],\n",
      "        [0.0022],\n",
      "        [0.0009],\n",
      "        [0.0036],\n",
      "        [0.0032],\n",
      "        [0.0036],\n",
      "        [0.0014],\n",
      "        [0.0052],\n",
      "        [0.0074],\n",
      "        [0.0117],\n",
      "        [0.0058],\n",
      "        [0.0075],\n",
      "        [0.0131],\n",
      "        [0.0069],\n",
      "        [0.0130],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0094],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0149],\n",
      "        [0.0072],\n",
      "        [0.0041],\n",
      "        [0.0157],\n",
      "        [0.0041],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0039],\n",
      "        [0.0133],\n",
      "        [0.0179],\n",
      "        [0.0141],\n",
      "        [0.0118],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0194],\n",
      "        [0.0115],\n",
      "        [0.0152],\n",
      "        [0.0178],\n",
      "        [0.0154],\n",
      "        [0.0225],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0092],\n",
      "        [0.0145],\n",
      "        [0.0180],\n",
      "        [0.0120],\n",
      "        [0.0120],\n",
      "        [0.0246],\n",
      "        [0.0135],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0254],\n",
      "        [0.0193],\n",
      "        [0.0132],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0201],\n",
      "        [0.0228],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0187],\n",
      "        [0.0254],\n",
      "        [0.0205],\n",
      "        [0.0190],\n",
      "        [0.0264],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0231],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0295],\n",
      "        [0.0368],\n",
      "        [0.0306],\n",
      "        [0.0373],\n",
      "        [0.0316],\n",
      "        [0.0352],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0312],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0354],\n",
      "        [0.0329],\n",
      "        [0.0298],\n",
      "        [0.0351],\n",
      "        [0.0358],\n",
      "        [0.0375],\n",
      "        [0.0323],\n",
      "        [0.0370],\n",
      "        [0.0394],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0425],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0397],\n",
      "        [0.0401],\n",
      "        [0.0477],\n",
      "        [0.0443],\n",
      "        [0.0565],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0467],\n",
      "        [0.0533],\n",
      "        [0.0525],\n",
      "        [0.0476],\n",
      "        [0.0554],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0553],\n",
      "        [0.0568],\n",
      "        [0.0590],\n",
      "        [0.0593],\n",
      "        [0.0563],\n",
      "        [0.0600],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0681],\n",
      "        [0.0749],\n",
      "        [0.0730],\n",
      "        [0.0767],\n",
      "        [0.0794],\n",
      "        [0.0718],\n",
      "        [0.0780],\n",
      "        [0.0808],\n",
      "        [0.0814],\n",
      "        [0.0789],\n",
      "        [0.0763],\n",
      "        [0.0884],\n",
      "        [0.0970],\n",
      "        [0.0997],\n",
      "        [0.0962],\n",
      "        [0.1031],\n",
      "        [0.0971],\n",
      "        [0.1044],\n",
      "        [0.1034],\n",
      "        [0.1128],\n",
      "        [0.1160],\n",
      "        [0.1139]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 99.04163908958435\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (7.688464620514424e-08, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [25, 31, 58, 3, 78, 30, 155, 112, 38, 6, 40, 151, 131, 26, 120, 55, 53, 157, 18, 104, 128, 27, 130, 23, 59, 127, 110, 54, 11, 107, 33, 32, 16, 8, 66, 12, 39, 70, 52, 69, 156, 28, 65, 108, 15, 154, 109, 64, 129, 22, 7, 24, 121, 147, 136, 29, 148, 126, 140, 34, 105, 132, 111, 145, 158, 21, 35, 94, 82, 14, 79, 19, 149, 103, 152, 150, 93, 43, 146, 95, 67, 106, 10, 81, 122, 57, 134, 77, 87, 133, 125, 119, 48, 144, 141, 20, 56, 2, 124, 135, 17, 9, 76, 63, 117, 86, 49, 92, 102, 113, 137, 37, 36, 100, 116, 123, 118, 143, 115, 114, 68, 80, 142, 41, 99, 13, 60, 153, 101, 51, 91, 84, 98, 4, 90, 5, 61, 96, 42, 138, 97, 62, 71, 74, 85, 75, 88, 50, 44, 47, 45, 83, 72, 89, 139, 73, 46] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6274],\n",
      "        [0.5473],\n",
      "        [0.3625],\n",
      "        [0.4967],\n",
      "        [0.2358],\n",
      "        [0.5591],\n",
      "        [0.5215],\n",
      "        [0.0989],\n",
      "        [0.4669],\n",
      "        [0.5475],\n",
      "        [0.4699],\n",
      "        [0.4969],\n",
      "        [0.0989],\n",
      "        [0.6131],\n",
      "        [0.0989],\n",
      "        [0.3508],\n",
      "        [0.3145],\n",
      "        [0.5070],\n",
      "        [0.6508],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5992],\n",
      "        [0.0989],\n",
      "        [0.6074],\n",
      "        [0.4005],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3464],\n",
      "        [0.6304],\n",
      "        [0.0989],\n",
      "        [0.5286],\n",
      "        [0.5145],\n",
      "        [0.6635],\n",
      "        [0.5981],\n",
      "        [0.3275],\n",
      "        [0.6238],\n",
      "        [0.4738],\n",
      "        [0.2763],\n",
      "        [0.2933],\n",
      "        [0.2859],\n",
      "        [0.5077],\n",
      "        [0.5739],\n",
      "        [0.3696],\n",
      "        [0.0989],\n",
      "        [0.6462],\n",
      "        [0.5253],\n",
      "        [0.0989],\n",
      "        [0.3823],\n",
      "        [0.0989],\n",
      "        [0.6249],\n",
      "        [0.5918],\n",
      "        [0.6249],\n",
      "        [0.0989],\n",
      "        [0.4455],\n",
      "        [0.0989],\n",
      "        [0.5805],\n",
      "        [0.4489],\n",
      "        [0.0989],\n",
      "        [0.4293],\n",
      "        [0.5161],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4368],\n",
      "        [0.5110],\n",
      "        [0.6512],\n",
      "        [0.5109],\n",
      "        [0.0989],\n",
      "        [0.1650],\n",
      "        [0.6053],\n",
      "        [0.2292],\n",
      "        [0.6470],\n",
      "        [0.4571],\n",
      "        [0.0989],\n",
      "        [0.5153],\n",
      "        [0.4708],\n",
      "        [0.0989],\n",
      "        [0.4106],\n",
      "        [0.4296],\n",
      "        [0.0989],\n",
      "        [0.2803],\n",
      "        [0.0989],\n",
      "        [0.5848],\n",
      "        [0.1914],\n",
      "        [0.0989],\n",
      "        [0.3256],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3026],\n",
      "        [0.4477],\n",
      "        [0.5030],\n",
      "        [0.6701],\n",
      "        [0.3217],\n",
      "        [0.4941],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6320],\n",
      "        [0.5837],\n",
      "        [0.2257],\n",
      "        [0.3564],\n",
      "        [0.0989],\n",
      "        [0.1183],\n",
      "        [0.2467],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4764],\n",
      "        [0.4853],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4452],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2700],\n",
      "        [0.2529],\n",
      "        [0.4403],\n",
      "        [0.4156],\n",
      "        [0.0989],\n",
      "        [0.6203],\n",
      "        [0.3943],\n",
      "        [0.5106],\n",
      "        [0.0989],\n",
      "        [0.2593],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4432],\n",
      "        [0.0989],\n",
      "        [0.4547],\n",
      "        [0.3792],\n",
      "        [0.0989],\n",
      "        [0.3882],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3576],\n",
      "        [0.2995],\n",
      "        [0.2684],\n",
      "        [0.0989],\n",
      "        [0.2692],\n",
      "        [0.1275],\n",
      "        [0.2056],\n",
      "        [0.4234],\n",
      "        [0.3647],\n",
      "        [0.3898],\n",
      "        [0.1144],\n",
      "        [0.2952],\n",
      "        [0.1128],\n",
      "        [0.4236],\n",
      "        [0.2787],\n",
      "        [0.3721]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0036],\n",
      "        [0.0036],\n",
      "        [0.0041],\n",
      "        [0.0039],\n",
      "        [0.0041],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0058],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0074],\n",
      "        [0.0075],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0085],\n",
      "        [0.0083],\n",
      "        [0.0092],\n",
      "        [0.0094],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0115],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0120],\n",
      "        [0.0132],\n",
      "        [0.0130],\n",
      "        [0.0133],\n",
      "        [0.0131],\n",
      "        [0.0135],\n",
      "        [0.0141],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0152],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0157],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0178],\n",
      "        [0.0180],\n",
      "        [0.0179],\n",
      "        [0.0187],\n",
      "        [0.0190],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0205],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0228],\n",
      "        [0.0225],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0246],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0264],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0329],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0352],\n",
      "        [0.0354],\n",
      "        [0.0358],\n",
      "        [0.0368],\n",
      "        [0.0370],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0397],\n",
      "        [0.0401],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0443],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0467],\n",
      "        [0.0476],\n",
      "        [0.0477],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0533],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0553],\n",
      "        [0.0554],\n",
      "        [0.0554],\n",
      "        [0.0563],\n",
      "        [0.0564],\n",
      "        [0.0565],\n",
      "        [0.0568],\n",
      "        [0.0590],\n",
      "        [0.0593],\n",
      "        [0.0600],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0681],\n",
      "        [0.0718],\n",
      "        [0.0718],\n",
      "        [0.0730],\n",
      "        [0.0749],\n",
      "        [0.0763],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0789],\n",
      "        [0.0794],\n",
      "        [0.0808],\n",
      "        [0.0814],\n",
      "        [0.0884],\n",
      "        [0.0962],\n",
      "        [0.0971],\n",
      "        [0.0970],\n",
      "        [0.0997],\n",
      "        [0.1031],\n",
      "        [0.1034],\n",
      "        [0.1044],\n",
      "        [0.1128],\n",
      "        [0.1139],\n",
      "        [0.1160],\n",
      "        [0.1447]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 1\n",
      "Number of shrink: 7\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0026],\n",
      "        [0.0028],\n",
      "        [0.0026],\n",
      "        [0.0004],\n",
      "        [0.0022],\n",
      "        [0.0013],\n",
      "        [0.0056],\n",
      "        [0.0036],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0035],\n",
      "        [0.0058],\n",
      "        [0.0040],\n",
      "        [0.0069],\n",
      "        [0.0087],\n",
      "        [0.0058],\n",
      "        [0.0056],\n",
      "        [0.0060],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0064],\n",
      "        [0.0083],\n",
      "        [0.0113],\n",
      "        [0.0080],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0129],\n",
      "        [0.0097],\n",
      "        [0.0118],\n",
      "        [0.0143],\n",
      "        [0.0141],\n",
      "        [0.0149],\n",
      "        [0.0111],\n",
      "        [0.0118],\n",
      "        [0.0110],\n",
      "        [0.0150],\n",
      "        [0.0125],\n",
      "        [0.0159],\n",
      "        [0.0134],\n",
      "        [0.0132],\n",
      "        [0.0128],\n",
      "        [0.0138],\n",
      "        [0.0154],\n",
      "        [0.0135],\n",
      "        [0.0181],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0180],\n",
      "        [0.0158],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0191],\n",
      "        [0.0173],\n",
      "        [0.0194],\n",
      "        [0.0175],\n",
      "        [0.0213],\n",
      "        [0.0199],\n",
      "        [0.0183],\n",
      "        [0.0225],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0205],\n",
      "        [0.0208],\n",
      "        [0.0204],\n",
      "        [0.0249],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0228],\n",
      "        [0.0254],\n",
      "        [0.0234],\n",
      "        [0.0283],\n",
      "        [0.0277],\n",
      "        [0.0260],\n",
      "        [0.0316],\n",
      "        [0.0297],\n",
      "        [0.0316],\n",
      "        [0.0280],\n",
      "        [0.0306],\n",
      "        [0.0327],\n",
      "        [0.0316],\n",
      "        [0.0341],\n",
      "        [0.0324],\n",
      "        [0.0326],\n",
      "        [0.0343],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0338],\n",
      "        [0.0374],\n",
      "        [0.0354],\n",
      "        [0.0346],\n",
      "        [0.0383],\n",
      "        [0.0357],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0417],\n",
      "        [0.0439],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0456],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0477],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0551],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0569],\n",
      "        [0.0554],\n",
      "        [0.0572],\n",
      "        [0.0579],\n",
      "        [0.0564],\n",
      "        [0.0545],\n",
      "        [0.0583],\n",
      "        [0.0606],\n",
      "        [0.0593],\n",
      "        [0.0614],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0697],\n",
      "        [0.0718],\n",
      "        [0.0733],\n",
      "        [0.0745],\n",
      "        [0.0749],\n",
      "        [0.0781],\n",
      "        [0.0767],\n",
      "        [0.0780],\n",
      "        [0.0805],\n",
      "        [0.0777],\n",
      "        [0.0795],\n",
      "        [0.0814],\n",
      "        [0.0869],\n",
      "        [0.0962],\n",
      "        [0.0982],\n",
      "        [0.0952],\n",
      "        [0.0982],\n",
      "        [0.1015],\n",
      "        [0.1034],\n",
      "        [0.1028],\n",
      "        [0.1128],\n",
      "        [0.1121],\n",
      "        [0.1145],\n",
      "        [0.1430]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 99.14683389663696\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.2417353900673334e-07, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [3, 30, 25, 78, 58, 31, 26, 151, 112, 6, 53, 157, 155, 40, 131, 27, 18, 38, 120, 104, 59, 128, 130, 55, 11, 127, 110, 23, 12, 8, 66, 107, 70, 28, 54, 156, 69, 65, 15, 32, 33, 39, 16, 108, 22, 52, 109, 64, 147, 29, 154, 129, 140, 121, 136, 126, 21, 105, 145, 132, 7, 148, 158, 24, 111, 34, 14, 94, 19, 82, 79, 35, 152, 103, 146, 149, 93, 95, 150, 106, 43, 81, 67, 122, 48, 134, 10, 77, 87, 133, 57, 125, 20, 119, 141, 2, 144, 124, 56, 135, 17, 76, 9, 117, 86, 63, 92, 102, 113, 137, 49, 100, 37, 36, 116, 123, 118, 115, 114, 13, 143, 80, 99, 68, 142, 41, 60, 101, 153, 51, 91, 84, 98, 4, 90, 5, 61, 96, 138, 71, 97, 42, 74, 62, 85, 75, 44, 88, 47, 50, 45, 72, 83, 139, 89, 73, 46, 1] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4949],\n",
      "        [0.5572],\n",
      "        [0.6252],\n",
      "        [0.2358],\n",
      "        [0.3609],\n",
      "        [0.5451],\n",
      "        [0.6113],\n",
      "        [0.4951],\n",
      "        [0.0989],\n",
      "        [0.5457],\n",
      "        [0.3129],\n",
      "        [0.5051],\n",
      "        [0.5195],\n",
      "        [0.4681],\n",
      "        [0.0989],\n",
      "        [0.5971],\n",
      "        [0.6490],\n",
      "        [0.4653],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3991],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3493],\n",
      "        [0.6285],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6053],\n",
      "        [0.6218],\n",
      "        [0.5962],\n",
      "        [0.3260],\n",
      "        [0.0989],\n",
      "        [0.2747],\n",
      "        [0.5718],\n",
      "        [0.3449],\n",
      "        [0.5058],\n",
      "        [0.2843],\n",
      "        [0.3681],\n",
      "        [0.6441],\n",
      "        [0.5123],\n",
      "        [0.5263],\n",
      "        [0.4723],\n",
      "        [0.6618],\n",
      "        [0.0989],\n",
      "        [0.6228],\n",
      "        [0.2919],\n",
      "        [0.0989],\n",
      "        [0.3808],\n",
      "        [0.4436],\n",
      "        [0.5785],\n",
      "        [0.5232],\n",
      "        [0.0989],\n",
      "        [0.4274],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6490],\n",
      "        [0.0989],\n",
      "        [0.4347],\n",
      "        [0.0989],\n",
      "        [0.5899],\n",
      "        [0.4469],\n",
      "        [0.5091],\n",
      "        [0.6229],\n",
      "        [0.0989],\n",
      "        [0.5142],\n",
      "        [0.6034],\n",
      "        [0.0989],\n",
      "        [0.6450],\n",
      "        [0.1650],\n",
      "        [0.2292],\n",
      "        [0.5091],\n",
      "        [0.5135],\n",
      "        [0.0989],\n",
      "        [0.4276],\n",
      "        [0.4553],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4687],\n",
      "        [0.0989],\n",
      "        [0.4089],\n",
      "        [0.1914],\n",
      "        [0.2788],\n",
      "        [0.0989],\n",
      "        [0.3012],\n",
      "        [0.0989],\n",
      "        [0.5829],\n",
      "        [0.2257],\n",
      "        [0.1523],\n",
      "        [0.0989],\n",
      "        [0.3243],\n",
      "        [0.0989],\n",
      "        [0.6680],\n",
      "        [0.0989],\n",
      "        [0.5025],\n",
      "        [0.4925],\n",
      "        [0.4457],\n",
      "        [0.0989],\n",
      "        [0.3204],\n",
      "        [0.0989],\n",
      "        [0.6301],\n",
      "        [0.2257],\n",
      "        [0.5818],\n",
      "        [0.0989],\n",
      "        [0.1183],\n",
      "        [0.3551],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2453],\n",
      "        [0.0989],\n",
      "        [0.4745],\n",
      "        [0.4836],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6183],\n",
      "        [0.4434],\n",
      "        [0.2529],\n",
      "        [0.0989],\n",
      "        [0.2684],\n",
      "        [0.4386],\n",
      "        [0.4140],\n",
      "        [0.3928],\n",
      "        [0.0989],\n",
      "        [0.5090],\n",
      "        [0.2578],\n",
      "        [0.0989],\n",
      "        [0.1255],\n",
      "        [0.0989],\n",
      "        [0.4416],\n",
      "        [0.0989],\n",
      "        [0.4532],\n",
      "        [0.3777],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2978],\n",
      "        [0.0989],\n",
      "        [0.3864],\n",
      "        [0.2671],\n",
      "        [0.3560],\n",
      "        [0.0989],\n",
      "        [0.2678],\n",
      "        [0.4215],\n",
      "        [0.1275],\n",
      "        [0.3631],\n",
      "        [0.2045],\n",
      "        [0.3883],\n",
      "        [0.2937],\n",
      "        [0.1144],\n",
      "        [0.4218],\n",
      "        [0.1128],\n",
      "        [0.2772],\n",
      "        [0.3704],\n",
      "        [0.5759]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0013],\n",
      "        [0.0026],\n",
      "        [0.0022],\n",
      "        [0.0026],\n",
      "        [0.0028],\n",
      "        [0.0040],\n",
      "        [0.0035],\n",
      "        [0.0036],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0056],\n",
      "        [0.0056],\n",
      "        [0.0059],\n",
      "        [0.0058],\n",
      "        [0.0064],\n",
      "        [0.0060],\n",
      "        [0.0056],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0113],\n",
      "        [0.0110],\n",
      "        [0.0111],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0132],\n",
      "        [0.0134],\n",
      "        [0.0138],\n",
      "        [0.0135],\n",
      "        [0.0141],\n",
      "        [0.0143],\n",
      "        [0.0150],\n",
      "        [0.0149],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0173],\n",
      "        [0.0175],\n",
      "        [0.0181],\n",
      "        [0.0180],\n",
      "        [0.0183],\n",
      "        [0.0191],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0203],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0206],\n",
      "        [0.0213],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0228],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0249],\n",
      "        [0.0260],\n",
      "        [0.0277],\n",
      "        [0.0280],\n",
      "        [0.0283],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0326],\n",
      "        [0.0338],\n",
      "        [0.0337],\n",
      "        [0.0341],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0346],\n",
      "        [0.0351],\n",
      "        [0.0354],\n",
      "        [0.0357],\n",
      "        [0.0374],\n",
      "        [0.0375],\n",
      "        [0.0383],\n",
      "        [0.0394],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0439],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0456],\n",
      "        [0.0477],\n",
      "        [0.0486],\n",
      "        [0.0493],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0545],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0569],\n",
      "        [0.0572],\n",
      "        [0.0579],\n",
      "        [0.0583],\n",
      "        [0.0593],\n",
      "        [0.0606],\n",
      "        [0.0614],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0697],\n",
      "        [0.0718],\n",
      "        [0.0733],\n",
      "        [0.0745],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0777],\n",
      "        [0.0780],\n",
      "        [0.0781],\n",
      "        [0.0795],\n",
      "        [0.0805],\n",
      "        [0.0814],\n",
      "        [0.0869],\n",
      "        [0.0952],\n",
      "        [0.0962],\n",
      "        [0.0982],\n",
      "        [0.0982],\n",
      "        [0.1015],\n",
      "        [0.1028],\n",
      "        [0.1034],\n",
      "        [0.1121],\n",
      "        [0.1128],\n",
      "        [0.1145],\n",
      "        [0.1430],\n",
      "        [0.1522]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 11\n",
      "Number of shrink: 13\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0037],\n",
      "        [0.0007],\n",
      "        [0.0047],\n",
      "        [0.0022],\n",
      "        [0.0036],\n",
      "        [0.0050],\n",
      "        [0.0019],\n",
      "        [0.0037],\n",
      "        [0.0036],\n",
      "        [0.0087],\n",
      "        [0.0050],\n",
      "        [0.0059],\n",
      "        [0.0054],\n",
      "        [0.0076],\n",
      "        [0.0058],\n",
      "        [0.0042],\n",
      "        [0.0036],\n",
      "        [0.0076],\n",
      "        [0.0069],\n",
      "        [0.0079],\n",
      "        [0.0069],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0096],\n",
      "        [0.0068],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0139],\n",
      "        [0.0084],\n",
      "        [0.0080],\n",
      "        [0.0104],\n",
      "        [0.0118],\n",
      "        [0.0113],\n",
      "        [0.0108],\n",
      "        [0.0141],\n",
      "        [0.0129],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0107],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0171],\n",
      "        [0.0180],\n",
      "        [0.0154],\n",
      "        [0.0131],\n",
      "        [0.0168],\n",
      "        [0.0164],\n",
      "        [0.0153],\n",
      "        [0.0172],\n",
      "        [0.0150],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0177],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0177],\n",
      "        [0.0203],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0237],\n",
      "        [0.0214],\n",
      "        [0.0210],\n",
      "        [0.0234],\n",
      "        [0.0221],\n",
      "        [0.0241],\n",
      "        [0.0203],\n",
      "        [0.0233],\n",
      "        [0.0213],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0271],\n",
      "        [0.0262],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0280],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0311],\n",
      "        [0.0316],\n",
      "        [0.0332],\n",
      "        [0.0324],\n",
      "        [0.0338],\n",
      "        [0.0326],\n",
      "        [0.0322],\n",
      "        [0.0337],\n",
      "        [0.0369],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0356],\n",
      "        [0.0345],\n",
      "        [0.0321],\n",
      "        [0.0351],\n",
      "        [0.0340],\n",
      "        [0.0327],\n",
      "        [0.0378],\n",
      "        [0.0375],\n",
      "        [0.0395],\n",
      "        [0.0394],\n",
      "        [0.0445],\n",
      "        [0.0417],\n",
      "        [0.0449],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0450],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0477],\n",
      "        [0.0505],\n",
      "        [0.0511],\n",
      "        [0.0510],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0521],\n",
      "        [0.0557],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0584],\n",
      "        [0.0576],\n",
      "        [0.0597],\n",
      "        [0.0595],\n",
      "        [0.0593],\n",
      "        [0.0607],\n",
      "        [0.0624],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0725],\n",
      "        [0.0718],\n",
      "        [0.0759],\n",
      "        [0.0756],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0765],\n",
      "        [0.0779],\n",
      "        [0.0798],\n",
      "        [0.0779],\n",
      "        [0.0815],\n",
      "        [0.0814],\n",
      "        [0.0857],\n",
      "        [0.0939],\n",
      "        [0.0962],\n",
      "        [0.0967],\n",
      "        [0.0994],\n",
      "        [0.1000],\n",
      "        [0.1014],\n",
      "        [0.1034],\n",
      "        [0.1113],\n",
      "        [0.1128],\n",
      "        [0.1131],\n",
      "        [0.1415],\n",
      "        [0.1487]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 99.28631687164307\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.023381509592582e-07, 30)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [30, 26, 78, 3, 112, 151, 58, 18, 27, 31, 25, 53, 155, 157, 131, 120, 59, 11, 40, 38, 104, 128, 12, 8, 130, 6, 55, 127, 110, 66, 28, 15, 70, 107, 69, 65, 156, 22, 54, 23, 64, 108, 29, 32, 33, 109, 52, 39, 147, 16, 21, 140, 129, 154, 121, 136, 14, 126, 105, 145, 132, 19, 158, 148, 111, 94, 24, 7, 82, 34, 79, 152, 35, 146, 103, 149, 93, 95, 150, 106, 81, 48, 20, 2, 122, 43, 134, 67, 77, 141, 87, 133, 125, 119, 57, 10, 124, 144, 135, 56, 76, 117, 86, 92, 17, 102, 113, 9, 63, 137, 49, 100, 37, 116, 36, 123, 13, 118, 115, 114, 80, 143, 99, 142, 68, 101, 41, 60, 153, 51, 91, 84, 98, 90, 4, 96, 61, 5, 71, 138, 97, 74, 42, 85, 62, 75, 44, 88, 47, 50, 45, 72, 83, 139, 89, 73, 46, 1, 0] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5552],\n",
      "        [0.6092],\n",
      "        [0.2358],\n",
      "        [0.4917],\n",
      "        [0.0989],\n",
      "        [0.4953],\n",
      "        [0.3599],\n",
      "        [0.6465],\n",
      "        [0.5948],\n",
      "        [0.5429],\n",
      "        [0.6231],\n",
      "        [0.3121],\n",
      "        [0.5197],\n",
      "        [0.5054],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3980],\n",
      "        [0.6255],\n",
      "        [0.4664],\n",
      "        [0.4634],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6192],\n",
      "        [0.5931],\n",
      "        [0.0989],\n",
      "        [0.5428],\n",
      "        [0.3484],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3246],\n",
      "        [0.5698],\n",
      "        [0.6413],\n",
      "        [0.2735],\n",
      "        [0.0989],\n",
      "        [0.2828],\n",
      "        [0.3669],\n",
      "        [0.5055],\n",
      "        [0.6201],\n",
      "        [0.3438],\n",
      "        [0.6027],\n",
      "        [0.3797],\n",
      "        [0.0989],\n",
      "        [0.5761],\n",
      "        [0.5107],\n",
      "        [0.5244],\n",
      "        [0.0989],\n",
      "        [0.2910],\n",
      "        [0.4702],\n",
      "        [0.4436],\n",
      "        [0.6587],\n",
      "        [0.6464],\n",
      "        [0.4269],\n",
      "        [0.0989],\n",
      "        [0.5233],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6009],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4343],\n",
      "        [0.0989],\n",
      "        [0.6429],\n",
      "        [0.5092],\n",
      "        [0.4468],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6205],\n",
      "        [0.5868],\n",
      "        [0.1650],\n",
      "        [0.5125],\n",
      "        [0.2292],\n",
      "        [0.5137],\n",
      "        [0.5069],\n",
      "        [0.4276],\n",
      "        [0.0989],\n",
      "        [0.4556],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4692],\n",
      "        [0.0989],\n",
      "        [0.1914],\n",
      "        [0.2997],\n",
      "        [0.6654],\n",
      "        [0.4896],\n",
      "        [0.0989],\n",
      "        [0.4072],\n",
      "        [0.0989],\n",
      "        [0.2776],\n",
      "        [0.2257],\n",
      "        [0.5012],\n",
      "        [0.1522],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3230],\n",
      "        [0.5802],\n",
      "        [0.0989],\n",
      "        [0.4453],\n",
      "        [0.0989],\n",
      "        [0.3192],\n",
      "        [0.2257],\n",
      "        [0.0989],\n",
      "        [0.1183],\n",
      "        [0.0989],\n",
      "        [0.6272],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5790],\n",
      "        [0.3539],\n",
      "        [0.0989],\n",
      "        [0.2440],\n",
      "        [0.0989],\n",
      "        [0.4725],\n",
      "        [0.0989],\n",
      "        [0.4818],\n",
      "        [0.0989],\n",
      "        [0.6158],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2529],\n",
      "        [0.4428],\n",
      "        [0.0989],\n",
      "        [0.4382],\n",
      "        [0.2669],\n",
      "        [0.0989],\n",
      "        [0.4122],\n",
      "        [0.3915],\n",
      "        [0.5089],\n",
      "        [0.2568],\n",
      "        [0.0989],\n",
      "        [0.1254],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4388],\n",
      "        [0.0989],\n",
      "        [0.3766],\n",
      "        [0.4506],\n",
      "        [0.2966],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2655],\n",
      "        [0.3848],\n",
      "        [0.0989],\n",
      "        [0.3550],\n",
      "        [0.2665],\n",
      "        [0.4202],\n",
      "        [0.1274],\n",
      "        [0.3616],\n",
      "        [0.2033],\n",
      "        [0.3868],\n",
      "        [0.2923],\n",
      "        [0.1144],\n",
      "        [0.4210],\n",
      "        [0.1128],\n",
      "        [0.2758],\n",
      "        [0.3689],\n",
      "        [0.5724],\n",
      "        [0.5793]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0037],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0036],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0047],\n",
      "        [0.0050],\n",
      "        [0.0054],\n",
      "        [0.0059],\n",
      "        [0.0058],\n",
      "        [0.0069],\n",
      "        [0.0069],\n",
      "        [0.0068],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0084],\n",
      "        [0.0080],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0104],\n",
      "        [0.0108],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0131],\n",
      "        [0.0141],\n",
      "        [0.0139],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0150],\n",
      "        [0.0158],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0168],\n",
      "        [0.0171],\n",
      "        [0.0172],\n",
      "        [0.0180],\n",
      "        [0.0177],\n",
      "        [0.0177],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0203],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0210],\n",
      "        [0.0214],\n",
      "        [0.0221],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0241],\n",
      "        [0.0254],\n",
      "        [0.0262],\n",
      "        [0.0271],\n",
      "        [0.0279],\n",
      "        [0.0277],\n",
      "        [0.0280],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0311],\n",
      "        [0.0316],\n",
      "        [0.0324],\n",
      "        [0.0322],\n",
      "        [0.0321],\n",
      "        [0.0327],\n",
      "        [0.0326],\n",
      "        [0.0332],\n",
      "        [0.0337],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0356],\n",
      "        [0.0369],\n",
      "        [0.0375],\n",
      "        [0.0378],\n",
      "        [0.0394],\n",
      "        [0.0395],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0445],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0477],\n",
      "        [0.0505],\n",
      "        [0.0510],\n",
      "        [0.0511],\n",
      "        [0.0515],\n",
      "        [0.0521],\n",
      "        [0.0525],\n",
      "        [0.0543],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0557],\n",
      "        [0.0564],\n",
      "        [0.0576],\n",
      "        [0.0584],\n",
      "        [0.0593],\n",
      "        [0.0597],\n",
      "        [0.0595],\n",
      "        [0.0607],\n",
      "        [0.0624],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0725],\n",
      "        [0.0749],\n",
      "        [0.0756],\n",
      "        [0.0759],\n",
      "        [0.0765],\n",
      "        [0.0767],\n",
      "        [0.0779],\n",
      "        [0.0779],\n",
      "        [0.0798],\n",
      "        [0.0814],\n",
      "        [0.0815],\n",
      "        [0.0857],\n",
      "        [0.0939],\n",
      "        [0.0962],\n",
      "        [0.0967],\n",
      "        [0.0994],\n",
      "        [0.1000],\n",
      "        [0.1014],\n",
      "        [0.1034],\n",
      "        [0.1113],\n",
      "        [0.1128],\n",
      "        [0.1131],\n",
      "        [0.1415],\n",
      "        [0.1487],\n",
      "        [0.1588]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 28\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0030],\n",
      "        [0.0022],\n",
      "        [0.0089],\n",
      "        [0.0036],\n",
      "        [0.0083],\n",
      "        [0.0039],\n",
      "        [0.0020],\n",
      "        [0.0053],\n",
      "        [0.0031],\n",
      "        [0.0040],\n",
      "        [0.0063],\n",
      "        [0.0012],\n",
      "        [0.0105],\n",
      "        [0.0058],\n",
      "        [0.0069],\n",
      "        [0.0064],\n",
      "        [0.0048],\n",
      "        [0.0065],\n",
      "        [0.0073],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0068],\n",
      "        [0.0044],\n",
      "        [0.0083],\n",
      "        [0.0124],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0122],\n",
      "        [0.0082],\n",
      "        [0.0099],\n",
      "        [0.0118],\n",
      "        [0.0107],\n",
      "        [0.0121],\n",
      "        [0.0176],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0154],\n",
      "        [0.0164],\n",
      "        [0.0135],\n",
      "        [0.0146],\n",
      "        [0.0164],\n",
      "        [0.0156],\n",
      "        [0.0163],\n",
      "        [0.0211],\n",
      "        [0.0197],\n",
      "        [0.0170],\n",
      "        [0.0184],\n",
      "        [0.0180],\n",
      "        [0.0147],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0179],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0224],\n",
      "        [0.0207],\n",
      "        [0.0202],\n",
      "        [0.0260],\n",
      "        [0.0162],\n",
      "        [0.0221],\n",
      "        [0.0233],\n",
      "        [0.0235],\n",
      "        [0.0275],\n",
      "        [0.0242],\n",
      "        [0.0232],\n",
      "        [0.0254],\n",
      "        [0.0300],\n",
      "        [0.0266],\n",
      "        [0.0309],\n",
      "        [0.0277],\n",
      "        [0.0234],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0264],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0330],\n",
      "        [0.0312],\n",
      "        [0.0269],\n",
      "        [0.0326],\n",
      "        [0.0314],\n",
      "        [0.0337],\n",
      "        [0.0335],\n",
      "        [0.0340],\n",
      "        [0.0320],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0357],\n",
      "        [0.0399],\n",
      "        [0.0375],\n",
      "        [0.0358],\n",
      "        [0.0394],\n",
      "        [0.0396],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0460],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0486],\n",
      "        [0.0460],\n",
      "        [0.0455],\n",
      "        [0.0460],\n",
      "        [0.0476],\n",
      "        [0.0498],\n",
      "        [0.0510],\n",
      "        [0.0507],\n",
      "        [0.0515],\n",
      "        [0.0502],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0544],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0585],\n",
      "        [0.0593],\n",
      "        [0.0578],\n",
      "        [0.0603],\n",
      "        [0.0571],\n",
      "        [0.0615],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0774],\n",
      "        [0.0749],\n",
      "        [0.0766],\n",
      "        [0.0801],\n",
      "        [0.0747],\n",
      "        [0.0767],\n",
      "        [0.0779],\n",
      "        [0.0760],\n",
      "        [0.0781],\n",
      "        [0.0814],\n",
      "        [0.0825],\n",
      "        [0.0835],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.0982],\n",
      "        [0.1018],\n",
      "        [0.0999],\n",
      "        [0.1034],\n",
      "        [0.1115],\n",
      "        [0.1127],\n",
      "        [0.1109],\n",
      "        [0.1431],\n",
      "        [0.1426],\n",
      "        [0.1533]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 99.47718787193298\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 30 個區塊累積花費時間(s) 0.649247407913208\n",
      "<<The performance of 30 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.649247407913208\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 974.69\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1261.70\n",
      "The accuracy(2000) for l = 1: 89.31%\n",
      "The accuracy(3000) for l = 1: 98.11%\n",
      "The maximum error: tensor(3913.4062)\n",
      "The minimum error: tensor(24.7188)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 928.9\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1015.8\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1296.26953125\n",
      "The minimum error: 264.23046875\n",
      "------------------------------------------------------------\n",
      "0.8930817610062893\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<31>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.374458045385836e-07, 26)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [26, 151, 14, 74, 22, 27, 108, 54, 7, 21, 4, 23, 127, 8, 49, 55, 36, 116, 34, 11, 100, 124, 147, 126, 51, 123, 66, 106, 62, 156, 153, 65, 103, 61, 2, 24, 18, 50, 28, 19, 150, 60, 29, 104, 48, 25, 144, 105, 35, 17, 152, 10, 125, 136, 117, 132, 12, 122, 101, 15, 128, 143, 141, 107, 145, 90, 30, 20, 78, 75, 31, 154, 146, 3, 99, 89, 148, 91, 142, 16, 39, 102, 137, 77, 118, 63, 44, 130, 73, 83, 129, 121, 115, 53, 155, 140, 120, 131, 52, 6, 72, 113, 82, 88, 98, 109, 133, 13, 59, 45, 96, 5, 157, 33, 9, 158, 112, 32, 119, 114, 111, 110, 139, 76, 95, 138, 149, 37, 64, 97, 56, 47, 87, 80, 94, 86, 67, 92, 70, 57, 134, 0, 93, 38, 1, 81, 58, 71, 40, 84, 43, 46, 68, 41, 79]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0012],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0048],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0068],\n",
      "        [0.0063],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0082],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0124],\n",
      "        [0.0122],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0147],\n",
      "        [0.0146],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0164],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0170],\n",
      "        [0.0176],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0211],\n",
      "        [0.0224],\n",
      "        [0.0221],\n",
      "        [0.0234],\n",
      "        [0.0233],\n",
      "        [0.0232],\n",
      "        [0.0235],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0260],\n",
      "        [0.0264],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0335],\n",
      "        [0.0330],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0357],\n",
      "        [0.0355],\n",
      "        [0.0358],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0460],\n",
      "        [0.0460],\n",
      "        [0.0460],\n",
      "        [0.0476],\n",
      "        [0.0486],\n",
      "        [0.0480],\n",
      "        [0.0498],\n",
      "        [0.0502],\n",
      "        [0.0506],\n",
      "        [0.0510],\n",
      "        [0.0507],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0544],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0571],\n",
      "        [0.0578],\n",
      "        [0.0585],\n",
      "        [0.0593],\n",
      "        [0.0603],\n",
      "        [0.0615],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0747],\n",
      "        [0.0749],\n",
      "        [0.0760],\n",
      "        [0.0766],\n",
      "        [0.0767],\n",
      "        [0.0774],\n",
      "        [0.0779],\n",
      "        [0.0781],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0825],\n",
      "        [0.0835],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.0982],\n",
      "        [0.0999],\n",
      "        [0.1018],\n",
      "        [0.1034]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.374458045385836e-07, 26)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [26, 151, 14, 74, 22, 27, 108, 54, 7, 21, 4, 23, 127, 8, 49, 55, 36, 116, 34, 11, 100, 124, 147, 126, 51, 123, 66, 106, 62, 156, 153, 65, 103, 61, 2, 24, 18, 50, 28, 19, 150, 60, 29, 104, 48, 25, 144, 105, 35, 17, 152, 10, 125, 136, 117, 132, 12, 122, 101, 15, 128, 143, 141, 107, 145, 90, 30, 20, 78, 75, 31, 154, 146, 3, 99, 89, 148, 91, 142, 16, 39, 102, 137, 77, 118, 63, 44, 130, 73, 83, 129, 121, 115, 53, 155, 140, 120, 131, 52, 6, 72, 113, 82, 88, 98, 109, 133, 13, 59, 45, 96, 5, 157, 33, 9, 158, 112, 32, 119, 114, 111, 110, 139, 76, 95, 138, 149, 37, 64, 97, 56, 47, 87, 80, 94, 86, 67, 92, 70, 57, 134, 0, 93, 38, 1, 81, 58, 71, 40, 84, 43, 46, 68, 41, 79, 69] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5568],\n",
      "        [0.5239],\n",
      "        [0.6449],\n",
      "        [0.2358],\n",
      "        [0.6103],\n",
      "        [0.5448],\n",
      "        [0.0989],\n",
      "        [0.3595],\n",
      "        [0.6235],\n",
      "        [0.6238],\n",
      "        [0.5895],\n",
      "        [0.5959],\n",
      "        [0.0989],\n",
      "        [0.6175],\n",
      "        [0.3134],\n",
      "        [0.3975],\n",
      "        [0.4675],\n",
      "        [0.0989],\n",
      "        [0.4637],\n",
      "        [0.6387],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5000],\n",
      "        [0.0989],\n",
      "        [0.3486],\n",
      "        [0.0989],\n",
      "        [0.2721],\n",
      "        [0.0989],\n",
      "        [0.3247],\n",
      "        [0.5161],\n",
      "        [0.5100],\n",
      "        [0.2816],\n",
      "        [0.0989],\n",
      "        [0.3663],\n",
      "        [0.5390],\n",
      "        [0.5712],\n",
      "        [0.6198],\n",
      "        [0.3448],\n",
      "        [0.5129],\n",
      "        [0.6025],\n",
      "        [0.5266],\n",
      "        [0.3792],\n",
      "        [0.5260],\n",
      "        [0.0989],\n",
      "        [0.2922],\n",
      "        [0.5775],\n",
      "        [0.4520],\n",
      "        [0.0989],\n",
      "        [0.4710],\n",
      "        [0.6456],\n",
      "        [0.5102],\n",
      "        [0.5986],\n",
      "        [0.0989],\n",
      "        [0.4275],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6570],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6418],\n",
      "        [0.0989],\n",
      "        [0.4475],\n",
      "        [0.4366],\n",
      "        [0.0989],\n",
      "        [0.4602],\n",
      "        [0.0989],\n",
      "        [0.5135],\n",
      "        [0.6204],\n",
      "        [0.1649],\n",
      "        [0.2292],\n",
      "        [0.5074],\n",
      "        [0.5143],\n",
      "        [0.4739],\n",
      "        [0.5830],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5174],\n",
      "        [0.0989],\n",
      "        [0.4306],\n",
      "        [0.6645],\n",
      "        [0.4091],\n",
      "        [0.0989],\n",
      "        [0.4992],\n",
      "        [0.1914],\n",
      "        [0.0989],\n",
      "        [0.2779],\n",
      "        [0.3005],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3229],\n",
      "        [0.5209],\n",
      "        [0.4473],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3191],\n",
      "        [0.5771],\n",
      "        [0.2257],\n",
      "        [0.0989],\n",
      "        [0.1183],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.6257],\n",
      "        [0.3530],\n",
      "        [0.2450],\n",
      "        [0.0989],\n",
      "        [0.5752],\n",
      "        [0.5179],\n",
      "        [0.4732],\n",
      "        [0.6140],\n",
      "        [0.5045],\n",
      "        [0.0989],\n",
      "        [0.4823],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4441],\n",
      "        [0.2529],\n",
      "        [0.0989],\n",
      "        [0.4393],\n",
      "        [0.5125],\n",
      "        [0.4141],\n",
      "        [0.2667],\n",
      "        [0.0989],\n",
      "        [0.3908],\n",
      "        [0.2577],\n",
      "        [0.0989],\n",
      "        [0.1254],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2948],\n",
      "        [0.0989],\n",
      "        [0.2636],\n",
      "        [0.3756],\n",
      "        [0.0989],\n",
      "        [0.4339],\n",
      "        [0.0989],\n",
      "        [0.3865],\n",
      "        [0.4464],\n",
      "        [0.0989],\n",
      "        [0.3540],\n",
      "        [0.2643],\n",
      "        [0.4224],\n",
      "        [0.1274],\n",
      "        [0.3627],\n",
      "        [0.2045],\n",
      "        [0.2908],\n",
      "        [0.3886],\n",
      "        [0.1144],\n",
      "        [0.2736]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0010],\n",
      "        [0.0012],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0031],\n",
      "        [0.0036],\n",
      "        [0.0039],\n",
      "        [0.0048],\n",
      "        [0.0040],\n",
      "        [0.0044],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0068],\n",
      "        [0.0063],\n",
      "        [0.0064],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0082],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0083],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0107],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0124],\n",
      "        [0.0122],\n",
      "        [0.0128],\n",
      "        [0.0130],\n",
      "        [0.0135],\n",
      "        [0.0140],\n",
      "        [0.0147],\n",
      "        [0.0147],\n",
      "        [0.0146],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0164],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0170],\n",
      "        [0.0176],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0202],\n",
      "        [0.0207],\n",
      "        [0.0211],\n",
      "        [0.0224],\n",
      "        [0.0221],\n",
      "        [0.0234],\n",
      "        [0.0233],\n",
      "        [0.0232],\n",
      "        [0.0235],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0260],\n",
      "        [0.0264],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0312],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0320],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0335],\n",
      "        [0.0330],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0351],\n",
      "        [0.0357],\n",
      "        [0.0355],\n",
      "        [0.0358],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0460],\n",
      "        [0.0460],\n",
      "        [0.0460],\n",
      "        [0.0476],\n",
      "        [0.0486],\n",
      "        [0.0480],\n",
      "        [0.0498],\n",
      "        [0.0502],\n",
      "        [0.0506],\n",
      "        [0.0510],\n",
      "        [0.0507],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0544],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0571],\n",
      "        [0.0578],\n",
      "        [0.0585],\n",
      "        [0.0593],\n",
      "        [0.0603],\n",
      "        [0.0615],\n",
      "        [0.0660],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0747],\n",
      "        [0.0749],\n",
      "        [0.0760],\n",
      "        [0.0766],\n",
      "        [0.0767],\n",
      "        [0.0774],\n",
      "        [0.0779],\n",
      "        [0.0781],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0825],\n",
      "        [0.0835],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.0982],\n",
      "        [0.0999],\n",
      "        [0.1018],\n",
      "        [0.1034],\n",
      "        [0.1109]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 17\n",
      "Number of shrink: 16\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0089],\n",
      "        [0.0031],\n",
      "        [0.0096],\n",
      "        [0.0022],\n",
      "        [0.0110],\n",
      "        [0.0054],\n",
      "        [0.0037],\n",
      "        [0.0009],\n",
      "        [0.0125],\n",
      "        [0.0039],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0057],\n",
      "        [0.0145],\n",
      "        [0.0114],\n",
      "        [0.0106],\n",
      "        [0.0003],\n",
      "        [0.0069],\n",
      "        [0.0006],\n",
      "        [0.0162],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0067],\n",
      "        [0.0083],\n",
      "        [0.0042],\n",
      "        [0.0098],\n",
      "        [0.0150],\n",
      "        [0.0100],\n",
      "        [0.0165],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0165],\n",
      "        [0.0118],\n",
      "        [0.0174],\n",
      "        [0.0034],\n",
      "        [0.0205],\n",
      "        [0.0206],\n",
      "        [0.0077],\n",
      "        [0.0058],\n",
      "        [0.0056],\n",
      "        [0.0170],\n",
      "        [0.0199],\n",
      "        [0.0071],\n",
      "        [0.0154],\n",
      "        [0.0102],\n",
      "        [0.0249],\n",
      "        [0.0172],\n",
      "        [0.0164],\n",
      "        [0.0097],\n",
      "        [0.0247],\n",
      "        [0.0165],\n",
      "        [0.0256],\n",
      "        [0.0180],\n",
      "        [0.0188],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0118],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0276],\n",
      "        [0.0207],\n",
      "        [0.0200],\n",
      "        [0.0226],\n",
      "        [0.0221],\n",
      "        [0.0247],\n",
      "        [0.0233],\n",
      "        [0.0163],\n",
      "        [0.0149],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0196],\n",
      "        [0.0248],\n",
      "        [0.0280],\n",
      "        [0.0187],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0278],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0389],\n",
      "        [0.0248],\n",
      "        [0.0316],\n",
      "        [0.0309],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0272],\n",
      "        [0.0386],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0308],\n",
      "        [0.0340],\n",
      "        [0.0363],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0347],\n",
      "        [0.0318],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0384],\n",
      "        [0.0407],\n",
      "        [0.0406],\n",
      "        [0.0476],\n",
      "        [0.0405],\n",
      "        [0.0464],\n",
      "        [0.0432],\n",
      "        [0.0584],\n",
      "        [0.0488],\n",
      "        [0.0509],\n",
      "        [0.0444],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0566],\n",
      "        [0.0592],\n",
      "        [0.0514],\n",
      "        [0.0523],\n",
      "        [0.0593],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0800],\n",
      "        [0.0749],\n",
      "        [0.0808],\n",
      "        [0.0713],\n",
      "        [0.0767],\n",
      "        [0.0674],\n",
      "        [0.0779],\n",
      "        [0.0715],\n",
      "        [0.0706],\n",
      "        [0.0814],\n",
      "        [0.0772],\n",
      "        [0.0880],\n",
      "        [0.1023],\n",
      "        [0.0961],\n",
      "        [0.1035],\n",
      "        [0.0927],\n",
      "        [0.1051],\n",
      "        [0.1083],\n",
      "        [0.1034],\n",
      "        [0.1160]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 99.87532305717468\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6853240936143266e-07, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 36, 54, 74, 151, 2, 108, 21, 51, 27, 19, 127, 28, 147, 29, 116, 50, 100, 124, 126, 156, 26, 153, 35, 123, 14, 106, 48, 55, 22, 49, 103, 12, 4, 7, 23, 8, 20, 66, 104, 11, 30, 105, 65, 152, 62, 150, 144, 61, 125, 3, 136, 31, 117, 132, 122, 60, 143, 101, 18, 24, 128, 107, 141, 90, 78, 17, 145, 154, 39, 25, 75, 10, 63, 15, 99, 148, 146, 89, 91, 53, 142, 137, 102, 6, 77, 118, 130, 73, 155, 83, 129, 121, 52, 115, 140, 120, 16, 44, 13, 131, 5, 45, 59, 72, 113, 33, 82, 88, 32, 98, 109, 133, 157, 96, 158, 112, 37, 119, 64, 114, 111, 110, 139, 76, 56, 47, 95, 138, 9, 149, 97, 87, 80, 94, 0, 1, 57, 38, 86, 92, 134, 58, 93, 67, 70, 81, 71, 46, 84, 40, 79, 43, 68, 41, 135, 85] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4703],\n",
      "        [0.4742],\n",
      "        [0.3644],\n",
      "        [0.2358],\n",
      "        [0.5220],\n",
      "        [0.5480],\n",
      "        [0.0989],\n",
      "        [0.6317],\n",
      "        [0.3538],\n",
      "        [0.5533],\n",
      "        [0.6109],\n",
      "        [0.0989],\n",
      "        [0.5207],\n",
      "        [0.4983],\n",
      "        [0.5335],\n",
      "        [0.0989],\n",
      "        [0.3502],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5147],\n",
      "        [0.5647],\n",
      "        [0.5088],\n",
      "        [0.4776],\n",
      "        [0.0989],\n",
      "        [0.6526],\n",
      "        [0.0989],\n",
      "        [0.2976],\n",
      "        [0.4017],\n",
      "        [0.6182],\n",
      "        [0.3185],\n",
      "        [0.0989],\n",
      "        [0.6649],\n",
      "        [0.5977],\n",
      "        [0.6312],\n",
      "        [0.6040],\n",
      "        [0.6252],\n",
      "        [0.6290],\n",
      "        [0.2772],\n",
      "        [0.0989],\n",
      "        [0.6468],\n",
      "        [0.5204],\n",
      "        [0.0989],\n",
      "        [0.2875],\n",
      "        [0.5090],\n",
      "        [0.3307],\n",
      "        [0.5243],\n",
      "        [0.4510],\n",
      "        [0.3717],\n",
      "        [0.0989],\n",
      "        [0.5918],\n",
      "        [0.4279],\n",
      "        [0.5144],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3843],\n",
      "        [0.4464],\n",
      "        [0.0989],\n",
      "        [0.6276],\n",
      "        [0.5794],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4368],\n",
      "        [0.0989],\n",
      "        [0.1649],\n",
      "        [0.6533],\n",
      "        [0.4589],\n",
      "        [0.5130],\n",
      "        [0.4156],\n",
      "        [0.5860],\n",
      "        [0.2292],\n",
      "        [0.6063],\n",
      "        [0.2842],\n",
      "        [0.6491],\n",
      "        [0.0989],\n",
      "        [0.5153],\n",
      "        [0.4724],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3278],\n",
      "        [0.4305],\n",
      "        [0.4981],\n",
      "        [0.0989],\n",
      "        [0.5853],\n",
      "        [0.1914],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.5194],\n",
      "        [0.1522],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3241],\n",
      "        [0.0989],\n",
      "        [0.4468],\n",
      "        [0.0989],\n",
      "        [0.6722],\n",
      "        [0.3060],\n",
      "        [0.6333],\n",
      "        [0.0989],\n",
      "        [0.5833],\n",
      "        [0.2503],\n",
      "        [0.3583],\n",
      "        [0.2257],\n",
      "        [0.0989],\n",
      "        [0.4798],\n",
      "        [0.1183],\n",
      "        [0.0989],\n",
      "        [0.4886],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5162],\n",
      "        [0.0989],\n",
      "        [0.5027],\n",
      "        [0.0989],\n",
      "        [0.4205],\n",
      "        [0.0989],\n",
      "        [0.2729],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4435],\n",
      "        [0.2529],\n",
      "        [0.3954],\n",
      "        [0.2630],\n",
      "        [0.0989],\n",
      "        [0.4391],\n",
      "        [0.6222],\n",
      "        [0.5104],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.1254],\n",
      "        [0.0989],\n",
      "        [0.4440],\n",
      "        [0.4559],\n",
      "        [0.3809],\n",
      "        [0.3930],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3593],\n",
      "        [0.0989],\n",
      "        [0.3001],\n",
      "        [0.2684],\n",
      "        [0.0989],\n",
      "        [0.2688],\n",
      "        [0.2100],\n",
      "        [0.1274],\n",
      "        [0.4286],\n",
      "        [0.1144],\n",
      "        [0.3684],\n",
      "        [0.2960],\n",
      "        [0.3950],\n",
      "        [0.4218],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0022],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0096],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0106],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0125],\n",
      "        [0.0133],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0180],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0247],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0272],\n",
      "        [0.0276],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0280],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0350],\n",
      "        [0.0363],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0386],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0406],\n",
      "        [0.0407],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0432],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0444],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0464],\n",
      "        [0.0476],\n",
      "        [0.0488],\n",
      "        [0.0509],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0564],\n",
      "        [0.0566],\n",
      "        [0.0584],\n",
      "        [0.0592],\n",
      "        [0.0593],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0674],\n",
      "        [0.0706],\n",
      "        [0.0713],\n",
      "        [0.0715],\n",
      "        [0.0718],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0772],\n",
      "        [0.0779],\n",
      "        [0.0800],\n",
      "        [0.0808],\n",
      "        [0.0814],\n",
      "        [0.0880],\n",
      "        [0.0927],\n",
      "        [0.0961],\n",
      "        [0.1023],\n",
      "        [0.1034],\n",
      "        [0.1035],\n",
      "        [0.1051],\n",
      "        [0.1083],\n",
      "        [0.1121],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 7\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0022],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0096],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0106],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0125],\n",
      "        [0.0133],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0180],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0247],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0272],\n",
      "        [0.0276],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0280],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0350],\n",
      "        [0.0363],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0386],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0406],\n",
      "        [0.0407],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0432],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0444],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0464],\n",
      "        [0.0476],\n",
      "        [0.0488],\n",
      "        [0.0509],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0564],\n",
      "        [0.0566],\n",
      "        [0.0584],\n",
      "        [0.0592],\n",
      "        [0.0593],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0674],\n",
      "        [0.0706],\n",
      "        [0.0713],\n",
      "        [0.0715],\n",
      "        [0.0718],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0772],\n",
      "        [0.0779],\n",
      "        [0.0800],\n",
      "        [0.0808],\n",
      "        [0.0814],\n",
      "        [0.0880],\n",
      "        [0.0927],\n",
      "        [0.0961],\n",
      "        [0.1023],\n",
      "        [0.1034],\n",
      "        [0.1035],\n",
      "        [0.1051],\n",
      "        [0.1083],\n",
      "        [0.1121],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 99.97985816001892\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6853240936143266e-07, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 36, 54, 74, 151, 2, 108, 21, 51, 27, 19, 127, 28, 147, 29, 116, 50, 100, 124, 126, 156, 26, 153, 35, 123, 14, 106, 48, 55, 22, 49, 103, 12, 4, 7, 23, 8, 20, 66, 104, 11, 30, 105, 65, 152, 62, 150, 144, 61, 125, 3, 136, 31, 117, 132, 122, 60, 143, 101, 18, 24, 128, 107, 141, 90, 78, 17, 145, 154, 39, 25, 75, 10, 63, 15, 99, 148, 146, 89, 91, 53, 142, 137, 102, 6, 77, 118, 130, 73, 155, 83, 129, 121, 52, 115, 140, 120, 16, 44, 13, 131, 5, 45, 59, 72, 113, 33, 82, 88, 32, 98, 109, 133, 157, 96, 158, 112, 37, 119, 64, 114, 111, 110, 139, 76, 56, 47, 95, 138, 9, 149, 97, 87, 80, 94, 0, 1, 57, 38, 86, 92, 134, 58, 93, 67, 70, 81, 71, 46, 84, 40, 79, 43, 68, 41, 135, 85, 69] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4703],\n",
      "        [0.4742],\n",
      "        [0.3644],\n",
      "        [0.2358],\n",
      "        [0.5220],\n",
      "        [0.5480],\n",
      "        [0.0989],\n",
      "        [0.6317],\n",
      "        [0.3538],\n",
      "        [0.5533],\n",
      "        [0.6109],\n",
      "        [0.0989],\n",
      "        [0.5207],\n",
      "        [0.4983],\n",
      "        [0.5335],\n",
      "        [0.0989],\n",
      "        [0.3502],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5147],\n",
      "        [0.5647],\n",
      "        [0.5088],\n",
      "        [0.4776],\n",
      "        [0.0989],\n",
      "        [0.6526],\n",
      "        [0.0989],\n",
      "        [0.2976],\n",
      "        [0.4017],\n",
      "        [0.6182],\n",
      "        [0.3185],\n",
      "        [0.0989],\n",
      "        [0.6649],\n",
      "        [0.5977],\n",
      "        [0.6312],\n",
      "        [0.6040],\n",
      "        [0.6252],\n",
      "        [0.6290],\n",
      "        [0.2772],\n",
      "        [0.0989],\n",
      "        [0.6468],\n",
      "        [0.5204],\n",
      "        [0.0989],\n",
      "        [0.2875],\n",
      "        [0.5090],\n",
      "        [0.3307],\n",
      "        [0.5243],\n",
      "        [0.4510],\n",
      "        [0.3717],\n",
      "        [0.0989],\n",
      "        [0.5918],\n",
      "        [0.4279],\n",
      "        [0.5144],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3843],\n",
      "        [0.4464],\n",
      "        [0.0989],\n",
      "        [0.6276],\n",
      "        [0.5794],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4368],\n",
      "        [0.0989],\n",
      "        [0.1649],\n",
      "        [0.6533],\n",
      "        [0.4589],\n",
      "        [0.5130],\n",
      "        [0.4156],\n",
      "        [0.5860],\n",
      "        [0.2292],\n",
      "        [0.6063],\n",
      "        [0.2842],\n",
      "        [0.6491],\n",
      "        [0.0989],\n",
      "        [0.5153],\n",
      "        [0.4724],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3278],\n",
      "        [0.4305],\n",
      "        [0.4981],\n",
      "        [0.0989],\n",
      "        [0.5853],\n",
      "        [0.1914],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.5194],\n",
      "        [0.1522],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3241],\n",
      "        [0.0989],\n",
      "        [0.4468],\n",
      "        [0.0989],\n",
      "        [0.6722],\n",
      "        [0.3060],\n",
      "        [0.6333],\n",
      "        [0.0989],\n",
      "        [0.5833],\n",
      "        [0.2503],\n",
      "        [0.3583],\n",
      "        [0.2257],\n",
      "        [0.0989],\n",
      "        [0.4798],\n",
      "        [0.1183],\n",
      "        [0.0989],\n",
      "        [0.4886],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5162],\n",
      "        [0.0989],\n",
      "        [0.5027],\n",
      "        [0.0989],\n",
      "        [0.4205],\n",
      "        [0.0989],\n",
      "        [0.2729],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4435],\n",
      "        [0.2529],\n",
      "        [0.3954],\n",
      "        [0.2630],\n",
      "        [0.0989],\n",
      "        [0.4391],\n",
      "        [0.6222],\n",
      "        [0.5104],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.1254],\n",
      "        [0.0989],\n",
      "        [0.4440],\n",
      "        [0.4559],\n",
      "        [0.3809],\n",
      "        [0.3930],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3593],\n",
      "        [0.0989],\n",
      "        [0.3001],\n",
      "        [0.2684],\n",
      "        [0.0989],\n",
      "        [0.2688],\n",
      "        [0.2100],\n",
      "        [0.1274],\n",
      "        [0.4286],\n",
      "        [0.1144],\n",
      "        [0.3684],\n",
      "        [0.2960],\n",
      "        [0.3950],\n",
      "        [0.4218],\n",
      "        [0.1127],\n",
      "        [0.2787]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0022],\n",
      "        [0.0031],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0054],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0089],\n",
      "        [0.0092],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0096],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0106],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0126],\n",
      "        [0.0125],\n",
      "        [0.0133],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0165],\n",
      "        [0.0170],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0180],\n",
      "        [0.0187],\n",
      "        [0.0188],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0203],\n",
      "        [0.0206],\n",
      "        [0.0205],\n",
      "        [0.0207],\n",
      "        [0.0221],\n",
      "        [0.0226],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0247],\n",
      "        [0.0247],\n",
      "        [0.0248],\n",
      "        [0.0248],\n",
      "        [0.0249],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0272],\n",
      "        [0.0276],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0280],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0308],\n",
      "        [0.0308],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0347],\n",
      "        [0.0350],\n",
      "        [0.0363],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0386],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0406],\n",
      "        [0.0407],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0432],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0444],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0464],\n",
      "        [0.0476],\n",
      "        [0.0488],\n",
      "        [0.0509],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0562],\n",
      "        [0.0564],\n",
      "        [0.0566],\n",
      "        [0.0584],\n",
      "        [0.0592],\n",
      "        [0.0593],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0674],\n",
      "        [0.0706],\n",
      "        [0.0713],\n",
      "        [0.0715],\n",
      "        [0.0718],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0772],\n",
      "        [0.0779],\n",
      "        [0.0800],\n",
      "        [0.0808],\n",
      "        [0.0814],\n",
      "        [0.0880],\n",
      "        [0.0927],\n",
      "        [0.0961],\n",
      "        [0.1023],\n",
      "        [0.1034],\n",
      "        [0.1035],\n",
      "        [0.1051],\n",
      "        [0.1083],\n",
      "        [0.1121],\n",
      "        [0.1127],\n",
      "        [0.1160]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 4\n",
      "Number of shrink: 9\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0017],\n",
      "        [    0.0009],\n",
      "        [    0.0001],\n",
      "        [    0.0022],\n",
      "        [    0.0059],\n",
      "        [    0.0046],\n",
      "        [    0.0037],\n",
      "        [    0.0026],\n",
      "        [    0.0053],\n",
      "        [    0.0041],\n",
      "        [    0.0065],\n",
      "        [    0.0057],\n",
      "        [    0.0074],\n",
      "        [    0.0038],\n",
      "        [    0.0082],\n",
      "        [    0.0069],\n",
      "        [    0.0090],\n",
      "        [    0.0079],\n",
      "        [    0.0081],\n",
      "        [    0.0083],\n",
      "        [    0.0057],\n",
      "        [    0.0077],\n",
      "        [    0.0064],\n",
      "        [    0.0109],\n",
      "        [    0.0098],\n",
      "        [    0.0084],\n",
      "        [    0.0100],\n",
      "        [    0.0112],\n",
      "        [    0.0094],\n",
      "        [    0.0099],\n",
      "        [    0.0102],\n",
      "        [    0.0118],\n",
      "        [    0.0133],\n",
      "        [    0.0114],\n",
      "        [    0.0113],\n",
      "        [    0.0120],\n",
      "        [    0.0131],\n",
      "        [    0.0163],\n",
      "        [    0.0139],\n",
      "        [    0.0154],\n",
      "        [    0.0149],\n",
      "        [    0.0173],\n",
      "        [    0.0164],\n",
      "        [    0.0153],\n",
      "        [    0.0135],\n",
      "        [    0.0155],\n",
      "        [    0.0198],\n",
      "        [    0.0200],\n",
      "        [    0.0165],\n",
      "        [    0.0180],\n",
      "        [    0.0199],\n",
      "        [    0.0166],\n",
      "        [    0.0207],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0199],\n",
      "        [    0.0191],\n",
      "        [    0.0176],\n",
      "        [    0.0203],\n",
      "        [    0.0190],\n",
      "        [    0.0192],\n",
      "        [    0.0207],\n",
      "        [    0.0221],\n",
      "        [    0.0204],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0230],\n",
      "        [    0.0273],\n",
      "        [    0.0220],\n",
      "        [    0.0260],\n",
      "        [    0.0237],\n",
      "        [    0.0254],\n",
      "        [    0.0243],\n",
      "        [    0.0285],\n",
      "        [    0.0261],\n",
      "        [    0.0277],\n",
      "        [    0.0250],\n",
      "        [    0.0308],\n",
      "        [    0.0297],\n",
      "        [    0.0306],\n",
      "        [    0.0318],\n",
      "        [    0.0283],\n",
      "        [    0.0302],\n",
      "        [    0.0316],\n",
      "        [    0.0332],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0337],\n",
      "        [    0.0340],\n",
      "        [    0.0314],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0357],\n",
      "        [    0.0350],\n",
      "        [    0.0387],\n",
      "        [    0.0375],\n",
      "        [    0.0374],\n",
      "        [    0.0374],\n",
      "        [    0.0397],\n",
      "        [    0.0394],\n",
      "        [    0.0415],\n",
      "        [    0.0415],\n",
      "        [    0.0417],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0442],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0457],\n",
      "        [    0.0449],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0436],\n",
      "        [    0.0476],\n",
      "        [    0.0461],\n",
      "        [    0.0509],\n",
      "        [    0.0524],\n",
      "        [    0.0515],\n",
      "        [    0.0534],\n",
      "        [    0.0525],\n",
      "        [    0.0542],\n",
      "        [    0.0549],\n",
      "        [    0.0575],\n",
      "        [    0.0554],\n",
      "        [    0.0567],\n",
      "        [    0.0572],\n",
      "        [    0.0564],\n",
      "        [    0.0591],\n",
      "        [    0.0571],\n",
      "        [    0.0621],\n",
      "        [    0.0593],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0675],\n",
      "        [    0.0683],\n",
      "        [    0.0716],\n",
      "        [    0.0724],\n",
      "        [    0.0725],\n",
      "        [    0.0718],\n",
      "        [    0.0749],\n",
      "        [    0.0767],\n",
      "        [    0.0781],\n",
      "        [    0.0779],\n",
      "        [    0.0787],\n",
      "        [    0.0797],\n",
      "        [    0.0814],\n",
      "        [    0.0868],\n",
      "        [    0.0937],\n",
      "        [    0.0961],\n",
      "        [    0.1013],\n",
      "        [    0.1034],\n",
      "        [    0.1024],\n",
      "        [    0.1039],\n",
      "        [    0.1073],\n",
      "        [    0.1097],\n",
      "        [    0.1127],\n",
      "        [    0.1148]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 100.09692978858948\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.4663541492154764e-07, 54)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [54, 36, 34, 74, 21, 147, 108, 27, 2, 51, 127, 156, 151, 153, 116, 19, 28, 26, 100, 29, 124, 14, 126, 50, 55, 22, 123, 106, 49, 35, 48, 4, 7, 103, 23, 8, 12, 152, 66, 11, 65, 104, 62, 20, 105, 61, 136, 143, 30, 125, 60, 18, 117, 132, 24, 122, 150, 3, 144, 141, 101, 31, 128, 154, 107, 17, 90, 25, 10, 78, 148, 75, 39, 15, 145, 99, 142, 63, 89, 137, 91, 146, 155, 102, 53, 77, 118, 6, 130, 73, 83, 129, 121, 115, 52, 16, 44, 120, 140, 131, 13, 5, 72, 45, 59, 113, 157, 82, 88, 33, 98, 109, 133, 32, 158, 96, 112, 119, 37, 114, 64, 111, 110, 76, 95, 56, 9, 47, 139, 138, 97, 149, 87, 80, 94, 0, 1, 86, 57, 38, 92, 134, 93, 67, 58, 70, 81, 71, 46, 84, 40, 43, 79, 68, 41, 135, 85, 69, 42] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3634],\n",
      "        [0.4731],\n",
      "        [0.4692],\n",
      "        [0.2358],\n",
      "        [0.6304],\n",
      "        [0.4954],\n",
      "        [0.0989],\n",
      "        [0.5520],\n",
      "        [0.5468],\n",
      "        [0.3527],\n",
      "        [0.0989],\n",
      "        [0.5118],\n",
      "        [0.5191],\n",
      "        [0.5060],\n",
      "        [0.0989],\n",
      "        [0.6100],\n",
      "        [0.5191],\n",
      "        [0.5635],\n",
      "        [0.0989],\n",
      "        [0.5324],\n",
      "        [0.0989],\n",
      "        [0.6513],\n",
      "        [0.0989],\n",
      "        [0.3489],\n",
      "        [0.4005],\n",
      "        [0.6171],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3173],\n",
      "        [0.4763],\n",
      "        [0.2966],\n",
      "        [0.5965],\n",
      "        [0.6300],\n",
      "        [0.0989],\n",
      "        [0.6027],\n",
      "        [0.6239],\n",
      "        [0.6634],\n",
      "        [0.5060],\n",
      "        [0.2761],\n",
      "        [0.6455],\n",
      "        [0.2862],\n",
      "        [0.0989],\n",
      "        [0.3297],\n",
      "        [0.6277],\n",
      "        [0.0989],\n",
      "        [0.3708],\n",
      "        [0.4258],\n",
      "        [0.4439],\n",
      "        [0.5193],\n",
      "        [0.0989],\n",
      "        [0.3835],\n",
      "        [0.6260],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.5782],\n",
      "        [0.0989],\n",
      "        [0.5215],\n",
      "        [0.5906],\n",
      "        [0.4482],\n",
      "        [0.4345],\n",
      "        [0.0989],\n",
      "        [0.5133],\n",
      "        [0.0989],\n",
      "        [0.5102],\n",
      "        [0.0989],\n",
      "        [0.6516],\n",
      "        [0.0989],\n",
      "        [0.5848],\n",
      "        [0.6050],\n",
      "        [0.1649],\n",
      "        [0.5125],\n",
      "        [0.2292],\n",
      "        [0.4145],\n",
      "        [0.6477],\n",
      "        [0.4562],\n",
      "        [0.0989],\n",
      "        [0.4279],\n",
      "        [0.2829],\n",
      "        [0.0989],\n",
      "        [0.4973],\n",
      "        [0.0989],\n",
      "        [0.4695],\n",
      "        [0.5168],\n",
      "        [0.0989],\n",
      "        [0.3267],\n",
      "        [0.1914],\n",
      "        [0.0989],\n",
      "        [0.5839],\n",
      "        [0.0989],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.3230],\n",
      "        [0.6708],\n",
      "        [0.3048],\n",
      "        [0.0989],\n",
      "        [0.4444],\n",
      "        [0.0989],\n",
      "        [0.6320],\n",
      "        [0.5823],\n",
      "        [0.2257],\n",
      "        [0.2494],\n",
      "        [0.3572],\n",
      "        [0.0989],\n",
      "        [0.5135],\n",
      "        [0.1183],\n",
      "        [0.0989],\n",
      "        [0.4788],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4873],\n",
      "        [0.5000],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.4195],\n",
      "        [0.0989],\n",
      "        [0.2718],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2529],\n",
      "        [0.0989],\n",
      "        [0.3944],\n",
      "        [0.6208],\n",
      "        [0.2621],\n",
      "        [0.4410],\n",
      "        [0.4366],\n",
      "        [0.0989],\n",
      "        [0.5075],\n",
      "        [0.0989],\n",
      "        [0.1254],\n",
      "        [0.0989],\n",
      "        [0.4431],\n",
      "        [0.4549],\n",
      "        [0.0989],\n",
      "        [0.3797],\n",
      "        [0.3920],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.0989],\n",
      "        [0.2989],\n",
      "        [0.3584],\n",
      "        [0.2673],\n",
      "        [0.0989],\n",
      "        [0.2676],\n",
      "        [0.2090],\n",
      "        [0.1274],\n",
      "        [0.4276],\n",
      "        [0.3673],\n",
      "        [0.1144],\n",
      "        [0.2948],\n",
      "        [0.3940],\n",
      "        [0.4195],\n",
      "        [0.1127],\n",
      "        [0.2775],\n",
      "        [0.3756]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0009],\n",
      "        [    0.0017],\n",
      "        [    0.0022],\n",
      "        [    0.0026],\n",
      "        [    0.0038],\n",
      "        [    0.0037],\n",
      "        [    0.0041],\n",
      "        [    0.0046],\n",
      "        [    0.0053],\n",
      "        [    0.0057],\n",
      "        [    0.0057],\n",
      "        [    0.0059],\n",
      "        [    0.0064],\n",
      "        [    0.0069],\n",
      "        [    0.0065],\n",
      "        [    0.0074],\n",
      "        [    0.0077],\n",
      "        [    0.0079],\n",
      "        [    0.0082],\n",
      "        [    0.0081],\n",
      "        [    0.0084],\n",
      "        [    0.0083],\n",
      "        [    0.0090],\n",
      "        [    0.0094],\n",
      "        [    0.0099],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0109],\n",
      "        [    0.0112],\n",
      "        [    0.0114],\n",
      "        [    0.0113],\n",
      "        [    0.0118],\n",
      "        [    0.0120],\n",
      "        [    0.0131],\n",
      "        [    0.0133],\n",
      "        [    0.0135],\n",
      "        [    0.0139],\n",
      "        [    0.0149],\n",
      "        [    0.0153],\n",
      "        [    0.0154],\n",
      "        [    0.0155],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0165],\n",
      "        [    0.0166],\n",
      "        [    0.0176],\n",
      "        [    0.0173],\n",
      "        [    0.0180],\n",
      "        [    0.0191],\n",
      "        [    0.0190],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0192],\n",
      "        [    0.0199],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0200],\n",
      "        [    0.0204],\n",
      "        [    0.0203],\n",
      "        [    0.0207],\n",
      "        [    0.0207],\n",
      "        [    0.0220],\n",
      "        [    0.0221],\n",
      "        [    0.0230],\n",
      "        [    0.0233],\n",
      "        [    0.0237],\n",
      "        [    0.0243],\n",
      "        [    0.0242],\n",
      "        [    0.0250],\n",
      "        [    0.0254],\n",
      "        [    0.0260],\n",
      "        [    0.0261],\n",
      "        [    0.0273],\n",
      "        [    0.0277],\n",
      "        [    0.0283],\n",
      "        [    0.0285],\n",
      "        [    0.0297],\n",
      "        [    0.0302],\n",
      "        [    0.0306],\n",
      "        [    0.0308],\n",
      "        [    0.0314],\n",
      "        [    0.0316],\n",
      "        [    0.0318],\n",
      "        [    0.0323],\n",
      "        [    0.0326],\n",
      "        [    0.0332],\n",
      "        [    0.0337],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0357],\n",
      "        [    0.0374],\n",
      "        [    0.0374],\n",
      "        [    0.0375],\n",
      "        [    0.0387],\n",
      "        [    0.0394],\n",
      "        [    0.0397],\n",
      "        [    0.0415],\n",
      "        [    0.0417],\n",
      "        [    0.0415],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0436],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0442],\n",
      "        [    0.0449],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0457],\n",
      "        [    0.0461],\n",
      "        [    0.0476],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0524],\n",
      "        [    0.0525],\n",
      "        [    0.0534],\n",
      "        [    0.0542],\n",
      "        [    0.0549],\n",
      "        [    0.0554],\n",
      "        [    0.0564],\n",
      "        [    0.0567],\n",
      "        [    0.0571],\n",
      "        [    0.0572],\n",
      "        [    0.0575],\n",
      "        [    0.0591],\n",
      "        [    0.0593],\n",
      "        [    0.0621],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0675],\n",
      "        [    0.0683],\n",
      "        [    0.0716],\n",
      "        [    0.0718],\n",
      "        [    0.0724],\n",
      "        [    0.0725],\n",
      "        [    0.0749],\n",
      "        [    0.0767],\n",
      "        [    0.0779],\n",
      "        [    0.0787],\n",
      "        [    0.0781],\n",
      "        [    0.0797],\n",
      "        [    0.0814],\n",
      "        [    0.0868],\n",
      "        [    0.0937],\n",
      "        [    0.0961],\n",
      "        [    0.1013],\n",
      "        [    0.1024],\n",
      "        [    0.1034],\n",
      "        [    0.1039],\n",
      "        [    0.1073],\n",
      "        [    0.1097],\n",
      "        [    0.1127],\n",
      "        [    0.1148],\n",
      "        [    0.1482]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 4\n",
      "Number of shrink: 9\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0016],\n",
      "        [0.0023],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0026],\n",
      "        [0.0037],\n",
      "        [0.0034],\n",
      "        [0.0052],\n",
      "        [0.0060],\n",
      "        [0.0057],\n",
      "        [0.0046],\n",
      "        [0.0071],\n",
      "        [0.0053],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0079],\n",
      "        [0.0070],\n",
      "        [0.0079],\n",
      "        [0.0090],\n",
      "        [0.0081],\n",
      "        [0.0077],\n",
      "        [0.0083],\n",
      "        [0.0097],\n",
      "        [0.0087],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0095],\n",
      "        [0.0116],\n",
      "        [0.0119],\n",
      "        [0.0109],\n",
      "        [0.0107],\n",
      "        [0.0118],\n",
      "        [0.0113],\n",
      "        [0.0126],\n",
      "        [0.0140],\n",
      "        [0.0125],\n",
      "        [0.0133],\n",
      "        [0.0143],\n",
      "        [0.0146],\n",
      "        [0.0154],\n",
      "        [0.0148],\n",
      "        [0.0169],\n",
      "        [0.0164],\n",
      "        [0.0158],\n",
      "        [0.0156],\n",
      "        [0.0165],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0186],\n",
      "        [0.0199],\n",
      "        [0.0209],\n",
      "        [0.0204],\n",
      "        [0.0212],\n",
      "        [0.0193],\n",
      "        [0.0203],\n",
      "        [0.0215],\n",
      "        [0.0207],\n",
      "        [0.0209],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0233],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0239],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0254],\n",
      "        [0.0286],\n",
      "        [0.0277],\n",
      "        [0.0271],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0293],\n",
      "        [0.0306],\n",
      "        [0.0318],\n",
      "        [0.0300],\n",
      "        [0.0316],\n",
      "        [0.0325],\n",
      "        [0.0323],\n",
      "        [0.0326],\n",
      "        [0.0338],\n",
      "        [0.0337],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0398],\n",
      "        [0.0394],\n",
      "        [0.0406],\n",
      "        [0.0421],\n",
      "        [0.0417],\n",
      "        [0.0422],\n",
      "        [0.0424],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0450],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0463],\n",
      "        [0.0450],\n",
      "        [0.0476],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0530],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0574],\n",
      "        [0.0565],\n",
      "        [0.0578],\n",
      "        [0.0586],\n",
      "        [0.0602],\n",
      "        [0.0592],\n",
      "        [0.0632],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0688],\n",
      "        [0.0723],\n",
      "        [0.0718],\n",
      "        [0.0731],\n",
      "        [0.0733],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0779],\n",
      "        [0.0780],\n",
      "        [0.0788],\n",
      "        [0.0789],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0943],\n",
      "        [0.0961],\n",
      "        [0.1006],\n",
      "        [0.1017],\n",
      "        [0.1034],\n",
      "        [0.1031],\n",
      "        [0.1066],\n",
      "        [0.1087],\n",
      "        [0.1127],\n",
      "        [0.1140],\n",
      "        [0.1475]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 100.21475958824158\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 31 個區塊累積花費時間(s) 0.5635890960693359\n",
      "<<The performance of 31 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.5635890960693359\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 936.55\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1192.17\n",
      "The accuracy(2000) for l = 1: 90.57%\n",
      "The accuracy(3000) for l = 1: 99.37%\n",
      "The maximum error: tensor(3765.7109)\n",
      "The minimum error: tensor(21.1016)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 2026.4\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 2101.8\n",
      "The accuracy(2000) for l = 1: 50.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 2695.96484375\n",
      "The minimum error: 1368.640625\n",
      "------------------------------------------------------------\n",
      "0.9056603773584906\n",
      "<class 'float'>\n",
      "0.5\n",
      "<class 'float'>\n",
      "The <<32>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1793886187660974e-06, 50)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [50, 32, 17, 70, 30, 143, 23, 104, 152, 149, 123, 47, 112, 147, 22, 15, 10, 24, 96, 120, 122, 51, 25, 18, 46, 45, 119, 102, 0, 3, 19, 31, 99, 44, 148, 4, 62, 7, 8, 61, 58, 100, 132, 57, 139, 101, 16, 121, 26, 56, 20, 14, 113, 137, 128, 118, 97, 124, 150, 146, 140, 27, 103, 13, 21, 86, 6, 144, 74, 71, 11, 35, 138, 95, 141, 59, 133, 85, 151, 87, 98, 142, 73, 49, 114, 126, 2, 69, 79, 125, 117, 111, 48, 40, 12, 116, 127, 136, 9, 68, 1, 41, 109, 153, 55, 78, 84, 94, 105, 154, 29, 129, 28, 92, 108, 115, 110, 33, 155, 107, 60, 106, 72, 5, 91, 52, 43, 135, 93, 134, 156, 145, 83, 76, 90, 82, 53, 34, 88, 130, 89, 63, 66, 54, 77, 67, 42, 158, 80, 36, 39, 64, 75, 157, 37]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0046],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0109],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0143],\n",
      "        [0.0140],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0183],\n",
      "        [0.0186],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0271],\n",
      "        [0.0277],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0293],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0398],\n",
      "        [0.0406],\n",
      "        [0.0417],\n",
      "        [0.0421],\n",
      "        [0.0422],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0424],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0455],\n",
      "        [0.0463],\n",
      "        [0.0476],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0530],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0564],\n",
      "        [0.0574],\n",
      "        [0.0578],\n",
      "        [0.0586],\n",
      "        [0.0592],\n",
      "        [0.0602],\n",
      "        [0.0625],\n",
      "        [0.0632],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0731],\n",
      "        [0.0733],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0779],\n",
      "        [0.0780],\n",
      "        [0.0789],\n",
      "        [0.0788],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0943],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.1006],\n",
      "        [0.1017],\n",
      "        [0.1031],\n",
      "        [0.1034],\n",
      "        [0.1058],\n",
      "        [0.1066]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1793886187660974e-06, 50)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [50, 32, 17, 70, 30, 143, 23, 104, 152, 149, 123, 47, 112, 147, 22, 15, 10, 24, 96, 120, 122, 51, 25, 18, 46, 45, 119, 102, 0, 3, 19, 31, 99, 44, 148, 4, 62, 7, 8, 61, 58, 100, 132, 57, 139, 101, 16, 121, 26, 56, 20, 14, 113, 137, 128, 118, 97, 124, 150, 146, 140, 27, 103, 13, 21, 86, 6, 144, 74, 71, 11, 35, 138, 95, 141, 59, 133, 85, 151, 87, 98, 142, 73, 49, 114, 126, 2, 69, 79, 125, 117, 111, 48, 40, 12, 116, 127, 136, 9, 68, 1, 41, 109, 153, 55, 78, 84, 94, 105, 154, 29, 129, 28, 92, 108, 115, 110, 33, 155, 107, 60, 106, 72, 5, 91, 52, 43, 135, 93, 134, 156, 145, 83, 76, 90, 82, 53, 34, 88, 130, 89, 63, 66, 54, 77, 67, 42, 158, 80, 36, 39, 64, 75, 157, 37, 131] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3626],\n",
      "        [0.4724],\n",
      "        [0.6299],\n",
      "        [0.2358],\n",
      "        [0.4686],\n",
      "        [0.4943],\n",
      "        [0.5514],\n",
      "        [0.0988],\n",
      "        [0.5107],\n",
      "        [0.5048],\n",
      "        [0.0988],\n",
      "        [0.3520],\n",
      "        [0.0988],\n",
      "        [0.5180],\n",
      "        [0.5628],\n",
      "        [0.6095],\n",
      "        [0.6507],\n",
      "        [0.5185],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3998],\n",
      "        [0.5316],\n",
      "        [0.6165],\n",
      "        [0.3482],\n",
      "        [0.3166],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5960],\n",
      "        [0.6294],\n",
      "        [0.6020],\n",
      "        [0.4756],\n",
      "        [0.0988],\n",
      "        [0.2959],\n",
      "        [0.5050],\n",
      "        [0.6234],\n",
      "        [0.2755],\n",
      "        [0.6448],\n",
      "        [0.6627],\n",
      "        [0.2855],\n",
      "        [0.3290],\n",
      "        [0.0988],\n",
      "        [0.4247],\n",
      "        [0.3700],\n",
      "        [0.4428],\n",
      "        [0.0988],\n",
      "        [0.6270],\n",
      "        [0.0988],\n",
      "        [0.5186],\n",
      "        [0.3828],\n",
      "        [0.5775],\n",
      "        [0.6255],\n",
      "        [0.0988],\n",
      "        [0.4334],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5092],\n",
      "        [0.5204],\n",
      "        [0.4470],\n",
      "        [0.5125],\n",
      "        [0.0988],\n",
      "        [0.6512],\n",
      "        [0.5843],\n",
      "        [0.0988],\n",
      "        [0.6043],\n",
      "        [0.5113],\n",
      "        [0.1649],\n",
      "        [0.2292],\n",
      "        [0.6470],\n",
      "        [0.4138],\n",
      "        [0.4267],\n",
      "        [0.0988],\n",
      "        [0.4550],\n",
      "        [0.2823],\n",
      "        [0.4965],\n",
      "        [0.0988],\n",
      "        [0.5154],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4685],\n",
      "        [0.1914],\n",
      "        [0.3260],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5833],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3223],\n",
      "        [0.3042],\n",
      "        [0.6701],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4433],\n",
      "        [0.6311],\n",
      "        [0.2257],\n",
      "        [0.5818],\n",
      "        [0.2488],\n",
      "        [0.0988],\n",
      "        [0.5123],\n",
      "        [0.3566],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4989],\n",
      "        [0.4781],\n",
      "        [0.0988],\n",
      "        [0.4866],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4189],\n",
      "        [0.5105],\n",
      "        [0.0988],\n",
      "        [0.2710],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.6203],\n",
      "        [0.0988],\n",
      "        [0.3937],\n",
      "        [0.2614],\n",
      "        [0.4399],\n",
      "        [0.0988],\n",
      "        [0.4356],\n",
      "        [0.5217],\n",
      "        [0.5063],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3791],\n",
      "        [0.3912],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2981],\n",
      "        [0.2665],\n",
      "        [0.3577],\n",
      "        [0.0988],\n",
      "        [0.2669],\n",
      "        [0.2084],\n",
      "        [0.5155],\n",
      "        [0.1274],\n",
      "        [0.4269],\n",
      "        [0.3667],\n",
      "        [0.2940],\n",
      "        [0.1144],\n",
      "        [0.5179],\n",
      "        [0.3934],\n",
      "        [0.4185]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0016],\n",
      "        [0.0021],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0046],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0070],\n",
      "        [0.0071],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0087],\n",
      "        [0.0090],\n",
      "        [0.0093],\n",
      "        [0.0097],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0109],\n",
      "        [0.0107],\n",
      "        [0.0113],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0126],\n",
      "        [0.0133],\n",
      "        [0.0143],\n",
      "        [0.0140],\n",
      "        [0.0146],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0158],\n",
      "        [0.0165],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0183],\n",
      "        [0.0186],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0193],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0266],\n",
      "        [0.0271],\n",
      "        [0.0277],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0293],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0318],\n",
      "        [0.0323],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0337],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0367],\n",
      "        [0.0367],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0398],\n",
      "        [0.0406],\n",
      "        [0.0417],\n",
      "        [0.0421],\n",
      "        [0.0422],\n",
      "        [0.0425],\n",
      "        [0.0425],\n",
      "        [0.0424],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0450],\n",
      "        [0.0455],\n",
      "        [0.0463],\n",
      "        [0.0476],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0530],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0565],\n",
      "        [0.0564],\n",
      "        [0.0574],\n",
      "        [0.0578],\n",
      "        [0.0586],\n",
      "        [0.0592],\n",
      "        [0.0602],\n",
      "        [0.0625],\n",
      "        [0.0632],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0731],\n",
      "        [0.0733],\n",
      "        [0.0749],\n",
      "        [0.0767],\n",
      "        [0.0779],\n",
      "        [0.0780],\n",
      "        [0.0789],\n",
      "        [0.0788],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0943],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.1006],\n",
      "        [0.1017],\n",
      "        [0.1031],\n",
      "        [0.1034],\n",
      "        [0.1058],\n",
      "        [0.1066],\n",
      "        [0.1087]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 19\n",
      "Number of shrink: 17\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0054],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0063],\n",
      "        [0.0096],\n",
      "        [0.0002],\n",
      "        [0.0037],\n",
      "        [0.0092],\n",
      "        [0.0077],\n",
      "        [0.0057],\n",
      "        [0.0065],\n",
      "        [0.0069],\n",
      "        [0.0203],\n",
      "        [0.0037],\n",
      "        [0.0123],\n",
      "        [0.0027],\n",
      "        [0.0110],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0081],\n",
      "        [0.0124],\n",
      "        [0.0049],\n",
      "        [0.0102],\n",
      "        [0.0091],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0073],\n",
      "        [0.0054],\n",
      "        [0.0072],\n",
      "        [0.0157],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0007],\n",
      "        [0.0073],\n",
      "        [0.0123],\n",
      "        [0.0093],\n",
      "        [0.0184],\n",
      "        [0.0137],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0068],\n",
      "        [0.0155],\n",
      "        [0.0058],\n",
      "        [0.0164],\n",
      "        [0.0213],\n",
      "        [0.0180],\n",
      "        [0.0218],\n",
      "        [0.0181],\n",
      "        [0.0151],\n",
      "        [0.0139],\n",
      "        [0.0192],\n",
      "        [0.0092],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0071],\n",
      "        [0.0344],\n",
      "        [0.0320],\n",
      "        [0.0252],\n",
      "        [0.0222],\n",
      "        [0.0174],\n",
      "        [0.0197],\n",
      "        [0.0233],\n",
      "        [0.0180],\n",
      "        [0.0114],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0203],\n",
      "        [0.0294],\n",
      "        [0.0172],\n",
      "        [0.0277],\n",
      "        [0.0397],\n",
      "        [0.0299],\n",
      "        [0.0265],\n",
      "        [0.0297],\n",
      "        [0.0162],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0437],\n",
      "        [0.0323],\n",
      "        [0.0334],\n",
      "        [0.0327],\n",
      "        [0.0337],\n",
      "        [0.0389],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0371],\n",
      "        [0.0343],\n",
      "        [0.0319],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0496],\n",
      "        [0.0450],\n",
      "        [0.0417],\n",
      "        [0.0459],\n",
      "        [0.0437],\n",
      "        [0.0425],\n",
      "        [0.0286],\n",
      "        [0.0426],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0317],\n",
      "        [0.0491],\n",
      "        [0.0455],\n",
      "        [0.0503],\n",
      "        [0.0476],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0562],\n",
      "        [0.0403],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0512],\n",
      "        [0.0564],\n",
      "        [0.0578],\n",
      "        [0.0578],\n",
      "        [0.0682],\n",
      "        [0.0592],\n",
      "        [0.0694],\n",
      "        [0.0489],\n",
      "        [0.0758],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0718],\n",
      "        [0.0735],\n",
      "        [0.0763],\n",
      "        [0.0749],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0770],\n",
      "        [0.0778],\n",
      "        [0.0792],\n",
      "        [0.0814],\n",
      "        [0.0849],\n",
      "        [0.0945],\n",
      "        [0.0823],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.0996],\n",
      "        [0.1022],\n",
      "        [0.1035],\n",
      "        [0.0923],\n",
      "        [0.1050],\n",
      "        [0.1001]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 100.61573147773743\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.6835956507275114e-08, 148)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [148, 23, 50, 70, 17, 10, 104, 22, 18, 32, 139, 123, 3, 47, 30, 112, 132, 19, 150, 4, 0, 149, 96, 120, 51, 122, 152, 45, 137, 7, 143, 119, 102, 46, 24, 144, 15, 99, 25, 44, 62, 14, 61, 58, 20, 100, 57, 31, 151, 101, 138, 13, 56, 121, 6, 8, 113, 128, 21, 118, 147, 97, 11, 124, 16, 26, 103, 86, 74, 27, 71, 133, 95, 153, 35, 85, 59, 87, 154, 98, 12, 140, 73, 114, 49, 126, 40, 69, 146, 79, 125, 117, 111, 48, 116, 2, 127, 141, 155, 68, 55, 109, 41, 78, 142, 84, 94, 105, 9, 129, 1, 92, 29, 156, 136, 28, 108, 5, 115, 110, 107, 60, 106, 72, 33, 91, 43, 52, 93, 83, 76, 90, 135, 134, 82, 53, 88, 145, 34, 130, 63, 66, 89, 54, 77, 158, 67, 157, 42, 80, 36, 39, 131, 64, 75, 37, 81] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4919],\n",
      "        [0.5478],\n",
      "        [0.3617],\n",
      "        [0.2358],\n",
      "        [0.6253],\n",
      "        [0.6457],\n",
      "        [0.0988],\n",
      "        [0.5595],\n",
      "        [0.6121],\n",
      "        [0.4685],\n",
      "        [0.4322],\n",
      "        [0.0988],\n",
      "        [0.6242],\n",
      "        [0.3515],\n",
      "        [0.4647],\n",
      "        [0.0988],\n",
      "        [0.4159],\n",
      "        [0.5979],\n",
      "        [0.4954],\n",
      "        [0.6180],\n",
      "        [0.5924],\n",
      "        [0.4918],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3992],\n",
      "        [0.0988],\n",
      "        [0.4969],\n",
      "        [0.3162],\n",
      "        [0.4234],\n",
      "        [0.6399],\n",
      "        [0.4821],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3477],\n",
      "        [0.5154],\n",
      "        [0.4988],\n",
      "        [0.6043],\n",
      "        [0.0988],\n",
      "        [0.5282],\n",
      "        [0.2956],\n",
      "        [0.2745],\n",
      "        [0.6209],\n",
      "        [0.2847],\n",
      "        [0.3289],\n",
      "        [0.5741],\n",
      "        [0.0988],\n",
      "        [0.3698],\n",
      "        [0.4715],\n",
      "        [0.5016],\n",
      "        [0.0988],\n",
      "        [0.4169],\n",
      "        [0.6461],\n",
      "        [0.3825],\n",
      "        [0.0988],\n",
      "        [0.5987],\n",
      "        [0.6583],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5808],\n",
      "        [0.0988],\n",
      "        [0.5048],\n",
      "        [0.0988],\n",
      "        [0.6419],\n",
      "        [0.0988],\n",
      "        [0.6226],\n",
      "        [0.5149],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.5088],\n",
      "        [0.2292],\n",
      "        [0.4936],\n",
      "        [0.0988],\n",
      "        [0.4985],\n",
      "        [0.4110],\n",
      "        [0.0988],\n",
      "        [0.2816],\n",
      "        [0.0988],\n",
      "        [0.4856],\n",
      "        [0.0988],\n",
      "        [0.6652],\n",
      "        [0.4362],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.3251],\n",
      "        [0.0988],\n",
      "        [0.3018],\n",
      "        [0.2257],\n",
      "        [0.5069],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3216],\n",
      "        [0.0988],\n",
      "        [0.5782],\n",
      "        [0.0988],\n",
      "        [0.4438],\n",
      "        [0.4969],\n",
      "        [0.2257],\n",
      "        [0.3563],\n",
      "        [0.0988],\n",
      "        [0.2472],\n",
      "        [0.1183],\n",
      "        [0.4567],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6267],\n",
      "        [0.0988],\n",
      "        [0.5779],\n",
      "        [0.0988],\n",
      "        [0.4740],\n",
      "        [0.5081],\n",
      "        [0.4334],\n",
      "        [0.4826],\n",
      "        [0.0988],\n",
      "        [0.6150],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2706],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.4157],\n",
      "        [0.0988],\n",
      "        [0.2614],\n",
      "        [0.3932],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4303],\n",
      "        [0.4264],\n",
      "        [0.0988],\n",
      "        [0.3787],\n",
      "        [0.0988],\n",
      "        [0.4938],\n",
      "        [0.3882],\n",
      "        [0.0988],\n",
      "        [0.2971],\n",
      "        [0.2655],\n",
      "        [0.0988],\n",
      "        [0.3573],\n",
      "        [0.0988],\n",
      "        [0.5018],\n",
      "        [0.2657],\n",
      "        [0.5044],\n",
      "        [0.2082],\n",
      "        [0.1274],\n",
      "        [0.4241],\n",
      "        [0.3645],\n",
      "        [0.4098],\n",
      "        [0.2931],\n",
      "        [0.1144],\n",
      "        [0.3918],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0002],\n",
      "        [0.0017],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0058],\n",
      "        [0.0057],\n",
      "        [0.0054],\n",
      "        [0.0065],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0068],\n",
      "        [0.0072],\n",
      "        [0.0071],\n",
      "        [0.0073],\n",
      "        [0.0073],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0092],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0123],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0122],\n",
      "        [0.0123],\n",
      "        [0.0139],\n",
      "        [0.0137],\n",
      "        [0.0148],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0181],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0184],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0197],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0203],\n",
      "        [0.0207],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0265],\n",
      "        [0.0277],\n",
      "        [0.0286],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0299],\n",
      "        [0.0306],\n",
      "        [0.0317],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0320],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0334],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.0340],\n",
      "        [0.0344],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0371],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0394],\n",
      "        [0.0397],\n",
      "        [0.0403],\n",
      "        [0.0417],\n",
      "        [0.0426],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0450],\n",
      "        [0.0455],\n",
      "        [0.0459],\n",
      "        [0.0476],\n",
      "        [0.0491],\n",
      "        [0.0489],\n",
      "        [0.0496],\n",
      "        [0.0503],\n",
      "        [0.0509],\n",
      "        [0.0512],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0562],\n",
      "        [0.0564],\n",
      "        [0.0578],\n",
      "        [0.0578],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0682],\n",
      "        [0.0694],\n",
      "        [0.0718],\n",
      "        [0.0735],\n",
      "        [0.0749],\n",
      "        [0.0758],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0770],\n",
      "        [0.0778],\n",
      "        [0.0779],\n",
      "        [0.0792],\n",
      "        [0.0814],\n",
      "        [0.0823],\n",
      "        [0.0849],\n",
      "        [0.0923],\n",
      "        [0.0945],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.0996],\n",
      "        [0.1001],\n",
      "        [0.1022],\n",
      "        [0.1035],\n",
      "        [0.1050],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 10\n",
      "Number of shrink: 12\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0018],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0032],\n",
      "        [0.0037],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0050],\n",
      "        [0.0057],\n",
      "        [0.0055],\n",
      "        [0.0054],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0063],\n",
      "        [0.0076],\n",
      "        [0.0052],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0091],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0092],\n",
      "        [0.0082],\n",
      "        [0.0112],\n",
      "        [0.0099],\n",
      "        [0.0088],\n",
      "        [0.0097],\n",
      "        [0.0107],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0090],\n",
      "        [0.0109],\n",
      "        [0.0101],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0112],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0146],\n",
      "        [0.0159],\n",
      "        [0.0155],\n",
      "        [0.0154],\n",
      "        [0.0167],\n",
      "        [0.0156],\n",
      "        [0.0138],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0178],\n",
      "        [0.0192],\n",
      "        [0.0180],\n",
      "        [0.0185],\n",
      "        [0.0180],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0202],\n",
      "        [0.0199],\n",
      "        [0.0218],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0207],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0254],\n",
      "        [0.0260],\n",
      "        [0.0277],\n",
      "        [0.0265],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0290],\n",
      "        [0.0306],\n",
      "        [0.0295],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0327],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0323],\n",
      "        [0.0338],\n",
      "        [0.0347],\n",
      "        [0.0340],\n",
      "        [0.0356],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0361],\n",
      "        [0.0375],\n",
      "        [0.0391],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0384],\n",
      "        [0.0417],\n",
      "        [0.0413],\n",
      "        [0.0425],\n",
      "        [0.0430],\n",
      "        [0.0437],\n",
      "        [0.0447],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0447],\n",
      "        [0.0455],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0469],\n",
      "        [0.0503],\n",
      "        [0.0499],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0540],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0558],\n",
      "        [0.0564],\n",
      "        [0.0569],\n",
      "        [0.0565],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0686],\n",
      "        [0.0698],\n",
      "        [0.0718],\n",
      "        [0.0722],\n",
      "        [0.0749],\n",
      "        [0.0772],\n",
      "        [0.0760],\n",
      "        [0.0768],\n",
      "        [0.0782],\n",
      "        [0.0787],\n",
      "        [0.0779],\n",
      "        [0.0781],\n",
      "        [0.0814],\n",
      "        [0.0801],\n",
      "        [0.0858],\n",
      "        [0.0904],\n",
      "        [0.0934],\n",
      "        [0.0961],\n",
      "        [0.0980],\n",
      "        [0.1003],\n",
      "        [0.0996],\n",
      "        [0.1033],\n",
      "        [0.1035],\n",
      "        [0.1058],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 100.74992895126343\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.931020333107881e-07, 23)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [23, 50, 148, 17, 70, 10, 104, 22, 139, 18, 32, 150, 3, 47, 123, 30, 132, 112, 19, 4, 0, 96, 120, 122, 137, 149, 46, 51, 119, 102, 45, 7, 144, 143, 24, 44, 152, 25, 99, 15, 62, 14, 151, 61, 31, 100, 20, 58, 101, 138, 57, 121, 8, 13, 6, 113, 56, 128, 118, 97, 16, 21, 11, 124, 26, 147, 103, 86, 74, 27, 71, 133, 153, 95, 35, 59, 154, 85, 87, 98, 12, 49, 140, 73, 114, 126, 69, 79, 125, 117, 111, 40, 146, 48, 116, 155, 2, 127, 141, 55, 68, 109, 41, 78, 84, 142, 94, 105, 9, 1, 129, 156, 92, 29, 28, 136, 108, 5, 115, 110, 60, 107, 106, 72, 33, 91, 52, 43, 93, 83, 76, 90, 135, 134, 82, 53, 88, 34, 130, 145, 89, 63, 54, 66, 158, 77, 67, 157, 42, 80, 36, 131, 39, 64, 75, 37, 81, 65] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5486],\n",
      "        [0.3630],\n",
      "        [0.4907],\n",
      "        [0.6255],\n",
      "        [0.2358],\n",
      "        [0.6461],\n",
      "        [0.0988],\n",
      "        [0.5600],\n",
      "        [0.4314],\n",
      "        [0.6123],\n",
      "        [0.4687],\n",
      "        [0.4935],\n",
      "        [0.6242],\n",
      "        [0.3527],\n",
      "        [0.0988],\n",
      "        [0.4646],\n",
      "        [0.4155],\n",
      "        [0.0988],\n",
      "        [0.5982],\n",
      "        [0.6184],\n",
      "        [0.5928],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4229],\n",
      "        [0.4905],\n",
      "        [0.3489],\n",
      "        [0.4003],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3170],\n",
      "        [0.6403],\n",
      "        [0.4976],\n",
      "        [0.4809],\n",
      "        [0.5155],\n",
      "        [0.2966],\n",
      "        [0.4949],\n",
      "        [0.5288],\n",
      "        [0.0988],\n",
      "        [0.6052],\n",
      "        [0.2756],\n",
      "        [0.6205],\n",
      "        [0.4992],\n",
      "        [0.2855],\n",
      "        [0.4717],\n",
      "        [0.0988],\n",
      "        [0.5745],\n",
      "        [0.3301],\n",
      "        [0.0988],\n",
      "        [0.4162],\n",
      "        [0.3709],\n",
      "        [0.0988],\n",
      "        [0.6587],\n",
      "        [0.6464],\n",
      "        [0.5992],\n",
      "        [0.0988],\n",
      "        [0.3837],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6233],\n",
      "        [0.5813],\n",
      "        [0.6427],\n",
      "        [0.0988],\n",
      "        [0.5154],\n",
      "        [0.5032],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.5091],\n",
      "        [0.2292],\n",
      "        [0.4931],\n",
      "        [0.4963],\n",
      "        [0.0988],\n",
      "        [0.4112],\n",
      "        [0.2824],\n",
      "        [0.4834],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6652],\n",
      "        [0.3263],\n",
      "        [0.4355],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3022],\n",
      "        [0.5057],\n",
      "        [0.3226],\n",
      "        [0.0988],\n",
      "        [0.4950],\n",
      "        [0.5780],\n",
      "        [0.0988],\n",
      "        [0.4430],\n",
      "        [0.3576],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.2480],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.4556],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6270],\n",
      "        [0.5784],\n",
      "        [0.0988],\n",
      "        [0.5062],\n",
      "        [0.0988],\n",
      "        [0.4741],\n",
      "        [0.4831],\n",
      "        [0.4328],\n",
      "        [0.0988],\n",
      "        [0.6153],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2713],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.4161],\n",
      "        [0.0988],\n",
      "        [0.3945],\n",
      "        [0.2623],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4299],\n",
      "        [0.4260],\n",
      "        [0.0988],\n",
      "        [0.3800],\n",
      "        [0.0988],\n",
      "        [0.3885],\n",
      "        [0.0988],\n",
      "        [0.4924],\n",
      "        [0.0988],\n",
      "        [0.2983],\n",
      "        [0.3584],\n",
      "        [0.2663],\n",
      "        [0.4997],\n",
      "        [0.0988],\n",
      "        [0.2666],\n",
      "        [0.5025],\n",
      "        [0.2093],\n",
      "        [0.1274],\n",
      "        [0.4244],\n",
      "        [0.4093],\n",
      "        [0.3652],\n",
      "        [0.2942],\n",
      "        [0.1144],\n",
      "        [0.3926],\n",
      "        [0.1127],\n",
      "        [0.2767]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0005],\n",
      "        [0.0018],\n",
      "        [0.0023],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0037],\n",
      "        [0.0042],\n",
      "        [0.0050],\n",
      "        [0.0050],\n",
      "        [0.0053],\n",
      "        [0.0052],\n",
      "        [0.0055],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0076],\n",
      "        [0.0076],\n",
      "        [0.0077],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0091],\n",
      "        [0.0090],\n",
      "        [0.0092],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0099],\n",
      "        [0.0097],\n",
      "        [0.0101],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0118],\n",
      "        [0.0118],\n",
      "        [0.0114],\n",
      "        [0.0134],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0146],\n",
      "        [0.0156],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0165],\n",
      "        [0.0167],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0178],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0202],\n",
      "        [0.0211],\n",
      "        [0.0207],\n",
      "        [0.0212],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0249],\n",
      "        [0.0254],\n",
      "        [0.0260],\n",
      "        [0.0265],\n",
      "        [0.0277],\n",
      "        [0.0292],\n",
      "        [0.0290],\n",
      "        [0.0295],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0319],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0347],\n",
      "        [0.0356],\n",
      "        [0.0361],\n",
      "        [0.0375],\n",
      "        [0.0384],\n",
      "        [0.0391],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0413],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0430],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0447],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0447],\n",
      "        [0.0455],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0499],\n",
      "        [0.0503],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0540],\n",
      "        [0.0542],\n",
      "        [0.0549],\n",
      "        [0.0554],\n",
      "        [0.0558],\n",
      "        [0.0564],\n",
      "        [0.0565],\n",
      "        [0.0569],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0686],\n",
      "        [0.0698],\n",
      "        [0.0718],\n",
      "        [0.0722],\n",
      "        [0.0749],\n",
      "        [0.0760],\n",
      "        [0.0768],\n",
      "        [0.0772],\n",
      "        [0.0779],\n",
      "        [0.0782],\n",
      "        [0.0781],\n",
      "        [0.0787],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0858],\n",
      "        [0.0904],\n",
      "        [0.0934],\n",
      "        [0.0961],\n",
      "        [0.0980],\n",
      "        [0.0996],\n",
      "        [0.1003],\n",
      "        [0.1033],\n",
      "        [0.1035],\n",
      "        [0.1058],\n",
      "        [0.1127],\n",
      "        [0.1140]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 9\n",
      "Number of shrink: 12\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0007],\n",
      "        [0.0052],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0039],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0030],\n",
      "        [0.0044],\n",
      "        [0.0060],\n",
      "        [0.0011],\n",
      "        [0.0045],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0066],\n",
      "        [0.0036],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0066],\n",
      "        [0.0076],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0063],\n",
      "        [0.0122],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0072],\n",
      "        [0.0130],\n",
      "        [0.0117],\n",
      "        [0.0113],\n",
      "        [0.0163],\n",
      "        [0.0125],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0131],\n",
      "        [0.0088],\n",
      "        [0.0137],\n",
      "        [0.0160],\n",
      "        [0.0154],\n",
      "        [0.0153],\n",
      "        [0.0149],\n",
      "        [0.0164],\n",
      "        [0.0145],\n",
      "        [0.0160],\n",
      "        [0.0179],\n",
      "        [0.0178],\n",
      "        [0.0173],\n",
      "        [0.0178],\n",
      "        [0.0192],\n",
      "        [0.0185],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0193],\n",
      "        [0.0213],\n",
      "        [0.0208],\n",
      "        [0.0218],\n",
      "        [0.0247],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0256],\n",
      "        [0.0254],\n",
      "        [0.0230],\n",
      "        [0.0218],\n",
      "        [0.0277],\n",
      "        [0.0291],\n",
      "        [0.0300],\n",
      "        [0.0248],\n",
      "        [0.0297],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0348],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0384],\n",
      "        [0.0360],\n",
      "        [0.0375],\n",
      "        [0.0333],\n",
      "        [0.0396],\n",
      "        [0.0394],\n",
      "        [0.0429],\n",
      "        [0.0419],\n",
      "        [0.0417],\n",
      "        [0.0425],\n",
      "        [0.0420],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0469],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0442],\n",
      "        [0.0456],\n",
      "        [0.0455],\n",
      "        [0.0417],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0509],\n",
      "        [0.0526],\n",
      "        [0.0509],\n",
      "        [0.0500],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0546],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0571],\n",
      "        [0.0562],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0710],\n",
      "        [0.0723],\n",
      "        [0.0718],\n",
      "        [0.0724],\n",
      "        [0.0749],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0801],\n",
      "        [0.0779],\n",
      "        [0.0774],\n",
      "        [0.0785],\n",
      "        [0.0782],\n",
      "        [0.0746],\n",
      "        [0.0814],\n",
      "        [0.0852],\n",
      "        [0.0850],\n",
      "        [0.0925],\n",
      "        [0.0961],\n",
      "        [0.0980],\n",
      "        [0.0971],\n",
      "        [0.1011],\n",
      "        [0.1029],\n",
      "        [0.1035],\n",
      "        [0.1064],\n",
      "        [0.1127],\n",
      "        [0.1131]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 100.88257002830505\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5076295767357806e-10, 23)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [23, 50, 150, 17, 70, 139, 10, 132, 104, 22, 18, 3, 148, 47, 123, 137, 32, 4, 30, 112, 19, 144, 0, 96, 120, 122, 151, 46, 51, 45, 119, 102, 7, 44, 24, 99, 149, 25, 15, 62, 14, 143, 61, 138, 58, 20, 100, 57, 31, 101, 152, 13, 8, 6, 121, 56, 113, 128, 21, 118, 97, 124, 16, 11, 153, 26, 103, 133, 86, 74, 154, 147, 71, 27, 95, 35, 85, 59, 87, 98, 12, 73, 49, 114, 155, 126, 69, 79, 125, 117, 140, 111, 40, 48, 116, 146, 127, 2, 55, 68, 156, 41, 109, 141, 78, 84, 9, 94, 105, 1, 129, 142, 92, 29, 5, 108, 28, 115, 110, 136, 107, 60, 106, 72, 33, 91, 43, 52, 93, 83, 76, 90, 135, 82, 134, 53, 88, 158, 34, 130, 63, 89, 66, 54, 145, 77, 157, 67, 42, 80, 131, 36, 39, 64, 75, 37, 81, 65, 38] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5481],\n",
      "        [0.3628],\n",
      "        [0.4893],\n",
      "        [0.6253],\n",
      "        [0.2358],\n",
      "        [0.4294],\n",
      "        [0.6469],\n",
      "        [0.4127],\n",
      "        [0.0988],\n",
      "        [0.5597],\n",
      "        [0.6117],\n",
      "        [0.6232],\n",
      "        [0.4874],\n",
      "        [0.3524],\n",
      "        [0.0988],\n",
      "        [0.4204],\n",
      "        [0.4680],\n",
      "        [0.6174],\n",
      "        [0.4644],\n",
      "        [0.0988],\n",
      "        [0.5977],\n",
      "        [0.4947],\n",
      "        [0.5927],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4941],\n",
      "        [0.3489],\n",
      "        [0.4002],\n",
      "        [0.3168],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6402],\n",
      "        [0.2965],\n",
      "        [0.5147],\n",
      "        [0.0988],\n",
      "        [0.4874],\n",
      "        [0.5280],\n",
      "        [0.6045],\n",
      "        [0.2745],\n",
      "        [0.6201],\n",
      "        [0.4786],\n",
      "        [0.2847],\n",
      "        [0.4142],\n",
      "        [0.3291],\n",
      "        [0.5743],\n",
      "        [0.0988],\n",
      "        [0.3702],\n",
      "        [0.4712],\n",
      "        [0.0988],\n",
      "        [0.4898],\n",
      "        [0.6459],\n",
      "        [0.6590],\n",
      "        [0.5985],\n",
      "        [0.0988],\n",
      "        [0.3830],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5804],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6228],\n",
      "        [0.6428],\n",
      "        [0.4916],\n",
      "        [0.5148],\n",
      "        [0.0988],\n",
      "        [0.4901],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4787],\n",
      "        [0.5003],\n",
      "        [0.2291],\n",
      "        [0.5084],\n",
      "        [0.0988],\n",
      "        [0.4114],\n",
      "        [0.0988],\n",
      "        [0.2815],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6649],\n",
      "        [0.1914],\n",
      "        [0.3262],\n",
      "        [0.0988],\n",
      "        [0.4900],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4334],\n",
      "        [0.0988],\n",
      "        [0.3031],\n",
      "        [0.3227],\n",
      "        [0.0988],\n",
      "        [0.5029],\n",
      "        [0.0988],\n",
      "        [0.5775],\n",
      "        [0.3570],\n",
      "        [0.2257],\n",
      "        [0.5010],\n",
      "        [0.2489],\n",
      "        [0.0988],\n",
      "        [0.4406],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.6275],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5783],\n",
      "        [0.0988],\n",
      "        [0.4534],\n",
      "        [0.0988],\n",
      "        [0.4741],\n",
      "        [0.6138],\n",
      "        [0.0988],\n",
      "        [0.4821],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4304],\n",
      "        [0.0988],\n",
      "        [0.2707],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.4158],\n",
      "        [0.0988],\n",
      "        [0.2630],\n",
      "        [0.3939],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4275],\n",
      "        [0.0988],\n",
      "        [0.4235],\n",
      "        [0.3798],\n",
      "        [0.0988],\n",
      "        [0.4942],\n",
      "        [0.3883],\n",
      "        [0.0988],\n",
      "        [0.2976],\n",
      "        [0.0988],\n",
      "        [0.2658],\n",
      "        [0.3580],\n",
      "        [0.4894],\n",
      "        [0.0988],\n",
      "        [0.4971],\n",
      "        [0.2660],\n",
      "        [0.2102],\n",
      "        [0.1274],\n",
      "        [0.4068],\n",
      "        [0.4244],\n",
      "        [0.3660],\n",
      "        [0.2938],\n",
      "        [0.1144],\n",
      "        [0.3932],\n",
      "        [0.1127],\n",
      "        [0.2759],\n",
      "        [0.3747]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0044],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0060],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0097],\n",
      "        [0.0113],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0131],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0173],\n",
      "        [0.0178],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0277],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0333],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0375],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0396],\n",
      "        [0.0419],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0456],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0500],\n",
      "        [0.0509],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0562],\n",
      "        [0.0571],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0710],\n",
      "        [0.0718],\n",
      "        [0.0723],\n",
      "        [0.0724],\n",
      "        [0.0749],\n",
      "        [0.0746],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0774],\n",
      "        [0.0779],\n",
      "        [0.0782],\n",
      "        [0.0785],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0850],\n",
      "        [0.0852],\n",
      "        [0.0925],\n",
      "        [0.0961],\n",
      "        [0.0971],\n",
      "        [0.0980],\n",
      "        [0.1011],\n",
      "        [0.1029],\n",
      "        [0.1035],\n",
      "        [0.1064],\n",
      "        [0.1127],\n",
      "        [0.1131],\n",
      "        [0.1473]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 7\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0044],\n",
      "        [0.0045],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0060],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0076],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0097],\n",
      "        [0.0113],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0131],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0173],\n",
      "        [0.0178],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0277],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0333],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0375],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0396],\n",
      "        [0.0419],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0456],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0500],\n",
      "        [0.0509],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0562],\n",
      "        [0.0571],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0710],\n",
      "        [0.0718],\n",
      "        [0.0723],\n",
      "        [0.0724],\n",
      "        [0.0749],\n",
      "        [0.0746],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0774],\n",
      "        [0.0779],\n",
      "        [0.0782],\n",
      "        [0.0785],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0850],\n",
      "        [0.0852],\n",
      "        [0.0925],\n",
      "        [0.0961],\n",
      "        [0.0971],\n",
      "        [0.0980],\n",
      "        [0.1011],\n",
      "        [0.1029],\n",
      "        [0.1035],\n",
      "        [0.1064],\n",
      "        [0.1127],\n",
      "        [0.1131],\n",
      "        [0.1473]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 100.98503279685974\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 32 個區塊累積花費時間(s) 0.5992846488952637\n",
      "<<The performance of 32 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.5992846488952637\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 941.64\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1204.67\n",
      "The accuracy(2000) for l = 1: 89.94%\n",
      "The accuracy(3000) for l = 1: 99.37%\n",
      "The maximum error: tensor(3760.6562)\n",
      "The minimum error: tensor(4.3594)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 1067.5\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 1086.3\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 1382.0390625\n",
      "The minimum error: 841.64453125\n",
      "------------------------------------------------------------\n",
      "0.89937106918239\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<33>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5076295767357806e-10, 19)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [19, 46, 146, 13, 66, 135, 6, 128, 100, 18, 14, 144, 43, 119, 133, 28, 0, 26, 108, 15, 140, 92, 116, 118, 147, 42, 47, 41, 115, 98, 3, 40, 20, 95, 145, 21, 11, 58, 10, 139, 57, 134, 54, 16, 96, 53, 27, 97, 148, 9, 4, 2, 117, 52, 109, 124, 17, 114, 93, 120, 12, 7, 149, 22, 99, 129, 82, 70, 150, 143, 67, 23, 91, 31, 81, 55, 83, 94, 8, 69, 45, 110, 155, 151, 122, 65, 75, 121, 113, 136, 107, 36, 44, 112, 158, 142, 123, 51, 64, 152, 37, 105, 156, 137, 74, 80, 5, 90, 101, 125, 138, 88, 25, 1, 104, 24, 111, 106, 132, 157, 103, 56, 102, 68, 29, 87, 39, 48, 89, 79, 72, 86, 131, 78, 130, 49, 84, 154, 30, 126, 59, 85, 62, 50, 141, 73, 153, 63, 38, 76, 127, 32, 35, 60, 71]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0044],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0060],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0097],\n",
      "        [0.0113],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0131],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0173],\n",
      "        [0.0178],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0277],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0375],\n",
      "        [0.0374],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0419],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0427],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0500],\n",
      "        [0.0509],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0539],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0562],\n",
      "        [0.0571],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0710],\n",
      "        [0.0718],\n",
      "        [0.0723],\n",
      "        [0.0724],\n",
      "        [0.0749],\n",
      "        [0.0746],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0774],\n",
      "        [0.0779],\n",
      "        [0.0782],\n",
      "        [0.0785],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0850],\n",
      "        [0.0852],\n",
      "        [0.0925],\n",
      "        [0.0961],\n",
      "        [0.0971],\n",
      "        [0.0980],\n",
      "        [0.1011],\n",
      "        [0.1029],\n",
      "        [0.1035]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.5076295767357806e-10, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [19, 46, 146, 13, 66, 135, 6, 128, 100, 18, 14, 144, 43, 119, 133, 28, 0, 26, 108, 15, 140, 92, 116, 118, 147, 42, 47, 41, 115, 98, 3, 40, 20, 95, 145, 21, 11, 58, 10, 139, 57, 134, 54, 16, 96, 53, 27, 97, 148, 9, 4, 2, 117, 52, 109, 124, 17, 114, 93, 120, 12, 7, 149, 22, 99, 129, 82, 70, 150, 143, 67, 23, 91, 31, 81, 55, 83, 94, 8, 69, 45, 110, 155, 151, 122, 65, 75, 121, 113, 136, 107, 36, 44, 112, 158, 142, 123, 51, 64, 152, 37, 105, 156, 137, 74, 80, 5, 90, 101, 125, 138, 88, 25, 1, 104, 24, 111, 106, 132, 157, 103, 56, 102, 68, 29, 87, 39, 48, 89, 79, 72, 86, 131, 78, 130, 49, 84, 154, 30, 126, 59, 85, 62, 50, 141, 73, 153, 63, 38, 76, 127, 32, 35, 60, 71, 33] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5481],\n",
      "        [0.3628],\n",
      "        [0.4893],\n",
      "        [0.6253],\n",
      "        [0.2358],\n",
      "        [0.4294],\n",
      "        [0.6469],\n",
      "        [0.4127],\n",
      "        [0.0988],\n",
      "        [0.5597],\n",
      "        [0.6117],\n",
      "        [0.4874],\n",
      "        [0.3524],\n",
      "        [0.0988],\n",
      "        [0.4204],\n",
      "        [0.4680],\n",
      "        [0.6174],\n",
      "        [0.4644],\n",
      "        [0.0988],\n",
      "        [0.5977],\n",
      "        [0.4947],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4941],\n",
      "        [0.3489],\n",
      "        [0.4002],\n",
      "        [0.3168],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6402],\n",
      "        [0.2965],\n",
      "        [0.5147],\n",
      "        [0.0988],\n",
      "        [0.4874],\n",
      "        [0.5280],\n",
      "        [0.6045],\n",
      "        [0.2745],\n",
      "        [0.6201],\n",
      "        [0.4786],\n",
      "        [0.2847],\n",
      "        [0.4142],\n",
      "        [0.3291],\n",
      "        [0.5743],\n",
      "        [0.0988],\n",
      "        [0.3702],\n",
      "        [0.4712],\n",
      "        [0.0988],\n",
      "        [0.4898],\n",
      "        [0.6459],\n",
      "        [0.6590],\n",
      "        [0.5985],\n",
      "        [0.0988],\n",
      "        [0.3830],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5804],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6228],\n",
      "        [0.6428],\n",
      "        [0.4916],\n",
      "        [0.5148],\n",
      "        [0.0988],\n",
      "        [0.4901],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4787],\n",
      "        [0.5003],\n",
      "        [0.2291],\n",
      "        [0.5084],\n",
      "        [0.0988],\n",
      "        [0.4114],\n",
      "        [0.0988],\n",
      "        [0.2815],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6649],\n",
      "        [0.1914],\n",
      "        [0.3262],\n",
      "        [0.0988],\n",
      "        [0.4782],\n",
      "        [0.4900],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4334],\n",
      "        [0.0988],\n",
      "        [0.3031],\n",
      "        [0.3227],\n",
      "        [0.0988],\n",
      "        [0.4810],\n",
      "        [0.5029],\n",
      "        [0.0988],\n",
      "        [0.3570],\n",
      "        [0.2257],\n",
      "        [0.5010],\n",
      "        [0.2489],\n",
      "        [0.0988],\n",
      "        [0.4752],\n",
      "        [0.4406],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.6275],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4534],\n",
      "        [0.0988],\n",
      "        [0.4741],\n",
      "        [0.6138],\n",
      "        [0.0988],\n",
      "        [0.4821],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4304],\n",
      "        [0.4770],\n",
      "        [0.0988],\n",
      "        [0.2707],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.4158],\n",
      "        [0.0988],\n",
      "        [0.2630],\n",
      "        [0.3939],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4275],\n",
      "        [0.0988],\n",
      "        [0.4235],\n",
      "        [0.3798],\n",
      "        [0.0988],\n",
      "        [0.4942],\n",
      "        [0.3883],\n",
      "        [0.0988],\n",
      "        [0.2976],\n",
      "        [0.0988],\n",
      "        [0.2658],\n",
      "        [0.3580],\n",
      "        [0.4894],\n",
      "        [0.0988],\n",
      "        [0.4971],\n",
      "        [0.2660],\n",
      "        [0.2102],\n",
      "        [0.1274],\n",
      "        [0.4068],\n",
      "        [0.4244],\n",
      "        [0.3660],\n",
      "        [0.2938],\n",
      "        [0.1144],\n",
      "        [0.3932]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0030],\n",
      "        [0.0039],\n",
      "        [0.0036],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0044],\n",
      "        [0.0052],\n",
      "        [0.0056],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0060],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0097],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0097],\n",
      "        [0.0113],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0125],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0131],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0145],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0160],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0163],\n",
      "        [0.0173],\n",
      "        [0.0178],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0193],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0211],\n",
      "        [0.0213],\n",
      "        [0.0218],\n",
      "        [0.0218],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0247],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0277],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0300],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0333],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0360],\n",
      "        [0.0375],\n",
      "        [0.0374],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0419],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0420],\n",
      "        [0.0425],\n",
      "        [0.0427],\n",
      "        [0.0429],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0442],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0469],\n",
      "        [0.0476],\n",
      "        [0.0489],\n",
      "        [0.0500],\n",
      "        [0.0509],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0539],\n",
      "        [0.0542],\n",
      "        [0.0546],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0564],\n",
      "        [0.0562],\n",
      "        [0.0571],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0710],\n",
      "        [0.0718],\n",
      "        [0.0723],\n",
      "        [0.0724],\n",
      "        [0.0749],\n",
      "        [0.0746],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0774],\n",
      "        [0.0779],\n",
      "        [0.0782],\n",
      "        [0.0785],\n",
      "        [0.0801],\n",
      "        [0.0814],\n",
      "        [0.0850],\n",
      "        [0.0852],\n",
      "        [0.0925],\n",
      "        [0.0961],\n",
      "        [0.0971],\n",
      "        [0.0980],\n",
      "        [0.1011],\n",
      "        [0.1029],\n",
      "        [0.1035],\n",
      "        [0.1064]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 11\n",
      "Number of shrink: 13\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0011],\n",
      "        [0.0006],\n",
      "        [0.0027],\n",
      "        [0.0037],\n",
      "        [0.0022],\n",
      "        [0.0013],\n",
      "        [0.0030],\n",
      "        [0.0019],\n",
      "        [0.0037],\n",
      "        [0.0025],\n",
      "        [0.0033],\n",
      "        [0.0079],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0041],\n",
      "        [0.0073],\n",
      "        [0.0049],\n",
      "        [0.0078],\n",
      "        [0.0069],\n",
      "        [0.0061],\n",
      "        [0.0050],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0046],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0091],\n",
      "        [0.0116],\n",
      "        [0.0129],\n",
      "        [0.0118],\n",
      "        [0.0152],\n",
      "        [0.0133],\n",
      "        [0.0130],\n",
      "        [0.0120],\n",
      "        [0.0118],\n",
      "        [0.0152],\n",
      "        [0.0132],\n",
      "        [0.0127],\n",
      "        [0.0144],\n",
      "        [0.0138],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0178],\n",
      "        [0.0164],\n",
      "        [0.0203],\n",
      "        [0.0164],\n",
      "        [0.0181],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0183],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0185],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0221],\n",
      "        [0.0206],\n",
      "        [0.0177],\n",
      "        [0.0229],\n",
      "        [0.0222],\n",
      "        [0.0220],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0212],\n",
      "        [0.0273],\n",
      "        [0.0254],\n",
      "        [0.0267],\n",
      "        [0.0277],\n",
      "        [0.0300],\n",
      "        [0.0297],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0316],\n",
      "        [0.0309],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0290],\n",
      "        [0.0296],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0365],\n",
      "        [0.0350],\n",
      "        [0.0353],\n",
      "        [0.0362],\n",
      "        [0.0375],\n",
      "        [0.0337],\n",
      "        [0.0411],\n",
      "        [0.0394],\n",
      "        [0.0421],\n",
      "        [0.0416],\n",
      "        [0.0379],\n",
      "        [0.0423],\n",
      "        [0.0425],\n",
      "        [0.0391],\n",
      "        [0.0447],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0451],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0491],\n",
      "        [0.0476],\n",
      "        [0.0502],\n",
      "        [0.0487],\n",
      "        [0.0509],\n",
      "        [0.0520],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0545],\n",
      "        [0.0502],\n",
      "        [0.0542],\n",
      "        [0.0551],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0571],\n",
      "        [0.0564],\n",
      "        [0.0565],\n",
      "        [0.0570],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0727],\n",
      "        [0.0718],\n",
      "        [0.0737],\n",
      "        [0.0726],\n",
      "        [0.0749],\n",
      "        [0.0708],\n",
      "        [0.0775],\n",
      "        [0.0768],\n",
      "        [0.0770],\n",
      "        [0.0779],\n",
      "        [0.0780],\n",
      "        [0.0790],\n",
      "        [0.0826],\n",
      "        [0.0814],\n",
      "        [0.0807],\n",
      "        [0.0848],\n",
      "        [0.0925],\n",
      "        [0.0961],\n",
      "        [0.0955],\n",
      "        [0.0969],\n",
      "        [0.1009],\n",
      "        [0.1028],\n",
      "        [0.1035],\n",
      "        [0.1058]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 101.36323738098145\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.607636074320908e-07, 46)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [46, 19, 135, 128, 66, 18, 146, 6, 13, 100, 14, 133, 140, 147, 0, 43, 119, 15, 108, 28, 26, 92, 144, 116, 118, 42, 47, 41, 3, 115, 98, 58, 95, 40, 10, 134, 20, 11, 57, 21, 16, 54, 145, 139, 96, 53, 2, 97, 9, 27, 149, 117, 4, 52, 17, 109, 124, 114, 148, 93, 7, 120, 150, 12, 129, 99, 22, 82, 70, 67, 23, 143, 91, 155, 81, 151, 31, 55, 83, 8, 94, 69, 45, 110, 158, 122, 65, 75, 121, 113, 107, 36, 44, 136, 112, 152, 156, 123, 142, 64, 51, 37, 105, 74, 80, 5, 137, 90, 101, 125, 88, 138, 1, 157, 25, 104, 111, 24, 106, 103, 132, 102, 56, 68, 87, 39, 29, 48, 89, 79, 72, 86, 154, 78, 131, 49, 130, 84, 126, 30, 59, 85, 62, 50, 153, 73, 141, 63, 38, 127, 76, 32, 35, 60, 71, 33, 61] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3629],\n",
      "        [0.5469],\n",
      "        [0.4276],\n",
      "        [0.4111],\n",
      "        [0.2358],\n",
      "        [0.5583],\n",
      "        [0.4856],\n",
      "        [0.6460],\n",
      "        [0.6241],\n",
      "        [0.0988],\n",
      "        [0.6106],\n",
      "        [0.4182],\n",
      "        [0.4924],\n",
      "        [0.4899],\n",
      "        [0.6157],\n",
      "        [0.3526],\n",
      "        [0.0988],\n",
      "        [0.5968],\n",
      "        [0.0988],\n",
      "        [0.4666],\n",
      "        [0.4631],\n",
      "        [0.0988],\n",
      "        [0.4846],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3491],\n",
      "        [0.4003],\n",
      "        [0.3165],\n",
      "        [0.6397],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2742],\n",
      "        [0.0988],\n",
      "        [0.2962],\n",
      "        [0.6188],\n",
      "        [0.4124],\n",
      "        [0.5136],\n",
      "        [0.6036],\n",
      "        [0.2842],\n",
      "        [0.5272],\n",
      "        [0.5728],\n",
      "        [0.3286],\n",
      "        [0.4844],\n",
      "        [0.4765],\n",
      "        [0.0988],\n",
      "        [0.3698],\n",
      "        [0.5972],\n",
      "        [0.0988],\n",
      "        [0.6450],\n",
      "        [0.4694],\n",
      "        [0.4875],\n",
      "        [0.0988],\n",
      "        [0.6586],\n",
      "        [0.3828],\n",
      "        [0.5795],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4858],\n",
      "        [0.0988],\n",
      "        [0.6422],\n",
      "        [0.0988],\n",
      "        [0.4751],\n",
      "        [0.6218],\n",
      "        [0.4892],\n",
      "        [0.0988],\n",
      "        [0.5137],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.2291],\n",
      "        [0.5073],\n",
      "        [0.4978],\n",
      "        [0.0988],\n",
      "        [0.4744],\n",
      "        [0.0988],\n",
      "        [0.4863],\n",
      "        [0.4104],\n",
      "        [0.2810],\n",
      "        [0.0988],\n",
      "        [0.6642],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.3262],\n",
      "        [0.0988],\n",
      "        [0.4773],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3027],\n",
      "        [0.3225],\n",
      "        [0.4317],\n",
      "        [0.0988],\n",
      "        [0.4971],\n",
      "        [0.4716],\n",
      "        [0.0988],\n",
      "        [0.5002],\n",
      "        [0.2257],\n",
      "        [0.3568],\n",
      "        [0.2486],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.6266],\n",
      "        [0.4388],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4512],\n",
      "        [0.6124],\n",
      "        [0.4733],\n",
      "        [0.4729],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4809],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4286],\n",
      "        [0.0988],\n",
      "        [0.2702],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.2627],\n",
      "        [0.4148],\n",
      "        [0.3941],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4903],\n",
      "        [0.0988],\n",
      "        [0.4258],\n",
      "        [0.3796],\n",
      "        [0.4221],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3871],\n",
      "        [0.2971],\n",
      "        [0.0988],\n",
      "        [0.2656],\n",
      "        [0.3575],\n",
      "        [0.4928],\n",
      "        [0.0988],\n",
      "        [0.4870],\n",
      "        [0.2656],\n",
      "        [0.2102],\n",
      "        [0.4052],\n",
      "        [0.1274],\n",
      "        [0.4233],\n",
      "        [0.3658],\n",
      "        [0.2936],\n",
      "        [0.1144],\n",
      "        [0.3926],\n",
      "        [0.2756]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0011],\n",
      "        [0.0013],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0030],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0033],\n",
      "        [0.0041],\n",
      "        [0.0050],\n",
      "        [0.0046],\n",
      "        [0.0049],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0061],\n",
      "        [0.0069],\n",
      "        [0.0073],\n",
      "        [0.0078],\n",
      "        [0.0079],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0088],\n",
      "        [0.0092],\n",
      "        [0.0095],\n",
      "        [0.0091],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0120],\n",
      "        [0.0118],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0129],\n",
      "        [0.0130],\n",
      "        [0.0132],\n",
      "        [0.0133],\n",
      "        [0.0138],\n",
      "        [0.0144],\n",
      "        [0.0152],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0166],\n",
      "        [0.0164],\n",
      "        [0.0164],\n",
      "        [0.0178],\n",
      "        [0.0177],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0183],\n",
      "        [0.0185],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0203],\n",
      "        [0.0204],\n",
      "        [0.0206],\n",
      "        [0.0208],\n",
      "        [0.0212],\n",
      "        [0.0221],\n",
      "        [0.0220],\n",
      "        [0.0222],\n",
      "        [0.0229],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0267],\n",
      "        [0.0273],\n",
      "        [0.0277],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0296],\n",
      "        [0.0300],\n",
      "        [0.0304],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0337],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0353],\n",
      "        [0.0362],\n",
      "        [0.0365],\n",
      "        [0.0375],\n",
      "        [0.0379],\n",
      "        [0.0391],\n",
      "        [0.0394],\n",
      "        [0.0411],\n",
      "        [0.0416],\n",
      "        [0.0421],\n",
      "        [0.0423],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0451],\n",
      "        [0.0447],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0491],\n",
      "        [0.0487],\n",
      "        [0.0502],\n",
      "        [0.0502],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0520],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.0551],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0565],\n",
      "        [0.0571],\n",
      "        [0.0570],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0708],\n",
      "        [0.0718],\n",
      "        [0.0727],\n",
      "        [0.0726],\n",
      "        [0.0737],\n",
      "        [0.0749],\n",
      "        [0.0768],\n",
      "        [0.0775],\n",
      "        [0.0770],\n",
      "        [0.0779],\n",
      "        [0.0780],\n",
      "        [0.0790],\n",
      "        [0.0807],\n",
      "        [0.0814],\n",
      "        [0.0826],\n",
      "        [0.0848],\n",
      "        [0.0925],\n",
      "        [0.0955],\n",
      "        [0.0961],\n",
      "        [0.0969],\n",
      "        [0.1009],\n",
      "        [0.1028],\n",
      "        [0.1035],\n",
      "        [0.1058],\n",
      "        [0.1128]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 12\n",
      "Number of shrink: 13\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0016],\n",
      "        [0.0002],\n",
      "        [0.0042],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0040],\n",
      "        [0.0016],\n",
      "        [0.0042],\n",
      "        [0.0025],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0054],\n",
      "        [0.0068],\n",
      "        [0.0047],\n",
      "        [0.0038],\n",
      "        [0.0029],\n",
      "        [0.0057],\n",
      "        [0.0072],\n",
      "        [0.0069],\n",
      "        [0.0053],\n",
      "        [0.0062],\n",
      "        [0.0079],\n",
      "        [0.0061],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0063],\n",
      "        [0.0117],\n",
      "        [0.0116],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0137],\n",
      "        [0.0118],\n",
      "        [0.0096],\n",
      "        [0.0124],\n",
      "        [0.0149],\n",
      "        [0.0116],\n",
      "        [0.0124],\n",
      "        [0.0149],\n",
      "        [0.0118],\n",
      "        [0.0153],\n",
      "        [0.0167],\n",
      "        [0.0130],\n",
      "        [0.0127],\n",
      "        [0.0154],\n",
      "        [0.0177],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0169],\n",
      "        [0.0164],\n",
      "        [0.0182],\n",
      "        [0.0179],\n",
      "        [0.0169],\n",
      "        [0.0206],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0204],\n",
      "        [0.0216],\n",
      "        [0.0208],\n",
      "        [0.0216],\n",
      "        [0.0211],\n",
      "        [0.0207],\n",
      "        [0.0222],\n",
      "        [0.0212],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0277],\n",
      "        [0.0285],\n",
      "        [0.0297],\n",
      "        [0.0292],\n",
      "        [0.0278],\n",
      "        [0.0286],\n",
      "        [0.0306],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0301],\n",
      "        [0.0327],\n",
      "        [0.0331],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0380],\n",
      "        [0.0340],\n",
      "        [0.0332],\n",
      "        [0.0375],\n",
      "        [0.0373],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0392],\n",
      "        [0.0416],\n",
      "        [0.0397],\n",
      "        [0.0395],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0441],\n",
      "        [0.0418],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0461],\n",
      "        [0.0481],\n",
      "        [0.0491],\n",
      "        [0.0484],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0506],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0533],\n",
      "        [0.0548],\n",
      "        [0.0534],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0541],\n",
      "        [0.0550],\n",
      "        [0.0550],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0698],\n",
      "        [0.0718],\n",
      "        [0.0714],\n",
      "        [0.0707],\n",
      "        [0.0729],\n",
      "        [0.0749],\n",
      "        [0.0768],\n",
      "        [0.0753],\n",
      "        [0.0791],\n",
      "        [0.0779],\n",
      "        [0.0797],\n",
      "        [0.0766],\n",
      "        [0.0799],\n",
      "        [0.0814],\n",
      "        [0.0812],\n",
      "        [0.0865],\n",
      "        [0.0898],\n",
      "        [0.0957],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1034],\n",
      "        [0.1047],\n",
      "        [0.1035],\n",
      "        [0.1088],\n",
      "        [0.1142]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 101.50303196907043\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.034443369325345e-08, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [19, 146, 46, 128, 66, 13, 43, 0, 100, 18, 6, 135, 14, 147, 28, 133, 119, 144, 26, 42, 140, 108, 15, 92, 116, 118, 3, 40, 115, 98, 41, 21, 20, 47, 95, 10, 11, 139, 145, 58, 57, 134, 16, 96, 27, 97, 2, 54, 9, 4, 53, 117, 149, 17, 109, 124, 114, 148, 93, 120, 129, 52, 12, 22, 150, 7, 99, 82, 70, 143, 67, 23, 91, 31, 55, 155, 151, 81, 45, 83, 8, 94, 69, 110, 158, 136, 122, 65, 44, 75, 121, 113, 107, 152, 112, 36, 156, 142, 123, 37, 51, 64, 137, 105, 74, 80, 5, 90, 101, 125, 138, 88, 1, 25, 157, 24, 104, 111, 106, 132, 56, 39, 103, 102, 48, 29, 68, 87, 89, 79, 72, 86, 154, 49, 131, 78, 130, 84, 30, 50, 126, 85, 59, 62, 153, 141, 73, 63, 38, 127, 76, 32, 35, 71, 60, 33, 77, 61] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5482],\n",
      "        [0.4866],\n",
      "        [0.3650],\n",
      "        [0.4111],\n",
      "        [0.2358],\n",
      "        [0.6253],\n",
      "        [0.3551],\n",
      "        [0.6146],\n",
      "        [0.0988],\n",
      "        [0.5598],\n",
      "        [0.6472],\n",
      "        [0.4305],\n",
      "        [0.6112],\n",
      "        [0.4900],\n",
      "        [0.4687],\n",
      "        [0.4196],\n",
      "        [0.0988],\n",
      "        [0.4865],\n",
      "        [0.4647],\n",
      "        [0.3516],\n",
      "        [0.4943],\n",
      "        [0.0988],\n",
      "        [0.5978],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6401],\n",
      "        [0.2982],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3187],\n",
      "        [0.5288],\n",
      "        [0.5148],\n",
      "        [0.4028],\n",
      "        [0.0988],\n",
      "        [0.6193],\n",
      "        [0.6042],\n",
      "        [0.4790],\n",
      "        [0.4865],\n",
      "        [0.2759],\n",
      "        [0.2859],\n",
      "        [0.4146],\n",
      "        [0.5743],\n",
      "        [0.0988],\n",
      "        [0.4709],\n",
      "        [0.0988],\n",
      "        [0.5967],\n",
      "        [0.3309],\n",
      "        [0.6455],\n",
      "        [0.6598],\n",
      "        [0.3719],\n",
      "        [0.0988],\n",
      "        [0.4881],\n",
      "        [0.5807],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4860],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4879],\n",
      "        [0.3851],\n",
      "        [0.6228],\n",
      "        [0.5154],\n",
      "        [0.4756],\n",
      "        [0.6432],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4997],\n",
      "        [0.2291],\n",
      "        [0.5086],\n",
      "        [0.0988],\n",
      "        [0.4126],\n",
      "        [0.2829],\n",
      "        [0.4740],\n",
      "        [0.4859],\n",
      "        [0.0988],\n",
      "        [0.3284],\n",
      "        [0.0988],\n",
      "        [0.6647],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.4767],\n",
      "        [0.4349],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.3247],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4965],\n",
      "        [0.0988],\n",
      "        [0.3055],\n",
      "        [0.4710],\n",
      "        [0.5021],\n",
      "        [0.0988],\n",
      "        [0.2515],\n",
      "        [0.3592],\n",
      "        [0.2257],\n",
      "        [0.4417],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.6276],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4542],\n",
      "        [0.0988],\n",
      "        [0.6119],\n",
      "        [0.4747],\n",
      "        [0.4722],\n",
      "        [0.4824],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4298],\n",
      "        [0.2719],\n",
      "        [0.2651],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3961],\n",
      "        [0.4169],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4893],\n",
      "        [0.3815],\n",
      "        [0.4271],\n",
      "        [0.0988],\n",
      "        [0.4229],\n",
      "        [0.0988],\n",
      "        [0.3892],\n",
      "        [0.3599],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2992],\n",
      "        [0.2673],\n",
      "        [0.4920],\n",
      "        [0.4884],\n",
      "        [0.0988],\n",
      "        [0.2674],\n",
      "        [0.2129],\n",
      "        [0.4054],\n",
      "        [0.1274],\n",
      "        [0.4257],\n",
      "        [0.3683],\n",
      "        [0.1144],\n",
      "        [0.2956],\n",
      "        [0.3955],\n",
      "        [0.1127],\n",
      "        [0.2770]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0002],\n",
      "        [0.0016],\n",
      "        [0.0016],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0025],\n",
      "        [0.0029],\n",
      "        [0.0038],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0040],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0054],\n",
      "        [0.0057],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0072],\n",
      "        [0.0079],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0096],\n",
      "        [0.0096],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0116],\n",
      "        [0.0117],\n",
      "        [0.0118],\n",
      "        [0.0124],\n",
      "        [0.0124],\n",
      "        [0.0127],\n",
      "        [0.0130],\n",
      "        [0.0137],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0164],\n",
      "        [0.0164],\n",
      "        [0.0160],\n",
      "        [0.0167],\n",
      "        [0.0169],\n",
      "        [0.0169],\n",
      "        [0.0177],\n",
      "        [0.0179],\n",
      "        [0.0182],\n",
      "        [0.0196],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0201],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0207],\n",
      "        [0.0206],\n",
      "        [0.0211],\n",
      "        [0.0212],\n",
      "        [0.0216],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0277],\n",
      "        [0.0278],\n",
      "        [0.0286],\n",
      "        [0.0285],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0301],\n",
      "        [0.0306],\n",
      "        [0.0314],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0331],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0373],\n",
      "        [0.0375],\n",
      "        [0.0380],\n",
      "        [0.0384],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0395],\n",
      "        [0.0397],\n",
      "        [0.0416],\n",
      "        [0.0418],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0441],\n",
      "        [0.0449],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0461],\n",
      "        [0.0476],\n",
      "        [0.0481],\n",
      "        [0.0484],\n",
      "        [0.0491],\n",
      "        [0.0506],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0533],\n",
      "        [0.0534],\n",
      "        [0.0541],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0550],\n",
      "        [0.0550],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0675],\n",
      "        [0.0698],\n",
      "        [0.0707],\n",
      "        [0.0714],\n",
      "        [0.0718],\n",
      "        [0.0729],\n",
      "        [0.0749],\n",
      "        [0.0753],\n",
      "        [0.0766],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0791],\n",
      "        [0.0797],\n",
      "        [0.0799],\n",
      "        [0.0812],\n",
      "        [0.0814],\n",
      "        [0.0865],\n",
      "        [0.0898],\n",
      "        [0.0957],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1034],\n",
      "        [0.1035],\n",
      "        [0.1047],\n",
      "        [0.1088],\n",
      "        [0.1127],\n",
      "        [0.1142]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 6\n",
      "Number of shrink: 10\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0020],\n",
      "        [    0.0010],\n",
      "        [    0.0014],\n",
      "        [    0.0022],\n",
      "        [    0.0028],\n",
      "        [    0.0035],\n",
      "        [    0.0029],\n",
      "        [    0.0037],\n",
      "        [    0.0036],\n",
      "        [    0.0043],\n",
      "        [    0.0042],\n",
      "        [    0.0039],\n",
      "        [    0.0045],\n",
      "        [    0.0053],\n",
      "        [    0.0054],\n",
      "        [    0.0057],\n",
      "        [    0.0060],\n",
      "        [    0.0063],\n",
      "        [    0.0066],\n",
      "        [    0.0067],\n",
      "        [    0.0069],\n",
      "        [    0.0072],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0096],\n",
      "        [    0.0102],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0109],\n",
      "        [    0.0122],\n",
      "        [    0.0118],\n",
      "        [    0.0111],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0126],\n",
      "        [    0.0126],\n",
      "        [    0.0129],\n",
      "        [    0.0130],\n",
      "        [    0.0141],\n",
      "        [    0.0150],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0165],\n",
      "        [    0.0164],\n",
      "        [    0.0160],\n",
      "        [    0.0160],\n",
      "        [    0.0163],\n",
      "        [    0.0172],\n",
      "        [    0.0171],\n",
      "        [    0.0179],\n",
      "        [    0.0177],\n",
      "        [    0.0192],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0199],\n",
      "        [    0.0206],\n",
      "        [    0.0204],\n",
      "        [    0.0208],\n",
      "        [    0.0205],\n",
      "        [    0.0203],\n",
      "        [    0.0215],\n",
      "        [    0.0216],\n",
      "        [    0.0212],\n",
      "        [    0.0214],\n",
      "        [    0.0222],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0254],\n",
      "        [    0.0254],\n",
      "        [    0.0258],\n",
      "        [    0.0277],\n",
      "        [    0.0279],\n",
      "        [    0.0292],\n",
      "        [    0.0281],\n",
      "        [    0.0289],\n",
      "        [    0.0297],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0315],\n",
      "        [    0.0316],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0325],\n",
      "        [    0.0333],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0345],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0368],\n",
      "        [    0.0375],\n",
      "        [    0.0380],\n",
      "        [    0.0380],\n",
      "        [    0.0395],\n",
      "        [    0.0394],\n",
      "        [    0.0396],\n",
      "        [    0.0404],\n",
      "        [    0.0416],\n",
      "        [    0.0418],\n",
      "        [    0.0425],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0458],\n",
      "        [    0.0476],\n",
      "        [    0.0472],\n",
      "        [    0.0484],\n",
      "        [    0.0487],\n",
      "        [    0.0505],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0525],\n",
      "        [    0.0534],\n",
      "        [    0.0541],\n",
      "        [    0.0546],\n",
      "        [    0.0542],\n",
      "        [    0.0548],\n",
      "        [    0.0554],\n",
      "        [    0.0550],\n",
      "        [    0.0554],\n",
      "        [    0.0564],\n",
      "        [    0.0592],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0691],\n",
      "        [    0.0711],\n",
      "        [    0.0718],\n",
      "        [    0.0718],\n",
      "        [    0.0731],\n",
      "        [    0.0749],\n",
      "        [    0.0755],\n",
      "        [    0.0772],\n",
      "        [    0.0768],\n",
      "        [    0.0779],\n",
      "        [    0.0784],\n",
      "        [    0.0789],\n",
      "        [    0.0792],\n",
      "        [    0.0813],\n",
      "        [    0.0814],\n",
      "        [    0.0860],\n",
      "        [    0.0900],\n",
      "        [    0.0951],\n",
      "        [    0.0961],\n",
      "        [    0.0990],\n",
      "        [    0.1038],\n",
      "        [    0.1035],\n",
      "        [    0.1041],\n",
      "        [    0.1087],\n",
      "        [    0.1127],\n",
      "        [    0.1137]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 101.62621736526489\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.0823311075446327e-08, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [19, 46, 128, 146, 66, 13, 0, 43, 100, 18, 135, 14, 6, 147, 133, 28, 144, 119, 26, 140, 42, 108, 15, 92, 116, 118, 3, 115, 98, 40, 41, 47, 21, 95, 20, 11, 139, 10, 145, 58, 57, 134, 16, 96, 2, 54, 97, 27, 9, 4, 53, 149, 117, 109, 124, 17, 114, 52, 93, 148, 129, 120, 12, 150, 22, 7, 99, 82, 70, 143, 67, 23, 91, 31, 155, 151, 55, 81, 45, 83, 8, 94, 69, 110, 158, 136, 122, 65, 75, 121, 44, 113, 107, 152, 112, 156, 36, 142, 123, 37, 51, 137, 64, 105, 74, 5, 80, 90, 101, 125, 138, 1, 88, 25, 157, 24, 104, 111, 106, 132, 103, 39, 56, 102, 29, 68, 48, 87, 89, 79, 72, 86, 154, 49, 131, 78, 130, 84, 30, 126, 50, 85, 59, 153, 62, 141, 73, 63, 38, 127, 76, 32, 71, 35, 60, 33, 77, 61, 34] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5479],\n",
      "        [0.3644],\n",
      "        [0.4105],\n",
      "        [0.4862],\n",
      "        [0.2358],\n",
      "        [0.6250],\n",
      "        [0.6137],\n",
      "        [0.3545],\n",
      "        [0.0988],\n",
      "        [0.5595],\n",
      "        [0.4306],\n",
      "        [0.6112],\n",
      "        [0.6472],\n",
      "        [0.4898],\n",
      "        [0.4195],\n",
      "        [0.4686],\n",
      "        [0.4866],\n",
      "        [0.0988],\n",
      "        [0.4646],\n",
      "        [0.4942],\n",
      "        [0.3513],\n",
      "        [0.0988],\n",
      "        [0.5979],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6401],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2976],\n",
      "        [0.3180],\n",
      "        [0.4022],\n",
      "        [0.5284],\n",
      "        [0.0988],\n",
      "        [0.5146],\n",
      "        [0.6040],\n",
      "        [0.4790],\n",
      "        [0.6188],\n",
      "        [0.4867],\n",
      "        [0.2752],\n",
      "        [0.2851],\n",
      "        [0.4147],\n",
      "        [0.5740],\n",
      "        [0.0988],\n",
      "        [0.5967],\n",
      "        [0.3301],\n",
      "        [0.0988],\n",
      "        [0.4708],\n",
      "        [0.6450],\n",
      "        [0.6596],\n",
      "        [0.3713],\n",
      "        [0.4876],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5803],\n",
      "        [0.0988],\n",
      "        [0.3847],\n",
      "        [0.0988],\n",
      "        [0.4855],\n",
      "        [0.4877],\n",
      "        [0.0988],\n",
      "        [0.6224],\n",
      "        [0.4752],\n",
      "        [0.5150],\n",
      "        [0.6430],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4996],\n",
      "        [0.2291],\n",
      "        [0.5082],\n",
      "        [0.0988],\n",
      "        [0.4125],\n",
      "        [0.4736],\n",
      "        [0.4856],\n",
      "        [0.2823],\n",
      "        [0.0988],\n",
      "        [0.3281],\n",
      "        [0.0988],\n",
      "        [0.6648],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.4761],\n",
      "        [0.4349],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.3242],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4960],\n",
      "        [0.0988],\n",
      "        [0.4705],\n",
      "        [0.3054],\n",
      "        [0.5018],\n",
      "        [0.0988],\n",
      "        [0.2513],\n",
      "        [0.3586],\n",
      "        [0.4418],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.6276],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4545],\n",
      "        [0.6110],\n",
      "        [0.0988],\n",
      "        [0.4746],\n",
      "        [0.4718],\n",
      "        [0.4824],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4297],\n",
      "        [0.0988],\n",
      "        [0.2646],\n",
      "        [0.2712],\n",
      "        [0.0988],\n",
      "        [0.4169],\n",
      "        [0.2529],\n",
      "        [0.3956],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4887],\n",
      "        [0.3811],\n",
      "        [0.4267],\n",
      "        [0.0988],\n",
      "        [0.4227],\n",
      "        [0.0988],\n",
      "        [0.3890],\n",
      "        [0.0988],\n",
      "        [0.3593],\n",
      "        [0.0988],\n",
      "        [0.2986],\n",
      "        [0.4913],\n",
      "        [0.2665],\n",
      "        [0.4882],\n",
      "        [0.0988],\n",
      "        [0.2668],\n",
      "        [0.2126],\n",
      "        [0.4048],\n",
      "        [0.1274],\n",
      "        [0.4253],\n",
      "        [0.1143],\n",
      "        [0.3688],\n",
      "        [0.2950],\n",
      "        [0.3955],\n",
      "        [0.1127],\n",
      "        [0.2764],\n",
      "        [0.3770]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0010],\n",
      "        [    0.0014],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0035],\n",
      "        [    0.0037],\n",
      "        [    0.0036],\n",
      "        [    0.0042],\n",
      "        [    0.0039],\n",
      "        [    0.0043],\n",
      "        [    0.0045],\n",
      "        [    0.0054],\n",
      "        [    0.0053],\n",
      "        [    0.0060],\n",
      "        [    0.0057],\n",
      "        [    0.0063],\n",
      "        [    0.0067],\n",
      "        [    0.0066],\n",
      "        [    0.0069],\n",
      "        [    0.0072],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0096],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0102],\n",
      "        [    0.0109],\n",
      "        [    0.0111],\n",
      "        [    0.0122],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0126],\n",
      "        [    0.0126],\n",
      "        [    0.0118],\n",
      "        [    0.0129],\n",
      "        [    0.0130],\n",
      "        [    0.0141],\n",
      "        [    0.0150],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0160],\n",
      "        [    0.0160],\n",
      "        [    0.0164],\n",
      "        [    0.0165],\n",
      "        [    0.0163],\n",
      "        [    0.0172],\n",
      "        [    0.0171],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0192],\n",
      "        [    0.0199],\n",
      "        [    0.0203],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0205],\n",
      "        [    0.0208],\n",
      "        [    0.0215],\n",
      "        [    0.0212],\n",
      "        [    0.0216],\n",
      "        [    0.0214],\n",
      "        [    0.0222],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0254],\n",
      "        [    0.0254],\n",
      "        [    0.0258],\n",
      "        [    0.0277],\n",
      "        [    0.0279],\n",
      "        [    0.0281],\n",
      "        [    0.0289],\n",
      "        [    0.0292],\n",
      "        [    0.0297],\n",
      "        [    0.0305],\n",
      "        [    0.0306],\n",
      "        [    0.0315],\n",
      "        [    0.0316],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0325],\n",
      "        [    0.0333],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0345],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0368],\n",
      "        [    0.0375],\n",
      "        [    0.0380],\n",
      "        [    0.0380],\n",
      "        [    0.0395],\n",
      "        [    0.0394],\n",
      "        [    0.0396],\n",
      "        [    0.0404],\n",
      "        [    0.0418],\n",
      "        [    0.0416],\n",
      "        [    0.0425],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0458],\n",
      "        [    0.0472],\n",
      "        [    0.0476],\n",
      "        [    0.0484],\n",
      "        [    0.0487],\n",
      "        [    0.0505],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0525],\n",
      "        [    0.0534],\n",
      "        [    0.0542],\n",
      "        [    0.0546],\n",
      "        [    0.0541],\n",
      "        [    0.0548],\n",
      "        [    0.0550],\n",
      "        [    0.0554],\n",
      "        [    0.0554],\n",
      "        [    0.0564],\n",
      "        [    0.0592],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0691],\n",
      "        [    0.0711],\n",
      "        [    0.0718],\n",
      "        [    0.0718],\n",
      "        [    0.0731],\n",
      "        [    0.0749],\n",
      "        [    0.0755],\n",
      "        [    0.0768],\n",
      "        [    0.0772],\n",
      "        [    0.0779],\n",
      "        [    0.0784],\n",
      "        [    0.0792],\n",
      "        [    0.0789],\n",
      "        [    0.0813],\n",
      "        [    0.0814],\n",
      "        [    0.0860],\n",
      "        [    0.0900],\n",
      "        [    0.0951],\n",
      "        [    0.0961],\n",
      "        [    0.0990],\n",
      "        [    0.1035],\n",
      "        [    0.1038],\n",
      "        [    0.1041],\n",
      "        [    0.1087],\n",
      "        [    0.1127],\n",
      "        [    0.1137],\n",
      "        [    0.1496]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 1\n",
      "Number of shrink: 7\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0044],\n",
      "        [    0.0020],\n",
      "        [    0.0017],\n",
      "        [    0.0058],\n",
      "        [    0.0022],\n",
      "        [    0.0072],\n",
      "        [    0.0012],\n",
      "        [    0.0064],\n",
      "        [    0.0037],\n",
      "        [    0.0007],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0000],\n",
      "        [    0.0005],\n",
      "        [    0.0019],\n",
      "        [    0.0093],\n",
      "        [    0.0098],\n",
      "        [    0.0057],\n",
      "        [    0.0099],\n",
      "        [    0.0030],\n",
      "        [    0.0095],\n",
      "        [    0.0069],\n",
      "        [    0.0027],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0051],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0130],\n",
      "        [    0.0080],\n",
      "        [    0.0085],\n",
      "        [    0.0163],\n",
      "        [    0.0118],\n",
      "        [    0.0159],\n",
      "        [    0.0170],\n",
      "        [    0.0162],\n",
      "        [    0.0077],\n",
      "        [    0.0167],\n",
      "        [    0.0098],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0103],\n",
      "        [    0.0154],\n",
      "        [    0.0114],\n",
      "        [    0.0127],\n",
      "        [    0.0164],\n",
      "        [    0.0201],\n",
      "        [    0.0123],\n",
      "        [    0.0216],\n",
      "        [    0.0142],\n",
      "        [    0.0138],\n",
      "        [    0.0179],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0149],\n",
      "        [    0.0199],\n",
      "        [    0.0171],\n",
      "        [    0.0204],\n",
      "        [    0.0243],\n",
      "        [    0.0198],\n",
      "        [    0.0208],\n",
      "        [    0.0258],\n",
      "        [    0.0175],\n",
      "        [    0.0257],\n",
      "        [    0.0173],\n",
      "        [    0.0222],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0291],\n",
      "        [    0.0254],\n",
      "        [    0.0298],\n",
      "        [    0.0277],\n",
      "        [    0.0317],\n",
      "        [    0.0241],\n",
      "        [    0.0251],\n",
      "        [    0.0325],\n",
      "        [    0.0297],\n",
      "        [    0.0335],\n",
      "        [    0.0306],\n",
      "        [    0.0271],\n",
      "        [    0.0316],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0288],\n",
      "        [    0.0369],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0372],\n",
      "        [    0.0345],\n",
      "        [    0.0350],\n",
      "        [    0.0331],\n",
      "        [    0.0375],\n",
      "        [    0.0344],\n",
      "        [    0.0347],\n",
      "        [    0.0433],\n",
      "        [    0.0394],\n",
      "        [    0.0427],\n",
      "        [    0.0432],\n",
      "        [    0.0456],\n",
      "        [    0.0416],\n",
      "        [    0.0425],\n",
      "        [    0.0437],\n",
      "        [    0.0482],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0496],\n",
      "        [    0.0432],\n",
      "        [    0.0476],\n",
      "        [    0.0524],\n",
      "        [    0.0451],\n",
      "        [    0.0547],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0525],\n",
      "        [    0.0566],\n",
      "        [    0.0542],\n",
      "        [    0.0573],\n",
      "        [    0.0574],\n",
      "        [    0.0548],\n",
      "        [    0.0587],\n",
      "        [    0.0554],\n",
      "        [    0.0582],\n",
      "        [    0.0564],\n",
      "        [    0.0592],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0655],\n",
      "        [    0.0741],\n",
      "        [    0.0750],\n",
      "        [    0.0718],\n",
      "        [    0.0761],\n",
      "        [    0.0749],\n",
      "        [    0.0789],\n",
      "        [    0.0768],\n",
      "        [    0.0801],\n",
      "        [    0.0779],\n",
      "        [    0.0755],\n",
      "        [    0.0753],\n",
      "        [    0.0762],\n",
      "        [    0.0849],\n",
      "        [    0.0814],\n",
      "        [    0.0832],\n",
      "        [    0.0930],\n",
      "        [    0.0923],\n",
      "        [    0.0961],\n",
      "        [    0.0954],\n",
      "        [    0.1035],\n",
      "        [    0.1002],\n",
      "        [    0.1012],\n",
      "        [    0.1050],\n",
      "        [    0.1127],\n",
      "        [    0.1108],\n",
      "        [    0.1460]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 101.73223328590393\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 33 個區塊累積花費時間(s) 0.5740199089050293\n",
      "<<The performance of 33 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.5740199089050293\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 941.89\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1202.93\n",
      "The accuracy(2000) for l = 1: 89.94%\n",
      "The accuracy(3000) for l = 1: 99.37%\n",
      "The maximum error: tensor(3728.4492)\n",
      "The minimum error: tensor(0.4180)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 615.4\n",
      "The MAPE for l = 1: 0.0%\n",
      "The RMSE for l = 1: 644.6\n",
      "The accuracy(2000) for l = 1: 100.0%\n",
      "The accuracy(3000) for l = 1: 100.0%\n",
      "The maximum error: 789.7734375\n",
      "The minimum error: 296.7421875\n",
      "------------------------------------------------------------\n",
      "0.89937106918239\n",
      "<class 'float'>\n",
      "1.0\n",
      "<class 'float'>\n",
      "The <<34>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.784581785268529e-08, 2)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [2, 131, 143, 10, 14, 124, 129, 42, 62, 11, 136, 96, 15, 142, 115, 39, 9, 104, 6, 37, 88, 112, 43, 114, 24, 38, 140, 54, 111, 94, 22, 12, 53, 130, 156, 91, 5, 50, 36, 145, 49, 13, 92, 16, 135, 17, 93, 141, 7, 48, 146, 3, 113, 105, 120, 125, 110, 89, 23, 116, 0, 95, 78, 66, 151, 144, 155, 147, 8, 63, 18, 87, 4, 158, 154, 139, 77, 19, 79, 157, 90, 27, 65, 106, 51, 148, 41, 118, 61, 71, 117, 152, 109, 32, 103, 132, 40, 108, 119, 60, 33, 101, 138, 47, 70, 76, 86, 97, 153, 121, 133, 84, 1, 134, 100, 107, 21, 102, 99, 20, 98, 64, 83, 128, 35, 52, 44, 25, 85, 150, 75, 68, 82, 74, 45, 127, 80, 149, 55, 126, 58, 122, 81, 26, 46, 69, 59, 137, 123, 34, 28, 72, 31, 56, 67]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0027],\n",
      "        [    0.0030],\n",
      "        [    0.0037],\n",
      "        [    0.0044],\n",
      "        [    0.0058],\n",
      "        [    0.0057],\n",
      "        [    0.0064],\n",
      "        [    0.0072],\n",
      "        [    0.0069],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0082],\n",
      "        [    0.0093],\n",
      "        [    0.0095],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0099],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0123],\n",
      "        [    0.0127],\n",
      "        [    0.0130],\n",
      "        [    0.0138],\n",
      "        [    0.0142],\n",
      "        [    0.0149],\n",
      "        [    0.0154],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0175],\n",
      "        [    0.0173],\n",
      "        [    0.0179],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0201],\n",
      "        [    0.0208],\n",
      "        [    0.0216],\n",
      "        [    0.0222],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0241],\n",
      "        [    0.0243],\n",
      "        [    0.0248],\n",
      "        [    0.0251],\n",
      "        [    0.0258],\n",
      "        [    0.0254],\n",
      "        [    0.0257],\n",
      "        [    0.0277],\n",
      "        [    0.0271],\n",
      "        [    0.0285],\n",
      "        [    0.0288],\n",
      "        [    0.0291],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0306],\n",
      "        [    0.0311],\n",
      "        [    0.0316],\n",
      "        [    0.0317],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0325],\n",
      "        [    0.0331],\n",
      "        [    0.0335],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0345],\n",
      "        [    0.0347],\n",
      "        [    0.0350],\n",
      "        [    0.0369],\n",
      "        [    0.0372],\n",
      "        [    0.0375],\n",
      "        [    0.0394],\n",
      "        [    0.0416],\n",
      "        [    0.0427],\n",
      "        [    0.0425],\n",
      "        [    0.0433],\n",
      "        [    0.0432],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0451],\n",
      "        [    0.0455],\n",
      "        [    0.0456],\n",
      "        [    0.0476],\n",
      "        [    0.0482],\n",
      "        [    0.0496],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0524],\n",
      "        [    0.0525],\n",
      "        [    0.0542],\n",
      "        [    0.0547],\n",
      "        [    0.0548],\n",
      "        [    0.0554],\n",
      "        [    0.0564],\n",
      "        [    0.0566],\n",
      "        [    0.0573],\n",
      "        [    0.0574],\n",
      "        [    0.0582],\n",
      "        [    0.0587],\n",
      "        [    0.0592],\n",
      "        [    0.0655],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0718],\n",
      "        [    0.0741],\n",
      "        [    0.0750],\n",
      "        [    0.0749],\n",
      "        [    0.0753],\n",
      "        [    0.0755],\n",
      "        [    0.0761],\n",
      "        [    0.0762],\n",
      "        [    0.0768],\n",
      "        [    0.0779],\n",
      "        [    0.0789],\n",
      "        [    0.0801],\n",
      "        [    0.0814],\n",
      "        [    0.0832],\n",
      "        [    0.0849],\n",
      "        [    0.0923],\n",
      "        [    0.0930],\n",
      "        [    0.0954],\n",
      "        [    0.0961],\n",
      "        [    0.1002],\n",
      "        [    0.1012],\n",
      "        [    0.1035]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.784581785268529e-08, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [2, 131, 143, 10, 14, 124, 129, 42, 62, 11, 136, 96, 15, 142, 115, 39, 9, 104, 6, 37, 88, 112, 43, 114, 24, 38, 140, 54, 111, 94, 22, 12, 53, 130, 156, 91, 5, 50, 36, 145, 49, 13, 92, 16, 135, 17, 93, 141, 7, 48, 146, 3, 113, 105, 120, 125, 110, 89, 23, 116, 0, 95, 78, 66, 151, 144, 155, 147, 8, 63, 18, 87, 4, 158, 154, 139, 77, 19, 79, 157, 90, 27, 65, 106, 51, 148, 41, 118, 61, 71, 117, 152, 109, 32, 103, 132, 40, 108, 119, 60, 33, 101, 138, 47, 70, 76, 86, 97, 153, 121, 133, 84, 1, 134, 100, 107, 21, 102, 99, 20, 98, 64, 83, 128, 35, 52, 44, 25, 85, 150, 75, 68, 82, 74, 45, 127, 80, 149, 55, 126, 58, 122, 81, 26, 46, 69, 59, 137, 123, 34, 28, 72, 31, 56, 67, 29] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6430],\n",
      "        [0.4267],\n",
      "        [0.4858],\n",
      "        [0.6068],\n",
      "        [0.5551],\n",
      "        [0.4074],\n",
      "        [0.4160],\n",
      "        [0.3615],\n",
      "        [0.2358],\n",
      "        [0.5933],\n",
      "        [0.4904],\n",
      "        [0.0988],\n",
      "        [0.5436],\n",
      "        [0.4824],\n",
      "        [0.0988],\n",
      "        [0.3516],\n",
      "        [0.6206],\n",
      "        [0.0988],\n",
      "        [0.6147],\n",
      "        [0.3151],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3996],\n",
      "        [0.0988],\n",
      "        [0.4646],\n",
      "        [0.3484],\n",
      "        [0.4827],\n",
      "        [0.2720],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4610],\n",
      "        [0.5693],\n",
      "        [0.2820],\n",
      "        [0.4111],\n",
      "        [0.4691],\n",
      "        [0.0988],\n",
      "        [0.6409],\n",
      "        [0.3269],\n",
      "        [0.2948],\n",
      "        [0.4836],\n",
      "        [0.3685],\n",
      "        [0.5760],\n",
      "        [0.0988],\n",
      "        [0.5106],\n",
      "        [0.4754],\n",
      "        [0.5243],\n",
      "        [0.0988],\n",
      "        [0.4828],\n",
      "        [0.5996],\n",
      "        [0.3816],\n",
      "        [0.4715],\n",
      "        [0.6389],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4869],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4672],\n",
      "        [0.0988],\n",
      "        [0.6551],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4696],\n",
      "        [0.4818],\n",
      "        [0.4631],\n",
      "        [0.4818],\n",
      "        [0.6181],\n",
      "        [0.2291],\n",
      "        [0.5109],\n",
      "        [0.0988],\n",
      "        [0.6604],\n",
      "        [0.4598],\n",
      "        [0.4724],\n",
      "        [0.4959],\n",
      "        [0.0988],\n",
      "        [0.5042],\n",
      "        [0.0988],\n",
      "        [0.4595],\n",
      "        [0.0988],\n",
      "        [0.4088],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.2789],\n",
      "        [0.4923],\n",
      "        [0.3251],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.4669],\n",
      "        [0.0988],\n",
      "        [0.3022],\n",
      "        [0.0988],\n",
      "        [0.4313],\n",
      "        [0.3215],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.2483],\n",
      "        [0.0988],\n",
      "        [0.4980],\n",
      "        [0.3557],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4683],\n",
      "        [0.0988],\n",
      "        [0.4380],\n",
      "        [0.0988],\n",
      "        [0.6235],\n",
      "        [0.4508],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4706],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4783],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.4265],\n",
      "        [0.2620],\n",
      "        [0.2679],\n",
      "        [0.3928],\n",
      "        [0.4132],\n",
      "        [0.0988],\n",
      "        [0.4850],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3781],\n",
      "        [0.4235],\n",
      "        [0.0988],\n",
      "        [0.4874],\n",
      "        [0.2956],\n",
      "        [0.4197],\n",
      "        [0.2638],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3857],\n",
      "        [0.3564],\n",
      "        [0.0988],\n",
      "        [0.2640],\n",
      "        [0.4847],\n",
      "        [0.4020],\n",
      "        [0.2097],\n",
      "        [0.4218],\n",
      "        [0.1274],\n",
      "        [0.3652],\n",
      "        [0.2920],\n",
      "        [0.1143],\n",
      "        [0.3918]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0004],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0007],\n",
      "        [    0.0017],\n",
      "        [    0.0019],\n",
      "        [    0.0020],\n",
      "        [    0.0022],\n",
      "        [    0.0027],\n",
      "        [    0.0030],\n",
      "        [    0.0037],\n",
      "        [    0.0044],\n",
      "        [    0.0058],\n",
      "        [    0.0057],\n",
      "        [    0.0064],\n",
      "        [    0.0072],\n",
      "        [    0.0069],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0085],\n",
      "        [    0.0082],\n",
      "        [    0.0093],\n",
      "        [    0.0095],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0099],\n",
      "        [    0.0103],\n",
      "        [    0.0110],\n",
      "        [    0.0115],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0123],\n",
      "        [    0.0127],\n",
      "        [    0.0130],\n",
      "        [    0.0138],\n",
      "        [    0.0142],\n",
      "        [    0.0149],\n",
      "        [    0.0154],\n",
      "        [    0.0159],\n",
      "        [    0.0162],\n",
      "        [    0.0163],\n",
      "        [    0.0164],\n",
      "        [    0.0167],\n",
      "        [    0.0170],\n",
      "        [    0.0171],\n",
      "        [    0.0175],\n",
      "        [    0.0173],\n",
      "        [    0.0179],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0198],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0201],\n",
      "        [    0.0208],\n",
      "        [    0.0216],\n",
      "        [    0.0222],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0241],\n",
      "        [    0.0243],\n",
      "        [    0.0248],\n",
      "        [    0.0251],\n",
      "        [    0.0258],\n",
      "        [    0.0254],\n",
      "        [    0.0257],\n",
      "        [    0.0277],\n",
      "        [    0.0271],\n",
      "        [    0.0285],\n",
      "        [    0.0288],\n",
      "        [    0.0291],\n",
      "        [    0.0297],\n",
      "        [    0.0298],\n",
      "        [    0.0306],\n",
      "        [    0.0311],\n",
      "        [    0.0316],\n",
      "        [    0.0317],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0325],\n",
      "        [    0.0331],\n",
      "        [    0.0335],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0344],\n",
      "        [    0.0345],\n",
      "        [    0.0347],\n",
      "        [    0.0350],\n",
      "        [    0.0369],\n",
      "        [    0.0372],\n",
      "        [    0.0375],\n",
      "        [    0.0394],\n",
      "        [    0.0416],\n",
      "        [    0.0427],\n",
      "        [    0.0425],\n",
      "        [    0.0433],\n",
      "        [    0.0432],\n",
      "        [    0.0437],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0451],\n",
      "        [    0.0455],\n",
      "        [    0.0456],\n",
      "        [    0.0476],\n",
      "        [    0.0482],\n",
      "        [    0.0496],\n",
      "        [    0.0509],\n",
      "        [    0.0515],\n",
      "        [    0.0524],\n",
      "        [    0.0525],\n",
      "        [    0.0542],\n",
      "        [    0.0547],\n",
      "        [    0.0548],\n",
      "        [    0.0554],\n",
      "        [    0.0564],\n",
      "        [    0.0566],\n",
      "        [    0.0573],\n",
      "        [    0.0574],\n",
      "        [    0.0582],\n",
      "        [    0.0587],\n",
      "        [    0.0592],\n",
      "        [    0.0655],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0718],\n",
      "        [    0.0741],\n",
      "        [    0.0750],\n",
      "        [    0.0749],\n",
      "        [    0.0753],\n",
      "        [    0.0755],\n",
      "        [    0.0761],\n",
      "        [    0.0762],\n",
      "        [    0.0768],\n",
      "        [    0.0779],\n",
      "        [    0.0789],\n",
      "        [    0.0801],\n",
      "        [    0.0814],\n",
      "        [    0.0832],\n",
      "        [    0.0849],\n",
      "        [    0.0923],\n",
      "        [    0.0930],\n",
      "        [    0.0954],\n",
      "        [    0.0961],\n",
      "        [    0.1002],\n",
      "        [    0.1012],\n",
      "        [    0.1035],\n",
      "        [    0.1050]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 6\n",
      "Number of shrink: 10\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0047],\n",
      "        [0.0043],\n",
      "        [0.0040],\n",
      "        [0.0047],\n",
      "        [0.0041],\n",
      "        [0.0013],\n",
      "        [0.0053],\n",
      "        [0.0012],\n",
      "        [0.0022],\n",
      "        [0.0075],\n",
      "        [0.0066],\n",
      "        [0.0037],\n",
      "        [0.0003],\n",
      "        [0.0022],\n",
      "        [0.0057],\n",
      "        [0.0031],\n",
      "        [0.0023],\n",
      "        [0.0069],\n",
      "        [0.0123],\n",
      "        [0.0112],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0115],\n",
      "        [0.0082],\n",
      "        [0.0053],\n",
      "        [0.0064],\n",
      "        [0.0060],\n",
      "        [0.0130],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0059],\n",
      "        [0.0154],\n",
      "        [0.0142],\n",
      "        [0.0149],\n",
      "        [0.0085],\n",
      "        [0.0118],\n",
      "        [0.0172],\n",
      "        [0.0163],\n",
      "        [0.0102],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0198],\n",
      "        [0.0154],\n",
      "        [0.0114],\n",
      "        [0.0125],\n",
      "        [0.0118],\n",
      "        [0.0164],\n",
      "        [0.0130],\n",
      "        [0.0123],\n",
      "        [0.0203],\n",
      "        [0.0209],\n",
      "        [0.0219],\n",
      "        [0.0179],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0204],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0162],\n",
      "        [0.0208],\n",
      "        [0.0168],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0279],\n",
      "        [0.0210],\n",
      "        [0.0281],\n",
      "        [0.0286],\n",
      "        [0.0211],\n",
      "        [0.0254],\n",
      "        [0.0212],\n",
      "        [0.0277],\n",
      "        [0.0320],\n",
      "        [0.0251],\n",
      "        [0.0324],\n",
      "        [0.0254],\n",
      "        [0.0297],\n",
      "        [0.0255],\n",
      "        [0.0306],\n",
      "        [0.0277],\n",
      "        [0.0317],\n",
      "        [0.0277],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0292],\n",
      "        [0.0364],\n",
      "        [0.0302],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0376],\n",
      "        [0.0345],\n",
      "        [0.0382],\n",
      "        [0.0350],\n",
      "        [0.0330],\n",
      "        [0.0342],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0416],\n",
      "        [0.0393],\n",
      "        [0.0425],\n",
      "        [0.0393],\n",
      "        [0.0401],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0483],\n",
      "        [0.0455],\n",
      "        [0.0418],\n",
      "        [0.0476],\n",
      "        [0.0437],\n",
      "        [0.0459],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0481],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0503],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0536],\n",
      "        [0.0545],\n",
      "        [0.0538],\n",
      "        [0.0552],\n",
      "        [0.0547],\n",
      "        [0.0592],\n",
      "        [0.0688],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0718],\n",
      "        [0.0709],\n",
      "        [0.0719],\n",
      "        [0.0749],\n",
      "        [0.0789],\n",
      "        [0.0788],\n",
      "        [0.0730],\n",
      "        [0.0791],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0752],\n",
      "        [0.0771],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0813],\n",
      "        [0.0950],\n",
      "        [0.0898],\n",
      "        [0.0993],\n",
      "        [0.0961],\n",
      "        [0.1039],\n",
      "        [0.1043],\n",
      "        [0.1035],\n",
      "        [0.1092]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 102.09670734405518\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.352479017668884e-09, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [15, 42, 124, 142, 62, 9, 39, 96, 14, 143, 131, 2, 10, 129, 24, 115, 140, 22, 38, 136, 104, 11, 88, 112, 114, 156, 111, 94, 36, 37, 16, 43, 17, 7, 91, 6, 135, 141, 54, 53, 130, 12, 92, 50, 23, 93, 0, 5, 145, 49, 113, 105, 120, 13, 110, 89, 125, 48, 8, 146, 116, 144, 18, 3, 95, 78, 66, 158, 139, 19, 63, 157, 87, 27, 151, 155, 147, 51, 77, 41, 79, 4, 90, 65, 154, 106, 132, 118, 61, 71, 40, 117, 109, 103, 148, 108, 152, 32, 138, 119, 33, 47, 133, 60, 101, 70, 1, 76, 86, 97, 121, 134, 84, 21, 153, 20, 100, 107, 102, 128, 52, 99, 35, 98, 25, 44, 64, 83, 85, 75, 68, 82, 150, 45, 74, 127, 126, 80, 26, 122, 46, 81, 55, 58, 149, 137, 69, 59, 34, 123, 72, 28, 67, 31, 56, 29, 73] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5482],\n",
      "        [0.3647],\n",
      "        [0.4104],\n",
      "        [0.4861],\n",
      "        [0.2358],\n",
      "        [0.6254],\n",
      "        [0.3549],\n",
      "        [0.0988],\n",
      "        [0.5599],\n",
      "        [0.4894],\n",
      "        [0.4307],\n",
      "        [0.6477],\n",
      "        [0.6119],\n",
      "        [0.4194],\n",
      "        [0.4686],\n",
      "        [0.0988],\n",
      "        [0.4865],\n",
      "        [0.4650],\n",
      "        [0.3515],\n",
      "        [0.4940],\n",
      "        [0.0988],\n",
      "        [0.5982],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4724],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2976],\n",
      "        [0.3182],\n",
      "        [0.5150],\n",
      "        [0.4026],\n",
      "        [0.5288],\n",
      "        [0.6043],\n",
      "        [0.0988],\n",
      "        [0.6193],\n",
      "        [0.4791],\n",
      "        [0.4865],\n",
      "        [0.2752],\n",
      "        [0.2852],\n",
      "        [0.4146],\n",
      "        [0.5744],\n",
      "        [0.0988],\n",
      "        [0.3305],\n",
      "        [0.4710],\n",
      "        [0.0988],\n",
      "        [0.6600],\n",
      "        [0.6459],\n",
      "        [0.4872],\n",
      "        [0.3716],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5808],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4876],\n",
      "        [0.3848],\n",
      "        [0.6228],\n",
      "        [0.4748],\n",
      "        [0.0988],\n",
      "        [0.4851],\n",
      "        [0.5154],\n",
      "        [0.6434],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4631],\n",
      "        [0.4996],\n",
      "        [0.5085],\n",
      "        [0.2291],\n",
      "        [0.4629],\n",
      "        [0.0988],\n",
      "        [0.4128],\n",
      "        [0.4733],\n",
      "        [0.4664],\n",
      "        [0.4852],\n",
      "        [0.2823],\n",
      "        [0.0988],\n",
      "        [0.3283],\n",
      "        [0.0988],\n",
      "        [0.6654],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.4760],\n",
      "        [0.0988],\n",
      "        [0.4352],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.3245],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4956],\n",
      "        [0.0988],\n",
      "        [0.4701],\n",
      "        [0.3057],\n",
      "        [0.5020],\n",
      "        [0.0988],\n",
      "        [0.2517],\n",
      "        [0.3588],\n",
      "        [0.4418],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.6280],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4544],\n",
      "        [0.0988],\n",
      "        [0.4749],\n",
      "        [0.4715],\n",
      "        [0.4827],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4295],\n",
      "        [0.2714],\n",
      "        [0.0988],\n",
      "        [0.2647],\n",
      "        [0.0988],\n",
      "        [0.4172],\n",
      "        [0.3958],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4884],\n",
      "        [0.3812],\n",
      "        [0.0988],\n",
      "        [0.4266],\n",
      "        [0.4228],\n",
      "        [0.0988],\n",
      "        [0.3893],\n",
      "        [0.0988],\n",
      "        [0.3594],\n",
      "        [0.0988],\n",
      "        [0.2989],\n",
      "        [0.2667],\n",
      "        [0.4910],\n",
      "        [0.4883],\n",
      "        [0.0988],\n",
      "        [0.2669],\n",
      "        [0.2129],\n",
      "        [0.4047],\n",
      "        [0.1274],\n",
      "        [0.4257],\n",
      "        [0.1143],\n",
      "        [0.3689],\n",
      "        [0.2952],\n",
      "        [0.3959],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0022],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0031],\n",
      "        [0.0037],\n",
      "        [0.0041],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0149],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0163],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0168],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0179],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0204],\n",
      "        [0.0203],\n",
      "        [0.0211],\n",
      "        [0.0209],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0219],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0255],\n",
      "        [0.0254],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0306],\n",
      "        [0.0320],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0382],\n",
      "        [0.0393],\n",
      "        [0.0394],\n",
      "        [0.0393],\n",
      "        [0.0401],\n",
      "        [0.0418],\n",
      "        [0.0416],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0459],\n",
      "        [0.0476],\n",
      "        [0.0481],\n",
      "        [0.0483],\n",
      "        [0.0503],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0536],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.0547],\n",
      "        [0.0552],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0688],\n",
      "        [0.0709],\n",
      "        [0.0718],\n",
      "        [0.0719],\n",
      "        [0.0730],\n",
      "        [0.0749],\n",
      "        [0.0752],\n",
      "        [0.0768],\n",
      "        [0.0771],\n",
      "        [0.0779],\n",
      "        [0.0788],\n",
      "        [0.0791],\n",
      "        [0.0789],\n",
      "        [0.0813],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0898],\n",
      "        [0.0950],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1035],\n",
      "        [0.1039],\n",
      "        [0.1043],\n",
      "        [0.1092],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 7\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0022],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0031],\n",
      "        [0.0037],\n",
      "        [0.0041],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0149],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0163],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0168],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0179],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0204],\n",
      "        [0.0203],\n",
      "        [0.0211],\n",
      "        [0.0209],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0219],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0255],\n",
      "        [0.0254],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0306],\n",
      "        [0.0320],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0382],\n",
      "        [0.0393],\n",
      "        [0.0394],\n",
      "        [0.0393],\n",
      "        [0.0401],\n",
      "        [0.0418],\n",
      "        [0.0416],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0459],\n",
      "        [0.0476],\n",
      "        [0.0481],\n",
      "        [0.0483],\n",
      "        [0.0503],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0536],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.0547],\n",
      "        [0.0552],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0688],\n",
      "        [0.0709],\n",
      "        [0.0718],\n",
      "        [0.0719],\n",
      "        [0.0730],\n",
      "        [0.0749],\n",
      "        [0.0752],\n",
      "        [0.0768],\n",
      "        [0.0771],\n",
      "        [0.0779],\n",
      "        [0.0788],\n",
      "        [0.0791],\n",
      "        [0.0789],\n",
      "        [0.0813],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0898],\n",
      "        [0.0950],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1035],\n",
      "        [0.1039],\n",
      "        [0.1043],\n",
      "        [0.1092],\n",
      "        [0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 102.20084691047668\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.352479017668884e-09, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [15, 42, 124, 142, 62, 9, 39, 96, 14, 143, 131, 2, 10, 129, 24, 115, 140, 22, 38, 136, 104, 11, 88, 112, 114, 156, 111, 94, 36, 37, 16, 43, 17, 7, 91, 6, 135, 141, 54, 53, 130, 12, 92, 50, 23, 93, 0, 5, 145, 49, 113, 105, 120, 13, 110, 89, 125, 48, 8, 146, 116, 144, 18, 3, 95, 78, 66, 158, 139, 19, 63, 157, 87, 27, 151, 155, 147, 51, 77, 41, 79, 4, 90, 65, 154, 106, 132, 118, 61, 71, 40, 117, 109, 103, 148, 108, 152, 32, 138, 119, 33, 47, 133, 60, 101, 70, 1, 76, 86, 97, 121, 134, 84, 21, 153, 20, 100, 107, 102, 128, 52, 99, 35, 98, 25, 44, 64, 83, 85, 75, 68, 82, 150, 45, 74, 127, 126, 80, 26, 122, 46, 81, 55, 58, 149, 137, 69, 59, 34, 123, 72, 28, 67, 31, 56, 29, 73, 57] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5482],\n",
      "        [0.3647],\n",
      "        [0.4104],\n",
      "        [0.4861],\n",
      "        [0.2358],\n",
      "        [0.6254],\n",
      "        [0.3549],\n",
      "        [0.0988],\n",
      "        [0.5599],\n",
      "        [0.4894],\n",
      "        [0.4307],\n",
      "        [0.6477],\n",
      "        [0.6119],\n",
      "        [0.4194],\n",
      "        [0.4686],\n",
      "        [0.0988],\n",
      "        [0.4865],\n",
      "        [0.4650],\n",
      "        [0.3515],\n",
      "        [0.4940],\n",
      "        [0.0988],\n",
      "        [0.5982],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4724],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2976],\n",
      "        [0.3182],\n",
      "        [0.5150],\n",
      "        [0.4026],\n",
      "        [0.5288],\n",
      "        [0.6043],\n",
      "        [0.0988],\n",
      "        [0.6193],\n",
      "        [0.4791],\n",
      "        [0.4865],\n",
      "        [0.2752],\n",
      "        [0.2852],\n",
      "        [0.4146],\n",
      "        [0.5744],\n",
      "        [0.0988],\n",
      "        [0.3305],\n",
      "        [0.4710],\n",
      "        [0.0988],\n",
      "        [0.6600],\n",
      "        [0.6459],\n",
      "        [0.4872],\n",
      "        [0.3716],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5808],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4876],\n",
      "        [0.3848],\n",
      "        [0.6228],\n",
      "        [0.4748],\n",
      "        [0.0988],\n",
      "        [0.4851],\n",
      "        [0.5154],\n",
      "        [0.6434],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.4631],\n",
      "        [0.4996],\n",
      "        [0.5085],\n",
      "        [0.2291],\n",
      "        [0.4629],\n",
      "        [0.0988],\n",
      "        [0.4128],\n",
      "        [0.4733],\n",
      "        [0.4664],\n",
      "        [0.4852],\n",
      "        [0.2823],\n",
      "        [0.0988],\n",
      "        [0.3283],\n",
      "        [0.0988],\n",
      "        [0.6654],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.4760],\n",
      "        [0.0988],\n",
      "        [0.4352],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.3245],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4956],\n",
      "        [0.0988],\n",
      "        [0.4701],\n",
      "        [0.3057],\n",
      "        [0.5020],\n",
      "        [0.0988],\n",
      "        [0.2517],\n",
      "        [0.3588],\n",
      "        [0.4418],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.6280],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4544],\n",
      "        [0.0988],\n",
      "        [0.4749],\n",
      "        [0.4715],\n",
      "        [0.4827],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4295],\n",
      "        [0.2714],\n",
      "        [0.0988],\n",
      "        [0.2647],\n",
      "        [0.0988],\n",
      "        [0.4172],\n",
      "        [0.3958],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4884],\n",
      "        [0.3812],\n",
      "        [0.0988],\n",
      "        [0.4266],\n",
      "        [0.4228],\n",
      "        [0.0988],\n",
      "        [0.3893],\n",
      "        [0.0988],\n",
      "        [0.3594],\n",
      "        [0.0988],\n",
      "        [0.2989],\n",
      "        [0.2667],\n",
      "        [0.4910],\n",
      "        [0.4883],\n",
      "        [0.0988],\n",
      "        [0.2669],\n",
      "        [0.2129],\n",
      "        [0.4047],\n",
      "        [0.1274],\n",
      "        [0.4257],\n",
      "        [0.1143],\n",
      "        [0.3689],\n",
      "        [0.2952],\n",
      "        [0.3959],\n",
      "        [0.1127],\n",
      "        [0.2766]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0022],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0031],\n",
      "        [0.0037],\n",
      "        [0.0041],\n",
      "        [0.0040],\n",
      "        [0.0043],\n",
      "        [0.0047],\n",
      "        [0.0047],\n",
      "        [0.0053],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0064],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0102],\n",
      "        [0.0112],\n",
      "        [0.0114],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0118],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0149],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0163],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0168],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0179],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0204],\n",
      "        [0.0203],\n",
      "        [0.0211],\n",
      "        [0.0209],\n",
      "        [0.0208],\n",
      "        [0.0210],\n",
      "        [0.0212],\n",
      "        [0.0219],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0251],\n",
      "        [0.0254],\n",
      "        [0.0255],\n",
      "        [0.0254],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0281],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0306],\n",
      "        [0.0320],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0324],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0342],\n",
      "        [0.0343],\n",
      "        [0.0345],\n",
      "        [0.0350],\n",
      "        [0.0364],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0382],\n",
      "        [0.0393],\n",
      "        [0.0394],\n",
      "        [0.0393],\n",
      "        [0.0401],\n",
      "        [0.0418],\n",
      "        [0.0416],\n",
      "        [0.0425],\n",
      "        [0.0437],\n",
      "        [0.0437],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0459],\n",
      "        [0.0476],\n",
      "        [0.0481],\n",
      "        [0.0483],\n",
      "        [0.0503],\n",
      "        [0.0509],\n",
      "        [0.0515],\n",
      "        [0.0525],\n",
      "        [0.0536],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.0547],\n",
      "        [0.0552],\n",
      "        [0.0554],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0688],\n",
      "        [0.0709],\n",
      "        [0.0718],\n",
      "        [0.0719],\n",
      "        [0.0730],\n",
      "        [0.0749],\n",
      "        [0.0752],\n",
      "        [0.0768],\n",
      "        [0.0771],\n",
      "        [0.0779],\n",
      "        [0.0788],\n",
      "        [0.0791],\n",
      "        [0.0789],\n",
      "        [0.0813],\n",
      "        [0.0814],\n",
      "        [0.0861],\n",
      "        [0.0898],\n",
      "        [0.0950],\n",
      "        [0.0961],\n",
      "        [0.0993],\n",
      "        [0.1035],\n",
      "        [0.1039],\n",
      "        [0.1043],\n",
      "        [0.1092],\n",
      "        [0.1127],\n",
      "        [0.1139]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 13\n",
      "Number of shrink: 14\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0030],\n",
      "        [0.0024],\n",
      "        [0.0007],\n",
      "        [0.0015],\n",
      "        [0.0022],\n",
      "        [0.0006],\n",
      "        [0.0020],\n",
      "        [0.0037],\n",
      "        [0.0068],\n",
      "        [0.0040],\n",
      "        [0.0059],\n",
      "        [0.0081],\n",
      "        [0.0077],\n",
      "        [0.0057],\n",
      "        [0.0032],\n",
      "        [0.0057],\n",
      "        [0.0049],\n",
      "        [0.0037],\n",
      "        [0.0055],\n",
      "        [0.0076],\n",
      "        [0.0069],\n",
      "        [0.0105],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0083],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0096],\n",
      "        [0.0115],\n",
      "        [0.0092],\n",
      "        [0.0125],\n",
      "        [0.0093],\n",
      "        [0.0091],\n",
      "        [0.0118],\n",
      "        [0.0155],\n",
      "        [0.0112],\n",
      "        [0.0117],\n",
      "        [0.0139],\n",
      "        [0.0153],\n",
      "        [0.0166],\n",
      "        [0.0183],\n",
      "        [0.0154],\n",
      "        [0.0174],\n",
      "        [0.0142],\n",
      "        [0.0164],\n",
      "        [0.0133],\n",
      "        [0.0205],\n",
      "        [0.0175],\n",
      "        [0.0184],\n",
      "        [0.0179],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0225],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0189],\n",
      "        [0.0214],\n",
      "        [0.0180],\n",
      "        [0.0212],\n",
      "        [0.0208],\n",
      "        [0.0206],\n",
      "        [0.0189],\n",
      "        [0.0254],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0252],\n",
      "        [0.0239],\n",
      "        [0.0231],\n",
      "        [0.0254],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0256],\n",
      "        [0.0277],\n",
      "        [0.0284],\n",
      "        [0.0286],\n",
      "        [0.0279],\n",
      "        [0.0297],\n",
      "        [0.0292],\n",
      "        [0.0305],\n",
      "        [0.0349],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0321],\n",
      "        [0.0327],\n",
      "        [0.0314],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0328],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0363],\n",
      "        [0.0375],\n",
      "        [0.0374],\n",
      "        [0.0407],\n",
      "        [0.0380],\n",
      "        [0.0394],\n",
      "        [0.0371],\n",
      "        [0.0390],\n",
      "        [0.0402],\n",
      "        [0.0416],\n",
      "        [0.0425],\n",
      "        [0.0438],\n",
      "        [0.0401],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0443],\n",
      "        [0.0476],\n",
      "        [0.0458],\n",
      "        [0.0480],\n",
      "        [0.0481],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0533],\n",
      "        [0.0527],\n",
      "        [0.0542],\n",
      "        [0.0532],\n",
      "        [0.0548],\n",
      "        [0.0525],\n",
      "        [0.0541],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0683],\n",
      "        [0.0699],\n",
      "        [0.0718],\n",
      "        [0.0719],\n",
      "        [0.0732],\n",
      "        [0.0749],\n",
      "        [0.0731],\n",
      "        [0.0768],\n",
      "        [0.0761],\n",
      "        [0.0779],\n",
      "        [0.0796],\n",
      "        [0.0799],\n",
      "        [0.0784],\n",
      "        [0.0802],\n",
      "        [0.0814],\n",
      "        [0.0867],\n",
      "        [0.0883],\n",
      "        [0.0945],\n",
      "        [0.0961],\n",
      "        [0.1012],\n",
      "        [0.1035],\n",
      "        [0.1065],\n",
      "        [0.1053],\n",
      "        [0.1113],\n",
      "        [0.1127],\n",
      "        [0.1147]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 102.34674715995789\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.6403598124270502e-07, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [9, 124, 142, 39, 62, 42, 15, 24, 96, 22, 143, 140, 38, 115, 131, 129, 14, 104, 10, 136, 2, 88, 112, 156, 114, 7, 16, 36, 17, 111, 94, 11, 141, 135, 37, 91, 43, 0, 54, 23, 53, 92, 6, 93, 130, 50, 145, 8, 113, 12, 49, 125, 18, 105, 120, 110, 5, 89, 116, 146, 144, 48, 95, 13, 19, 78, 139, 66, 158, 63, 3, 27, 151, 157, 87, 51, 155, 147, 41, 77, 79, 132, 90, 154, 65, 106, 40, 118, 61, 71, 117, 109, 4, 103, 148, 152, 33, 108, 138, 47, 119, 1, 133, 32, 60, 101, 70, 76, 134, 86, 97, 121, 21, 84, 20, 153, 100, 107, 102, 25, 52, 128, 35, 99, 44, 98, 64, 83, 85, 75, 68, 82, 150, 45, 74, 127, 26, 126, 80, 46, 122, 81, 149, 55, 58, 137, 69, 59, 34, 123, 72, 28, 67, 56, 31, 29, 73, 57, 30] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6284],\n",
      "        [0.4098],\n",
      "        [0.4868],\n",
      "        [0.3560],\n",
      "        [0.2358],\n",
      "        [0.3658],\n",
      "        [0.5509],\n",
      "        [0.4708],\n",
      "        [0.0988],\n",
      "        [0.4673],\n",
      "        [0.4893],\n",
      "        [0.4876],\n",
      "        [0.3524],\n",
      "        [0.0988],\n",
      "        [0.4323],\n",
      "        [0.4198],\n",
      "        [0.5626],\n",
      "        [0.0988],\n",
      "        [0.6149],\n",
      "        [0.4950],\n",
      "        [0.6511],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4725],\n",
      "        [0.0988],\n",
      "        [0.6075],\n",
      "        [0.5172],\n",
      "        [0.2982],\n",
      "        [0.5313],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6012],\n",
      "        [0.4878],\n",
      "        [0.4804],\n",
      "        [0.3186],\n",
      "        [0.0988],\n",
      "        [0.4036],\n",
      "        [0.6635],\n",
      "        [0.2761],\n",
      "        [0.4731],\n",
      "        [0.2863],\n",
      "        [0.0988],\n",
      "        [0.6225],\n",
      "        [0.0988],\n",
      "        [0.4162],\n",
      "        [0.3316],\n",
      "        [0.4873],\n",
      "        [0.6260],\n",
      "        [0.0988],\n",
      "        [0.5773],\n",
      "        [0.3727],\n",
      "        [0.4861],\n",
      "        [0.5178],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6491],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4751],\n",
      "        [0.4855],\n",
      "        [0.3859],\n",
      "        [0.0988],\n",
      "        [0.5836],\n",
      "        [0.5109],\n",
      "        [0.0988],\n",
      "        [0.5012],\n",
      "        [0.1649],\n",
      "        [0.4631],\n",
      "        [0.2291],\n",
      "        [0.6470],\n",
      "        [0.4149],\n",
      "        [0.4731],\n",
      "        [0.4631],\n",
      "        [0.0988],\n",
      "        [0.2835],\n",
      "        [0.4666],\n",
      "        [0.4853],\n",
      "        [0.3293],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4368],\n",
      "        [0.0988],\n",
      "        [0.4757],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.3259],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6682],\n",
      "        [0.0988],\n",
      "        [0.4955],\n",
      "        [0.4700],\n",
      "        [0.2539],\n",
      "        [0.0988],\n",
      "        [0.5032],\n",
      "        [0.3600],\n",
      "        [0.0988],\n",
      "        [0.6317],\n",
      "        [0.4434],\n",
      "        [0.3081],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.4560],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4772],\n",
      "        [0.0988],\n",
      "        [0.4849],\n",
      "        [0.4712],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4194],\n",
      "        [0.2726],\n",
      "        [0.4298],\n",
      "        [0.2660],\n",
      "        [0.0988],\n",
      "        [0.3970],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4879],\n",
      "        [0.3823],\n",
      "        [0.0988],\n",
      "        [0.4266],\n",
      "        [0.3914],\n",
      "        [0.4225],\n",
      "        [0.0988],\n",
      "        [0.3604],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4905],\n",
      "        [0.2997],\n",
      "        [0.2676],\n",
      "        [0.4894],\n",
      "        [0.0988],\n",
      "        [0.2676],\n",
      "        [0.2144],\n",
      "        [0.4042],\n",
      "        [0.1274],\n",
      "        [0.4276],\n",
      "        [0.1143],\n",
      "        [0.2962],\n",
      "        [0.3714],\n",
      "        [0.3981],\n",
      "        [0.1127],\n",
      "        [0.2775],\n",
      "        [0.3797]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0007],\n",
      "        [0.0015],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0024],\n",
      "        [0.0030],\n",
      "        [0.0032],\n",
      "        [0.0037],\n",
      "        [0.0037],\n",
      "        [0.0040],\n",
      "        [0.0049],\n",
      "        [0.0055],\n",
      "        [0.0057],\n",
      "        [0.0059],\n",
      "        [0.0057],\n",
      "        [0.0068],\n",
      "        [0.0069],\n",
      "        [0.0077],\n",
      "        [0.0076],\n",
      "        [0.0081],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0082],\n",
      "        [0.0091],\n",
      "        [0.0092],\n",
      "        [0.0096],\n",
      "        [0.0093],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0117],\n",
      "        [0.0112],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0125],\n",
      "        [0.0133],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.0153],\n",
      "        [0.0154],\n",
      "        [0.0155],\n",
      "        [0.0164],\n",
      "        [0.0166],\n",
      "        [0.0174],\n",
      "        [0.0175],\n",
      "        [0.0180],\n",
      "        [0.0179],\n",
      "        [0.0183],\n",
      "        [0.0184],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0205],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0212],\n",
      "        [0.0206],\n",
      "        [0.0214],\n",
      "        [0.0222],\n",
      "        [0.0225],\n",
      "        [0.0231],\n",
      "        [0.0233],\n",
      "        [0.0239],\n",
      "        [0.0242],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0277],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0279],\n",
      "        [0.0284],\n",
      "        [0.0286],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0314],\n",
      "        [0.0317],\n",
      "        [0.0321],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0328],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.0350],\n",
      "        [0.0363],\n",
      "        [0.0374],\n",
      "        [0.0371],\n",
      "        [0.0375],\n",
      "        [0.0380],\n",
      "        [0.0390],\n",
      "        [0.0394],\n",
      "        [0.0401],\n",
      "        [0.0402],\n",
      "        [0.0407],\n",
      "        [0.0416],\n",
      "        [0.0425],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0443],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0458],\n",
      "        [0.0476],\n",
      "        [0.0481],\n",
      "        [0.0480],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0533],\n",
      "        [0.0532],\n",
      "        [0.0542],\n",
      "        [0.0541],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0683],\n",
      "        [0.0699],\n",
      "        [0.0718],\n",
      "        [0.0719],\n",
      "        [0.0731],\n",
      "        [0.0732],\n",
      "        [0.0749],\n",
      "        [0.0761],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0784],\n",
      "        [0.0796],\n",
      "        [0.0799],\n",
      "        [0.0802],\n",
      "        [0.0814],\n",
      "        [0.0867],\n",
      "        [0.0883],\n",
      "        [0.0945],\n",
      "        [0.0961],\n",
      "        [0.1012],\n",
      "        [0.1035],\n",
      "        [0.1053],\n",
      "        [0.1065],\n",
      "        [0.1113],\n",
      "        [0.1127],\n",
      "        [0.1147],\n",
      "        [0.1523]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 3\n",
      "Number of shrink: 8\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0045],\n",
      "        [    0.0021],\n",
      "        [    0.0055],\n",
      "        [    0.0054],\n",
      "        [    0.0022],\n",
      "        [    0.0009],\n",
      "        [    0.0022],\n",
      "        [    0.0077],\n",
      "        [    0.0037],\n",
      "        [    0.0082],\n",
      "        [    0.0001],\n",
      "        [    0.0086],\n",
      "        [    0.0088],\n",
      "        [    0.0057],\n",
      "        [    0.0019],\n",
      "        [    0.0025],\n",
      "        [    0.0017],\n",
      "        [    0.0069],\n",
      "        [    0.0021],\n",
      "        [    0.0040],\n",
      "        [    0.0032],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0121],\n",
      "        [    0.0082],\n",
      "        [    0.0142],\n",
      "        [    0.0144],\n",
      "        [    0.0126],\n",
      "        [    0.0140],\n",
      "        [    0.0098],\n",
      "        [    0.0100],\n",
      "        [    0.0053],\n",
      "        [    0.0157],\n",
      "        [    0.0149],\n",
      "        [    0.0085],\n",
      "        [    0.0118],\n",
      "        [    0.0095],\n",
      "        [    0.0183],\n",
      "        [    0.0104],\n",
      "        [    0.0186],\n",
      "        [    0.0118],\n",
      "        [    0.0154],\n",
      "        [    0.0099],\n",
      "        [    0.0164],\n",
      "        [    0.0127],\n",
      "        [    0.0138],\n",
      "        [    0.0134],\n",
      "        [    0.0232],\n",
      "        [    0.0179],\n",
      "        [    0.0132],\n",
      "        [    0.0150],\n",
      "        [    0.0181],\n",
      "        [    0.0235],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0199],\n",
      "        [    0.0152],\n",
      "        [    0.0204],\n",
      "        [    0.0208],\n",
      "        [    0.0170],\n",
      "        [    0.0250],\n",
      "        [    0.0179],\n",
      "        [    0.0222],\n",
      "        [    0.0175],\n",
      "        [    0.0280],\n",
      "        [    0.0233],\n",
      "        [    0.0280],\n",
      "        [    0.0242],\n",
      "        [    0.0292],\n",
      "        [    0.0254],\n",
      "        [    0.0203],\n",
      "        [    0.0300],\n",
      "        [    0.0235],\n",
      "        [    0.0316],\n",
      "        [    0.0277],\n",
      "        [    0.0317],\n",
      "        [    0.0246],\n",
      "        [    0.0246],\n",
      "        [    0.0324],\n",
      "        [    0.0297],\n",
      "        [    0.0305],\n",
      "        [    0.0352],\n",
      "        [    0.0317],\n",
      "        [    0.0284],\n",
      "        [    0.0323],\n",
      "        [    0.0327],\n",
      "        [    0.0361],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0346],\n",
      "        [    0.0301],\n",
      "        [    0.0350],\n",
      "        [    0.0321],\n",
      "        [    0.0334],\n",
      "        [    0.0408],\n",
      "        [    0.0375],\n",
      "        [    0.0417],\n",
      "        [    0.0425],\n",
      "        [    0.0394],\n",
      "        [    0.0452],\n",
      "        [    0.0439],\n",
      "        [    0.0371],\n",
      "        [    0.0416],\n",
      "        [    0.0425],\n",
      "        [    0.0438],\n",
      "        [    0.0441],\n",
      "        [    0.0483],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0504],\n",
      "        [    0.0476],\n",
      "        [    0.0526],\n",
      "        [    0.0443],\n",
      "        [    0.0509],\n",
      "        [    0.0516],\n",
      "        [    0.0525],\n",
      "        [    0.0568],\n",
      "        [    0.0566],\n",
      "        [    0.0563],\n",
      "        [    0.0564],\n",
      "        [    0.0542],\n",
      "        [    0.0573],\n",
      "        [    0.0548],\n",
      "        [    0.0553],\n",
      "        [    0.0564],\n",
      "        [    0.0592],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0640],\n",
      "        [    0.0731],\n",
      "        [    0.0718],\n",
      "        [    0.0747],\n",
      "        [    0.0775],\n",
      "        [    0.0761],\n",
      "        [    0.0749],\n",
      "        [    0.0793],\n",
      "        [    0.0768],\n",
      "        [    0.0779],\n",
      "        [    0.0743],\n",
      "        [    0.0763],\n",
      "        [    0.0769],\n",
      "        [    0.0839],\n",
      "        [    0.0814],\n",
      "        [    0.0839],\n",
      "        [    0.0915],\n",
      "        [    0.0917],\n",
      "        [    0.0961],\n",
      "        [    0.0972],\n",
      "        [    0.1035],\n",
      "        [    0.1021],\n",
      "        [    0.1026],\n",
      "        [    0.1072],\n",
      "        [    0.1127],\n",
      "        [    0.1115],\n",
      "        [    0.1484]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 102.45912170410156\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 34 個區塊累積花費時間(s) 0.551541805267334\n",
      "<<The performance of 34 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.551541805267334\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 943.71\n",
      "The MAPE for l = 1: 0.02%\n",
      "The RMSE for l = 1: 1201.37\n",
      "The accuracy(2000) for l = 1: 90.57%\n",
      "The accuracy(3000) for l = 1: 99.37%\n",
      "The maximum error: tensor(3789.0430)\n",
      "The minimum error: tensor(1.4961)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 4240.6\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 4456.4\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 25.0%\n",
      "The maximum error: 5411.0078125\n",
      "The minimum error: 2029.94140625\n",
      "------------------------------------------------------------\n",
      "0.9056603773584906\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<35>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.078917635932157e-08, 139)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [139, 38, 10, 6, 120, 127, 58, 11, 125, 92, 132, 5, 35, 7, 138, 111, 100, 20, 84, 108, 110, 18, 34, 33, 136, 39, 107, 2, 90, 50, 49, 87, 152, 32, 126, 8, 141, 46, 3, 1, 13, 12, 131, 45, 88, 137, 89, 142, 9, 44, 109, 121, 19, 101, 116, 106, 85, 112, 91, 4, 74, 147, 14, 62, 143, 151, 140, 59, 83, 135, 150, 15, 154, 73, 0, 23, 75, 47, 153, 86, 144, 61, 37, 102, 148, 114, 57, 67, 113, 105, 99, 128, 36, 28, 104, 115, 29, 134, 56, 97, 43, 66, 129, 72, 149, 82, 93, 117, 80, 130, 17, 96, 103, 98, 16, 95, 94, 60, 31, 124, 48, 79, 40, 21, 81, 146, 71, 64, 78, 70, 41, 145, 76, 123, 122, 51, 118, 54, 22, 77, 42, 155, 65, 55, 133, 30, 119, 68, 24, 52, 27, 63, 25, 53, 69]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0009],\n",
      "        [    0.0017],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0019],\n",
      "        [    0.0022],\n",
      "        [    0.0022],\n",
      "        [    0.0025],\n",
      "        [    0.0037],\n",
      "        [    0.0040],\n",
      "        [    0.0045],\n",
      "        [    0.0054],\n",
      "        [    0.0053],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0069],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0088],\n",
      "        [    0.0085],\n",
      "        [    0.0086],\n",
      "        [    0.0095],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0104],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0121],\n",
      "        [    0.0126],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0134],\n",
      "        [    0.0138],\n",
      "        [    0.0142],\n",
      "        [    0.0152],\n",
      "        [    0.0140],\n",
      "        [    0.0144],\n",
      "        [    0.0149],\n",
      "        [    0.0150],\n",
      "        [    0.0154],\n",
      "        [    0.0157],\n",
      "        [    0.0164],\n",
      "        [    0.0170],\n",
      "        [    0.0175],\n",
      "        [    0.0179],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0186],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0208],\n",
      "        [    0.0222],\n",
      "        [    0.0232],\n",
      "        [    0.0233],\n",
      "        [    0.0235],\n",
      "        [    0.0235],\n",
      "        [    0.0242],\n",
      "        [    0.0246],\n",
      "        [    0.0246],\n",
      "        [    0.0250],\n",
      "        [    0.0254],\n",
      "        [    0.0277],\n",
      "        [    0.0280],\n",
      "        [    0.0284],\n",
      "        [    0.0280],\n",
      "        [    0.0292],\n",
      "        [    0.0297],\n",
      "        [    0.0301],\n",
      "        [    0.0300],\n",
      "        [    0.0305],\n",
      "        [    0.0317],\n",
      "        [    0.0316],\n",
      "        [    0.0317],\n",
      "        [    0.0321],\n",
      "        [    0.0323],\n",
      "        [    0.0324],\n",
      "        [    0.0327],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0346],\n",
      "        [    0.0350],\n",
      "        [    0.0352],\n",
      "        [    0.0361],\n",
      "        [    0.0371],\n",
      "        [    0.0375],\n",
      "        [    0.0394],\n",
      "        [    0.0408],\n",
      "        [    0.0417],\n",
      "        [    0.0416],\n",
      "        [    0.0425],\n",
      "        [    0.0425],\n",
      "        [    0.0438],\n",
      "        [    0.0439],\n",
      "        [    0.0441],\n",
      "        [    0.0443],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0476],\n",
      "        [    0.0483],\n",
      "        [    0.0504],\n",
      "        [    0.0509],\n",
      "        [    0.0516],\n",
      "        [    0.0525],\n",
      "        [    0.0526],\n",
      "        [    0.0542],\n",
      "        [    0.0548],\n",
      "        [    0.0553],\n",
      "        [    0.0564],\n",
      "        [    0.0563],\n",
      "        [    0.0566],\n",
      "        [    0.0564],\n",
      "        [    0.0573],\n",
      "        [    0.0568],\n",
      "        [    0.0592],\n",
      "        [    0.0640],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0718],\n",
      "        [    0.0731],\n",
      "        [    0.0743],\n",
      "        [    0.0749],\n",
      "        [    0.0747],\n",
      "        [    0.0761],\n",
      "        [    0.0763],\n",
      "        [    0.0768],\n",
      "        [    0.0769],\n",
      "        [    0.0775],\n",
      "        [    0.0779],\n",
      "        [    0.0793],\n",
      "        [    0.0794],\n",
      "        [    0.0814],\n",
      "        [    0.0839],\n",
      "        [    0.0839],\n",
      "        [    0.0915],\n",
      "        [    0.0917],\n",
      "        [    0.0961],\n",
      "        [    0.0972],\n",
      "        [    0.1021],\n",
      "        [    0.1026],\n",
      "        [    0.1035],\n",
      "        [    0.1072],\n",
      "        [    0.1115],\n",
      "        [    0.1127]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (6.078917635932157e-08, 139)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [139, 38, 10, 6, 120, 127, 58, 11, 125, 92, 132, 5, 35, 7, 138, 111, 100, 20, 84, 108, 110, 18, 34, 33, 136, 39, 107, 2, 90, 50, 49, 87, 152, 32, 126, 8, 141, 46, 3, 1, 13, 12, 131, 45, 88, 137, 89, 142, 9, 44, 109, 121, 19, 101, 116, 106, 85, 112, 91, 4, 74, 147, 14, 62, 143, 151, 140, 59, 83, 135, 150, 15, 154, 73, 0, 23, 75, 47, 153, 86, 144, 61, 37, 102, 148, 114, 57, 67, 113, 105, 99, 128, 36, 28, 104, 115, 29, 134, 56, 97, 43, 66, 129, 72, 149, 82, 93, 117, 80, 130, 17, 96, 103, 98, 16, 95, 94, 60, 31, 124, 48, 79, 40, 21, 81, 146, 71, 64, 78, 70, 41, 145, 76, 123, 122, 51, 118, 54, 22, 77, 42, 155, 65, 55, 133, 30, 119, 68, 24, 52, 27, 63, 25, 53, 69, 26] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4853],\n",
      "        [0.3626],\n",
      "        [0.5575],\n",
      "        [0.6093],\n",
      "        [0.4070],\n",
      "        [0.4283],\n",
      "        [0.2358],\n",
      "        [0.5458],\n",
      "        [0.4166],\n",
      "        [0.0988],\n",
      "        [0.4915],\n",
      "        [0.6233],\n",
      "        [0.3526],\n",
      "        [0.5959],\n",
      "        [0.4828],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4662],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4628],\n",
      "        [0.3491],\n",
      "        [0.3156],\n",
      "        [0.4839],\n",
      "        [0.4006],\n",
      "        [0.0988],\n",
      "        [0.6169],\n",
      "        [0.0988],\n",
      "        [0.2726],\n",
      "        [0.2827],\n",
      "        [0.0988],\n",
      "        [0.4688],\n",
      "        [0.2952],\n",
      "        [0.4124],\n",
      "        [0.5721],\n",
      "        [0.4832],\n",
      "        [0.3280],\n",
      "        [0.6024],\n",
      "        [0.6438],\n",
      "        [0.5266],\n",
      "        [0.5121],\n",
      "        [0.4767],\n",
      "        [0.3693],\n",
      "        [0.0988],\n",
      "        [0.4838],\n",
      "        [0.0988],\n",
      "        [0.4709],\n",
      "        [0.5786],\n",
      "        [0.3824],\n",
      "        [0.0988],\n",
      "        [0.4853],\n",
      "        [0.4686],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6208],\n",
      "        [0.0988],\n",
      "        [0.4690],\n",
      "        [0.5131],\n",
      "        [0.1649],\n",
      "        [0.4813],\n",
      "        [0.4629],\n",
      "        [0.4811],\n",
      "        [0.2291],\n",
      "        [0.0988],\n",
      "        [0.4971],\n",
      "        [0.4720],\n",
      "        [0.5060],\n",
      "        [0.4591],\n",
      "        [0.0988],\n",
      "        [0.6634],\n",
      "        [0.4104],\n",
      "        [0.0988],\n",
      "        [0.2797],\n",
      "        [0.4590],\n",
      "        [0.0988],\n",
      "        [0.4913],\n",
      "        [0.1914],\n",
      "        [0.3262],\n",
      "        [0.0988],\n",
      "        [0.4659],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4330],\n",
      "        [0.3226],\n",
      "        [0.3046],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2502],\n",
      "        [0.4996],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.3565],\n",
      "        [0.1183],\n",
      "        [0.4397],\n",
      "        [0.0988],\n",
      "        [0.4674],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4520],\n",
      "        [0.4726],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4803],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.2628],\n",
      "        [0.4268],\n",
      "        [0.2686],\n",
      "        [0.0988],\n",
      "        [0.3938],\n",
      "        [0.4151],\n",
      "        [0.0988],\n",
      "        [0.4836],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3790],\n",
      "        [0.4864],\n",
      "        [0.0988],\n",
      "        [0.4238],\n",
      "        [0.4196],\n",
      "        [0.2965],\n",
      "        [0.0988],\n",
      "        [0.2646],\n",
      "        [0.3871],\n",
      "        [0.0988],\n",
      "        [0.3572],\n",
      "        [0.4346],\n",
      "        [0.0988],\n",
      "        [0.2647],\n",
      "        [0.4857],\n",
      "        [0.2112],\n",
      "        [0.4014],\n",
      "        [0.1274],\n",
      "        [0.4235],\n",
      "        [0.2930],\n",
      "        [0.3675],\n",
      "        [0.1143],\n",
      "        [0.3940],\n",
      "        [0.2742],\n",
      "        [0.1127],\n",
      "        [0.3758]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0009],\n",
      "        [    0.0017],\n",
      "        [    0.0021],\n",
      "        [    0.0021],\n",
      "        [    0.0019],\n",
      "        [    0.0022],\n",
      "        [    0.0022],\n",
      "        [    0.0025],\n",
      "        [    0.0037],\n",
      "        [    0.0040],\n",
      "        [    0.0045],\n",
      "        [    0.0054],\n",
      "        [    0.0053],\n",
      "        [    0.0055],\n",
      "        [    0.0057],\n",
      "        [    0.0069],\n",
      "        [    0.0077],\n",
      "        [    0.0080],\n",
      "        [    0.0081],\n",
      "        [    0.0082],\n",
      "        [    0.0082],\n",
      "        [    0.0088],\n",
      "        [    0.0085],\n",
      "        [    0.0086],\n",
      "        [    0.0095],\n",
      "        [    0.0098],\n",
      "        [    0.0099],\n",
      "        [    0.0100],\n",
      "        [    0.0104],\n",
      "        [    0.0118],\n",
      "        [    0.0118],\n",
      "        [    0.0121],\n",
      "        [    0.0126],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0134],\n",
      "        [    0.0138],\n",
      "        [    0.0142],\n",
      "        [    0.0152],\n",
      "        [    0.0140],\n",
      "        [    0.0144],\n",
      "        [    0.0149],\n",
      "        [    0.0150],\n",
      "        [    0.0154],\n",
      "        [    0.0157],\n",
      "        [    0.0164],\n",
      "        [    0.0170],\n",
      "        [    0.0175],\n",
      "        [    0.0179],\n",
      "        [    0.0179],\n",
      "        [    0.0181],\n",
      "        [    0.0186],\n",
      "        [    0.0192],\n",
      "        [    0.0194],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0208],\n",
      "        [    0.0222],\n",
      "        [    0.0232],\n",
      "        [    0.0233],\n",
      "        [    0.0235],\n",
      "        [    0.0235],\n",
      "        [    0.0242],\n",
      "        [    0.0246],\n",
      "        [    0.0246],\n",
      "        [    0.0250],\n",
      "        [    0.0254],\n",
      "        [    0.0277],\n",
      "        [    0.0280],\n",
      "        [    0.0284],\n",
      "        [    0.0280],\n",
      "        [    0.0292],\n",
      "        [    0.0297],\n",
      "        [    0.0301],\n",
      "        [    0.0300],\n",
      "        [    0.0305],\n",
      "        [    0.0317],\n",
      "        [    0.0316],\n",
      "        [    0.0317],\n",
      "        [    0.0321],\n",
      "        [    0.0323],\n",
      "        [    0.0324],\n",
      "        [    0.0327],\n",
      "        [    0.0334],\n",
      "        [    0.0338],\n",
      "        [    0.0340],\n",
      "        [    0.0341],\n",
      "        [    0.0343],\n",
      "        [    0.0346],\n",
      "        [    0.0350],\n",
      "        [    0.0352],\n",
      "        [    0.0361],\n",
      "        [    0.0371],\n",
      "        [    0.0375],\n",
      "        [    0.0394],\n",
      "        [    0.0408],\n",
      "        [    0.0417],\n",
      "        [    0.0416],\n",
      "        [    0.0425],\n",
      "        [    0.0425],\n",
      "        [    0.0438],\n",
      "        [    0.0439],\n",
      "        [    0.0441],\n",
      "        [    0.0443],\n",
      "        [    0.0448],\n",
      "        [    0.0449],\n",
      "        [    0.0455],\n",
      "        [    0.0476],\n",
      "        [    0.0483],\n",
      "        [    0.0504],\n",
      "        [    0.0509],\n",
      "        [    0.0516],\n",
      "        [    0.0525],\n",
      "        [    0.0526],\n",
      "        [    0.0542],\n",
      "        [    0.0548],\n",
      "        [    0.0553],\n",
      "        [    0.0564],\n",
      "        [    0.0563],\n",
      "        [    0.0566],\n",
      "        [    0.0564],\n",
      "        [    0.0573],\n",
      "        [    0.0568],\n",
      "        [    0.0592],\n",
      "        [    0.0640],\n",
      "        [    0.0660],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0718],\n",
      "        [    0.0731],\n",
      "        [    0.0743],\n",
      "        [    0.0749],\n",
      "        [    0.0747],\n",
      "        [    0.0761],\n",
      "        [    0.0763],\n",
      "        [    0.0768],\n",
      "        [    0.0769],\n",
      "        [    0.0775],\n",
      "        [    0.0779],\n",
      "        [    0.0793],\n",
      "        [    0.0794],\n",
      "        [    0.0814],\n",
      "        [    0.0839],\n",
      "        [    0.0839],\n",
      "        [    0.0915],\n",
      "        [    0.0917],\n",
      "        [    0.0961],\n",
      "        [    0.0972],\n",
      "        [    0.1021],\n",
      "        [    0.1026],\n",
      "        [    0.1035],\n",
      "        [    0.1072],\n",
      "        [    0.1115],\n",
      "        [    0.1127],\n",
      "        [    0.1484]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 10\n",
      "Number of shrink: 12\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0007],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0004],\n",
      "        [0.0048],\n",
      "        [0.0022],\n",
      "        [0.0038],\n",
      "        [0.0045],\n",
      "        [0.0037],\n",
      "        [0.0070],\n",
      "        [0.0061],\n",
      "        [0.0053],\n",
      "        [0.0037],\n",
      "        [0.0034],\n",
      "        [0.0057],\n",
      "        [0.0069],\n",
      "        [0.0088],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0096],\n",
      "        [0.0087],\n",
      "        [0.0085],\n",
      "        [0.0062],\n",
      "        [0.0099],\n",
      "        [0.0098],\n",
      "        [0.0088],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0115],\n",
      "        [0.0118],\n",
      "        [0.0098],\n",
      "        [0.0128],\n",
      "        [0.0153],\n",
      "        [0.0114],\n",
      "        [0.0153],\n",
      "        [0.0138],\n",
      "        [0.0159],\n",
      "        [0.0135],\n",
      "        [0.0159],\n",
      "        [0.0158],\n",
      "        [0.0122],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0129],\n",
      "        [0.0164],\n",
      "        [0.0188],\n",
      "        [0.0154],\n",
      "        [0.0181],\n",
      "        [0.0179],\n",
      "        [0.0184],\n",
      "        [0.0197],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0222],\n",
      "        [0.0247],\n",
      "        [0.0233],\n",
      "        [0.0256],\n",
      "        [0.0252],\n",
      "        [0.0243],\n",
      "        [0.0265],\n",
      "        [0.0269],\n",
      "        [0.0230],\n",
      "        [0.0254],\n",
      "        [0.0277],\n",
      "        [0.0250],\n",
      "        [0.0302],\n",
      "        [0.0292],\n",
      "        [0.0273],\n",
      "        [0.0297],\n",
      "        [0.0287],\n",
      "        [0.0311],\n",
      "        [0.0305],\n",
      "        [0.0320],\n",
      "        [0.0294],\n",
      "        [0.0317],\n",
      "        [0.0341],\n",
      "        [0.0323],\n",
      "        [0.0321],\n",
      "        [0.0327],\n",
      "        [0.0357],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0324],\n",
      "        [0.0360],\n",
      "        [0.0368],\n",
      "        [0.0375],\n",
      "        [0.0394],\n",
      "        [0.0412],\n",
      "        [0.0388],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0421],\n",
      "        [0.0438],\n",
      "        [0.0413],\n",
      "        [0.0441],\n",
      "        [0.0463],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0456],\n",
      "        [0.0517],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0567],\n",
      "        [0.0541],\n",
      "        [0.0569],\n",
      "        [0.0564],\n",
      "        [0.0568],\n",
      "        [0.0583],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0717],\n",
      "        [0.0731],\n",
      "        [0.0762],\n",
      "        [0.0749],\n",
      "        [0.0726],\n",
      "        [0.0741],\n",
      "        [0.0763],\n",
      "        [0.0768],\n",
      "        [0.0769],\n",
      "        [0.0785],\n",
      "        [0.0779],\n",
      "        [0.0791],\n",
      "        [0.0778],\n",
      "        [0.0814],\n",
      "        [0.0840],\n",
      "        [0.0810],\n",
      "        [0.0918],\n",
      "        [0.0935],\n",
      "        [0.0961],\n",
      "        [0.0959],\n",
      "        [0.1019],\n",
      "        [0.1020],\n",
      "        [0.1035],\n",
      "        [0.1064],\n",
      "        [0.1114],\n",
      "        [0.1127],\n",
      "        [0.1475]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 102.85178256034851\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (3.0126102501526475e-08, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 120, 6, 38, 139, 58, 7, 138, 92, 11, 125, 127, 35, 111, 5, 136, 100, 132, 84, 108, 110, 33, 34, 20, 2, 152, 107, 18, 39, 90, 50, 49, 8, 87, 131, 32, 137, 1, 46, 126, 141, 45, 88, 9, 3, 13, 89, 12, 109, 44, 121, 142, 101, 116, 106, 19, 85, 112, 91, 140, 74, 62, 135, 4, 14, 59, 147, 143, 151, 154, 83, 0, 153, 15, 73, 150, 75, 23, 86, 37, 128, 61, 47, 102, 114, 57, 144, 67, 113, 105, 99, 148, 36, 28, 104, 134, 115, 129, 29, 56, 43, 97, 66, 72, 82, 93, 130, 117, 149, 80, 96, 103, 17, 98, 16, 124, 95, 94, 60, 79, 31, 48, 40, 21, 81, 71, 146, 64, 78, 70, 123, 41, 122, 76, 51, 145, 118, 54, 155, 77, 22, 42, 133, 65, 55, 30, 119, 24, 68, 52, 27, 63, 25, 53, 69, 26, 156] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5562],\n",
      "        [0.4087],\n",
      "        [0.6080],\n",
      "        [0.3627],\n",
      "        [0.4871],\n",
      "        [0.2358],\n",
      "        [0.5944],\n",
      "        [0.4848],\n",
      "        [0.0988],\n",
      "        [0.5442],\n",
      "        [0.4186],\n",
      "        [0.4312],\n",
      "        [0.3527],\n",
      "        [0.0988],\n",
      "        [0.6216],\n",
      "        [0.4864],\n",
      "        [0.0988],\n",
      "        [0.4945],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3156],\n",
      "        [0.3492],\n",
      "        [0.4651],\n",
      "        [0.6158],\n",
      "        [0.4710],\n",
      "        [0.0988],\n",
      "        [0.4614],\n",
      "        [0.4010],\n",
      "        [0.0988],\n",
      "        [0.2725],\n",
      "        [0.2825],\n",
      "        [0.5704],\n",
      "        [0.0988],\n",
      "        [0.4794],\n",
      "        [0.2950],\n",
      "        [0.4866],\n",
      "        [0.6421],\n",
      "        [0.3280],\n",
      "        [0.4150],\n",
      "        [0.4852],\n",
      "        [0.3695],\n",
      "        [0.0988],\n",
      "        [0.5765],\n",
      "        [0.6006],\n",
      "        [0.5247],\n",
      "        [0.0988],\n",
      "        [0.5106],\n",
      "        [0.0988],\n",
      "        [0.3826],\n",
      "        [0.4856],\n",
      "        [0.4727],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4676],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4832],\n",
      "        [0.0988],\n",
      "        [0.1649],\n",
      "        [0.5001],\n",
      "        [0.6193],\n",
      "        [0.5115],\n",
      "        [0.2291],\n",
      "        [0.4710],\n",
      "        [0.4832],\n",
      "        [0.4652],\n",
      "        [0.4609],\n",
      "        [0.0988],\n",
      "        [0.6620],\n",
      "        [0.4612],\n",
      "        [0.5048],\n",
      "        [0.0988],\n",
      "        [0.4738],\n",
      "        [0.0988],\n",
      "        [0.4094],\n",
      "        [0.0988],\n",
      "        [0.3265],\n",
      "        [0.4358],\n",
      "        [0.1914],\n",
      "        [0.2795],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.4933],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4683],\n",
      "        [0.3227],\n",
      "        [0.3043],\n",
      "        [0.0988],\n",
      "        [0.5025],\n",
      "        [0.0988],\n",
      "        [0.4423],\n",
      "        [0.2497],\n",
      "        [0.2257],\n",
      "        [0.3569],\n",
      "        [0.0988],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4548],\n",
      "        [0.0988],\n",
      "        [0.4695],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4713],\n",
      "        [0.0988],\n",
      "        [0.4792],\n",
      "        [0.4290],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.2625],\n",
      "        [0.2683],\n",
      "        [0.3942],\n",
      "        [0.4136],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4855],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4259],\n",
      "        [0.3790],\n",
      "        [0.4217],\n",
      "        [0.0988],\n",
      "        [0.2965],\n",
      "        [0.4883],\n",
      "        [0.0988],\n",
      "        [0.2646],\n",
      "        [0.4362],\n",
      "        [0.0988],\n",
      "        [0.3860],\n",
      "        [0.3574],\n",
      "        [0.4886],\n",
      "        [0.0988],\n",
      "        [0.2648],\n",
      "        [0.2109],\n",
      "        [0.4032],\n",
      "        [0.4223],\n",
      "        [0.1274],\n",
      "        [0.2927],\n",
      "        [0.3670],\n",
      "        [0.1143],\n",
      "        [0.3931],\n",
      "        [0.2741],\n",
      "        [0.1127],\n",
      "        [0.3749],\n",
      "        [0.4102]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0003],\n",
      "        [0.0004],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0017],\n",
      "        [0.0022],\n",
      "        [0.0037],\n",
      "        [0.0034],\n",
      "        [0.0037],\n",
      "        [0.0038],\n",
      "        [0.0045],\n",
      "        [0.0048],\n",
      "        [0.0053],\n",
      "        [0.0057],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0085],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0088],\n",
      "        [0.0098],\n",
      "        [0.0098],\n",
      "        [0.0096],\n",
      "        [0.0099],\n",
      "        [0.0100],\n",
      "        [0.0103],\n",
      "        [0.0115],\n",
      "        [0.0114],\n",
      "        [0.0118],\n",
      "        [0.0122],\n",
      "        [0.0128],\n",
      "        [0.0129],\n",
      "        [0.0135],\n",
      "        [0.0138],\n",
      "        [0.0153],\n",
      "        [0.0153],\n",
      "        [0.0152],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0159],\n",
      "        [0.0159],\n",
      "        [0.0164],\n",
      "        [0.0158],\n",
      "        [0.0179],\n",
      "        [0.0181],\n",
      "        [0.0184],\n",
      "        [0.0188],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0197],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0222],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0243],\n",
      "        [0.0250],\n",
      "        [0.0247],\n",
      "        [0.0252],\n",
      "        [0.0254],\n",
      "        [0.0256],\n",
      "        [0.0265],\n",
      "        [0.0269],\n",
      "        [0.0273],\n",
      "        [0.0277],\n",
      "        [0.0287],\n",
      "        [0.0294],\n",
      "        [0.0292],\n",
      "        [0.0297],\n",
      "        [0.0302],\n",
      "        [0.0305],\n",
      "        [0.0311],\n",
      "        [0.0317],\n",
      "        [0.0321],\n",
      "        [0.0324],\n",
      "        [0.0323],\n",
      "        [0.0320],\n",
      "        [0.0327],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0357],\n",
      "        [0.0360],\n",
      "        [0.0368],\n",
      "        [0.0375],\n",
      "        [0.0388],\n",
      "        [0.0394],\n",
      "        [0.0413],\n",
      "        [0.0412],\n",
      "        [0.0416],\n",
      "        [0.0421],\n",
      "        [0.0424],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0456],\n",
      "        [0.0455],\n",
      "        [0.0463],\n",
      "        [0.0476],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0517],\n",
      "        [0.0525],\n",
      "        [0.0538],\n",
      "        [0.0541],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0567],\n",
      "        [0.0569],\n",
      "        [0.0568],\n",
      "        [0.0583],\n",
      "        [0.0592],\n",
      "        [0.0660],\n",
      "        [0.0660],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0717],\n",
      "        [0.0726],\n",
      "        [0.0731],\n",
      "        [0.0741],\n",
      "        [0.0749],\n",
      "        [0.0763],\n",
      "        [0.0762],\n",
      "        [0.0768],\n",
      "        [0.0769],\n",
      "        [0.0778],\n",
      "        [0.0779],\n",
      "        [0.0785],\n",
      "        [0.0791],\n",
      "        [0.0810],\n",
      "        [0.0814],\n",
      "        [0.0840],\n",
      "        [0.0918],\n",
      "        [0.0935],\n",
      "        [0.0959],\n",
      "        [0.0961],\n",
      "        [0.1019],\n",
      "        [0.1020],\n",
      "        [0.1035],\n",
      "        [0.1064],\n",
      "        [0.1114],\n",
      "        [0.1127],\n",
      "        [0.1475],\n",
      "        [0.1614]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 6\n",
      "Number of shrink: 10\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0020],\n",
      "        [0.0018],\n",
      "        [0.0006],\n",
      "        [0.0043],\n",
      "        [0.0022],\n",
      "        [0.0047],\n",
      "        [0.0007],\n",
      "        [0.0037],\n",
      "        [0.0028],\n",
      "        [0.0075],\n",
      "        [0.0078],\n",
      "        [0.0043],\n",
      "        [0.0057],\n",
      "        [0.0050],\n",
      "        [0.0033],\n",
      "        [0.0069],\n",
      "        [0.0101],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0094],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0097],\n",
      "        [0.0071],\n",
      "        [0.0098],\n",
      "        [0.0087],\n",
      "        [0.0109],\n",
      "        [0.0100],\n",
      "        [0.0113],\n",
      "        [0.0125],\n",
      "        [0.0123],\n",
      "        [0.0118],\n",
      "        [0.0091],\n",
      "        [0.0121],\n",
      "        [0.0101],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.0181],\n",
      "        [0.0178],\n",
      "        [0.0162],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0164],\n",
      "        [0.0154],\n",
      "        [0.0179],\n",
      "        [0.0191],\n",
      "        [0.0188],\n",
      "        [0.0215],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0199],\n",
      "        [0.0191],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0222],\n",
      "        [0.0207],\n",
      "        [0.0233],\n",
      "        [0.0243],\n",
      "        [0.0221],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0254],\n",
      "        [0.0282],\n",
      "        [0.0291],\n",
      "        [0.0298],\n",
      "        [0.0246],\n",
      "        [0.0277],\n",
      "        [0.0297],\n",
      "        [0.0267],\n",
      "        [0.0284],\n",
      "        [0.0297],\n",
      "        [0.0330],\n",
      "        [0.0305],\n",
      "        [0.0303],\n",
      "        [0.0317],\n",
      "        [0.0309],\n",
      "        [0.0294],\n",
      "        [0.0323],\n",
      "        [0.0309],\n",
      "        [0.0327],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0366],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0384],\n",
      "        [0.0348],\n",
      "        [0.0376],\n",
      "        [0.0375],\n",
      "        [0.0356],\n",
      "        [0.0394],\n",
      "        [0.0385],\n",
      "        [0.0405],\n",
      "        [0.0416],\n",
      "        [0.0410],\n",
      "        [0.0424],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0427],\n",
      "        [0.0455],\n",
      "        [0.0490],\n",
      "        [0.0476],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0509],\n",
      "        [0.0525],\n",
      "        [0.0531],\n",
      "        [0.0513],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0574],\n",
      "        [0.0592],\n",
      "        [0.0659],\n",
      "        [0.0683],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0717],\n",
      "        [0.0699],\n",
      "        [0.0721],\n",
      "        [0.0716],\n",
      "        [0.0749],\n",
      "        [0.0775],\n",
      "        [0.0787],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0752],\n",
      "        [0.0779],\n",
      "        [0.0779],\n",
      "        [0.0781],\n",
      "        [0.0781],\n",
      "        [0.0814],\n",
      "        [0.0850],\n",
      "        [0.0908],\n",
      "        [0.0960],\n",
      "        [0.0965],\n",
      "        [0.0961],\n",
      "        [0.1029],\n",
      "        [0.1029],\n",
      "        [0.1035],\n",
      "        [0.1072],\n",
      "        [0.1126],\n",
      "        [0.1127],\n",
      "        [0.1484],\n",
      "        [0.1588]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 102.97342419624329\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.0144206303029932e-07, 38)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [38, 138, 10, 6, 120, 58, 11, 136, 92, 7, 35, 139, 5, 111, 100, 152, 125, 127, 34, 84, 108, 20, 110, 18, 131, 33, 107, 2, 90, 132, 137, 39, 50, 87, 32, 8, 49, 1, 46, 13, 3, 88, 12, 45, 89, 9, 141, 126, 109, 121, 19, 101, 116, 44, 106, 85, 140, 112, 142, 135, 91, 74, 4, 62, 14, 154, 59, 153, 83, 147, 15, 143, 128, 73, 151, 0, 23, 75, 37, 47, 86, 61, 102, 150, 114, 57, 67, 113, 105, 36, 99, 134, 144, 104, 28, 129, 148, 115, 29, 43, 56, 97, 130, 66, 72, 82, 93, 117, 80, 149, 17, 96, 124, 103, 98, 16, 95, 94, 60, 40, 48, 31, 79, 21, 81, 71, 64, 78, 146, 123, 122, 70, 41, 76, 155, 118, 51, 22, 54, 77, 133, 42, 145, 65, 55, 30, 119, 68, 24, 27, 52, 63, 25, 53, 69, 26, 156, 158] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3641],\n",
      "        [0.4875],\n",
      "        [0.5567],\n",
      "        [0.6090],\n",
      "        [0.4111],\n",
      "        [0.2358],\n",
      "        [0.5451],\n",
      "        [0.4892],\n",
      "        [0.0988],\n",
      "        [0.5954],\n",
      "        [0.3537],\n",
      "        [0.4897],\n",
      "        [0.6228],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4738],\n",
      "        [0.4216],\n",
      "        [0.4342],\n",
      "        [0.3503],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4659],\n",
      "        [0.0988],\n",
      "        [0.4623],\n",
      "        [0.4826],\n",
      "        [0.3165],\n",
      "        [0.0988],\n",
      "        [0.6167],\n",
      "        [0.0988],\n",
      "        [0.4976],\n",
      "        [0.4894],\n",
      "        [0.4020],\n",
      "        [0.2735],\n",
      "        [0.0988],\n",
      "        [0.2957],\n",
      "        [0.5713],\n",
      "        [0.2835],\n",
      "        [0.6432],\n",
      "        [0.3289],\n",
      "        [0.5254],\n",
      "        [0.6015],\n",
      "        [0.0988],\n",
      "        [0.5110],\n",
      "        [0.3705],\n",
      "        [0.0988],\n",
      "        [0.5773],\n",
      "        [0.4877],\n",
      "        [0.4177],\n",
      "        [0.0988],\n",
      "        [0.4860],\n",
      "        [0.4681],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3836],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4854],\n",
      "        [0.0988],\n",
      "        [0.4754],\n",
      "        [0.5030],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6202],\n",
      "        [0.1649],\n",
      "        [0.5124],\n",
      "        [0.4637],\n",
      "        [0.2291],\n",
      "        [0.4639],\n",
      "        [0.0988],\n",
      "        [0.4737],\n",
      "        [0.5056],\n",
      "        [0.4858],\n",
      "        [0.4388],\n",
      "        [0.0988],\n",
      "        [0.4680],\n",
      "        [0.6631],\n",
      "        [0.4101],\n",
      "        [0.0988],\n",
      "        [0.3276],\n",
      "        [0.2805],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.4766],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3239],\n",
      "        [0.0988],\n",
      "        [0.5057],\n",
      "        [0.4958],\n",
      "        [0.0988],\n",
      "        [0.3051],\n",
      "        [0.4451],\n",
      "        [0.4710],\n",
      "        [0.0988],\n",
      "        [0.2505],\n",
      "        [0.3579],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.4577],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4721],\n",
      "        [0.4722],\n",
      "        [0.0988],\n",
      "        [0.4318],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4799],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.3952],\n",
      "        [0.2693],\n",
      "        [0.2634],\n",
      "        [0.0988],\n",
      "        [0.4145],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4879],\n",
      "        [0.4286],\n",
      "        [0.4242],\n",
      "        [0.0988],\n",
      "        [0.3801],\n",
      "        [0.0988],\n",
      "        [0.4389],\n",
      "        [0.0988],\n",
      "        [0.2976],\n",
      "        [0.3866],\n",
      "        [0.2656],\n",
      "        [0.0988],\n",
      "        [0.4915],\n",
      "        [0.3584],\n",
      "        [0.4908],\n",
      "        [0.0988],\n",
      "        [0.2658],\n",
      "        [0.2118],\n",
      "        [0.4057],\n",
      "        [0.1274],\n",
      "        [0.4229],\n",
      "        [0.3679],\n",
      "        [0.2938],\n",
      "        [0.1143],\n",
      "        [0.3940],\n",
      "        [0.2753],\n",
      "        [0.1127],\n",
      "        [0.3758],\n",
      "        [0.4128],\n",
      "        [0.4183]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0007],\n",
      "        [0.0009],\n",
      "        [0.0018],\n",
      "        [0.0020],\n",
      "        [0.0022],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0037],\n",
      "        [0.0047],\n",
      "        [0.0043],\n",
      "        [0.0043],\n",
      "        [0.0050],\n",
      "        [0.0057],\n",
      "        [0.0069],\n",
      "        [0.0071],\n",
      "        [0.0075],\n",
      "        [0.0078],\n",
      "        [0.0076],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0087],\n",
      "        [0.0091],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0097],\n",
      "        [0.0100],\n",
      "        [0.0101],\n",
      "        [0.0101],\n",
      "        [0.0109],\n",
      "        [0.0113],\n",
      "        [0.0118],\n",
      "        [0.0121],\n",
      "        [0.0123],\n",
      "        [0.0125],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.0151],\n",
      "        [0.0151],\n",
      "        [0.0154],\n",
      "        [0.0154],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0162],\n",
      "        [0.0178],\n",
      "        [0.0181],\n",
      "        [0.0179],\n",
      "        [0.0188],\n",
      "        [0.0191],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0191],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0215],\n",
      "        [0.0221],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0243],\n",
      "        [0.0242],\n",
      "        [0.0246],\n",
      "        [0.0254],\n",
      "        [0.0267],\n",
      "        [0.0277],\n",
      "        [0.0282],\n",
      "        [0.0284],\n",
      "        [0.0291],\n",
      "        [0.0294],\n",
      "        [0.0297],\n",
      "        [0.0298],\n",
      "        [0.0297],\n",
      "        [0.0303],\n",
      "        [0.0305],\n",
      "        [0.0309],\n",
      "        [0.0309],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0330],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0348],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0366],\n",
      "        [0.0375],\n",
      "        [0.0376],\n",
      "        [0.0385],\n",
      "        [0.0384],\n",
      "        [0.0394],\n",
      "        [0.0405],\n",
      "        [0.0410],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0427],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0490],\n",
      "        [0.0509],\n",
      "        [0.0509],\n",
      "        [0.0513],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0531],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0564],\n",
      "        [0.0574],\n",
      "        [0.0592],\n",
      "        [0.0659],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0683],\n",
      "        [0.0699],\n",
      "        [0.0716],\n",
      "        [0.0717],\n",
      "        [0.0721],\n",
      "        [0.0749],\n",
      "        [0.0752],\n",
      "        [0.0768],\n",
      "        [0.0775],\n",
      "        [0.0779],\n",
      "        [0.0779],\n",
      "        [0.0779],\n",
      "        [0.0781],\n",
      "        [0.0781],\n",
      "        [0.0787],\n",
      "        [0.0814],\n",
      "        [0.0850],\n",
      "        [0.0908],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.0965],\n",
      "        [0.1029],\n",
      "        [0.1029],\n",
      "        [0.1035],\n",
      "        [0.1072],\n",
      "        [0.1126],\n",
      "        [0.1127],\n",
      "        [0.1484],\n",
      "        [0.1588],\n",
      "        [0.2054]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 8\n",
      "Number of shrink: 11\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0022],\n",
      "        [0.0047],\n",
      "        [0.0017],\n",
      "        [0.0025],\n",
      "        [0.0057],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0020],\n",
      "        [0.0037],\n",
      "        [0.0058],\n",
      "        [0.0029],\n",
      "        [0.0094],\n",
      "        [0.0039],\n",
      "        [0.0057],\n",
      "        [0.0069],\n",
      "        [0.0018],\n",
      "        [0.0116],\n",
      "        [0.0127],\n",
      "        [0.0063],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0070],\n",
      "        [0.0082],\n",
      "        [0.0077],\n",
      "        [0.0042],\n",
      "        [0.0107],\n",
      "        [0.0098],\n",
      "        [0.0111],\n",
      "        [0.0100],\n",
      "        [0.0152],\n",
      "        [0.0047],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0118],\n",
      "        [0.0109],\n",
      "        [0.0131],\n",
      "        [0.0140],\n",
      "        [0.0156],\n",
      "        [0.0162],\n",
      "        [0.0145],\n",
      "        [0.0137],\n",
      "        [0.0154],\n",
      "        [0.0142],\n",
      "        [0.0178],\n",
      "        [0.0164],\n",
      "        [0.0175],\n",
      "        [0.0229],\n",
      "        [0.0225],\n",
      "        [0.0179],\n",
      "        [0.0198],\n",
      "        [0.0178],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0207],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0155],\n",
      "        [0.0208],\n",
      "        [0.0263],\n",
      "        [0.0167],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0228],\n",
      "        [0.0243],\n",
      "        [0.0236],\n",
      "        [0.0190],\n",
      "        [0.0254],\n",
      "        [0.0215],\n",
      "        [0.0277],\n",
      "        [0.0336],\n",
      "        [0.0277],\n",
      "        [0.0344],\n",
      "        [0.0245],\n",
      "        [0.0297],\n",
      "        [0.0352],\n",
      "        [0.0308],\n",
      "        [0.0294],\n",
      "        [0.0305],\n",
      "        [0.0294],\n",
      "        [0.0296],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0384],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0336],\n",
      "        [0.0350],\n",
      "        [0.0302],\n",
      "        [0.0420],\n",
      "        [0.0375],\n",
      "        [0.0389],\n",
      "        [0.0333],\n",
      "        [0.0435],\n",
      "        [0.0394],\n",
      "        [0.0389],\n",
      "        [0.0394],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0378],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0476],\n",
      "        [0.0544],\n",
      "        [0.0498],\n",
      "        [0.0509],\n",
      "        [0.0471],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0519],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0544],\n",
      "        [0.0545],\n",
      "        [0.0549],\n",
      "        [0.0564],\n",
      "        [0.0563],\n",
      "        [0.0592],\n",
      "        [0.0659],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0736],\n",
      "        [0.0659],\n",
      "        [0.0676],\n",
      "        [0.0717],\n",
      "        [0.0708],\n",
      "        [0.0749],\n",
      "        [0.0699],\n",
      "        [0.0768],\n",
      "        [0.0787],\n",
      "        [0.0766],\n",
      "        [0.0795],\n",
      "        [0.0779],\n",
      "        [0.0728],\n",
      "        [0.0765],\n",
      "        [0.0840],\n",
      "        [0.0814],\n",
      "        [0.0864],\n",
      "        [0.0901],\n",
      "        [0.0998],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.1042],\n",
      "        [0.1043],\n",
      "        [0.1035],\n",
      "        [0.1081],\n",
      "        [0.1138],\n",
      "        [0.1127],\n",
      "        [0.1494],\n",
      "        [0.1535],\n",
      "        [0.2000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 103.10020208358765\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 159\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.1592343273368897e-06, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [10, 152, 38, 58, 11, 136, 35, 6, 92, 5, 131, 137, 138, 111, 7, 120, 34, 100, 20, 18, 84, 108, 110, 139, 107, 90, 2, 33, 32, 87, 125, 39, 50, 127, 8, 49, 3, 12, 13, 132, 140, 88, 1, 46, 89, 9, 135, 19, 109, 45, 154, 101, 116, 121, 106, 85, 112, 44, 153, 91, 4, 126, 141, 74, 14, 128, 62, 59, 142, 83, 15, 23, 47, 73, 37, 75, 0, 134, 86, 61, 102, 129, 147, 36, 114, 57, 67, 113, 143, 105, 151, 99, 104, 130, 150, 29, 28, 115, 43, 144, 56, 97, 148, 66, 72, 82, 93, 117, 124, 80, 17, 96, 103, 16, 98, 95, 149, 40, 48, 94, 31, 60, 79, 21, 81, 123, 71, 64, 78, 122, 155, 41, 70, 133, 146, 76, 42, 118, 22, 77, 51, 54, 65, 145, 55, 30, 68, 24, 119, 63, 27, 52, 25, 69, 53, 26, 156, 158, 157] 數值 torch.Size([159, 1])\n",
      "目前模型的Data狀態 torch.Size([159, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5575],\n",
      "        [0.4790],\n",
      "        [0.3656],\n",
      "        [0.2358],\n",
      "        [0.5456],\n",
      "        [0.4945],\n",
      "        [0.3551],\n",
      "        [0.6098],\n",
      "        [0.0988],\n",
      "        [0.6238],\n",
      "        [0.4875],\n",
      "        [0.4948],\n",
      "        [0.4929],\n",
      "        [0.0988],\n",
      "        [0.5964],\n",
      "        [0.4148],\n",
      "        [0.3516],\n",
      "        [0.0988],\n",
      "        [0.4669],\n",
      "        [0.4632],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4947],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6180],\n",
      "        [0.3178],\n",
      "        [0.2969],\n",
      "        [0.0988],\n",
      "        [0.4258],\n",
      "        [0.4037],\n",
      "        [0.2751],\n",
      "        [0.4390],\n",
      "        [0.5721],\n",
      "        [0.2849],\n",
      "        [0.6028],\n",
      "        [0.5122],\n",
      "        [0.5261],\n",
      "        [0.5027],\n",
      "        [0.4906],\n",
      "        [0.0988],\n",
      "        [0.6442],\n",
      "        [0.3304],\n",
      "        [0.0988],\n",
      "        [0.5785],\n",
      "        [0.5084],\n",
      "        [0.4694],\n",
      "        [0.0988],\n",
      "        [0.3721],\n",
      "        [0.4692],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4870],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3852],\n",
      "        [0.4691],\n",
      "        [0.0988],\n",
      "        [0.6211],\n",
      "        [0.4221],\n",
      "        [0.4927],\n",
      "        [0.0988],\n",
      "        [0.5131],\n",
      "        [0.4437],\n",
      "        [0.1649],\n",
      "        [0.2291],\n",
      "        [0.4803],\n",
      "        [0.0988],\n",
      "        [0.5063],\n",
      "        [0.4111],\n",
      "        [0.2819],\n",
      "        [0.0988],\n",
      "        [0.3291],\n",
      "        [0.0988],\n",
      "        [0.6641],\n",
      "        [0.5111],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.4503],\n",
      "        [0.4791],\n",
      "        [0.3251],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.4911],\n",
      "        [0.0988],\n",
      "        [0.4734],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4625],\n",
      "        [0.4820],\n",
      "        [0.2520],\n",
      "        [0.3064],\n",
      "        [0.0988],\n",
      "        [0.3595],\n",
      "        [0.5012],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.4760],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4360],\n",
      "        [0.0988],\n",
      "        [0.4733],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4810],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4776],\n",
      "        [0.3966],\n",
      "        [0.2707],\n",
      "        [0.0988],\n",
      "        [0.2644],\n",
      "        [0.2529],\n",
      "        [0.0988],\n",
      "        [0.4156],\n",
      "        [0.0988],\n",
      "        [0.4326],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.4281],\n",
      "        [0.4441],\n",
      "        [0.3814],\n",
      "        [0.0988],\n",
      "        [0.4968],\n",
      "        [0.4932],\n",
      "        [0.0988],\n",
      "        [0.3600],\n",
      "        [0.0988],\n",
      "        [0.3879],\n",
      "        [0.0988],\n",
      "        [0.2989],\n",
      "        [0.2671],\n",
      "        [0.0988],\n",
      "        [0.4961],\n",
      "        [0.2672],\n",
      "        [0.2125],\n",
      "        [0.1274],\n",
      "        [0.4241],\n",
      "        [0.4095],\n",
      "        [0.1143],\n",
      "        [0.3691],\n",
      "        [0.2952],\n",
      "        [0.3948],\n",
      "        [0.1127],\n",
      "        [0.2765],\n",
      "        [0.3768],\n",
      "        [0.4181],\n",
      "        [0.4238],\n",
      "        [0.4146]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0018],\n",
      "        [0.0022],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0020],\n",
      "        [0.0029],\n",
      "        [0.0025],\n",
      "        [0.0037],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0047],\n",
      "        [0.0047],\n",
      "        [0.0057],\n",
      "        [0.0058],\n",
      "        [0.0057],\n",
      "        [0.0063],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0077],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0094],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0111],\n",
      "        [0.0107],\n",
      "        [0.0109],\n",
      "        [0.0118],\n",
      "        [0.0116],\n",
      "        [0.0126],\n",
      "        [0.0129],\n",
      "        [0.0127],\n",
      "        [0.0131],\n",
      "        [0.0140],\n",
      "        [0.0137],\n",
      "        [0.0142],\n",
      "        [0.0145],\n",
      "        [0.0152],\n",
      "        [0.0155],\n",
      "        [0.0154],\n",
      "        [0.0156],\n",
      "        [0.0162],\n",
      "        [0.0164],\n",
      "        [0.0175],\n",
      "        [0.0167],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0178],\n",
      "        [0.0190],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0207],\n",
      "        [0.0215],\n",
      "        [0.0222],\n",
      "        [0.0228],\n",
      "        [0.0225],\n",
      "        [0.0229],\n",
      "        [0.0233],\n",
      "        [0.0236],\n",
      "        [0.0245],\n",
      "        [0.0243],\n",
      "        [0.0254],\n",
      "        [0.0263],\n",
      "        [0.0277],\n",
      "        [0.0277],\n",
      "        [0.0294],\n",
      "        [0.0296],\n",
      "        [0.0297],\n",
      "        [0.0294],\n",
      "        [0.0305],\n",
      "        [0.0308],\n",
      "        [0.0302],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0333],\n",
      "        [0.0336],\n",
      "        [0.0336],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0344],\n",
      "        [0.0346],\n",
      "        [0.0352],\n",
      "        [0.0350],\n",
      "        [0.0375],\n",
      "        [0.0378],\n",
      "        [0.0384],\n",
      "        [0.0389],\n",
      "        [0.0389],\n",
      "        [0.0394],\n",
      "        [0.0394],\n",
      "        [0.0420],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0435],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0471],\n",
      "        [0.0476],\n",
      "        [0.0498],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0519],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0544],\n",
      "        [0.0544],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.0549],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0563],\n",
      "        [0.0592],\n",
      "        [0.0659],\n",
      "        [0.0659],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0676],\n",
      "        [0.0699],\n",
      "        [0.0708],\n",
      "        [0.0717],\n",
      "        [0.0728],\n",
      "        [0.0736],\n",
      "        [0.0749],\n",
      "        [0.0765],\n",
      "        [0.0768],\n",
      "        [0.0766],\n",
      "        [0.0779],\n",
      "        [0.0787],\n",
      "        [0.0795],\n",
      "        [0.0814],\n",
      "        [0.0840],\n",
      "        [0.0864],\n",
      "        [0.0901],\n",
      "        [0.0961],\n",
      "        [0.0977],\n",
      "        [0.0998],\n",
      "        [0.1035],\n",
      "        [0.1042],\n",
      "        [0.1043],\n",
      "        [0.1081],\n",
      "        [0.1127],\n",
      "        [0.1138],\n",
      "        [0.1494],\n",
      "        [0.1535],\n",
      "        [0.2000],\n",
      "        [0.2024]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 8\n",
      "Number of shrink: 11\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0006],\n",
      "        [0.0033],\n",
      "        [0.0025],\n",
      "        [0.0022],\n",
      "        [0.0032],\n",
      "        [0.0070],\n",
      "        [0.0027],\n",
      "        [0.0021],\n",
      "        [0.0037],\n",
      "        [0.0049],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0095],\n",
      "        [0.0057],\n",
      "        [0.0050],\n",
      "        [0.0091],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0079],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0082],\n",
      "        [0.0144],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0105],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0118],\n",
      "        [0.0158],\n",
      "        [0.0127],\n",
      "        [0.0136],\n",
      "        [0.0172],\n",
      "        [0.0126],\n",
      "        [0.0145],\n",
      "        [0.0148],\n",
      "        [0.0157],\n",
      "        [0.0155],\n",
      "        [0.0200],\n",
      "        [0.0108],\n",
      "        [0.0154],\n",
      "        [0.0150],\n",
      "        [0.0167],\n",
      "        [0.0164],\n",
      "        [0.0161],\n",
      "        [0.0116],\n",
      "        [0.0176],\n",
      "        [0.0179],\n",
      "        [0.0186],\n",
      "        [0.0140],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0207],\n",
      "        [0.0199],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0216],\n",
      "        [0.0163],\n",
      "        [0.0222],\n",
      "        [0.0237],\n",
      "        [0.0265],\n",
      "        [0.0275],\n",
      "        [0.0233],\n",
      "        [0.0245],\n",
      "        [0.0198],\n",
      "        [0.0243],\n",
      "        [0.0255],\n",
      "        [0.0310],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0297],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0292],\n",
      "        [0.0305],\n",
      "        [0.0304],\n",
      "        [0.0252],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0288],\n",
      "        [0.0385],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0394],\n",
      "        [0.0346],\n",
      "        [0.0401],\n",
      "        [0.0350],\n",
      "        [0.0375],\n",
      "        [0.0333],\n",
      "        [0.0434],\n",
      "        [0.0389],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0389],\n",
      "        [0.0471],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0483],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0430],\n",
      "        [0.0476],\n",
      "        [0.0502],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0542],\n",
      "        [0.0594],\n",
      "        [0.0541],\n",
      "        [0.0540],\n",
      "        [0.0548],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0620],\n",
      "        [0.0659],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0640],\n",
      "        [0.0649],\n",
      "        [0.0703],\n",
      "        [0.0717],\n",
      "        [0.0677],\n",
      "        [0.0786],\n",
      "        [0.0749],\n",
      "        [0.0759],\n",
      "        [0.0768],\n",
      "        [0.0769],\n",
      "        [0.0779],\n",
      "        [0.0792],\n",
      "        [0.0798],\n",
      "        [0.0814],\n",
      "        [0.0888],\n",
      "        [0.0871],\n",
      "        [0.0902],\n",
      "        [0.0961],\n",
      "        [0.0974],\n",
      "        [0.1032],\n",
      "        [0.1035],\n",
      "        [0.1043],\n",
      "        [0.1047],\n",
      "        [0.1077],\n",
      "        [0.1127],\n",
      "        [0.1143],\n",
      "        [0.1493],\n",
      "        [0.1486],\n",
      "        [0.1949],\n",
      "        [0.1975]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 103.22851872444153\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 35 個區塊累積花費時間(s) 0.575920581817627\n",
      "<<The performance of 35 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 0.575920581817627\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1034.95\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1368.20\n",
      "The accuracy(2000) for l = 1: 87.42%\n",
      "The accuracy(3000) for l = 1: 97.48%\n",
      "The maximum error: tensor(5041.9883)\n",
      "The minimum error: tensor(8.9609)\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 6811.8\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 6913.5\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "The maximum error: 8404.05078125\n",
      "The minimum error: 5327.19140625\n",
      "------------------------------------------------------------\n",
      "0.8742138364779874\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<36>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.234597466596824e-08, 127)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [127, 133, 6, 54, 2, 34, 31, 7, 148, 88, 1, 3, 107, 30, 96, 16, 132, 80, 104, 14, 106, 116, 134, 103, 86, 28, 29, 136, 131, 83, 35, 4, 46, 150, 135, 45, 9, 8, 84, 121, 85, 5, 149, 42, 123, 15, 105, 41, 97, 112, 124, 102, 128, 81, 117, 108, 40, 87, 70, 0, 58, 10, 130, 55, 122, 137, 79, 11, 125, 33, 43, 19, 69, 71, 138, 82, 57, 98, 126, 32, 110, 53, 63, 109, 101, 95, 100, 143, 25, 39, 111, 24, 139, 147, 52, 93, 120, 146, 62, 68, 78, 89, 113, 140, 76, 144, 13, 92, 99, 94, 12, 44, 36, 91, 27, 90, 56, 17, 75, 77, 145, 119, 118, 151, 67, 60, 74, 129, 37, 66, 72, 38, 18, 114, 73, 142, 47, 50, 61, 51, 141, 26, 64, 20, 115, 59, 23, 48, 21, 65, 49, 152, 22, 154, 153]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0037],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0070],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0091],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0126],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0164],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0179],\n",
      "        [0.0186],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0252],\n",
      "        [0.0255],\n",
      "        [0.0265],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0292],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0310],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0333],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0375],\n",
      "        [0.0385],\n",
      "        [0.0389],\n",
      "        [0.0389],\n",
      "        [0.0394],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0401],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0430],\n",
      "        [0.0434],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0471],\n",
      "        [0.0476],\n",
      "        [0.0483],\n",
      "        [0.0502],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0540],\n",
      "        [0.0541],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0594],\n",
      "        [0.0620],\n",
      "        [0.0640],\n",
      "        [0.0649],\n",
      "        [0.0659],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0677],\n",
      "        [0.0703],\n",
      "        [0.0717],\n",
      "        [0.0749],\n",
      "        [0.0759],\n",
      "        [0.0769],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0786],\n",
      "        [0.0792],\n",
      "        [0.0798],\n",
      "        [0.0814],\n",
      "        [0.0871],\n",
      "        [0.0888],\n",
      "        [0.0902],\n",
      "        [0.0961],\n",
      "        [0.0974],\n",
      "        [0.1032],\n",
      "        [0.1035],\n",
      "        [0.1043],\n",
      "        [0.1047],\n",
      "        [0.1077],\n",
      "        [0.1127],\n",
      "        [0.1143],\n",
      "        [0.1486],\n",
      "        [0.1493],\n",
      "        [0.1949],\n",
      "        [0.1975]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 156\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.234597466596824e-08, 127)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [127, 133, 6, 54, 2, 34, 31, 7, 148, 88, 1, 3, 107, 30, 96, 16, 132, 80, 104, 14, 106, 116, 134, 103, 86, 28, 29, 136, 131, 83, 35, 4, 46, 150, 135, 45, 9, 8, 84, 121, 85, 5, 149, 42, 123, 15, 105, 41, 97, 112, 124, 102, 128, 81, 117, 108, 40, 87, 70, 0, 58, 10, 130, 55, 122, 137, 79, 11, 125, 33, 43, 19, 69, 71, 138, 82, 57, 98, 126, 32, 110, 53, 63, 109, 101, 95, 100, 143, 25, 39, 111, 24, 139, 147, 52, 93, 120, 146, 62, 68, 78, 89, 113, 140, 76, 144, 13, 92, 99, 94, 12, 44, 36, 91, 27, 90, 56, 17, 75, 77, 145, 119, 118, 151, 67, 60, 74, 129, 37, 66, 72, 38, 18, 114, 73, 142, 47, 50, 61, 51, 141, 26, 64, 20, 115, 59, 23, 48, 21, 65, 49, 152, 22, 154, 153, 155] 數值 torch.Size([156, 1])\n",
      "目前模型的Data狀態 torch.Size([156, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.4920],\n",
      "        [0.5001],\n",
      "        [0.5564],\n",
      "        [0.2358],\n",
      "        [0.6094],\n",
      "        [0.3659],\n",
      "        [0.3554],\n",
      "        [0.5448],\n",
      "        [0.4842],\n",
      "        [0.0988],\n",
      "        [0.6229],\n",
      "        [0.5956],\n",
      "        [0.0988],\n",
      "        [0.3519],\n",
      "        [0.0988],\n",
      "        [0.4669],\n",
      "        [0.4996],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4631],\n",
      "        [0.0988],\n",
      "        [0.4182],\n",
      "        [0.4977],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2970],\n",
      "        [0.3179],\n",
      "        [0.4953],\n",
      "        [0.5135],\n",
      "        [0.0988],\n",
      "        [0.4038],\n",
      "        [0.5716],\n",
      "        [0.2758],\n",
      "        [0.4743],\n",
      "        [0.4997],\n",
      "        [0.2855],\n",
      "        [0.5251],\n",
      "        [0.5107],\n",
      "        [0.0988],\n",
      "        [0.4300],\n",
      "        [0.0988],\n",
      "        [0.5772],\n",
      "        [0.4743],\n",
      "        [0.3309],\n",
      "        [0.4435],\n",
      "        [0.4696],\n",
      "        [0.0988],\n",
      "        [0.3729],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4484],\n",
      "        [0.0988],\n",
      "        [0.5075],\n",
      "        [0.0988],\n",
      "        [0.4878],\n",
      "        [0.0988],\n",
      "        [0.3861],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.6202],\n",
      "        [0.1649],\n",
      "        [0.5121],\n",
      "        [0.5161],\n",
      "        [0.2291],\n",
      "        [0.4261],\n",
      "        [0.4973],\n",
      "        [0.0988],\n",
      "        [0.5057],\n",
      "        [0.4547],\n",
      "        [0.3294],\n",
      "        [0.2824],\n",
      "        [0.4108],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4849],\n",
      "        [0.0988],\n",
      "        [0.1914],\n",
      "        [0.0988],\n",
      "        [0.4671],\n",
      "        [0.3255],\n",
      "        [0.0988],\n",
      "        [0.2257],\n",
      "        [0.1522],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4840],\n",
      "        [0.2521],\n",
      "        [0.3600],\n",
      "        [0.0988],\n",
      "        [0.3066],\n",
      "        [0.4961],\n",
      "        [0.4784],\n",
      "        [0.2257],\n",
      "        [0.0988],\n",
      "        [0.4401],\n",
      "        [0.4870],\n",
      "        [0.1183],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5063],\n",
      "        [0.0988],\n",
      "        [0.4808],\n",
      "        [0.4728],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4804],\n",
      "        [0.2713],\n",
      "        [0.3969],\n",
      "        [0.0988],\n",
      "        [0.2644],\n",
      "        [0.0988],\n",
      "        [0.2529],\n",
      "        [0.4155],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4825],\n",
      "        [0.4365],\n",
      "        [0.4318],\n",
      "        [0.4491],\n",
      "        [0.0988],\n",
      "        [0.1254],\n",
      "        [0.0988],\n",
      "        [0.5019],\n",
      "        [0.3819],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3606],\n",
      "        [0.3876],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4982],\n",
      "        [0.2993],\n",
      "        [0.2675],\n",
      "        [0.0988],\n",
      "        [0.2680],\n",
      "        [0.5009],\n",
      "        [0.2125],\n",
      "        [0.1274],\n",
      "        [0.4237],\n",
      "        [0.4130],\n",
      "        [0.1143],\n",
      "        [0.3693],\n",
      "        [0.2956],\n",
      "        [0.3945],\n",
      "        [0.1127],\n",
      "        [0.2771],\n",
      "        [0.4230],\n",
      "        [0.3766],\n",
      "        [0.4289],\n",
      "        [0.4195],\n",
      "        [0.4150]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0025],\n",
      "        [0.0027],\n",
      "        [0.0032],\n",
      "        [0.0033],\n",
      "        [0.0037],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0057],\n",
      "        [0.0060],\n",
      "        [0.0069],\n",
      "        [0.0070],\n",
      "        [0.0070],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0079],\n",
      "        [0.0082],\n",
      "        [0.0091],\n",
      "        [0.0095],\n",
      "        [0.0098],\n",
      "        [0.0100],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0116],\n",
      "        [0.0118],\n",
      "        [0.0127],\n",
      "        [0.0126],\n",
      "        [0.0136],\n",
      "        [0.0140],\n",
      "        [0.0144],\n",
      "        [0.0145],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0154],\n",
      "        [0.0158],\n",
      "        [0.0164],\n",
      "        [0.0161],\n",
      "        [0.0163],\n",
      "        [0.0167],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0179],\n",
      "        [0.0186],\n",
      "        [0.0192],\n",
      "        [0.0194],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0207],\n",
      "        [0.0208],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0233],\n",
      "        [0.0237],\n",
      "        [0.0243],\n",
      "        [0.0245],\n",
      "        [0.0252],\n",
      "        [0.0255],\n",
      "        [0.0265],\n",
      "        [0.0275],\n",
      "        [0.0277],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0292],\n",
      "        [0.0290],\n",
      "        [0.0297],\n",
      "        [0.0297],\n",
      "        [0.0305],\n",
      "        [0.0310],\n",
      "        [0.0317],\n",
      "        [0.0323],\n",
      "        [0.0327],\n",
      "        [0.0333],\n",
      "        [0.0332],\n",
      "        [0.0338],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0343],\n",
      "        [0.0346],\n",
      "        [0.0350],\n",
      "        [0.0375],\n",
      "        [0.0385],\n",
      "        [0.0389],\n",
      "        [0.0389],\n",
      "        [0.0394],\n",
      "        [0.0392],\n",
      "        [0.0394],\n",
      "        [0.0401],\n",
      "        [0.0416],\n",
      "        [0.0424],\n",
      "        [0.0430],\n",
      "        [0.0434],\n",
      "        [0.0438],\n",
      "        [0.0441],\n",
      "        [0.0448],\n",
      "        [0.0449],\n",
      "        [0.0455],\n",
      "        [0.0471],\n",
      "        [0.0476],\n",
      "        [0.0483],\n",
      "        [0.0502],\n",
      "        [0.0509],\n",
      "        [0.0516],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0540],\n",
      "        [0.0541],\n",
      "        [0.0542],\n",
      "        [0.0548],\n",
      "        [0.0548],\n",
      "        [0.0553],\n",
      "        [0.0564],\n",
      "        [0.0564],\n",
      "        [0.0592],\n",
      "        [0.0594],\n",
      "        [0.0620],\n",
      "        [0.0640],\n",
      "        [0.0649],\n",
      "        [0.0659],\n",
      "        [0.0667],\n",
      "        [0.0674],\n",
      "        [0.0677],\n",
      "        [0.0703],\n",
      "        [0.0717],\n",
      "        [0.0749],\n",
      "        [0.0759],\n",
      "        [0.0769],\n",
      "        [0.0768],\n",
      "        [0.0779],\n",
      "        [0.0786],\n",
      "        [0.0792],\n",
      "        [0.0798],\n",
      "        [0.0814],\n",
      "        [0.0871],\n",
      "        [0.0888],\n",
      "        [0.0902],\n",
      "        [0.0961],\n",
      "        [0.0974],\n",
      "        [0.1032],\n",
      "        [0.1035],\n",
      "        [0.1043],\n",
      "        [0.1047],\n",
      "        [0.1077],\n",
      "        [0.1127],\n",
      "        [0.1143],\n",
      "        [0.1486],\n",
      "        [0.1493],\n",
      "        [0.1949],\n",
      "        [0.1975],\n",
      "        [0.2085]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 62\n",
      "Number of shrink: 38\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0024],\n",
      "        [    0.0074],\n",
      "        [    0.0004],\n",
      "        [    0.0021],\n",
      "        [    0.0015],\n",
      "        [    0.0001],\n",
      "        [    0.0058],\n",
      "        [    0.0044],\n",
      "        [    0.0118],\n",
      "        [    0.0037],\n",
      "        [    0.0053],\n",
      "        [    0.0048],\n",
      "        [    0.0057],\n",
      "        [    0.0093],\n",
      "        [    0.0069],\n",
      "        [    0.0055],\n",
      "        [    0.0125],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0061],\n",
      "        [    0.0082],\n",
      "        [    0.0122],\n",
      "        [    0.0151],\n",
      "        [    0.0098],\n",
      "        [    0.0101],\n",
      "        [    0.0123],\n",
      "        [    0.0075],\n",
      "        [    0.0089],\n",
      "        [    0.0072],\n",
      "        [    0.0119],\n",
      "        [    0.0073],\n",
      "        [    0.0134],\n",
      "        [    0.0124],\n",
      "        [    0.0036],\n",
      "        [    0.0179],\n",
      "        [    0.0134],\n",
      "        [    0.0192],\n",
      "        [    0.0188],\n",
      "        [    0.0154],\n",
      "        [    0.0227],\n",
      "        [    0.0163],\n",
      "        [    0.0168],\n",
      "        [    0.0068],\n",
      "        [    0.0161],\n",
      "        [    0.0219],\n",
      "        [    0.0149],\n",
      "        [    0.0179],\n",
      "        [    0.0178],\n",
      "        [    0.0192],\n",
      "        [    0.0195],\n",
      "        [    0.0157],\n",
      "        [    0.0199],\n",
      "        [    0.0224],\n",
      "        [    0.0204],\n",
      "        [    0.0189],\n",
      "        [    0.0208],\n",
      "        [    0.0206],\n",
      "        [    0.0222],\n",
      "        [    0.0233],\n",
      "        [    0.0242],\n",
      "        [    0.0243],\n",
      "        [    0.0261],\n",
      "        [    0.0212],\n",
      "        [    0.0255],\n",
      "        [    0.0311],\n",
      "        [    0.0295],\n",
      "        [    0.0276],\n",
      "        [    0.0293],\n",
      "        [    0.0254],\n",
      "        [    0.0311],\n",
      "        [    0.0308],\n",
      "        [    0.0293],\n",
      "        [    0.0296],\n",
      "        [    0.0305],\n",
      "        [    0.0336],\n",
      "        [    0.0317],\n",
      "        [    0.0322],\n",
      "        [    0.0327],\n",
      "        [    0.0303],\n",
      "        [    0.0344],\n",
      "        [    0.0338],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0344],\n",
      "        [    0.0346],\n",
      "        [    0.0350],\n",
      "        [    0.0376],\n",
      "        [    0.0434],\n",
      "        [    0.0378],\n",
      "        [    0.0397],\n",
      "        [    0.0394],\n",
      "        [    0.0401],\n",
      "        [    0.0426],\n",
      "        [    0.0491],\n",
      "        [    0.0416],\n",
      "        [    0.0424],\n",
      "        [    0.0381],\n",
      "        [    0.0513],\n",
      "        [    0.0438],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0448],\n",
      "        [    0.0456],\n",
      "        [    0.0511],\n",
      "        [    0.0476],\n",
      "        [    0.0535],\n",
      "        [    0.0505],\n",
      "        [    0.0509],\n",
      "        [    0.0516],\n",
      "        [    0.0524],\n",
      "        [    0.0528],\n",
      "        [    0.0557],\n",
      "        [    0.0588],\n",
      "        [    0.0542],\n",
      "        [    0.0551],\n",
      "        [    0.0548],\n",
      "        [    0.0553],\n",
      "        [    0.0543],\n",
      "        [    0.0564],\n",
      "        [    0.0592],\n",
      "        [    0.0652],\n",
      "        [    0.0577],\n",
      "        [    0.0605],\n",
      "        [    0.0541],\n",
      "        [    0.0659],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0633],\n",
      "        [    0.0736],\n",
      "        [    0.0717],\n",
      "        [    0.0748],\n",
      "        [    0.0774],\n",
      "        [    0.0752],\n",
      "        [    0.0768],\n",
      "        [    0.0779],\n",
      "        [    0.0837],\n",
      "        [    0.0777],\n",
      "        [    0.0797],\n",
      "        [    0.0814],\n",
      "        [    0.0873],\n",
      "        [    0.0939],\n",
      "        [    0.0912],\n",
      "        [    0.0960],\n",
      "        [    0.0972],\n",
      "        [    0.1066],\n",
      "        [    0.1035],\n",
      "        [    0.1054],\n",
      "        [    0.1037],\n",
      "        [    0.1075],\n",
      "        [    0.1126],\n",
      "        [    0.1144],\n",
      "        [    0.1365],\n",
      "        [    0.1497],\n",
      "        [    0.1827],\n",
      "        [    0.1856],\n",
      "        [    0.1960]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 103.75824022293091\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 157\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.512291423315219e-08, 34)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [34, 2, 6, 54, 127, 150, 88, 7, 3, 16, 1, 107, 31, 14, 149, 96, 131, 35, 29, 133, 80, 104, 106, 136, 30, 103, 86, 148, 83, 116, 28, 132, 46, 45, 4, 15, 134, 84, 42, 124, 85, 5, 41, 135, 105, 8, 117, 9, 97, 112, 102, 81, 40, 108, 130, 123, 87, 121, 128, 70, 58, 0, 125, 55, 10, 79, 11, 19, 137, 69, 126, 71, 43, 122, 33, 82, 57, 98, 138, 110, 53, 63, 32, 109, 101, 95, 25, 100, 120, 111, 39, 24, 52, 93, 139, 143, 62, 68, 78, 89, 113, 76, 147, 13, 92, 140, 146, 99, 94, 12, 144, 151, 91, 17, 90, 27, 44, 56, 75, 119, 36, 77, 118, 129, 145, 67, 60, 74, 66, 37, 72, 18, 114, 38, 47, 73, 50, 61, 142, 51, 26, 141, 64, 20, 59, 48, 23, 115, 21, 65, 49, 152, 22, 154, 153, 155, 156] 數值 torch.Size([157, 1])\n",
      "目前模型的Data狀態 torch.Size([157, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.3636],\n",
      "        [0.6087],\n",
      "        [0.5562],\n",
      "        [0.2357],\n",
      "        [0.4941],\n",
      "        [0.4847],\n",
      "        [0.0988],\n",
      "        [0.5435],\n",
      "        [0.5955],\n",
      "        [0.4685],\n",
      "        [0.6225],\n",
      "        [0.0988],\n",
      "        [0.3522],\n",
      "        [0.4649],\n",
      "        [0.4838],\n",
      "        [0.0988],\n",
      "        [0.5179],\n",
      "        [0.3984],\n",
      "        [0.3146],\n",
      "        [0.5069],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4972],\n",
      "        [0.3486],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4926],\n",
      "        [0.0988],\n",
      "        [0.4214],\n",
      "        [0.2955],\n",
      "        [0.5051],\n",
      "        [0.2746],\n",
      "        [0.2844],\n",
      "        [0.5724],\n",
      "        [0.4723],\n",
      "        [0.5033],\n",
      "        [0.0988],\n",
      "        [0.3303],\n",
      "        [0.4525],\n",
      "        [0.0988],\n",
      "        [0.5779],\n",
      "        [0.3721],\n",
      "        [0.5032],\n",
      "        [0.0988],\n",
      "        [0.5076],\n",
      "        [0.4860],\n",
      "        [0.5214],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3851],\n",
      "        [0.0988],\n",
      "        [0.5200],\n",
      "        [0.4483],\n",
      "        [0.0988],\n",
      "        [0.4368],\n",
      "        [0.5099],\n",
      "        [0.0988],\n",
      "        [0.1648],\n",
      "        [0.6197],\n",
      "        [0.4582],\n",
      "        [0.2291],\n",
      "        [0.5105],\n",
      "        [0.0988],\n",
      "        [0.5047],\n",
      "        [0.4111],\n",
      "        [0.4994],\n",
      "        [0.0988],\n",
      "        [0.4700],\n",
      "        [0.0988],\n",
      "        [0.2807],\n",
      "        [0.4308],\n",
      "        [0.3274],\n",
      "        [0.0988],\n",
      "        [0.1913],\n",
      "        [0.0988],\n",
      "        [0.4875],\n",
      "        [0.0988],\n",
      "        [0.2256],\n",
      "        [0.1521],\n",
      "        [0.3243],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2532],\n",
      "        [0.0988],\n",
      "        [0.4450],\n",
      "        [0.0988],\n",
      "        [0.3593],\n",
      "        [0.3076],\n",
      "        [0.2256],\n",
      "        [0.0988],\n",
      "        [0.4993],\n",
      "        [0.4889],\n",
      "        [0.1182],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4874],\n",
      "        [0.4726],\n",
      "        [0.0988],\n",
      "        [0.5103],\n",
      "        [0.4949],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4802],\n",
      "        [0.4860],\n",
      "        [0.4599],\n",
      "        [0.0988],\n",
      "        [0.4176],\n",
      "        [0.0988],\n",
      "        [0.2642],\n",
      "        [0.2696],\n",
      "        [0.2528],\n",
      "        [0.0988],\n",
      "        [0.4408],\n",
      "        [0.3923],\n",
      "        [0.0988],\n",
      "        [0.4353],\n",
      "        [0.5062],\n",
      "        [0.4884],\n",
      "        [0.0988],\n",
      "        [0.1253],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3786],\n",
      "        [0.0988],\n",
      "        [0.3894],\n",
      "        [0.0988],\n",
      "        [0.3591],\n",
      "        [0.2978],\n",
      "        [0.0988],\n",
      "        [0.2673],\n",
      "        [0.0988],\n",
      "        [0.5033],\n",
      "        [0.2681],\n",
      "        [0.2115],\n",
      "        [0.5060],\n",
      "        [0.1273],\n",
      "        [0.4236],\n",
      "        [0.1143],\n",
      "        [0.2946],\n",
      "        [0.3703],\n",
      "        [0.4163],\n",
      "        [0.3943],\n",
      "        [0.1126],\n",
      "        [0.2771],\n",
      "        [0.4351],\n",
      "        [0.3771],\n",
      "        [0.4410],\n",
      "        [0.4315],\n",
      "        [0.4276],\n",
      "        [0.4395]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0015],\n",
      "        [    0.0004],\n",
      "        [    0.0021],\n",
      "        [    0.0024],\n",
      "        [    0.0036],\n",
      "        [    0.0037],\n",
      "        [    0.0044],\n",
      "        [    0.0048],\n",
      "        [    0.0055],\n",
      "        [    0.0053],\n",
      "        [    0.0057],\n",
      "        [    0.0058],\n",
      "        [    0.0061],\n",
      "        [    0.0068],\n",
      "        [    0.0069],\n",
      "        [    0.0072],\n",
      "        [    0.0073],\n",
      "        [    0.0075],\n",
      "        [    0.0074],\n",
      "        [    0.0080],\n",
      "        [    0.0080],\n",
      "        [    0.0082],\n",
      "        [    0.0089],\n",
      "        [    0.0093],\n",
      "        [    0.0098],\n",
      "        [    0.0101],\n",
      "        [    0.0118],\n",
      "        [    0.0119],\n",
      "        [    0.0122],\n",
      "        [    0.0123],\n",
      "        [    0.0125],\n",
      "        [    0.0124],\n",
      "        [    0.0134],\n",
      "        [    0.0134],\n",
      "        [    0.0149],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0161],\n",
      "        [    0.0157],\n",
      "        [    0.0163],\n",
      "        [    0.0168],\n",
      "        [    0.0178],\n",
      "        [    0.0179],\n",
      "        [    0.0179],\n",
      "        [    0.0188],\n",
      "        [    0.0189],\n",
      "        [    0.0192],\n",
      "        [    0.0192],\n",
      "        [    0.0195],\n",
      "        [    0.0199],\n",
      "        [    0.0204],\n",
      "        [    0.0206],\n",
      "        [    0.0208],\n",
      "        [    0.0212],\n",
      "        [    0.0219],\n",
      "        [    0.0222],\n",
      "        [    0.0227],\n",
      "        [    0.0224],\n",
      "        [    0.0233],\n",
      "        [    0.0243],\n",
      "        [    0.0242],\n",
      "        [    0.0254],\n",
      "        [    0.0255],\n",
      "        [    0.0261],\n",
      "        [    0.0276],\n",
      "        [    0.0293],\n",
      "        [    0.0293],\n",
      "        [    0.0295],\n",
      "        [    0.0296],\n",
      "        [    0.0303],\n",
      "        [    0.0305],\n",
      "        [    0.0308],\n",
      "        [    0.0311],\n",
      "        [    0.0311],\n",
      "        [    0.0317],\n",
      "        [    0.0322],\n",
      "        [    0.0327],\n",
      "        [    0.0336],\n",
      "        [    0.0338],\n",
      "        [    0.0339],\n",
      "        [    0.0340],\n",
      "        [    0.0344],\n",
      "        [    0.0344],\n",
      "        [    0.0346],\n",
      "        [    0.0350],\n",
      "        [    0.0378],\n",
      "        [    0.0376],\n",
      "        [    0.0381],\n",
      "        [    0.0394],\n",
      "        [    0.0397],\n",
      "        [    0.0401],\n",
      "        [    0.0416],\n",
      "        [    0.0424],\n",
      "        [    0.0426],\n",
      "        [    0.0434],\n",
      "        [    0.0438],\n",
      "        [    0.0441],\n",
      "        [    0.0448],\n",
      "        [    0.0448],\n",
      "        [    0.0456],\n",
      "        [    0.0476],\n",
      "        [    0.0491],\n",
      "        [    0.0505],\n",
      "        [    0.0509],\n",
      "        [    0.0511],\n",
      "        [    0.0513],\n",
      "        [    0.0516],\n",
      "        [    0.0524],\n",
      "        [    0.0528],\n",
      "        [    0.0535],\n",
      "        [    0.0541],\n",
      "        [    0.0542],\n",
      "        [    0.0543],\n",
      "        [    0.0548],\n",
      "        [    0.0551],\n",
      "        [    0.0557],\n",
      "        [    0.0553],\n",
      "        [    0.0564],\n",
      "        [    0.0577],\n",
      "        [    0.0588],\n",
      "        [    0.0592],\n",
      "        [    0.0605],\n",
      "        [    0.0633],\n",
      "        [    0.0652],\n",
      "        [    0.0659],\n",
      "        [    0.0667],\n",
      "        [    0.0674],\n",
      "        [    0.0717],\n",
      "        [    0.0736],\n",
      "        [    0.0748],\n",
      "        [    0.0752],\n",
      "        [    0.0768],\n",
      "        [    0.0774],\n",
      "        [    0.0777],\n",
      "        [    0.0779],\n",
      "        [    0.0797],\n",
      "        [    0.0814],\n",
      "        [    0.0837],\n",
      "        [    0.0873],\n",
      "        [    0.0912],\n",
      "        [    0.0939],\n",
      "        [    0.0960],\n",
      "        [    0.0972],\n",
      "        [    0.1035],\n",
      "        [    0.1037],\n",
      "        [    0.1054],\n",
      "        [    0.1066],\n",
      "        [    0.1075],\n",
      "        [    0.1126],\n",
      "        [    0.1144],\n",
      "        [    0.1365],\n",
      "        [    0.1497],\n",
      "        [    0.1827],\n",
      "        [    0.1856],\n",
      "        [    0.1960],\n",
      "        [    0.2265]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 63\n",
      "Number of shrink: 37\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0029],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0021],\n",
      "        [0.0066],\n",
      "        [0.0114],\n",
      "        [0.0038],\n",
      "        [0.0004],\n",
      "        [0.0098],\n",
      "        [0.0022],\n",
      "        [0.0010],\n",
      "        [0.0056],\n",
      "        [0.0048],\n",
      "        [0.0014],\n",
      "        [0.0068],\n",
      "        [0.0070],\n",
      "        [0.0004],\n",
      "        [0.0066],\n",
      "        [0.0075],\n",
      "        [0.0186],\n",
      "        [0.0080],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0044],\n",
      "        [0.0089],\n",
      "        [0.0099],\n",
      "        [0.0101],\n",
      "        [0.0247],\n",
      "        [0.0119],\n",
      "        [0.0145],\n",
      "        [0.0110],\n",
      "        [0.0214],\n",
      "        [0.0162],\n",
      "        [0.0178],\n",
      "        [0.0197],\n",
      "        [0.0063],\n",
      "        [0.0244],\n",
      "        [0.0153],\n",
      "        [0.0224],\n",
      "        [0.0084],\n",
      "        [0.0163],\n",
      "        [0.0230],\n",
      "        [0.0242],\n",
      "        [0.0248],\n",
      "        [0.0179],\n",
      "        [0.0170],\n",
      "        [0.0179],\n",
      "        [0.0184],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0263],\n",
      "        [0.0208],\n",
      "        [0.0143],\n",
      "        [0.0305],\n",
      "        [0.0222],\n",
      "        [0.0327],\n",
      "        [0.0280],\n",
      "        [0.0232],\n",
      "        [0.0243],\n",
      "        [0.0202],\n",
      "        [0.0192],\n",
      "        [0.0255],\n",
      "        [0.0233],\n",
      "        [0.0276],\n",
      "        [0.0250],\n",
      "        [0.0233],\n",
      "        [0.0344],\n",
      "        [0.0296],\n",
      "        [0.0259],\n",
      "        [0.0305],\n",
      "        [0.0260],\n",
      "        [0.0390],\n",
      "        [0.0286],\n",
      "        [0.0317],\n",
      "        [0.0322],\n",
      "        [0.0327],\n",
      "        [0.0385],\n",
      "        [0.0338],\n",
      "        [0.0339],\n",
      "        [0.0340],\n",
      "        [0.0312],\n",
      "        [0.0344],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.0327],\n",
      "        [0.0376],\n",
      "        [0.0306],\n",
      "        [0.0395],\n",
      "        [0.0342],\n",
      "        [0.0453],\n",
      "        [0.0415],\n",
      "        [0.0424],\n",
      "        [0.0493],\n",
      "        [0.0518],\n",
      "        [0.0439],\n",
      "        [0.0440],\n",
      "        [0.0448],\n",
      "        [0.0448],\n",
      "        [0.0456],\n",
      "        [0.0475],\n",
      "        [0.0620],\n",
      "        [0.0453],\n",
      "        [0.0509],\n",
      "        [0.0598],\n",
      "        [0.0619],\n",
      "        [0.0516],\n",
      "        [0.0524],\n",
      "        [0.0479],\n",
      "        [0.0608],\n",
      "        [0.0385],\n",
      "        [0.0541],\n",
      "        [0.0456],\n",
      "        [0.0548],\n",
      "        [0.0520],\n",
      "        [0.0512],\n",
      "        [0.0552],\n",
      "        [0.0563],\n",
      "        [0.0517],\n",
      "        [0.0580],\n",
      "        [0.0592],\n",
      "        [0.0570],\n",
      "        [0.0554],\n",
      "        [0.0737],\n",
      "        [0.0659],\n",
      "        [0.0668],\n",
      "        [0.0674],\n",
      "        [0.0717],\n",
      "        [0.0717],\n",
      "        [0.0748],\n",
      "        [0.0674],\n",
      "        [0.0768],\n",
      "        [0.0734],\n",
      "        [0.0813],\n",
      "        [0.0778],\n",
      "        [0.0835],\n",
      "        [0.0815],\n",
      "        [0.0922],\n",
      "        [0.0908],\n",
      "        [0.0887],\n",
      "        [0.1029],\n",
      "        [0.0960],\n",
      "        [0.1023],\n",
      "        [0.1036],\n",
      "        [0.1079],\n",
      "        [0.1119],\n",
      "        [0.1095],\n",
      "        [0.1132],\n",
      "        [0.1126],\n",
      "        [0.1187],\n",
      "        [0.1188],\n",
      "        [0.1556],\n",
      "        [0.1653],\n",
      "        [0.1680],\n",
      "        [0.1780],\n",
      "        [0.2063]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 104.05116987228394\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 158\n",
      "剩餘X 資料 torch.Size([159, 18])\n",
      "剩餘Y 資料 torch.Size([159, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.0378906267760613e-07, 131)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引: [131, 7, 1, 14, 16, 54, 34, 88, 136, 31, 6, 107, 15, 2, 127, 35, 96, 149, 29, 104, 80, 106, 124, 30, 103, 3, 86, 28, 150, 83, 130, 116, 84, 46, 85, 8, 45, 105, 117, 9, 133, 97, 112, 125, 0, 4, 102, 81, 108, 132, 87, 42, 5, 19, 70, 10, 41, 58, 148, 134, 11, 135, 55, 126, 43, 40, 79, 128, 33, 69, 123, 71, 120, 32, 82, 57, 121, 25, 98, 110, 53, 63, 39, 109, 137, 101, 95, 100, 138, 151, 122, 111, 52, 93, 62, 68, 78, 89, 24, 13, 113, 17, 76, 12, 139, 92, 44, 143, 99, 119, 27, 94, 91, 90, 129, 56, 75, 118, 36, 77, 140, 144, 147, 146, 67, 60, 74, 18, 66, 37, 38, 145, 72, 114, 73, 47, 61, 50, 26, 51, 142, 64, 20, 141, 59, 48, 115, 23, 65, 21, 49, 152, 22, 154, 153, 155, 156, 157] 數值 torch.Size([158, 1])\n",
      "目前模型的Data狀態 torch.Size([158, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5255],\n",
      "        [0.5475],\n",
      "        [0.6268],\n",
      "        [0.4724],\n",
      "        [0.4761],\n",
      "        [0.2357],\n",
      "        [0.3663],\n",
      "        [0.0988],\n",
      "        [0.5017],\n",
      "        [0.3532],\n",
      "        [0.5617],\n",
      "        [0.0988],\n",
      "        [0.4810],\n",
      "        [0.6133],\n",
      "        [0.4983],\n",
      "        [0.3978],\n",
      "        [0.0988],\n",
      "        [0.4974],\n",
      "        [0.3146],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4597],\n",
      "        [0.3490],\n",
      "        [0.0988],\n",
      "        [0.6005],\n",
      "        [0.0988],\n",
      "        [0.2968],\n",
      "        [0.4997],\n",
      "        [0.0988],\n",
      "        [0.5269],\n",
      "        [0.4236],\n",
      "        [0.0988],\n",
      "        [0.2784],\n",
      "        [0.0988],\n",
      "        [0.5095],\n",
      "        [0.2888],\n",
      "        [0.0988],\n",
      "        [0.4851],\n",
      "        [0.5222],\n",
      "        [0.5182],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4643],\n",
      "        [0.6238],\n",
      "        [0.5787],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5139],\n",
      "        [0.0988],\n",
      "        [0.3365],\n",
      "        [0.5841],\n",
      "        [0.4171],\n",
      "        [0.0988],\n",
      "        [0.5134],\n",
      "        [0.3785],\n",
      "        [0.1648],\n",
      "        [0.5055],\n",
      "        [0.5127],\n",
      "        [0.5090],\n",
      "        [0.5102],\n",
      "        [0.2290],\n",
      "        [0.4744],\n",
      "        [0.2855],\n",
      "        [0.3908],\n",
      "        [0.0988],\n",
      "        [0.5155],\n",
      "        [0.3299],\n",
      "        [0.0988],\n",
      "        [0.4568],\n",
      "        [0.0988],\n",
      "        [0.4525],\n",
      "        [0.3275],\n",
      "        [0.0988],\n",
      "        [0.1913],\n",
      "        [0.4468],\n",
      "        [0.2583],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.2256],\n",
      "        [0.1521],\n",
      "        [0.3647],\n",
      "        [0.0988],\n",
      "        [0.5042],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.4924],\n",
      "        [0.4755],\n",
      "        [0.4387],\n",
      "        [0.0988],\n",
      "        [0.2256],\n",
      "        [0.0988],\n",
      "        [0.1182],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3127],\n",
      "        [0.4777],\n",
      "        [0.0988],\n",
      "        [0.4263],\n",
      "        [0.0988],\n",
      "        [0.4850],\n",
      "        [0.5060],\n",
      "        [0.0988],\n",
      "        [0.2740],\n",
      "        [0.4973],\n",
      "        [0.0988],\n",
      "        [0.4468],\n",
      "        [0.2673],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.5142],\n",
      "        [0.2528],\n",
      "        [0.0988],\n",
      "        [0.4387],\n",
      "        [0.3930],\n",
      "        [0.0988],\n",
      "        [0.5190],\n",
      "        [0.4933],\n",
      "        [0.5002],\n",
      "        [0.5055],\n",
      "        [0.0988],\n",
      "        [0.1253],\n",
      "        [0.0988],\n",
      "        [0.3971],\n",
      "        [0.0988],\n",
      "        [0.3804],\n",
      "        [0.3631],\n",
      "        [0.4969],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.0988],\n",
      "        [0.3014],\n",
      "        [0.0988],\n",
      "        [0.2711],\n",
      "        [0.2140],\n",
      "        [0.2716],\n",
      "        [0.5117],\n",
      "        [0.1273],\n",
      "        [0.4286],\n",
      "        [0.5150],\n",
      "        [0.1142],\n",
      "        [0.2988],\n",
      "        [0.4192],\n",
      "        [0.3769],\n",
      "        [0.1126],\n",
      "        [0.3999],\n",
      "        [0.2815],\n",
      "        [0.4528],\n",
      "        [0.3829],\n",
      "        [0.4584],\n",
      "        [0.4490],\n",
      "        [0.4455],\n",
      "        [0.4597],\n",
      "        [0.4550]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0004],\n",
      "        [0.0010],\n",
      "        [0.0014],\n",
      "        [0.0022],\n",
      "        [0.0021],\n",
      "        [0.0029],\n",
      "        [0.0038],\n",
      "        [0.0044],\n",
      "        [0.0048],\n",
      "        [0.0059],\n",
      "        [0.0056],\n",
      "        [0.0063],\n",
      "        [0.0060],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0070],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0084],\n",
      "        [0.0089],\n",
      "        [0.0099],\n",
      "        [0.0098],\n",
      "        [0.0101],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0143],\n",
      "        [0.0145],\n",
      "        [0.0153],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0170],\n",
      "        [0.0178],\n",
      "        [0.0179],\n",
      "        [0.0179],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0192],\n",
      "        [0.0202],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0204],\n",
      "        [0.0208],\n",
      "        [0.0214],\n",
      "        [0.0222],\n",
      "        [0.0224],\n",
      "        [0.0230],\n",
      "        [0.0233],\n",
      "        [0.0232],\n",
      "        [0.0233],\n",
      "        [0.0242],\n",
      "        [0.0243],\n",
      "        [0.0247],\n",
      "        [0.0244],\n",
      "        [0.0250],\n",
      "        [0.0248],\n",
      "        [0.0255],\n",
      "        [0.0259],\n",
      "        [0.0260],\n",
      "        [0.0263],\n",
      "        [0.0276],\n",
      "        [0.0280],\n",
      "        [0.0286],\n",
      "        [0.0296],\n",
      "        [0.0305],\n",
      "        [0.0305],\n",
      "        [0.0306],\n",
      "        [0.0312],\n",
      "        [0.0317],\n",
      "        [0.0322],\n",
      "        [0.0327],\n",
      "        [0.0327],\n",
      "        [0.0327],\n",
      "        [0.0338],\n",
      "        [0.0339],\n",
      "        [0.0340],\n",
      "        [0.0342],\n",
      "        [0.0344],\n",
      "        [0.0344],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.0376],\n",
      "        [0.0385],\n",
      "        [0.0385],\n",
      "        [0.0390],\n",
      "        [0.0395],\n",
      "        [0.0415],\n",
      "        [0.0424],\n",
      "        [0.0439],\n",
      "        [0.0440],\n",
      "        [0.0448],\n",
      "        [0.0448],\n",
      "        [0.0453],\n",
      "        [0.0453],\n",
      "        [0.0456],\n",
      "        [0.0456],\n",
      "        [0.0475],\n",
      "        [0.0479],\n",
      "        [0.0493],\n",
      "        [0.0509],\n",
      "        [0.0512],\n",
      "        [0.0518],\n",
      "        [0.0516],\n",
      "        [0.0517],\n",
      "        [0.0520],\n",
      "        [0.0524],\n",
      "        [0.0541],\n",
      "        [0.0548],\n",
      "        [0.0554],\n",
      "        [0.0552],\n",
      "        [0.0563],\n",
      "        [0.0570],\n",
      "        [0.0580],\n",
      "        [0.0592],\n",
      "        [0.0598],\n",
      "        [0.0608],\n",
      "        [0.0620],\n",
      "        [0.0619],\n",
      "        [0.0659],\n",
      "        [0.0668],\n",
      "        [0.0674],\n",
      "        [0.0674],\n",
      "        [0.0717],\n",
      "        [0.0717],\n",
      "        [0.0734],\n",
      "        [0.0737],\n",
      "        [0.0748],\n",
      "        [0.0768],\n",
      "        [0.0778],\n",
      "        [0.0813],\n",
      "        [0.0815],\n",
      "        [0.0835],\n",
      "        [0.0887],\n",
      "        [0.0908],\n",
      "        [0.0922],\n",
      "        [0.0960],\n",
      "        [0.1023],\n",
      "        [0.1029],\n",
      "        [0.1036],\n",
      "        [0.1079],\n",
      "        [0.1095],\n",
      "        [0.1119],\n",
      "        [0.1126],\n",
      "        [0.1132],\n",
      "        [0.1187],\n",
      "        [0.1188],\n",
      "        [0.1556],\n",
      "        [0.1653],\n",
      "        [0.1680],\n",
      "        [0.1780],\n",
      "        [0.2063],\n",
      "        [0.2562]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.31\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.31\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.31\n"
     ]
    }
   ],
   "source": [
    "evaluation_table_train = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"MAPE\",\"RMSE\",\"Accuracy(2000)\",\"Accuracy(3000)\",\"Maximum error\",\"Minimum error\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "evaluation_table_test = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"MAPE\",\"RMSE\",\"Accuracy(2000)\",\"Accuracy(3000)\",\"Maximum error\",\"Minimum error\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "forecasted_price = pd.DataFrame(columns=[\"Date\", \"Actual\", \"Forecasted_price\"])\n",
    "\n",
    "date, x_data, y_data= get_data(4)\n",
    "\n",
    "x_data = sc.fit_transform(x_data)\n",
    "y_data = sc.fit_transform(y_data[:,3].reshape(-1,1))\n",
    "threshold_for_error = 8000/(sc.data_max_-sc.data_min_)\n",
    "\n",
    "data = range(x_data.shape[0])\n",
    "# window_size => the length of training block\n",
    "window_size = 159\n",
    "# step_window => step size of each window\n",
    "step_window = 4\n",
    "# the split data\n",
    "splits = []\n",
    "\n",
    "adjust = 0\n",
    "\n",
    "## Moving window mechnism\n",
    "for i in range(window_size, len(data), step_window):\n",
    "    train = np.array(data[i-window_size:i])\n",
    "    test = np.array(data[i:i+step_window])\n",
    "#     test = np.array(data[i-window_size:i+step_window])\n",
    "    splits.append(('TRAIN:', train, 'TEST:', test))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i_block in range(len(splits)):\n",
    "# for i_block in range(-2,0,1):\n",
    "# for i_block in range(2):\n",
    "    block_start = time.time()\n",
    "    ## Record the number of each step\n",
    "    nb_step4 = 0\n",
    "    nb_step6_1 = 0\n",
    "    nb_step6_2 = 0\n",
    "    \n",
    "    print(\"The <<%d>> Block\" %(i_block+1))\n",
    "#     print(\"The <<%d>> Block\" %(len(splits)+i_block+1))\n",
    "#     print(\"The training block\\n\", y_data[splits[i_block][1]])\n",
    "#     print(\"The testing block\\n\", y_data[splits[i_block][3]])\n",
    "    \n",
    "    x_train = x_data[splits[i_block][1]]\n",
    "    x_test = x_data[splits[i_block][3]]\n",
    "    y_train = y_data[splits[i_block][1]]\n",
    "    y_test = y_data[splits[i_block][3]]\n",
    "    \n",
    "    x_train_scaled = torch.FloatTensor(x_train)\n",
    "    x_test_scaled = torch.FloatTensor(x_test)\n",
    "    y_train_scaled = torch.FloatTensor(y_train)\n",
    "    y_test = sc.inverse_transform(y_test)\n",
    "\n",
    "    \n",
    "#     if i_block == -2:\n",
    "    if i_block == 0:\n",
    "        lower = torch.mean(y_train_scaled)-0.3*torch.std(y_train_scaled)\n",
    "        upper = torch.mean(y_train_scaled)+0.3*torch.std(y_train_scaled)\n",
    "        nonoutlier_index = torch.nonzero((y_train_scaled[:,0]>lower)&(y_train_scaled[:,0]<upper)).reshape([-1])\n",
    "        print(\"初始值\",nonoutlier_index.shape)\n",
    "        initial_x = x_train_scaled[nonoutlier_index[:19]]\n",
    "        initial_y = y_train_scaled[nonoutlier_index[:19]]\n",
    "        \n",
    "#         x_train_scaled = np.delete(x_train_scaled, nonoutlier_index[:19], 0)\n",
    "#         y_train_scaled = np.delete(y_train_scaled, nonoutlier_index[:19], 0)\n",
    "#         print(initial_x.shape[0])\n",
    "        \n",
    "        network = Network(1,initial_x,initial_y)\n",
    "        \n",
    "        network.nb_node_acceptable = torch.IntTensor([1 for _ in range(initial_x.shape[0])])\n",
    "        network.threshold_for_error = round(threshold_for_error[0],2)\n",
    "        \n",
    "        initializing(network, initial_x, initial_y)\n",
    "        \n",
    "        print(\"<<Initializing後看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(network.y-yo))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        remainder = int(window_size) - initial_x.shape[0]-adjust\n",
    "#         remainder = int(window_size*0.9624) - initial_x.shape[0]\n",
    "        nb_step4 += initial_x.shape[0]\n",
    "    \n",
    "    else:\n",
    "#         print(\"新的Code待驗證\")\n",
    "#         print(network.state_dict())\n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        restart_index = int(x_train_scaled.shape[0])-step_window-adjust\n",
    "        print(\"其他區塊剛開始選的資料索引：\",sorted_index[:restart_index])\n",
    "        init_x = x_train_scaled[sorted_index[:int(x_train_scaled.shape[0])-step_window-adjust]].reshape(-1,x_train_scaled.shape[1])\n",
    "        init_y = y_train_scaled[sorted_index[:int(x_train_scaled.shape[0])-step_window-adjust]].reshape(-1,1)\n",
    "#         print(\"取得的x\",init_x.shape)\n",
    "#         print(\"取得的y\",init_y.shape)\n",
    "#         print(\"前\")\n",
    "#         print(network.y.shape)\n",
    "        network.setData(init_x, init_y)\n",
    "#         print(\"後\")\n",
    "#         print(network.y.shape)\n",
    "        network.nb_node_acceptable = torch.IntTensor([network.linear1.bias.data.shape[0] for _ in range(init_x.shape[0])])\n",
    "        network.nb_node_pruned = 0\n",
    "        \n",
    "        print(\"<<其他區塊剛開始時看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(network.y-yo))\n",
    "        \n",
    "        remainder = int(window_size) - init_x.shape[0]-adjust\n",
    "#         remainder = int(window_size*0.9624) - init_x.shape[0]\n",
    "#         x_train_scaled = np.delete(x_train_scaled, sorted_index[:restart_index], 0)\n",
    "#         y_train_scaled = np.delete(y_train_scaled, sorted_index[:restart_index], 0)\n",
    "        nb_step4 += init_x.shape[0]\n",
    "#         print(\"X 資料\",x_train_scaled.shape)\n",
    "#         print(\"Y 資料\",y_train_scaled.shape)\n",
    "#     network.limit = network.linear1.bias.data.shape[0]\n",
    "#     print(\"Limit for node pruned:\",network.limit)\n",
    "\n",
    "#     for i in range(2):\n",
    "#     for i in range(remainder):\n",
    "    for i in range(network.x.shape[0]+1, int(window_size)+1):\n",
    "#         if i_block == -2:\n",
    "#         if i_block == 0:\n",
    "#             print(\"現在訓練到第幾筆資料: %d\"%(i+x_train_scaled.shape[1]+1))\n",
    "        print(\"現在訓練到第幾筆資料: %d\"%(i))\n",
    "#         else:\n",
    "#             print(\"現在訓練到第幾筆資料: %d\"%(restart_index+i))\n",
    "        \n",
    "        print(\"剩餘X 資料\",x_train_scaled.shape)\n",
    "        print(\"剩餘Y 資料\",y_train_scaled.shape)\n",
    "        \n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        \n",
    "\n",
    "        ## Add new data for training\n",
    "        \n",
    "        print(\"現在要進去模型的數據，索引:\",sorted_index[:i],\"數值\",y_train_scaled[sorted_index[:i]].shape)\n",
    "        network.setData(x_train_scaled[sorted_index[:i]], y_train_scaled[sorted_index[:i]])\n",
    "#         network.addData(x_train_scaled[sorted_index[0]], y_train_scaled[sorted_index[0]])\n",
    "        print(\"目前模型的Data狀態\",network.y.shape)\n",
    "#         x_train_scaled = np.delete(x_train_scaled, sorted_index[0], 0)\n",
    "#         y_train_scaled = np.delete(y_train_scaled, sorted_index[0], 0)\n",
    "        \n",
    "        yo,loss = network.forward()\n",
    "        print(\"<<預測值>>\")\n",
    "        print(yo)\n",
    "        print(\"<<差異>>\")\n",
    "        print(torch.abs(yo-network.y))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        print(\"Loss值\")\n",
    "        print(loss)\n",
    "\n",
    "        pre_network = copy.deepcopy(network)\n",
    "        \n",
    "        if not torch.all(torch.abs(network.y-yo)<=network.threshold_for_error):\n",
    "            print(network.threshold_for_error)\n",
    "            network.acceptable = False\n",
    "            network = matching(network)\n",
    "            \n",
    "            print(\"<<Matching後看一下差異>>\")\n",
    "            yo,loss = network.forward()\n",
    "            print(torch.abs(yo-network.y))\n",
    "            print(\"threshold_for_error:\",threshold_for_error)\n",
    "            \n",
    "            if network.acceptable == False:\n",
    "                \n",
    "                network = copy.deepcopy(pre_network)\n",
    "                cramming(network)\n",
    "\n",
    "                if network.acceptable == False:\n",
    "                    sys.exit(0)  \n",
    "                \n",
    "                print(\"<<Cramming後看一下差異>>\")\n",
    "                yo,loss = network.forward()\n",
    "                print(torch.abs(yo-network.y))\n",
    "                print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "                nb_step6_2 += 1\n",
    "\n",
    "            else:\n",
    "                nb_step6_1 += 1\n",
    "\n",
    "        else:\n",
    "            nb_step4 += 1\n",
    "\n",
    "        network = reorganizing(network)\n",
    "        print(\"<<Reorganizing後看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(yo-network.y))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        \n",
    "        network.nb_node_acceptable = torch.cat([network.nb_node_acceptable, torch.IntTensor([network.linear1.bias.data.shape[0]])],0)\n",
    "        print(\"看一下 hidden node\")\n",
    "        print(network.nb_node_acceptable)\n",
    "       \n",
    "        print(\"使用裝置\",(list(network.parameters())[0].device))\n",
    "        print(\"累計時間(s)\",time.time()-start)\n",
    "#         print(network.state_dict())\n",
    "        print(\"-\"*90)\n",
    "\n",
    "    \n",
    "    block_end = time.time()\n",
    "    print(\"到第 %d 個區塊累積花費時間(s)\"%(i_block+1),block_end-block_start)\n",
    "#     print(\"到第 %d 個區塊累積花費時間(s)\"%(len(splits)+i_block+1),block_end-block_start)\n",
    "    print(\"<<The performance of %d block>>\"%(i_block+1))\n",
    "#     print(\"<<The performance of %d block>>\"%(len(splits)+i_block+1))\n",
    "    \n",
    "    evaluation_table_train, evaluation_table_test, forecasted_price = validation(date.iloc[splits[i_block][3][0],:], network, nb_step4, nb_step6_1, nb_step6_2, x_train_scaled, y_train_scaled,x_test_scaled, y_test, block_start, block_end,i_block+1,evaluation_table_train,evaluation_table_test,forecasted_price)\n",
    "\n",
    "    evaluation_table_train.to_csv(\"evaluation_table_train.csv\",index=False)\n",
    "#     evaluation_table_outlier.to_csv(\"evaluation_table_outlier.csv\",index=False)\n",
    "    evaluation_table_test.to_csv(\"evaluation_table_inferencing.csv\",index=False)\n",
    "    forecasted_price.to_csv(\"forecast_price.csv\", index=False)\n",
    "#     validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test_scaled, y_test, block_start, block_end,(len(splits)+i_block+1))\n",
    "end = time.time()\n",
    "print(\"總計時間(s)\", end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
